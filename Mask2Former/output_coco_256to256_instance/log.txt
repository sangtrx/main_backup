[07/11 11:16:37] detectron2 INFO: Rank of current process: 0. World size: 1
[07/11 11:16:39] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/11 11:16:39] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco_front2class/instance-segmentation/maskformer2_R50_bs16_50ep_front_2class_256x256.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', './output_coco_256to256_instance'], resume=False)
[07/11 11:16:39] detectron2 INFO: Contents of args.config_file=configs/coco_front2class/instance-segmentation/maskformer2_R50_bs16_50ep_front_2class_256x256.yaml:
[38;5;197m_BASE_[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mBase-COCO-InstanceSegmentation-256x256.yaml[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormer[39m[38;5;186m"[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMaskFormerHead[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mGN[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;242m# pixel decoder[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMSDeformAttnPixelDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres2[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;186m"[39m[38;5;186mres3[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres4[39m[38;5;186m"[39m[38;5;15m,[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mres5[39m[38;5;186m"[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mMultiScaleMaskedTransformerDecoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mmulti_scale_pixel_decoder[39m[38;5;186m"[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m[38;5;15m [39m[38;5;242m#100[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m[38;5;15m  [39m[38;5;242m# 9 decoder layers, add one for the loss on learnable query[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrue[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFalse[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m

[07/11 11:16:39] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_val_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/11 11:16:39] detectron2 INFO: Full config saved to ./output_coco_256to256_instance/config.yaml
[07/11 11:16:39] d2.utils.env INFO: Using a generated random seed 39848665
[07/11 11:16:48] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[07/11 11:16:48] mask2former.data.dataset_mappers.coco_instance_new_baseline_dataset_mapper INFO: [COCOInstanceNewBaselineDatasetMapper] Full TransformGens used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=256, target_width=256), FixedSizeCrop(crop_size=(256, 256))]
[07/11 11:16:48] d2.data.datasets.coco INFO: Loaded 3017 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_train2017.json
[07/11 11:16:48] d2.data.build INFO: Removed 0 images with no usable annotations. 3017 images left.
[07/11 11:16:48] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 1302         |   defect   | 1715         |
|            |              |            |              |
|   total    | 3017         |            |              |[0m
[07/11 11:16:48] d2.data.build INFO: Using training sampler TrainingSampler
[07/11 11:16:48] d2.data.common INFO: Serializing 3017 elements to byte tensors and concatenating them all ...
[07/11 11:16:48] d2.data.common INFO: Serialized dataset takes 10.05 MiB
[07/11 11:16:48] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[07/11 11:16:48] d2.engine.train_loop INFO: Starting training from iteration 0
[07/11 11:16:59] d2.utils.events INFO:  eta: 3:10:00  iter: 19  total_loss: 82.64  loss_ce: 0.8095  loss_mask: 3.464  loss_dice: 3.493  loss_ce_0: 2.313  loss_mask_0: 3.379  loss_dice_0: 3.242  loss_ce_1: 0.7253  loss_mask_1: 3.446  loss_dice_1: 3.307  loss_ce_2: 0.8365  loss_mask_2: 3.444  loss_dice_2: 3.35  loss_ce_3: 0.8202  loss_mask_3: 3.441  loss_dice_3: 3.417  loss_ce_4: 0.7921  loss_mask_4: 3.446  loss_dice_4: 3.406  loss_ce_5: 0.7913  loss_mask_5: 3.429  loss_dice_5: 3.458  loss_ce_6: 0.8092  loss_mask_6: 3.437  loss_dice_6: 3.427  loss_ce_7: 0.805  loss_mask_7: 3.436  loss_dice_7: 3.414  loss_ce_8: 0.8238  loss_mask_8: 3.456  loss_dice_8: 3.398  time: 0.4086  data_time: 0.0107  lr: 0.0001  max_mem: 1568M
[07/11 11:17:08] d2.utils.events INFO:  eta: 3:17:41  iter: 39  total_loss: 60.8  loss_ce: 0.752  loss_mask: 2.948  loss_dice: 2.463  loss_ce_0: 2.264  loss_mask_0: 3.005  loss_dice_0: 2.624  loss_ce_1: 0.4296  loss_mask_1: 2.952  loss_dice_1: 2.325  loss_ce_2: 0.5315  loss_mask_2: 2.931  loss_dice_2: 2.408  loss_ce_3: 0.7022  loss_mask_3: 2.944  loss_dice_3: 2.453  loss_ce_4: 0.7529  loss_mask_4: 2.966  loss_dice_4: 2.44  loss_ce_5: 0.7559  loss_mask_5: 2.966  loss_dice_5: 2.467  loss_ce_6: 0.7542  loss_mask_6: 2.965  loss_dice_6: 2.428  loss_ce_7: 0.7467  loss_mask_7: 2.948  loss_dice_7: 2.419  loss_ce_8: 0.7382  loss_mask_8: 2.933  loss_dice_8: 2.506  time: 0.4046  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:17:15] d2.utils.events INFO:  eta: 3:17:53  iter: 59  total_loss: 30.25  loss_ce: 0.7417  loss_mask: 1.265  loss_dice: 0.9287  loss_ce_0: 2.233  loss_mask_0: 1.199  loss_dice_0: 1.205  loss_ce_1: 0.2557  loss_mask_1: 1.258  loss_dice_1: 0.9248  loss_ce_2: 0.4234  loss_mask_2: 1.222  loss_dice_2: 0.8893  loss_ce_3: 0.5873  loss_mask_3: 1.266  loss_dice_3: 0.8853  loss_ce_4: 0.6847  loss_mask_4: 1.245  loss_dice_4: 0.9206  loss_ce_5: 0.7041  loss_mask_5: 1.271  loss_dice_5: 0.9441  loss_ce_6: 0.7383  loss_mask_6: 1.267  loss_dice_6: 0.9184  loss_ce_7: 0.7261  loss_mask_7: 1.293  loss_dice_7: 0.9385  loss_ce_8: 0.7366  loss_mask_8: 1.262  loss_dice_8: 0.9211  time: 0.4000  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:17:23] d2.utils.events INFO:  eta: 3:17:46  iter: 79  total_loss: 25.17  loss_ce: 0.7497  loss_mask: 0.9778  loss_dice: 0.5756  loss_ce_0: 2.203  loss_mask_0: 0.8201  loss_dice_0: 0.7654  loss_ce_1: 0.1684  loss_mask_1: 0.9126  loss_dice_1: 0.5632  loss_ce_2: 0.1757  loss_mask_2: 0.9374  loss_dice_2: 0.5899  loss_ce_3: 0.3677  loss_mask_3: 0.9775  loss_dice_3: 0.5828  loss_ce_4: 0.5649  loss_mask_4: 0.9615  loss_dice_4: 0.5628  loss_ce_5: 0.6891  loss_mask_5: 0.9689  loss_dice_5: 0.5804  loss_ce_6: 0.7247  loss_mask_6: 0.9773  loss_dice_6: 0.5686  loss_ce_7: 0.7546  loss_mask_7: 1.01  loss_dice_7: 0.5875  loss_ce_8: 0.7493  loss_mask_8: 0.9959  loss_dice_8: 0.5651  time: 0.3996  data_time: 0.0017  lr: 0.0001  max_mem: 1568M
[07/11 11:17:31] d2.utils.events INFO:  eta: 3:17:49  iter: 99  total_loss: 18.96  loss_ce: 0.8082  loss_mask: 0.5247  loss_dice: 0.6906  loss_ce_0: 2.16  loss_mask_0: 0.5059  loss_dice_0: 0.8195  loss_ce_1: 0.08049  loss_mask_1: 0.4891  loss_dice_1: 0.6859  loss_ce_2: 0.02764  loss_mask_2: 0.4739  loss_dice_2: 0.6911  loss_ce_3: 0.03989  loss_mask_3: 0.5124  loss_dice_3: 0.6737  loss_ce_4: 0.1326  loss_mask_4: 0.4882  loss_dice_4: 0.7041  loss_ce_5: 0.3467  loss_mask_5: 0.4965  loss_dice_5: 0.6933  loss_ce_6: 0.5577  loss_mask_6: 0.5231  loss_dice_6: 0.6936  loss_ce_7: 0.6573  loss_mask_7: 0.5335  loss_dice_7: 0.7007  loss_ce_8: 0.7815  loss_mask_8: 0.4958  loss_dice_8: 0.6736  time: 0.3989  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:17:39] d2.utils.events INFO:  eta: 3:17:39  iter: 119  total_loss: 14.57  loss_ce: 0.6102  loss_mask: 0.5496  loss_dice: 0.3718  loss_ce_0: 2.087  loss_mask_0: 0.5359  loss_dice_0: 0.4215  loss_ce_1: 0.1554  loss_mask_1: 0.5693  loss_dice_1: 0.3988  loss_ce_2: 0.1651  loss_mask_2: 0.5668  loss_dice_2: 0.3775  loss_ce_3: 0.1615  loss_mask_3: 0.5689  loss_dice_3: 0.3733  loss_ce_4: 0.1379  loss_mask_4: 0.5665  loss_dice_4: 0.3816  loss_ce_5: 0.1566  loss_mask_5: 0.5445  loss_dice_5: 0.3922  loss_ce_6: 0.1815  loss_mask_6: 0.5601  loss_dice_6: 0.3933  loss_ce_7: 0.2265  loss_mask_7: 0.5623  loss_dice_7: 0.3813  loss_ce_8: 0.3484  loss_mask_8: 0.5685  loss_dice_8: 0.3659  time: 0.3975  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:17:47] d2.utils.events INFO:  eta: 3:17:41  iter: 139  total_loss: 13.72  loss_ce: 0.2085  loss_mask: 0.4745  loss_dice: 0.4909  loss_ce_0: 2.032  loss_mask_0: 0.4974  loss_dice_0: 0.5161  loss_ce_1: 0.1438  loss_mask_1: 0.4832  loss_dice_1: 0.5107  loss_ce_2: 0.1426  loss_mask_2: 0.4946  loss_dice_2: 0.5291  loss_ce_3: 0.1345  loss_mask_3: 0.4707  loss_dice_3: 0.518  loss_ce_4: 0.1233  loss_mask_4: 0.4721  loss_dice_4: 0.537  loss_ce_5: 0.1174  loss_mask_5: 0.5006  loss_dice_5: 0.521  loss_ce_6: 0.122  loss_mask_6: 0.5168  loss_dice_6: 0.4991  loss_ce_7: 0.1256  loss_mask_7: 0.501  loss_dice_7: 0.5508  loss_ce_8: 0.1553  loss_mask_8: 0.489  loss_dice_8: 0.5542  time: 0.3971  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:17:55] d2.utils.events INFO:  eta: 3:17:05  iter: 159  total_loss: 11.93  loss_ce: 0.1264  loss_mask: 0.4394  loss_dice: 0.3423  loss_ce_0: 1.976  loss_mask_0: 0.447  loss_dice_0: 0.3477  loss_ce_1: 0.1355  loss_mask_1: 0.4692  loss_dice_1: 0.3447  loss_ce_2: 0.1323  loss_mask_2: 0.4357  loss_dice_2: 0.3418  loss_ce_3: 0.137  loss_mask_3: 0.4576  loss_dice_3: 0.3419  loss_ce_4: 0.1341  loss_mask_4: 0.4404  loss_dice_4: 0.3358  loss_ce_5: 0.1321  loss_mask_5: 0.4585  loss_dice_5: 0.3163  loss_ce_6: 0.1433  loss_mask_6: 0.4595  loss_dice_6: 0.3358  loss_ce_7: 0.1249  loss_mask_7: 0.4838  loss_dice_7: 0.3427  loss_ce_8: 0.1095  loss_mask_8: 0.4439  loss_dice_8: 0.3609  time: 0.3966  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:18:03] d2.utils.events INFO:  eta: 3:16:48  iter: 179  total_loss: 10.58  loss_ce: 0.2027  loss_mask: 0.3621  loss_dice: 0.2948  loss_ce_0: 1.904  loss_mask_0: 0.3305  loss_dice_0: 0.295  loss_ce_1: 0.1628  loss_mask_1: 0.358  loss_dice_1: 0.305  loss_ce_2: 0.1661  loss_mask_2: 0.3606  loss_dice_2: 0.3002  loss_ce_3: 0.1645  loss_mask_3: 0.3496  loss_dice_3: 0.2986  loss_ce_4: 0.1612  loss_mask_4: 0.3689  loss_dice_4: 0.2935  loss_ce_5: 0.1561  loss_mask_5: 0.3495  loss_dice_5: 0.3137  loss_ce_6: 0.1572  loss_mask_6: 0.3731  loss_dice_6: 0.3073  loss_ce_7: 0.1666  loss_mask_7: 0.3568  loss_dice_7: 0.3046  loss_ce_8: 0.1937  loss_mask_8: 0.3361  loss_dice_8: 0.2973  time: 0.3967  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:18:11] d2.utils.events INFO:  eta: 3:17:00  iter: 199  total_loss: 12.81  loss_ce: 0.1198  loss_mask: 0.4595  loss_dice: 0.4403  loss_ce_0: 1.812  loss_mask_0: 0.4597  loss_dice_0: 0.455  loss_ce_1: 0.08306  loss_mask_1: 0.4305  loss_dice_1: 0.4562  loss_ce_2: 0.07799  loss_mask_2: 0.4507  loss_dice_2: 0.4413  loss_ce_3: 0.07853  loss_mask_3: 0.4699  loss_dice_3: 0.4442  loss_ce_4: 0.07725  loss_mask_4: 0.4597  loss_dice_4: 0.4206  loss_ce_5: 0.08423  loss_mask_5: 0.4469  loss_dice_5: 0.44  loss_ce_6: 0.08856  loss_mask_6: 0.4484  loss_dice_6: 0.4428  loss_ce_7: 0.1017  loss_mask_7: 0.462  loss_dice_7: 0.4203  loss_ce_8: 0.1246  loss_mask_8: 0.4537  loss_dice_8: 0.4455  time: 0.3968  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:18:19] d2.utils.events INFO:  eta: 3:16:42  iter: 219  total_loss: 8.71  loss_ce: 0.1046  loss_mask: 0.3155  loss_dice: 0.1941  loss_ce_0: 1.753  loss_mask_0: 0.3247  loss_dice_0: 0.1944  loss_ce_1: 0.1224  loss_mask_1: 0.3186  loss_dice_1: 0.194  loss_ce_2: 0.1272  loss_mask_2: 0.3055  loss_dice_2: 0.1918  loss_ce_3: 0.1222  loss_mask_3: 0.3128  loss_dice_3: 0.1883  loss_ce_4: 0.1262  loss_mask_4: 0.326  loss_dice_4: 0.194  loss_ce_5: 0.1158  loss_mask_5: 0.2952  loss_dice_5: 0.1916  loss_ce_6: 0.1098  loss_mask_6: 0.3165  loss_dice_6: 0.1935  loss_ce_7: 0.0994  loss_mask_7: 0.3087  loss_dice_7: 0.1922  loss_ce_8: 0.09455  loss_mask_8: 0.3146  loss_dice_8: 0.1887  time: 0.3965  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:18:27] d2.utils.events INFO:  eta: 3:16:43  iter: 239  total_loss: 9.06  loss_ce: 0.1557  loss_mask: 0.2803  loss_dice: 0.214  loss_ce_0: 1.668  loss_mask_0: 0.2751  loss_dice_0: 0.2179  loss_ce_1: 0.168  loss_mask_1: 0.2692  loss_dice_1: 0.2142  loss_ce_2: 0.1755  loss_mask_2: 0.2729  loss_dice_2: 0.2129  loss_ce_3: 0.163  loss_mask_3: 0.2846  loss_dice_3: 0.2091  loss_ce_4: 0.1612  loss_mask_4: 0.2848  loss_dice_4: 0.2183  loss_ce_5: 0.1641  loss_mask_5: 0.2868  loss_dice_5: 0.2202  loss_ce_6: 0.1629  loss_mask_6: 0.2725  loss_dice_6: 0.2091  loss_ce_7: 0.158  loss_mask_7: 0.2802  loss_dice_7: 0.2193  loss_ce_8: 0.1399  loss_mask_8: 0.2894  loss_dice_8: 0.2176  time: 0.3967  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:18:35] d2.utils.events INFO:  eta: 3:16:38  iter: 259  total_loss: 8.684  loss_ce: 0.1379  loss_mask: 0.1703  loss_dice: 0.3359  loss_ce_0: 1.571  loss_mask_0: 0.1753  loss_dice_0: 0.3171  loss_ce_1: 0.1246  loss_mask_1: 0.178  loss_dice_1: 0.3378  loss_ce_2: 0.1284  loss_mask_2: 0.1624  loss_dice_2: 0.3307  loss_ce_3: 0.1297  loss_mask_3: 0.1788  loss_dice_3: 0.3155  loss_ce_4: 0.1346  loss_mask_4: 0.1688  loss_dice_4: 0.3186  loss_ce_5: 0.1342  loss_mask_5: 0.1692  loss_dice_5: 0.3342  loss_ce_6: 0.132  loss_mask_6: 0.183  loss_dice_6: 0.3396  loss_ce_7: 0.1325  loss_mask_7: 0.179  loss_dice_7: 0.3451  loss_ce_8: 0.1353  loss_mask_8: 0.1768  loss_dice_8: 0.3313  time: 0.3967  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:18:43] d2.utils.events INFO:  eta: 3:16:44  iter: 279  total_loss: 9.107  loss_ce: 0.1284  loss_mask: 0.3296  loss_dice: 0.2729  loss_ce_0: 1.484  loss_mask_0: 0.3414  loss_dice_0: 0.2709  loss_ce_1: 0.1353  loss_mask_1: 0.3434  loss_dice_1: 0.2732  loss_ce_2: 0.1338  loss_mask_2: 0.3399  loss_dice_2: 0.2778  loss_ce_3: 0.1298  loss_mask_3: 0.3552  loss_dice_3: 0.2811  loss_ce_4: 0.1363  loss_mask_4: 0.3274  loss_dice_4: 0.2567  loss_ce_5: 0.1345  loss_mask_5: 0.3447  loss_dice_5: 0.273  loss_ce_6: 0.1324  loss_mask_6: 0.3373  loss_dice_6: 0.2676  loss_ce_7: 0.1317  loss_mask_7: 0.3393  loss_dice_7: 0.2612  loss_ce_8: 0.1295  loss_mask_8: 0.3526  loss_dice_8: 0.271  time: 0.3969  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:18:51] d2.utils.events INFO:  eta: 3:16:29  iter: 299  total_loss: 9.355  loss_ce: 0.1083  loss_mask: 0.3233  loss_dice: 0.3439  loss_ce_0: 1.414  loss_mask_0: 0.3162  loss_dice_0: 0.3219  loss_ce_1: 0.1126  loss_mask_1: 0.3169  loss_dice_1: 0.3389  loss_ce_2: 0.1118  loss_mask_2: 0.335  loss_dice_2: 0.341  loss_ce_3: 0.1096  loss_mask_3: 0.3196  loss_dice_3: 0.3326  loss_ce_4: 0.1103  loss_mask_4: 0.3309  loss_dice_4: 0.3514  loss_ce_5: 0.1074  loss_mask_5: 0.3216  loss_dice_5: 0.3289  loss_ce_6: 0.108  loss_mask_6: 0.3078  loss_dice_6: 0.3588  loss_ce_7: 0.1071  loss_mask_7: 0.3092  loss_dice_7: 0.3404  loss_ce_8: 0.1054  loss_mask_8: 0.3077  loss_dice_8: 0.3434  time: 0.3969  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:18:58] d2.utils.events INFO:  eta: 3:16:14  iter: 319  total_loss: 8.546  loss_ce: 0.09417  loss_mask: 0.3864  loss_dice: 0.2635  loss_ce_0: 1.349  loss_mask_0: 0.3714  loss_dice_0: 0.2717  loss_ce_1: 0.08386  loss_mask_1: 0.3783  loss_dice_1: 0.2676  loss_ce_2: 0.0847  loss_mask_2: 0.3572  loss_dice_2: 0.2515  loss_ce_3: 0.08762  loss_mask_3: 0.3661  loss_dice_3: 0.264  loss_ce_4: 0.08538  loss_mask_4: 0.3621  loss_dice_4: 0.2644  loss_ce_5: 0.08881  loss_mask_5: 0.3738  loss_dice_5: 0.261  loss_ce_6: 0.09068  loss_mask_6: 0.3708  loss_dice_6: 0.2606  loss_ce_7: 0.0975  loss_mask_7: 0.3783  loss_dice_7: 0.2624  loss_ce_8: 0.1008  loss_mask_8: 0.3597  loss_dice_8: 0.2641  time: 0.3966  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:19:06] d2.utils.events INFO:  eta: 3:16:06  iter: 339  total_loss: 7.698  loss_ce: 0.07408  loss_mask: 0.2828  loss_dice: 0.2342  loss_ce_0: 1.28  loss_mask_0: 0.2691  loss_dice_0: 0.2382  loss_ce_1: 0.07257  loss_mask_1: 0.2707  loss_dice_1: 0.2246  loss_ce_2: 0.07221  loss_mask_2: 0.2658  loss_dice_2: 0.2252  loss_ce_3: 0.07306  loss_mask_3: 0.2702  loss_dice_3: 0.2365  loss_ce_4: 0.07228  loss_mask_4: 0.272  loss_dice_4: 0.2227  loss_ce_5: 0.07281  loss_mask_5: 0.2761  loss_dice_5: 0.2428  loss_ce_6: 0.07426  loss_mask_6: 0.2685  loss_dice_6: 0.23  loss_ce_7: 0.07349  loss_mask_7: 0.2678  loss_dice_7: 0.2337  loss_ce_8: 0.07189  loss_mask_8: 0.2676  loss_dice_8: 0.2475  time: 0.3963  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:19:14] d2.utils.events INFO:  eta: 3:15:57  iter: 359  total_loss: 7.942  loss_ce: 0.1163  loss_mask: 0.2272  loss_dice: 0.234  loss_ce_0: 1.206  loss_mask_0: 0.2311  loss_dice_0: 0.239  loss_ce_1: 0.09991  loss_mask_1: 0.2363  loss_dice_1: 0.2279  loss_ce_2: 0.1048  loss_mask_2: 0.2302  loss_dice_2: 0.2466  loss_ce_3: 0.1069  loss_mask_3: 0.2358  loss_dice_3: 0.2269  loss_ce_4: 0.1022  loss_mask_4: 0.2359  loss_dice_4: 0.2306  loss_ce_5: 0.1021  loss_mask_5: 0.2316  loss_dice_5: 0.2393  loss_ce_6: 0.107  loss_mask_6: 0.2326  loss_dice_6: 0.2253  loss_ce_7: 0.103  loss_mask_7: 0.2346  loss_dice_7: 0.2393  loss_ce_8: 0.1098  loss_mask_8: 0.2382  loss_dice_8: 0.2375  time: 0.3965  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:19:22] d2.utils.events INFO:  eta: 3:15:49  iter: 379  total_loss: 8.077  loss_ce: 0.1355  loss_mask: 0.2538  loss_dice: 0.2488  loss_ce_0: 1.137  loss_mask_0: 0.252  loss_dice_0: 0.2501  loss_ce_1: 0.1372  loss_mask_1: 0.2337  loss_dice_1: 0.2427  loss_ce_2: 0.1344  loss_mask_2: 0.2476  loss_dice_2: 0.2474  loss_ce_3: 0.1341  loss_mask_3: 0.2531  loss_dice_3: 0.2497  loss_ce_4: 0.1339  loss_mask_4: 0.2424  loss_dice_4: 0.2446  loss_ce_5: 0.1339  loss_mask_5: 0.2456  loss_dice_5: 0.2419  loss_ce_6: 0.1326  loss_mask_6: 0.2461  loss_dice_6: 0.2468  loss_ce_7: 0.1329  loss_mask_7: 0.2495  loss_dice_7: 0.2448  loss_ce_8: 0.1381  loss_mask_8: 0.2582  loss_dice_8: 0.2511  time: 0.3965  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:19:30] d2.utils.events INFO:  eta: 3:15:41  iter: 399  total_loss: 7.763  loss_ce: 0.1205  loss_mask: 0.2848  loss_dice: 0.2295  loss_ce_0: 1.075  loss_mask_0: 0.2889  loss_dice_0: 0.2228  loss_ce_1: 0.1178  loss_mask_1: 0.2857  loss_dice_1: 0.241  loss_ce_2: 0.1192  loss_mask_2: 0.2837  loss_dice_2: 0.2328  loss_ce_3: 0.1202  loss_mask_3: 0.2875  loss_dice_3: 0.239  loss_ce_4: 0.1189  loss_mask_4: 0.2821  loss_dice_4: 0.2334  loss_ce_5: 0.1184  loss_mask_5: 0.2883  loss_dice_5: 0.2344  loss_ce_6: 0.1189  loss_mask_6: 0.2836  loss_dice_6: 0.2385  loss_ce_7: 0.117  loss_mask_7: 0.2767  loss_dice_7: 0.2384  loss_ce_8: 0.1197  loss_mask_8: 0.2715  loss_dice_8: 0.2301  time: 0.3965  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:19:38] d2.utils.events INFO:  eta: 3:15:30  iter: 419  total_loss: 7.079  loss_ce: 0.1415  loss_mask: 0.2826  loss_dice: 0.2507  loss_ce_0: 0.976  loss_mask_0: 0.2699  loss_dice_0: 0.2541  loss_ce_1: 0.1406  loss_mask_1: 0.2735  loss_dice_1: 0.2466  loss_ce_2: 0.1399  loss_mask_2: 0.2873  loss_dice_2: 0.2478  loss_ce_3: 0.1431  loss_mask_3: 0.27  loss_dice_3: 0.2518  loss_ce_4: 0.1401  loss_mask_4: 0.283  loss_dice_4: 0.247  loss_ce_5: 0.1425  loss_mask_5: 0.2805  loss_dice_5: 0.2485  loss_ce_6: 0.1412  loss_mask_6: 0.2811  loss_dice_6: 0.248  loss_ce_7: 0.1427  loss_mask_7: 0.2736  loss_dice_7: 0.2523  loss_ce_8: 0.1416  loss_mask_8: 0.2851  loss_dice_8: 0.2444  time: 0.3964  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:19:46] d2.utils.events INFO:  eta: 3:15:24  iter: 439  total_loss: 7.463  loss_ce: 0.1237  loss_mask: 0.2153  loss_dice: 0.2688  loss_ce_0: 0.9136  loss_mask_0: 0.2044  loss_dice_0: 0.2788  loss_ce_1: 0.1226  loss_mask_1: 0.2184  loss_dice_1: 0.2714  loss_ce_2: 0.1234  loss_mask_2: 0.2189  loss_dice_2: 0.2744  loss_ce_3: 0.123  loss_mask_3: 0.2183  loss_dice_3: 0.2727  loss_ce_4: 0.1231  loss_mask_4: 0.2217  loss_dice_4: 0.2782  loss_ce_5: 0.1236  loss_mask_5: 0.2054  loss_dice_5: 0.2742  loss_ce_6: 0.1236  loss_mask_6: 0.2139  loss_dice_6: 0.262  loss_ce_7: 0.1262  loss_mask_7: 0.2114  loss_dice_7: 0.278  loss_ce_8: 0.1236  loss_mask_8: 0.2223  loss_dice_8: 0.2783  time: 0.3964  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:19:54] d2.utils.events INFO:  eta: 3:15:14  iter: 459  total_loss: 7.092  loss_ce: 0.0612  loss_mask: 0.2902  loss_dice: 0.2274  loss_ce_0: 0.8492  loss_mask_0: 0.2993  loss_dice_0: 0.2418  loss_ce_1: 0.06114  loss_mask_1: 0.2891  loss_dice_1: 0.2281  loss_ce_2: 0.06114  loss_mask_2: 0.2891  loss_dice_2: 0.2429  loss_ce_3: 0.06172  loss_mask_3: 0.2764  loss_dice_3: 0.2373  loss_ce_4: 0.06161  loss_mask_4: 0.2894  loss_dice_4: 0.2347  loss_ce_5: 0.06271  loss_mask_5: 0.2919  loss_dice_5: 0.2453  loss_ce_6: 0.06039  loss_mask_6: 0.2905  loss_dice_6: 0.2378  loss_ce_7: 0.06323  loss_mask_7: 0.2924  loss_dice_7: 0.2397  loss_ce_8: 0.06043  loss_mask_8: 0.2841  loss_dice_8: 0.2467  time: 0.3964  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:20:02] d2.utils.events INFO:  eta: 3:15:10  iter: 479  total_loss: 7.403  loss_ce: 0.06852  loss_mask: 0.2853  loss_dice: 0.2691  loss_ce_0: 0.7924  loss_mask_0: 0.2812  loss_dice_0: 0.2574  loss_ce_1: 0.06786  loss_mask_1: 0.2791  loss_dice_1: 0.2617  loss_ce_2: 0.06641  loss_mask_2: 0.2809  loss_dice_2: 0.2554  loss_ce_3: 0.06436  loss_mask_3: 0.2748  loss_dice_3: 0.256  loss_ce_4: 0.06531  loss_mask_4: 0.284  loss_dice_4: 0.2814  loss_ce_5: 0.06683  loss_mask_5: 0.2784  loss_dice_5: 0.2484  loss_ce_6: 0.06934  loss_mask_6: 0.2834  loss_dice_6: 0.26  loss_ce_7: 0.0651  loss_mask_7: 0.2848  loss_dice_7: 0.2587  loss_ce_8: 0.06803  loss_mask_8: 0.2914  loss_dice_8: 0.2598  time: 0.3965  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:20:10] d2.utils.events INFO:  eta: 3:15:01  iter: 499  total_loss: 6.928  loss_ce: 0.1383  loss_mask: 0.2752  loss_dice: 0.2365  loss_ce_0: 0.7434  loss_mask_0: 0.2942  loss_dice_0: 0.2458  loss_ce_1: 0.135  loss_mask_1: 0.2844  loss_dice_1: 0.2438  loss_ce_2: 0.1324  loss_mask_2: 0.2741  loss_dice_2: 0.2413  loss_ce_3: 0.1318  loss_mask_3: 0.2861  loss_dice_3: 0.2334  loss_ce_4: 0.1338  loss_mask_4: 0.2924  loss_dice_4: 0.2403  loss_ce_5: 0.1315  loss_mask_5: 0.2729  loss_dice_5: 0.2376  loss_ce_6: 0.1329  loss_mask_6: 0.2703  loss_dice_6: 0.2357  loss_ce_7: 0.1323  loss_mask_7: 0.2835  loss_dice_7: 0.2381  loss_ce_8: 0.1371  loss_mask_8: 0.286  loss_dice_8: 0.2408  time: 0.3964  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:20:18] d2.utils.events INFO:  eta: 3:14:52  iter: 519  total_loss: 6.746  loss_ce: 0.1268  loss_mask: 0.295  loss_dice: 0.1916  loss_ce_0: 0.6996  loss_mask_0: 0.303  loss_dice_0: 0.193  loss_ce_1: 0.1267  loss_mask_1: 0.3149  loss_dice_1: 0.202  loss_ce_2: 0.1266  loss_mask_2: 0.3075  loss_dice_2: 0.1935  loss_ce_3: 0.1272  loss_mask_3: 0.2988  loss_dice_3: 0.1933  loss_ce_4: 0.1259  loss_mask_4: 0.293  loss_dice_4: 0.1915  loss_ce_5: 0.126  loss_mask_5: 0.2975  loss_dice_5: 0.1958  loss_ce_6: 0.1259  loss_mask_6: 0.3048  loss_dice_6: 0.1914  loss_ce_7: 0.127  loss_mask_7: 0.3033  loss_dice_7: 0.1994  loss_ce_8: 0.1272  loss_mask_8: 0.3043  loss_dice_8: 0.1953  time: 0.3964  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:20:26] d2.utils.events INFO:  eta: 3:14:27  iter: 539  total_loss: 8.029  loss_ce: 0.1379  loss_mask: 0.2952  loss_dice: 0.254  loss_ce_0: 0.6573  loss_mask_0: 0.308  loss_dice_0: 0.2531  loss_ce_1: 0.1319  loss_mask_1: 0.2916  loss_dice_1: 0.2467  loss_ce_2: 0.1387  loss_mask_2: 0.2986  loss_dice_2: 0.2489  loss_ce_3: 0.1354  loss_mask_3: 0.2904  loss_dice_3: 0.2674  loss_ce_4: 0.1363  loss_mask_4: 0.2835  loss_dice_4: 0.2598  loss_ce_5: 0.1393  loss_mask_5: 0.2995  loss_dice_5: 0.266  loss_ce_6: 0.1365  loss_mask_6: 0.2881  loss_dice_6: 0.25  loss_ce_7: 0.1328  loss_mask_7: 0.2816  loss_dice_7: 0.2563  loss_ce_8: 0.1378  loss_mask_8: 0.2899  loss_dice_8: 0.259  time: 0.3962  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:20:34] d2.utils.events INFO:  eta: 3:14:27  iter: 559  total_loss: 5.815  loss_ce: 0.1101  loss_mask: 0.188  loss_dice: 0.2084  loss_ce_0: 0.6198  loss_mask_0: 0.1837  loss_dice_0: 0.2146  loss_ce_1: 0.1122  loss_mask_1: 0.1875  loss_dice_1: 0.2066  loss_ce_2: 0.1096  loss_mask_2: 0.1935  loss_dice_2: 0.2115  loss_ce_3: 0.1099  loss_mask_3: 0.187  loss_dice_3: 0.2095  loss_ce_4: 0.1109  loss_mask_4: 0.1967  loss_dice_4: 0.2077  loss_ce_5: 0.1088  loss_mask_5: 0.1833  loss_dice_5: 0.2114  loss_ce_6: 0.1108  loss_mask_6: 0.1827  loss_dice_6: 0.2026  loss_ce_7: 0.1107  loss_mask_7: 0.1862  loss_dice_7: 0.2029  loss_ce_8: 0.1121  loss_mask_8: 0.1826  loss_dice_8: 0.2146  time: 0.3965  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:20:42] d2.utils.events INFO:  eta: 3:14:28  iter: 579  total_loss: 6.161  loss_ce: 0.1309  loss_mask: 0.2675  loss_dice: 0.1845  loss_ce_0: 0.5768  loss_mask_0: 0.2637  loss_dice_0: 0.1977  loss_ce_1: 0.1281  loss_mask_1: 0.2518  loss_dice_1: 0.177  loss_ce_2: 0.131  loss_mask_2: 0.2562  loss_dice_2: 0.1883  loss_ce_3: 0.1309  loss_mask_3: 0.2714  loss_dice_3: 0.1899  loss_ce_4: 0.1299  loss_mask_4: 0.2681  loss_dice_4: 0.1779  loss_ce_5: 0.1318  loss_mask_5: 0.2663  loss_dice_5: 0.186  loss_ce_6: 0.1334  loss_mask_6: 0.27  loss_dice_6: 0.191  loss_ce_7: 0.1333  loss_mask_7: 0.264  loss_dice_7: 0.1899  loss_ce_8: 0.1297  loss_mask_8: 0.268  loss_dice_8: 0.1876  time: 0.3968  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:20:51] d2.utils.events INFO:  eta: 3:14:27  iter: 599  total_loss: 7.628  loss_ce: 0.1303  loss_mask: 0.2156  loss_dice: 0.3133  loss_ce_0: 0.5475  loss_mask_0: 0.2108  loss_dice_0: 0.3118  loss_ce_1: 0.1328  loss_mask_1: 0.2168  loss_dice_1: 0.3184  loss_ce_2: 0.1409  loss_mask_2: 0.2176  loss_dice_2: 0.3205  loss_ce_3: 0.1408  loss_mask_3: 0.2266  loss_dice_3: 0.3301  loss_ce_4: 0.1388  loss_mask_4: 0.2162  loss_dice_4: 0.318  loss_ce_5: 0.13  loss_mask_5: 0.2193  loss_dice_5: 0.319  loss_ce_6: 0.1293  loss_mask_6: 0.2137  loss_dice_6: 0.3182  loss_ce_7: 0.1292  loss_mask_7: 0.2192  loss_dice_7: 0.3143  loss_ce_8: 0.1301  loss_mask_8: 0.2153  loss_dice_8: 0.324  time: 0.3983  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:21:00] d2.utils.events INFO:  eta: 3:14:41  iter: 619  total_loss: 6.759  loss_ce: 0.1262  loss_mask: 0.2639  loss_dice: 0.2335  loss_ce_0: 0.5125  loss_mask_0: 0.2702  loss_dice_0: 0.246  loss_ce_1: 0.1215  loss_mask_1: 0.2765  loss_dice_1: 0.2423  loss_ce_2: 0.1191  loss_mask_2: 0.2643  loss_dice_2: 0.2428  loss_ce_3: 0.1184  loss_mask_3: 0.2593  loss_dice_3: 0.2479  loss_ce_4: 0.1204  loss_mask_4: 0.2458  loss_dice_4: 0.2373  loss_ce_5: 0.1248  loss_mask_5: 0.2693  loss_dice_5: 0.2329  loss_ce_6: 0.1252  loss_mask_6: 0.2639  loss_dice_6: 0.238  loss_ce_7: 0.1222  loss_mask_7: 0.26  loss_dice_7: 0.2507  loss_ce_8: 0.1253  loss_mask_8: 0.2677  loss_dice_8: 0.241  time: 0.4006  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:21:10] d2.utils.events INFO:  eta: 3:15:03  iter: 639  total_loss: 7.463  loss_ce: 0.0935  loss_mask: 0.2659  loss_dice: 0.3037  loss_ce_0: 0.4817  loss_mask_0: 0.2612  loss_dice_0: 0.2886  loss_ce_1: 0.08661  loss_mask_1: 0.258  loss_dice_1: 0.2932  loss_ce_2: 0.1032  loss_mask_2: 0.2623  loss_dice_2: 0.2959  loss_ce_3: 0.1055  loss_mask_3: 0.2686  loss_dice_3: 0.2994  loss_ce_4: 0.1001  loss_mask_4: 0.2606  loss_dice_4: 0.2904  loss_ce_5: 0.09159  loss_mask_5: 0.2667  loss_dice_5: 0.2993  loss_ce_6: 0.09328  loss_mask_6: 0.2706  loss_dice_6: 0.3115  loss_ce_7: 0.08782  loss_mask_7: 0.2643  loss_dice_7: 0.2927  loss_ce_8: 0.08775  loss_mask_8: 0.2539  loss_dice_8: 0.3058  time: 0.4028  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:21:19] d2.utils.events INFO:  eta: 3:15:18  iter: 659  total_loss: 6.306  loss_ce: 0.08502  loss_mask: 0.2453  loss_dice: 0.1891  loss_ce_0: 0.4502  loss_mask_0: 0.2364  loss_dice_0: 0.1893  loss_ce_1: 0.08232  loss_mask_1: 0.2489  loss_dice_1: 0.1826  loss_ce_2: 0.09156  loss_mask_2: 0.2552  loss_dice_2: 0.1938  loss_ce_3: 0.08986  loss_mask_3: 0.2505  loss_dice_3: 0.1969  loss_ce_4: 0.08554  loss_mask_4: 0.2496  loss_dice_4: 0.1853  loss_ce_5: 0.08536  loss_mask_5: 0.2448  loss_dice_5: 0.1925  loss_ce_6: 0.08535  loss_mask_6: 0.2416  loss_dice_6: 0.1848  loss_ce_7: 0.08179  loss_mask_7: 0.2361  loss_dice_7: 0.1911  loss_ce_8: 0.08228  loss_mask_8: 0.2498  loss_dice_8: 0.1904  time: 0.4050  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:21:29] d2.utils.events INFO:  eta: 3:15:30  iter: 679  total_loss: 6.925  loss_ce: 0.133  loss_mask: 0.2398  loss_dice: 0.2212  loss_ce_0: 0.4294  loss_mask_0: 0.2386  loss_dice_0: 0.2144  loss_ce_1: 0.1409  loss_mask_1: 0.2553  loss_dice_1: 0.2103  loss_ce_2: 0.1361  loss_mask_2: 0.251  loss_dice_2: 0.2097  loss_ce_3: 0.138  loss_mask_3: 0.236  loss_dice_3: 0.226  loss_ce_4: 0.136  loss_mask_4: 0.2458  loss_dice_4: 0.2221  loss_ce_5: 0.1328  loss_mask_5: 0.244  loss_dice_5: 0.2132  loss_ce_6: 0.1362  loss_mask_6: 0.2546  loss_dice_6: 0.2065  loss_ce_7: 0.1323  loss_mask_7: 0.2337  loss_dice_7: 0.209  loss_ce_8: 0.1343  loss_mask_8: 0.2483  loss_dice_8: 0.2161  time: 0.4069  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:21:38] d2.utils.events INFO:  eta: 3:15:53  iter: 699  total_loss: 6.818  loss_ce: 0.1308  loss_mask: 0.2432  loss_dice: 0.2153  loss_ce_0: 0.4027  loss_mask_0: 0.2524  loss_dice_0: 0.2158  loss_ce_1: 0.1279  loss_mask_1: 0.2393  loss_dice_1: 0.2121  loss_ce_2: 0.1272  loss_mask_2: 0.2419  loss_dice_2: 0.2125  loss_ce_3: 0.1289  loss_mask_3: 0.2667  loss_dice_3: 0.211  loss_ce_4: 0.1258  loss_mask_4: 0.2483  loss_dice_4: 0.2114  loss_ce_5: 0.1286  loss_mask_5: 0.2681  loss_dice_5: 0.2137  loss_ce_6: 0.1309  loss_mask_6: 0.2503  loss_dice_6: 0.221  loss_ce_7: 0.1275  loss_mask_7: 0.2469  loss_dice_7: 0.2151  loss_ce_8: 0.1295  loss_mask_8: 0.268  loss_dice_8: 0.2133  time: 0.4087  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:21:47] d2.utils.events INFO:  eta: 3:16:12  iter: 719  total_loss: 7.088  loss_ce: 0.1222  loss_mask: 0.2824  loss_dice: 0.2747  loss_ce_0: 0.3705  loss_mask_0: 0.2705  loss_dice_0: 0.2779  loss_ce_1: 0.1218  loss_mask_1: 0.2721  loss_dice_1: 0.2827  loss_ce_2: 0.1205  loss_mask_2: 0.2923  loss_dice_2: 0.2977  loss_ce_3: 0.1211  loss_mask_3: 0.2846  loss_dice_3: 0.2779  loss_ce_4: 0.1218  loss_mask_4: 0.287  loss_dice_4: 0.279  loss_ce_5: 0.1222  loss_mask_5: 0.2807  loss_dice_5: 0.2931  loss_ce_6: 0.1205  loss_mask_6: 0.2829  loss_dice_6: 0.2809  loss_ce_7: 0.1228  loss_mask_7: 0.2777  loss_dice_7: 0.2861  loss_ce_8: 0.121  loss_mask_8: 0.2907  loss_dice_8: 0.2896  time: 0.4105  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:21:57] d2.utils.events INFO:  eta: 3:16:45  iter: 739  total_loss: 6.078  loss_ce: 0.06667  loss_mask: 0.2133  loss_dice: 0.1973  loss_ce_0: 0.3457  loss_mask_0: 0.2066  loss_dice_0: 0.1956  loss_ce_1: 0.06585  loss_mask_1: 0.2197  loss_dice_1: 0.2079  loss_ce_2: 0.06601  loss_mask_2: 0.217  loss_dice_2: 0.2096  loss_ce_3: 0.06652  loss_mask_3: 0.2189  loss_dice_3: 0.1972  loss_ce_4: 0.06593  loss_mask_4: 0.2166  loss_dice_4: 0.2014  loss_ce_5: 0.06642  loss_mask_5: 0.2247  loss_dice_5: 0.2036  loss_ce_6: 0.06636  loss_mask_6: 0.2174  loss_dice_6: 0.1956  loss_ce_7: 0.06678  loss_mask_7: 0.2209  loss_dice_7: 0.2055  loss_ce_8: 0.06676  loss_mask_8: 0.2262  loss_dice_8: 0.203  time: 0.4122  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:22:06] d2.utils.events INFO:  eta: 3:17:12  iter: 759  total_loss: 6.726  loss_ce: 0.09538  loss_mask: 0.2613  loss_dice: 0.2131  loss_ce_0: 0.3265  loss_mask_0: 0.2725  loss_dice_0: 0.1953  loss_ce_1: 0.09764  loss_mask_1: 0.2471  loss_dice_1: 0.2005  loss_ce_2: 0.0973  loss_mask_2: 0.2697  loss_dice_2: 0.2008  loss_ce_3: 0.09622  loss_mask_3: 0.2654  loss_dice_3: 0.1987  loss_ce_4: 0.1004  loss_mask_4: 0.2629  loss_dice_4: 0.1991  loss_ce_5: 0.09645  loss_mask_5: 0.2586  loss_dice_5: 0.1955  loss_ce_6: 0.09777  loss_mask_6: 0.2562  loss_dice_6: 0.2113  loss_ce_7: 0.09755  loss_mask_7: 0.2699  loss_dice_7: 0.207  loss_ce_8: 0.09856  loss_mask_8: 0.2583  loss_dice_8: 0.2022  time: 0.4137  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:22:16] d2.utils.events INFO:  eta: 3:17:32  iter: 779  total_loss: 6.063  loss_ce: 0.1132  loss_mask: 0.2668  loss_dice: 0.2032  loss_ce_0: 0.3049  loss_mask_0: 0.2632  loss_dice_0: 0.1989  loss_ce_1: 0.1129  loss_mask_1: 0.2556  loss_dice_1: 0.2001  loss_ce_2: 0.1147  loss_mask_2: 0.2661  loss_dice_2: 0.1949  loss_ce_3: 0.1125  loss_mask_3: 0.2708  loss_dice_3: 0.2021  loss_ce_4: 0.118  loss_mask_4: 0.2644  loss_dice_4: 0.1993  loss_ce_5: 0.1136  loss_mask_5: 0.2668  loss_dice_5: 0.2  loss_ce_6: 0.115  loss_mask_6: 0.2707  loss_dice_6: 0.2012  loss_ce_7: 0.1126  loss_mask_7: 0.2755  loss_dice_7: 0.2049  loss_ce_8: 0.1131  loss_mask_8: 0.2702  loss_dice_8: 0.2069  time: 0.4151  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:22:25] d2.utils.events INFO:  eta: 3:17:46  iter: 799  total_loss: 6.903  loss_ce: 0.05896  loss_mask: 0.2956  loss_dice: 0.2076  loss_ce_0: 0.2878  loss_mask_0: 0.2869  loss_dice_0: 0.21  loss_ce_1: 0.05418  loss_mask_1: 0.3029  loss_dice_1: 0.2224  loss_ce_2: 0.05744  loss_mask_2: 0.2961  loss_dice_2: 0.2082  loss_ce_3: 0.05786  loss_mask_3: 0.2833  loss_dice_3: 0.2162  loss_ce_4: 0.05243  loss_mask_4: 0.3027  loss_dice_4: 0.2131  loss_ce_5: 0.05961  loss_mask_5: 0.2893  loss_dice_5: 0.2149  loss_ce_6: 0.05746  loss_mask_6: 0.2869  loss_dice_6: 0.2085  loss_ce_7: 0.05503  loss_mask_7: 0.2954  loss_dice_7: 0.215  loss_ce_8: 0.05645  loss_mask_8: 0.2803  loss_dice_8: 0.2155  time: 0.4166  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:22:35] d2.utils.events INFO:  eta: 3:18:16  iter: 819  total_loss: 5.631  loss_ce: 0.1382  loss_mask: 0.2129  loss_dice: 0.1949  loss_ce_0: 0.2867  loss_mask_0: 0.215  loss_dice_0: 0.1803  loss_ce_1: 0.1343  loss_mask_1: 0.2049  loss_dice_1: 0.1823  loss_ce_2: 0.1351  loss_mask_2: 0.1982  loss_dice_2: 0.1779  loss_ce_3: 0.1333  loss_mask_3: 0.2262  loss_dice_3: 0.1788  loss_ce_4: 0.1309  loss_mask_4: 0.2188  loss_dice_4: 0.1846  loss_ce_5: 0.1381  loss_mask_5: 0.2107  loss_dice_5: 0.1801  loss_ce_6: 0.1338  loss_mask_6: 0.2165  loss_dice_6: 0.1697  loss_ce_7: 0.136  loss_mask_7: 0.2115  loss_dice_7: 0.1864  loss_ce_8: 0.1332  loss_mask_8: 0.2056  loss_dice_8: 0.1839  time: 0.4179  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:22:44] d2.utils.events INFO:  eta: 3:18:41  iter: 839  total_loss: 6.412  loss_ce: 0.1439  loss_mask: 0.2227  loss_dice: 0.2562  loss_ce_0: 0.2589  loss_mask_0: 0.2254  loss_dice_0: 0.2545  loss_ce_1: 0.1413  loss_mask_1: 0.2264  loss_dice_1: 0.2606  loss_ce_2: 0.1399  loss_mask_2: 0.2192  loss_dice_2: 0.2487  loss_ce_3: 0.141  loss_mask_3: 0.2355  loss_dice_3: 0.2615  loss_ce_4: 0.1396  loss_mask_4: 0.2167  loss_dice_4: 0.2529  loss_ce_5: 0.1388  loss_mask_5: 0.2347  loss_dice_5: 0.2492  loss_ce_6: 0.142  loss_mask_6: 0.248  loss_dice_6: 0.2535  loss_ce_7: 0.1401  loss_mask_7: 0.234  loss_dice_7: 0.2547  loss_ce_8: 0.1431  loss_mask_8: 0.2246  loss_dice_8: 0.2599  time: 0.4193  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:22:54] d2.utils.events INFO:  eta: 3:19:12  iter: 859  total_loss: 5.943  loss_ce: 0.1089  loss_mask: 0.25  loss_dice: 0.203  loss_ce_0: 0.2459  loss_mask_0: 0.2525  loss_dice_0: 0.2034  loss_ce_1: 0.1084  loss_mask_1: 0.2485  loss_dice_1: 0.205  loss_ce_2: 0.1095  loss_mask_2: 0.245  loss_dice_2: 0.2082  loss_ce_3: 0.1135  loss_mask_3: 0.2439  loss_dice_3: 0.2045  loss_ce_4: 0.1154  loss_mask_4: 0.2443  loss_dice_4: 0.1994  loss_ce_5: 0.1112  loss_mask_5: 0.2505  loss_dice_5: 0.2008  loss_ce_6: 0.1102  loss_mask_6: 0.2541  loss_dice_6: 0.2108  loss_ce_7: 0.1068  loss_mask_7: 0.2372  loss_dice_7: 0.2047  loss_ce_8: 0.1091  loss_mask_8: 0.247  loss_dice_8: 0.2039  time: 0.4206  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:23:03] d2.utils.events INFO:  eta: 3:19:27  iter: 879  total_loss: 6.102  loss_ce: 0.1241  loss_mask: 0.2554  loss_dice: 0.212  loss_ce_0: 0.2312  loss_mask_0: 0.2667  loss_dice_0: 0.2223  loss_ce_1: 0.1261  loss_mask_1: 0.2572  loss_dice_1: 0.2209  loss_ce_2: 0.1257  loss_mask_2: 0.2627  loss_dice_2: 0.2194  loss_ce_3: 0.1308  loss_mask_3: 0.2625  loss_dice_3: 0.2182  loss_ce_4: 0.1319  loss_mask_4: 0.2542  loss_dice_4: 0.2246  loss_ce_5: 0.1255  loss_mask_5: 0.2701  loss_dice_5: 0.2184  loss_ce_6: 0.1256  loss_mask_6: 0.2636  loss_dice_6: 0.2258  loss_ce_7: 0.1245  loss_mask_7: 0.2585  loss_dice_7: 0.2215  loss_ce_8: 0.1251  loss_mask_8: 0.264  loss_dice_8: 0.2158  time: 0.4218  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:23:13] d2.utils.events INFO:  eta: 3:19:51  iter: 899  total_loss: 5.962  loss_ce: 0.1461  loss_mask: 0.2199  loss_dice: 0.1833  loss_ce_0: 0.2378  loss_mask_0: 0.2172  loss_dice_0: 0.1847  loss_ce_1: 0.1487  loss_mask_1: 0.2302  loss_dice_1: 0.1855  loss_ce_2: 0.1498  loss_mask_2: 0.2293  loss_dice_2: 0.1782  loss_ce_3: 0.1557  loss_mask_3: 0.2217  loss_dice_3: 0.1863  loss_ce_4: 0.1663  loss_mask_4: 0.2241  loss_dice_4: 0.1839  loss_ce_5: 0.155  loss_mask_5: 0.2219  loss_dice_5: 0.1858  loss_ce_6: 0.1554  loss_mask_6: 0.2177  loss_dice_6: 0.1812  loss_ce_7: 0.149  loss_mask_7: 0.2239  loss_dice_7: 0.1812  loss_ce_8: 0.1485  loss_mask_8: 0.2159  loss_dice_8: 0.1843  time: 0.4230  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:23:22] d2.utils.events INFO:  eta: 3:20:04  iter: 919  total_loss: 5.907  loss_ce: 0.1204  loss_mask: 0.2503  loss_dice: 0.1806  loss_ce_0: 0.2135  loss_mask_0: 0.2492  loss_dice_0: 0.1737  loss_ce_1: 0.1196  loss_mask_1: 0.2499  loss_dice_1: 0.185  loss_ce_2: 0.1193  loss_mask_2: 0.2534  loss_dice_2: 0.1866  loss_ce_3: 0.1179  loss_mask_3: 0.2512  loss_dice_3: 0.1762  loss_ce_4: 0.1153  loss_mask_4: 0.2371  loss_dice_4: 0.181  loss_ce_5: 0.1185  loss_mask_5: 0.2417  loss_dice_5: 0.1903  loss_ce_6: 0.1123  loss_mask_6: 0.2449  loss_dice_6: 0.1754  loss_ce_7: 0.1195  loss_mask_7: 0.2581  loss_dice_7: 0.1832  loss_ce_8: 0.1196  loss_mask_8: 0.2531  loss_dice_8: 0.1872  time: 0.4242  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:23:32] d2.utils.events INFO:  eta: 3:20:41  iter: 939  total_loss: 6.754  loss_ce: 0.1022  loss_mask: 0.2042  loss_dice: 0.259  loss_ce_0: 0.2053  loss_mask_0: 0.2077  loss_dice_0: 0.2504  loss_ce_1: 0.1037  loss_mask_1: 0.2057  loss_dice_1: 0.2692  loss_ce_2: 0.104  loss_mask_2: 0.1952  loss_dice_2: 0.2611  loss_ce_3: 0.1063  loss_mask_3: 0.1977  loss_dice_3: 0.2688  loss_ce_4: 0.1064  loss_mask_4: 0.1938  loss_dice_4: 0.2524  loss_ce_5: 0.1056  loss_mask_5: 0.2025  loss_dice_5: 0.2648  loss_ce_6: 0.1155  loss_mask_6: 0.2089  loss_dice_6: 0.2646  loss_ce_7: 0.1029  loss_mask_7: 0.2002  loss_dice_7: 0.276  loss_ce_8: 0.1035  loss_mask_8: 0.1913  loss_dice_8: 0.2582  time: 0.4255  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:23:42] d2.utils.events INFO:  eta: 3:20:58  iter: 959  total_loss: 5.85  loss_ce: 0.09149  loss_mask: 0.246  loss_dice: 0.203  loss_ce_0: 0.1964  loss_mask_0: 0.2349  loss_dice_0: 0.1917  loss_ce_1: 0.09287  loss_mask_1: 0.227  loss_dice_1: 0.1918  loss_ce_2: 0.09136  loss_mask_2: 0.2354  loss_dice_2: 0.1858  loss_ce_3: 0.09369  loss_mask_3: 0.2417  loss_dice_3: 0.1866  loss_ce_4: 0.09334  loss_mask_4: 0.2311  loss_dice_4: 0.188  loss_ce_5: 0.0927  loss_mask_5: 0.2318  loss_dice_5: 0.2052  loss_ce_6: 0.09676  loss_mask_6: 0.2296  loss_dice_6: 0.1812  loss_ce_7: 0.09105  loss_mask_7: 0.2433  loss_dice_7: 0.2004  loss_ce_8: 0.09192  loss_mask_8: 0.2345  loss_dice_8: 0.1902  time: 0.4266  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:23:52] d2.utils.events INFO:  eta: 3:22:23  iter: 979  total_loss: 6.046  loss_ce: 0.09194  loss_mask: 0.2192  loss_dice: 0.2455  loss_ce_0: 0.1873  loss_mask_0: 0.2366  loss_dice_0: 0.2501  loss_ce_1: 0.09498  loss_mask_1: 0.2161  loss_dice_1: 0.2566  loss_ce_2: 0.09446  loss_mask_2: 0.2282  loss_dice_2: 0.2351  loss_ce_3: 0.09391  loss_mask_3: 0.2354  loss_dice_3: 0.252  loss_ce_4: 0.09575  loss_mask_4: 0.2178  loss_dice_4: 0.2467  loss_ce_5: 0.09435  loss_mask_5: 0.2188  loss_dice_5: 0.2508  loss_ce_6: 0.09684  loss_mask_6: 0.2259  loss_dice_6: 0.2386  loss_ce_7: 0.09396  loss_mask_7: 0.221  loss_dice_7: 0.254  loss_ce_8: 0.09421  loss_mask_8: 0.2261  loss_dice_8: 0.2531  time: 0.4287  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:24:03] d2.utils.events INFO:  eta: 3:23:13  iter: 999  total_loss: 5.588  loss_ce: 0.1063  loss_mask: 0.2348  loss_dice: 0.1481  loss_ce_0: 0.1791  loss_mask_0: 0.2306  loss_dice_0: 0.1436  loss_ce_1: 0.1086  loss_mask_1: 0.2433  loss_dice_1: 0.1415  loss_ce_2: 0.1087  loss_mask_2: 0.2343  loss_dice_2: 0.1407  loss_ce_3: 0.1081  loss_mask_3: 0.2347  loss_dice_3: 0.1499  loss_ce_4: 0.1088  loss_mask_4: 0.2388  loss_dice_4: 0.1411  loss_ce_5: 0.1082  loss_mask_5: 0.2402  loss_dice_5: 0.1469  loss_ce_6: 0.1101  loss_mask_6: 0.2431  loss_dice_6: 0.135  loss_ce_7: 0.1078  loss_mask_7: 0.2382  loss_dice_7: 0.1338  loss_ce_8: 0.1077  loss_mask_8: 0.2336  loss_dice_8: 0.1492  time: 0.4313  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:24:14] d2.utils.events INFO:  eta: 3:24:19  iter: 1019  total_loss: 5.732  loss_ce: 0.1229  loss_mask: 0.2281  loss_dice: 0.2014  loss_ce_0: 0.2  loss_mask_0: 0.2284  loss_dice_0: 0.2043  loss_ce_1: 0.1226  loss_mask_1: 0.2222  loss_dice_1: 0.195  loss_ce_2: 0.1221  loss_mask_2: 0.2211  loss_dice_2: 0.1958  loss_ce_3: 0.1227  loss_mask_3: 0.2147  loss_dice_3: 0.2026  loss_ce_4: 0.1229  loss_mask_4: 0.2292  loss_dice_4: 0.2059  loss_ce_5: 0.1229  loss_mask_5: 0.2286  loss_dice_5: 0.2078  loss_ce_6: 0.1213  loss_mask_6: 0.2257  loss_dice_6: 0.2028  loss_ce_7: 0.1235  loss_mask_7: 0.2325  loss_dice_7: 0.2016  loss_ce_8: 0.1232  loss_mask_8: 0.2294  loss_dice_8: 0.1955  time: 0.4337  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:24:26] d2.utils.events INFO:  eta: 3:25:24  iter: 1039  total_loss: 6.401  loss_ce: 0.1338  loss_mask: 0.2526  loss_dice: 0.2201  loss_ce_0: 0.1672  loss_mask_0: 0.2482  loss_dice_0: 0.2255  loss_ce_1: 0.1335  loss_mask_1: 0.2431  loss_dice_1: 0.2164  loss_ce_2: 0.1333  loss_mask_2: 0.2494  loss_dice_2: 0.221  loss_ce_3: 0.1331  loss_mask_3: 0.243  loss_dice_3: 0.2303  loss_ce_4: 0.133  loss_mask_4: 0.2596  loss_dice_4: 0.2124  loss_ce_5: 0.133  loss_mask_5: 0.2407  loss_dice_5: 0.2252  loss_ce_6: 0.1331  loss_mask_6: 0.2572  loss_dice_6: 0.2179  loss_ce_7: 0.1333  loss_mask_7: 0.2458  loss_dice_7: 0.2319  loss_ce_8: 0.1331  loss_mask_8: 0.2543  loss_dice_8: 0.2166  time: 0.4361  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:24:37] d2.utils.events INFO:  eta: 3:27:29  iter: 1059  total_loss: 6.524  loss_ce: 0.1058  loss_mask: 0.2552  loss_dice: 0.1953  loss_ce_0: 0.1619  loss_mask_0: 0.2588  loss_dice_0: 0.1987  loss_ce_1: 0.1049  loss_mask_1: 0.253  loss_dice_1: 0.2023  loss_ce_2: 0.1049  loss_mask_2: 0.2538  loss_dice_2: 0.1985  loss_ce_3: 0.105  loss_mask_3: 0.2645  loss_dice_3: 0.2067  loss_ce_4: 0.1051  loss_mask_4: 0.2646  loss_dice_4: 0.2053  loss_ce_5: 0.1052  loss_mask_5: 0.2625  loss_dice_5: 0.2063  loss_ce_6: 0.1044  loss_mask_6: 0.2614  loss_dice_6: 0.2026  loss_ce_7: 0.1055  loss_mask_7: 0.2574  loss_dice_7: 0.2037  loss_ce_8: 0.1047  loss_mask_8: 0.2565  loss_dice_8: 0.1956  time: 0.4385  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:24:48] d2.utils.events INFO:  eta: 3:28:41  iter: 1079  total_loss: 5.565  loss_ce: 0.1263  loss_mask: 0.2181  loss_dice: 0.172  loss_ce_0: 0.1569  loss_mask_0: 0.2175  loss_dice_0: 0.1786  loss_ce_1: 0.1292  loss_mask_1: 0.2242  loss_dice_1: 0.1712  loss_ce_2: 0.1264  loss_mask_2: 0.2237  loss_dice_2: 0.1778  loss_ce_3: 0.1262  loss_mask_3: 0.2284  loss_dice_3: 0.1725  loss_ce_4: 0.1264  loss_mask_4: 0.2291  loss_dice_4: 0.1725  loss_ce_5: 0.1261  loss_mask_5: 0.2228  loss_dice_5: 0.1753  loss_ce_6: 0.1263  loss_mask_6: 0.2256  loss_dice_6: 0.172  loss_ce_7: 0.1268  loss_mask_7: 0.2206  loss_dice_7: 0.1741  loss_ce_8: 0.1264  loss_mask_8: 0.2201  loss_dice_8: 0.1769  time: 0.4405  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:24:59] d2.utils.events INFO:  eta: 3:30:54  iter: 1099  total_loss: 6.097  loss_ce: 0.1343  loss_mask: 0.2428  loss_dice: 0.1896  loss_ce_0: 0.1537  loss_mask_0: 0.2288  loss_dice_0: 0.1819  loss_ce_1: 0.1344  loss_mask_1: 0.2335  loss_dice_1: 0.1927  loss_ce_2: 0.1353  loss_mask_2: 0.2398  loss_dice_2: 0.1908  loss_ce_3: 0.135  loss_mask_3: 0.2363  loss_dice_3: 0.1867  loss_ce_4: 0.1348  loss_mask_4: 0.2447  loss_dice_4: 0.1925  loss_ce_5: 0.1337  loss_mask_5: 0.2313  loss_dice_5: 0.1819  loss_ce_6: 0.1342  loss_mask_6: 0.2413  loss_dice_6: 0.179  loss_ce_7: 0.1341  loss_mask_7: 0.2377  loss_dice_7: 0.1902  loss_ce_8: 0.1349  loss_mask_8: 0.2306  loss_dice_8: 0.1858  time: 0.4427  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:25:10] d2.utils.events INFO:  eta: 3:33:36  iter: 1119  total_loss: 5.677  loss_ce: 0.1083  loss_mask: 0.2286  loss_dice: 0.2101  loss_ce_0: 0.1493  loss_mask_0: 0.22  loss_dice_0: 0.2022  loss_ce_1: 0.1081  loss_mask_1: 0.2263  loss_dice_1: 0.2017  loss_ce_2: 0.1079  loss_mask_2: 0.2095  loss_dice_2: 0.2098  loss_ce_3: 0.1077  loss_mask_3: 0.2172  loss_dice_3: 0.2159  loss_ce_4: 0.1081  loss_mask_4: 0.2164  loss_dice_4: 0.2106  loss_ce_5: 0.1081  loss_mask_5: 0.2225  loss_dice_5: 0.2054  loss_ce_6: 0.1077  loss_mask_6: 0.2258  loss_dice_6: 0.218  loss_ce_7: 0.1085  loss_mask_7: 0.2032  loss_dice_7: 0.2148  loss_ce_8: 0.1077  loss_mask_8: 0.231  loss_dice_8: 0.214  time: 0.4448  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:25:21] d2.utils.events INFO:  eta: 3:35:32  iter: 1139  total_loss: 6.012  loss_ce: 0.1268  loss_mask: 0.2593  loss_dice: 0.1891  loss_ce_0: 0.1601  loss_mask_0: 0.2551  loss_dice_0: 0.1777  loss_ce_1: 0.125  loss_mask_1: 0.2584  loss_dice_1: 0.1827  loss_ce_2: 0.1247  loss_mask_2: 0.257  loss_dice_2: 0.1814  loss_ce_3: 0.1248  loss_mask_3: 0.2577  loss_dice_3: 0.1814  loss_ce_4: 0.1242  loss_mask_4: 0.2561  loss_dice_4: 0.1843  loss_ce_5: 0.1259  loss_mask_5: 0.2527  loss_dice_5: 0.1798  loss_ce_6: 0.1255  loss_mask_6: 0.2539  loss_dice_6: 0.1802  loss_ce_7: 0.1253  loss_mask_7: 0.2498  loss_dice_7: 0.1788  loss_ce_8: 0.1248  loss_mask_8: 0.2586  loss_dice_8: 0.1761  time: 0.4467  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:25:33] d2.utils.events INFO:  eta: 3:37:25  iter: 1159  total_loss: 6.042  loss_ce: 0.134  loss_mask: 0.252  loss_dice: 0.2028  loss_ce_0: 0.1481  loss_mask_0: 0.2588  loss_dice_0: 0.2046  loss_ce_1: 0.1304  loss_mask_1: 0.2544  loss_dice_1: 0.2025  loss_ce_2: 0.1321  loss_mask_2: 0.2561  loss_dice_2: 0.2059  loss_ce_3: 0.1322  loss_mask_3: 0.2473  loss_dice_3: 0.2053  loss_ce_4: 0.1324  loss_mask_4: 0.247  loss_dice_4: 0.1983  loss_ce_5: 0.1335  loss_mask_5: 0.2583  loss_dice_5: 0.2002  loss_ce_6: 0.1328  loss_mask_6: 0.2461  loss_dice_6: 0.2058  loss_ce_7: 0.1332  loss_mask_7: 0.2566  loss_dice_7: 0.1973  loss_ce_8: 0.1322  loss_mask_8: 0.2472  loss_dice_8: 0.2084  time: 0.4487  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:25:44] d2.utils.events INFO:  eta: 3:39:37  iter: 1179  total_loss: 6.022  loss_ce: 0.1313  loss_mask: 0.2301  loss_dice: 0.1877  loss_ce_0: 0.1425  loss_mask_0: 0.2317  loss_dice_0: 0.1824  loss_ce_1: 0.1324  loss_mask_1: 0.2302  loss_dice_1: 0.1909  loss_ce_2: 0.1316  loss_mask_2: 0.2303  loss_dice_2: 0.1957  loss_ce_3: 0.1314  loss_mask_3: 0.2277  loss_dice_3: 0.1941  loss_ce_4: 0.1319  loss_mask_4: 0.2314  loss_dice_4: 0.1915  loss_ce_5: 0.1315  loss_mask_5: 0.2229  loss_dice_5: 0.1952  loss_ce_6: 0.1314  loss_mask_6: 0.2254  loss_dice_6: 0.1918  loss_ce_7: 0.1315  loss_mask_7: 0.2268  loss_dice_7: 0.1925  loss_ce_8: 0.1314  loss_mask_8: 0.2256  loss_dice_8: 0.1931  time: 0.4505  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:25:55] d2.utils.events INFO:  eta: 3:41:51  iter: 1199  total_loss: 6.017  loss_ce: 0.1313  loss_mask: 0.262  loss_dice: 0.1647  loss_ce_0: 0.1418  loss_mask_0: 0.2638  loss_dice_0: 0.1701  loss_ce_1: 0.1325  loss_mask_1: 0.2575  loss_dice_1: 0.17  loss_ce_2: 0.1328  loss_mask_2: 0.2634  loss_dice_2: 0.1715  loss_ce_3: 0.1321  loss_mask_3: 0.2506  loss_dice_3: 0.169  loss_ce_4: 0.132  loss_mask_4: 0.2625  loss_dice_4: 0.1646  loss_ce_5: 0.1323  loss_mask_5: 0.2604  loss_dice_5: 0.1655  loss_ce_6: 0.1308  loss_mask_6: 0.2709  loss_dice_6: 0.1694  loss_ce_7: 0.1305  loss_mask_7: 0.2508  loss_dice_7: 0.1683  loss_ce_8: 0.1312  loss_mask_8: 0.2514  loss_dice_8: 0.1663  time: 0.4522  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:26:06] d2.utils.events INFO:  eta: 3:43:17  iter: 1219  total_loss: 5.149  loss_ce: 0.1098  loss_mask: 0.2215  loss_dice: 0.1756  loss_ce_0: 0.1332  loss_mask_0: 0.2162  loss_dice_0: 0.1774  loss_ce_1: 0.1123  loss_mask_1: 0.2216  loss_dice_1: 0.1806  loss_ce_2: 0.1114  loss_mask_2: 0.2196  loss_dice_2: 0.1806  loss_ce_3: 0.1109  loss_mask_3: 0.224  loss_dice_3: 0.1681  loss_ce_4: 0.1103  loss_mask_4: 0.2215  loss_dice_4: 0.1678  loss_ce_5: 0.1109  loss_mask_5: 0.2303  loss_dice_5: 0.1753  loss_ce_6: 0.1103  loss_mask_6: 0.209  loss_dice_6: 0.1686  loss_ce_7: 0.1108  loss_mask_7: 0.2127  loss_dice_7: 0.1813  loss_ce_8: 0.1109  loss_mask_8: 0.2217  loss_dice_8: 0.1669  time: 0.4540  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:26:17] d2.utils.events INFO:  eta: 3:44:24  iter: 1239  total_loss: 5.274  loss_ce: 0.04633  loss_mask: 0.221  loss_dice: 0.1781  loss_ce_0: 0.1257  loss_mask_0: 0.2217  loss_dice_0: 0.1845  loss_ce_1: 0.04457  loss_mask_1: 0.2157  loss_dice_1: 0.1851  loss_ce_2: 0.04511  loss_mask_2: 0.2219  loss_dice_2: 0.1906  loss_ce_3: 0.04534  loss_mask_3: 0.2214  loss_dice_3: 0.1845  loss_ce_4: 0.04544  loss_mask_4: 0.2278  loss_dice_4: 0.1856  loss_ce_5: 0.04588  loss_mask_5: 0.223  loss_dice_5: 0.1871  loss_ce_6: 0.04505  loss_mask_6: 0.226  loss_dice_6: 0.1878  loss_ce_7: 0.04586  loss_mask_7: 0.2223  loss_dice_7: 0.1839  loss_ce_8: 0.04541  loss_mask_8: 0.2361  loss_dice_8: 0.1836  time: 0.4556  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:26:28] d2.utils.events INFO:  eta: 3:45:25  iter: 1259  total_loss: 6.019  loss_ce: 0.07805  loss_mask: 0.2194  loss_dice: 0.2106  loss_ce_0: 0.1195  loss_mask_0: 0.2224  loss_dice_0: 0.209  loss_ce_1: 0.0764  loss_mask_1: 0.2339  loss_dice_1: 0.2134  loss_ce_2: 0.07685  loss_mask_2: 0.224  loss_dice_2: 0.2125  loss_ce_3: 0.07873  loss_mask_3: 0.2159  loss_dice_3: 0.2123  loss_ce_4: 0.07961  loss_mask_4: 0.2133  loss_dice_4: 0.2099  loss_ce_5: 0.07768  loss_mask_5: 0.2189  loss_dice_5: 0.2144  loss_ce_6: 0.07651  loss_mask_6: 0.22  loss_dice_6: 0.2144  loss_ce_7: 0.07548  loss_mask_7: 0.226  loss_dice_7: 0.212  loss_ce_8: 0.0766  loss_mask_8: 0.2248  loss_dice_8: 0.2029  time: 0.4572  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:26:39] d2.utils.events INFO:  eta: 3:46:53  iter: 1279  total_loss: 5.57  loss_ce: 0.1232  loss_mask: 0.2481  loss_dice: 0.1579  loss_ce_0: 0.1427  loss_mask_0: 0.228  loss_dice_0: 0.151  loss_ce_1: 0.1223  loss_mask_1: 0.2402  loss_dice_1: 0.1549  loss_ce_2: 0.1228  loss_mask_2: 0.233  loss_dice_2: 0.1557  loss_ce_3: 0.1223  loss_mask_3: 0.2378  loss_dice_3: 0.1569  loss_ce_4: 0.1209  loss_mask_4: 0.2343  loss_dice_4: 0.1602  loss_ce_5: 0.123  loss_mask_5: 0.2269  loss_dice_5: 0.1612  loss_ce_6: 0.1227  loss_mask_6: 0.2363  loss_dice_6: 0.1576  loss_ce_7: 0.1209  loss_mask_7: 0.2373  loss_dice_7: 0.1624  loss_ce_8: 0.1231  loss_mask_8: 0.2253  loss_dice_8: 0.1534  time: 0.4587  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:26:51] d2.utils.events INFO:  eta: 3:48:29  iter: 1299  total_loss: 6.273  loss_ce: 0.1658  loss_mask: 0.2343  loss_dice: 0.218  loss_ce_0: 0.1172  loss_mask_0: 0.224  loss_dice_0: 0.2108  loss_ce_1: 0.1974  loss_mask_1: 0.2247  loss_dice_1: 0.2125  loss_ce_2: 0.1972  loss_mask_2: 0.2328  loss_dice_2: 0.2257  loss_ce_3: 0.192  loss_mask_3: 0.23  loss_dice_3: 0.2243  loss_ce_4: 0.1888  loss_mask_4: 0.2265  loss_dice_4: 0.231  loss_ce_5: 0.1855  loss_mask_5: 0.2211  loss_dice_5: 0.2125  loss_ce_6: 0.1948  loss_mask_6: 0.2297  loss_dice_6: 0.2143  loss_ce_7: 0.1921  loss_mask_7: 0.2246  loss_dice_7: 0.2096  loss_ce_8: 0.1861  loss_mask_8: 0.2231  loss_dice_8: 0.2173  time: 0.4603  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:27:02] d2.utils.events INFO:  eta: 3:49:47  iter: 1319  total_loss: 5.442  loss_ce: 0.06981  loss_mask: 0.1955  loss_dice: 0.198  loss_ce_0: 0.1158  loss_mask_0: 0.1911  loss_dice_0: 0.1966  loss_ce_1: 0.1025  loss_mask_1: 0.1917  loss_dice_1: 0.1964  loss_ce_2: 0.09338  loss_mask_2: 0.191  loss_dice_2: 0.2009  loss_ce_3: 0.08391  loss_mask_3: 0.1896  loss_dice_3: 0.1928  loss_ce_4: 0.0818  loss_mask_4: 0.1988  loss_dice_4: 0.2027  loss_ce_5: 0.0818  loss_mask_5: 0.1879  loss_dice_5: 0.1928  loss_ce_6: 0.09124  loss_mask_6: 0.1881  loss_dice_6: 0.1988  loss_ce_7: 0.09212  loss_mask_7: 0.1965  loss_dice_7: 0.2021  loss_ce_8: 0.08041  loss_mask_8: 0.1989  loss_dice_8: 0.1881  time: 0.4618  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:27:13] d2.utils.events INFO:  eta: 3:51:27  iter: 1339  total_loss: 5.438  loss_ce: 0.05914  loss_mask: 0.2238  loss_dice: 0.1821  loss_ce_0: 0.1108  loss_mask_0: 0.223  loss_dice_0: 0.1813  loss_ce_1: 0.05159  loss_mask_1: 0.236  loss_dice_1: 0.1849  loss_ce_2: 0.05235  loss_mask_2: 0.2296  loss_dice_2: 0.187  loss_ce_3: 0.05385  loss_mask_3: 0.2343  loss_dice_3: 0.184  loss_ce_4: 0.05457  loss_mask_4: 0.2299  loss_dice_4: 0.1843  loss_ce_5: 0.05607  loss_mask_5: 0.2348  loss_dice_5: 0.1882  loss_ce_6: 0.0543  loss_mask_6: 0.2321  loss_dice_6: 0.1836  loss_ce_7: 0.0548  loss_mask_7: 0.225  loss_dice_7: 0.1871  loss_ce_8: 0.0554  loss_mask_8: 0.2203  loss_dice_8: 0.1862  time: 0.4632  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:27:24] d2.utils.events INFO:  eta: 3:52:54  iter: 1359  total_loss: 6.278  loss_ce: 0.1027  loss_mask: 0.2539  loss_dice: 0.2257  loss_ce_0: 0.106  loss_mask_0: 0.2549  loss_dice_0: 0.2327  loss_ce_1: 0.09363  loss_mask_1: 0.26  loss_dice_1: 0.2265  loss_ce_2: 0.1041  loss_mask_2: 0.2582  loss_dice_2: 0.2239  loss_ce_3: 0.1049  loss_mask_3: 0.2561  loss_dice_3: 0.2271  loss_ce_4: 0.1037  loss_mask_4: 0.2583  loss_dice_4: 0.2171  loss_ce_5: 0.1055  loss_mask_5: 0.2537  loss_dice_5: 0.224  loss_ce_6: 0.103  loss_mask_6: 0.2606  loss_dice_6: 0.2184  loss_ce_7: 0.1032  loss_mask_7: 0.2564  loss_dice_7: 0.2273  loss_ce_8: 0.1053  loss_mask_8: 0.2619  loss_dice_8: 0.2282  time: 0.4646  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:27:35] d2.utils.events INFO:  eta: 3:54:34  iter: 1379  total_loss: 5.182  loss_ce: 0.07427  loss_mask: 0.2524  loss_dice: 0.1832  loss_ce_0: 0.1027  loss_mask_0: 0.2567  loss_dice_0: 0.1785  loss_ce_1: 0.0766  loss_mask_1: 0.2472  loss_dice_1: 0.1856  loss_ce_2: 0.07707  loss_mask_2: 0.2521  loss_dice_2: 0.1788  loss_ce_3: 0.0729  loss_mask_3: 0.2479  loss_dice_3: 0.1785  loss_ce_4: 0.07528  loss_mask_4: 0.2453  loss_dice_4: 0.1776  loss_ce_5: 0.0745  loss_mask_5: 0.2568  loss_dice_5: 0.1741  loss_ce_6: 0.07557  loss_mask_6: 0.257  loss_dice_6: 0.1779  loss_ce_7: 0.07435  loss_mask_7: 0.2487  loss_dice_7: 0.1765  loss_ce_8: 0.074  loss_mask_8: 0.2451  loss_dice_8: 0.1785  time: 0.4659  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:27:46] d2.utils.events INFO:  eta: 3:55:59  iter: 1399  total_loss: 6.146  loss_ce: 0.1437  loss_mask: 0.2323  loss_dice: 0.1729  loss_ce_0: 0.1182  loss_mask_0: 0.2474  loss_dice_0: 0.1771  loss_ce_1: 0.1434  loss_mask_1: 0.2431  loss_dice_1: 0.1779  loss_ce_2: 0.1445  loss_mask_2: 0.2365  loss_dice_2: 0.1778  loss_ce_3: 0.1441  loss_mask_3: 0.243  loss_dice_3: 0.1827  loss_ce_4: 0.1441  loss_mask_4: 0.2534  loss_dice_4: 0.1828  loss_ce_5: 0.1442  loss_mask_5: 0.2442  loss_dice_5: 0.1853  loss_ce_6: 0.144  loss_mask_6: 0.2472  loss_dice_6: 0.1752  loss_ce_7: 0.1441  loss_mask_7: 0.244  loss_dice_7: 0.1744  loss_ce_8: 0.144  loss_mask_8: 0.2575  loss_dice_8: 0.1811  time: 0.4673  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:27:58] d2.utils.events INFO:  eta: 3:57:26  iter: 1419  total_loss: 5.253  loss_ce: 0.15  loss_mask: 0.1944  loss_dice: 0.1512  loss_ce_0: 0.1337  loss_mask_0: 0.1974  loss_dice_0: 0.1421  loss_ce_1: 0.1498  loss_mask_1: 0.1975  loss_dice_1: 0.1439  loss_ce_2: 0.1506  loss_mask_2: 0.1915  loss_dice_2: 0.15  loss_ce_3: 0.1471  loss_mask_3: 0.1953  loss_dice_3: 0.1525  loss_ce_4: 0.1487  loss_mask_4: 0.1929  loss_dice_4: 0.1452  loss_ce_5: 0.149  loss_mask_5: 0.1893  loss_dice_5: 0.1445  loss_ce_6: 0.1494  loss_mask_6: 0.1796  loss_dice_6: 0.1502  loss_ce_7: 0.1493  loss_mask_7: 0.1919  loss_dice_7: 0.1431  loss_ce_8: 0.1478  loss_mask_8: 0.1954  loss_dice_8: 0.1485  time: 0.4685  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:28:09] d2.utils.events INFO:  eta: 3:58:58  iter: 1439  total_loss: 5.299  loss_ce: 0.1218  loss_mask: 0.1961  loss_dice: 0.202  loss_ce_0: 0.09641  loss_mask_0: 0.1896  loss_dice_0: 0.1967  loss_ce_1: 0.1228  loss_mask_1: 0.1995  loss_dice_1: 0.2068  loss_ce_2: 0.1222  loss_mask_2: 0.1926  loss_dice_2: 0.1966  loss_ce_3: 0.1244  loss_mask_3: 0.2047  loss_dice_3: 0.2006  loss_ce_4: 0.1231  loss_mask_4: 0.2026  loss_dice_4: 0.2011  loss_ce_5: 0.1229  loss_mask_5: 0.2083  loss_dice_5: 0.2081  loss_ce_6: 0.1226  loss_mask_6: 0.1943  loss_dice_6: 0.1982  loss_ce_7: 0.1226  loss_mask_7: 0.1986  loss_dice_7: 0.2088  loss_ce_8: 0.1234  loss_mask_8: 0.1955  loss_dice_8: 0.2009  time: 0.4697  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:28:20] d2.utils.events INFO:  eta: 4:00:56  iter: 1459  total_loss: 5.556  loss_ce: 0.127  loss_mask: 0.2071  loss_dice: 0.2231  loss_ce_0: 0.1242  loss_mask_0: 0.2039  loss_dice_0: 0.2238  loss_ce_1: 0.1269  loss_mask_1: 0.2135  loss_dice_1: 0.207  loss_ce_2: 0.1269  loss_mask_2: 0.2105  loss_dice_2: 0.2071  loss_ce_3: 0.1269  loss_mask_3: 0.2086  loss_dice_3: 0.218  loss_ce_4: 0.127  loss_mask_4: 0.2068  loss_dice_4: 0.2175  loss_ce_5: 0.1271  loss_mask_5: 0.214  loss_dice_5: 0.2126  loss_ce_6: 0.1268  loss_mask_6: 0.2005  loss_dice_6: 0.2268  loss_ce_7: 0.1267  loss_mask_7: 0.2039  loss_dice_7: 0.2156  loss_ce_8: 0.1269  loss_mask_8: 0.2134  loss_dice_8: 0.2288  time: 0.4710  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:28:31] d2.utils.events INFO:  eta: 4:02:51  iter: 1479  total_loss: 6.261  loss_ce: 0.1355  loss_mask: 0.2301  loss_dice: 0.2045  loss_ce_0: 0.09699  loss_mask_0: 0.2329  loss_dice_0: 0.213  loss_ce_1: 0.1369  loss_mask_1: 0.23  loss_dice_1: 0.2044  loss_ce_2: 0.1361  loss_mask_2: 0.2385  loss_dice_2: 0.2086  loss_ce_3: 0.1355  loss_mask_3: 0.2418  loss_dice_3: 0.2068  loss_ce_4: 0.1357  loss_mask_4: 0.23  loss_dice_4: 0.21  loss_ce_5: 0.1359  loss_mask_5: 0.2306  loss_dice_5: 0.2012  loss_ce_6: 0.1356  loss_mask_6: 0.2307  loss_dice_6: 0.2037  loss_ce_7: 0.1357  loss_mask_7: 0.222  loss_dice_7: 0.2008  loss_ce_8: 0.136  loss_mask_8: 0.2339  loss_dice_8: 0.21  time: 0.4721  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:28:42] d2.utils.events INFO:  eta: 4:05:03  iter: 1499  total_loss: 5.681  loss_ce: 0.09293  loss_mask: 0.2264  loss_dice: 0.1883  loss_ce_0: 0.09538  loss_mask_0: 0.2335  loss_dice_0: 0.1926  loss_ce_1: 0.09231  loss_mask_1: 0.2349  loss_dice_1: 0.1777  loss_ce_2: 0.09183  loss_mask_2: 0.2332  loss_dice_2: 0.174  loss_ce_3: 0.09119  loss_mask_3: 0.2378  loss_dice_3: 0.1865  loss_ce_4: 0.09197  loss_mask_4: 0.2251  loss_dice_4: 0.1885  loss_ce_5: 0.09254  loss_mask_5: 0.2174  loss_dice_5: 0.1798  loss_ce_6: 0.09197  loss_mask_6: 0.2534  loss_dice_6: 0.1856  loss_ce_7: 0.09258  loss_mask_7: 0.2231  loss_dice_7: 0.196  loss_ce_8: 0.09207  loss_mask_8: 0.2455  loss_dice_8: 0.1929  time: 0.4732  data_time: 0.0016  lr: 0.0001  max_mem: 1568M
[07/11 11:28:53] d2.utils.events INFO:  eta: 4:06:02  iter: 1519  total_loss: 4.626  loss_ce: 0.05388  loss_mask: 0.2083  loss_dice: 0.1789  loss_ce_0: 0.09117  loss_mask_0: 0.2094  loss_dice_0: 0.1831  loss_ce_1: 0.05279  loss_mask_1: 0.2129  loss_dice_1: 0.1771  loss_ce_2: 0.05298  loss_mask_2: 0.2216  loss_dice_2: 0.1885  loss_ce_3: 0.05209  loss_mask_3: 0.2117  loss_dice_3: 0.1801  loss_ce_4: 0.05234  loss_mask_4: 0.2244  loss_dice_4: 0.186  loss_ce_5: 0.05351  loss_mask_5: 0.2108  loss_dice_5: 0.1853  loss_ce_6: 0.05234  loss_mask_6: 0.2119  loss_dice_6: 0.184  loss_ce_7: 0.05371  loss_mask_7: 0.2188  loss_dice_7: 0.1839  loss_ce_8: 0.0526  loss_mask_8: 0.2049  loss_dice_8: 0.1898  time: 0.4744  data_time: 0.0032  lr: 0.0001  max_mem: 1568M
[07/11 11:29:05] d2.utils.events INFO:  eta: 4:07:09  iter: 1539  total_loss: 6.631  loss_ce: 0.03181  loss_mask: 0.2814  loss_dice: 0.2434  loss_ce_0: 0.08432  loss_mask_0: 0.273  loss_dice_0: 0.2359  loss_ce_1: 0.03023  loss_mask_1: 0.2769  loss_dice_1: 0.234  loss_ce_2: 0.03097  loss_mask_2: 0.2708  loss_dice_2: 0.2542  loss_ce_3: 0.03115  loss_mask_3: 0.2648  loss_dice_3: 0.2442  loss_ce_4: 0.03112  loss_mask_4: 0.2628  loss_dice_4: 0.2429  loss_ce_5: 0.03119  loss_mask_5: 0.2669  loss_dice_5: 0.2611  loss_ce_6: 0.03082  loss_mask_6: 0.2671  loss_dice_6: 0.2455  loss_ce_7: 0.03135  loss_mask_7: 0.2675  loss_dice_7: 0.2537  loss_ce_8: 0.031  loss_mask_8: 0.2691  loss_dice_8: 0.2374  time: 0.4755  data_time: 0.0018  lr: 0.0001  max_mem: 1568M
[07/11 11:29:16] d2.utils.events INFO:  eta: 4:07:58  iter: 1559  total_loss: 5.488  loss_ce: 0.0445  loss_mask: 0.2214  loss_dice: 0.216  loss_ce_0: 0.08137  loss_mask_0: 0.2206  loss_dice_0: 0.2154  loss_ce_1: 0.04357  loss_mask_1: 0.2252  loss_dice_1: 0.2082  loss_ce_2: 0.0441  loss_mask_2: 0.2231  loss_dice_2: 0.2134  loss_ce_3: 0.04493  loss_mask_3: 0.2202  loss_dice_3: 0.2189  loss_ce_4: 0.04489  loss_mask_4: 0.2178  loss_dice_4: 0.2078  loss_ce_5: 0.04376  loss_mask_5: 0.22  loss_dice_5: 0.2015  loss_ce_6: 0.0443  loss_mask_6: 0.2232  loss_dice_6: 0.2144  loss_ce_7: 0.04434  loss_mask_7: 0.2137  loss_dice_7: 0.2124  loss_ce_8: 0.04416  loss_mask_8: 0.2188  loss_dice_8: 0.207  time: 0.4765  data_time: 0.0021  lr: 0.0001  max_mem: 1568M
[07/11 11:29:27] d2.utils.events INFO:  eta: 4:08:57  iter: 1579  total_loss: 5.3  loss_ce: 0.1291  loss_mask: 0.1767  loss_dice: 0.1671  loss_ce_0: 0.1052  loss_mask_0: 0.1781  loss_dice_0: 0.1665  loss_ce_1: 0.1292  loss_mask_1: 0.1775  loss_dice_1: 0.1671  loss_ce_2: 0.1292  loss_mask_2: 0.1791  loss_dice_2: 0.165  loss_ce_3: 0.1279  loss_mask_3: 0.1818  loss_dice_3: 0.1663  loss_ce_4: 0.1286  loss_mask_4: 0.1814  loss_dice_4: 0.1693  loss_ce_5: 0.1292  loss_mask_5: 0.1765  loss_dice_5: 0.1628  loss_ce_6: 0.1286  loss_mask_6: 0.1805  loss_dice_6: 0.1617  loss_ce_7: 0.1292  loss_mask_7: 0.1737  loss_dice_7: 0.1645  loss_ce_8: 0.1291  loss_mask_8: 0.1849  loss_dice_8: 0.1676  time: 0.4775  data_time: 0.0016  lr: 0.0001  max_mem: 1568M
[07/11 11:29:38] d2.utils.events INFO:  eta: 4:09:44  iter: 1599  total_loss: 5.637  loss_ce: 0.1252  loss_mask: 0.2186  loss_dice: 0.2198  loss_ce_0: 0.1238  loss_mask_0: 0.2155  loss_dice_0: 0.2224  loss_ce_1: 0.1249  loss_mask_1: 0.2035  loss_dice_1: 0.2225  loss_ce_2: 0.125  loss_mask_2: 0.2134  loss_dice_2: 0.2215  loss_ce_3: 0.1217  loss_mask_3: 0.1977  loss_dice_3: 0.2245  loss_ce_4: 0.125  loss_mask_4: 0.2064  loss_dice_4: 0.2273  loss_ce_5: 0.1249  loss_mask_5: 0.1992  loss_dice_5: 0.2137  loss_ce_6: 0.1251  loss_mask_6: 0.2046  loss_dice_6: 0.2182  loss_ce_7: 0.1249  loss_mask_7: 0.2023  loss_dice_7: 0.2219  loss_ce_8: 0.1249  loss_mask_8: 0.2095  loss_dice_8: 0.2179  time: 0.4787  data_time: 0.0019  lr: 0.0001  max_mem: 1568M
[07/11 11:29:49] d2.utils.events INFO:  eta: 4:10:37  iter: 1619  total_loss: 5.863  loss_ce: 0.1436  loss_mask: 0.2211  loss_dice: 0.2206  loss_ce_0: 0.08396  loss_mask_0: 0.213  loss_dice_0: 0.2083  loss_ce_1: 0.1455  loss_mask_1: 0.2074  loss_dice_1: 0.2111  loss_ce_2: 0.1423  loss_mask_2: 0.2135  loss_dice_2: 0.2087  loss_ce_3: 0.1448  loss_mask_3: 0.2261  loss_dice_3: 0.2147  loss_ce_4: 0.1437  loss_mask_4: 0.205  loss_dice_4: 0.2171  loss_ce_5: 0.1426  loss_mask_5: 0.2082  loss_dice_5: 0.2173  loss_ce_6: 0.1435  loss_mask_6: 0.2252  loss_dice_6: 0.2133  loss_ce_7: 0.1432  loss_mask_7: 0.1954  loss_dice_7: 0.2132  loss_ce_8: 0.1427  loss_mask_8: 0.212  loss_dice_8: 0.2186  time: 0.4796  data_time: 0.0018  lr: 0.0001  max_mem: 1568M
[07/11 11:30:01] d2.utils.events INFO:  eta: 4:11:17  iter: 1639  total_loss: 5.772  loss_ce: 0.1315  loss_mask: 0.2016  loss_dice: 0.1953  loss_ce_0: 0.09922  loss_mask_0: 0.206  loss_dice_0: 0.192  loss_ce_1: 0.1347  loss_mask_1: 0.2063  loss_dice_1: 0.1915  loss_ce_2: 0.1344  loss_mask_2: 0.2112  loss_dice_2: 0.1996  loss_ce_3: 0.132  loss_mask_3: 0.2062  loss_dice_3: 0.1892  loss_ce_4: 0.1325  loss_mask_4: 0.2139  loss_dice_4: 0.1926  loss_ce_5: 0.1333  loss_mask_5: 0.2096  loss_dice_5: 0.2069  loss_ce_6: 0.1328  loss_mask_6: 0.2098  loss_dice_6: 0.1847  loss_ce_7: 0.1326  loss_mask_7: 0.2003  loss_dice_7: 0.1918  loss_ce_8: 0.1336  loss_mask_8: 0.2079  loss_dice_8: 0.1931  time: 0.4806  data_time: 0.0017  lr: 0.0001  max_mem: 1568M
[07/11 11:30:12] d2.utils.events INFO:  eta: 4:12:02  iter: 1659  total_loss: 5.123  loss_ce: 0.1048  loss_mask: 0.2203  loss_dice: 0.177  loss_ce_0: 0.084  loss_mask_0: 0.2094  loss_dice_0: 0.1678  loss_ce_1: 0.1064  loss_mask_1: 0.2175  loss_dice_1: 0.1691  loss_ce_2: 0.1058  loss_mask_2: 0.195  loss_dice_2: 0.1704  loss_ce_3: 0.1037  loss_mask_3: 0.2071  loss_dice_3: 0.1662  loss_ce_4: 0.1058  loss_mask_4: 0.2029  loss_dice_4: 0.172  loss_ce_5: 0.1058  loss_mask_5: 0.2183  loss_dice_5: 0.1744  loss_ce_6: 0.1049  loss_mask_6: 0.2009  loss_dice_6: 0.1738  loss_ce_7: 0.1062  loss_mask_7: 0.2153  loss_dice_7: 0.1703  loss_ce_8: 0.1063  loss_mask_8: 0.2028  loss_dice_8: 0.1723  time: 0.4816  data_time: 0.0026  lr: 0.0001  max_mem: 1568M
[07/11 11:30:23] d2.utils.events INFO:  eta: 4:12:44  iter: 1679  total_loss: 5.596  loss_ce: 0.143  loss_mask: 0.194  loss_dice: 0.1705  loss_ce_0: 0.111  loss_mask_0: 0.1959  loss_dice_0: 0.169  loss_ce_1: 0.14  loss_mask_1: 0.1931  loss_dice_1: 0.1735  loss_ce_2: 0.1415  loss_mask_2: 0.19  loss_dice_2: 0.1738  loss_ce_3: 0.1406  loss_mask_3: 0.1918  loss_dice_3: 0.1758  loss_ce_4: 0.1408  loss_mask_4: 0.1874  loss_dice_4: 0.1772  loss_ce_5: 0.142  loss_mask_5: 0.1917  loss_dice_5: 0.1754  loss_ce_6: 0.1415  loss_mask_6: 0.199  loss_dice_6: 0.1832  loss_ce_7: 0.1418  loss_mask_7: 0.1856  loss_dice_7: 0.1763  loss_ce_8: 0.1426  loss_mask_8: 0.1887  loss_dice_8: 0.1696  time: 0.4826  data_time: 0.0018  lr: 0.0001  max_mem: 1568M
[07/11 11:30:35] d2.utils.events INFO:  eta: 4:13:42  iter: 1699  total_loss: 5.555  loss_ce: 0.1183  loss_mask: 0.2332  loss_dice: 0.1637  loss_ce_0: 0.1044  loss_mask_0: 0.2415  loss_dice_0: 0.1783  loss_ce_1: 0.1171  loss_mask_1: 0.2346  loss_dice_1: 0.1766  loss_ce_2: 0.1157  loss_mask_2: 0.2262  loss_dice_2: 0.1739  loss_ce_3: 0.1143  loss_mask_3: 0.236  loss_dice_3: 0.1743  loss_ce_4: 0.116  loss_mask_4: 0.2424  loss_dice_4: 0.1729  loss_ce_5: 0.1166  loss_mask_5: 0.2416  loss_dice_5: 0.1697  loss_ce_6: 0.1155  loss_mask_6: 0.2348  loss_dice_6: 0.174  loss_ce_7: 0.117  loss_mask_7: 0.2345  loss_dice_7: 0.1704  loss_ce_8: 0.1169  loss_mask_8: 0.2355  loss_dice_8: 0.1773  time: 0.4836  data_time: 0.0017  lr: 0.0001  max_mem: 1568M
[07/11 11:30:46] d2.utils.events INFO:  eta: 4:13:53  iter: 1719  total_loss: 5.695  loss_ce: 0.1299  loss_mask: 0.2227  loss_dice: 0.2209  loss_ce_0: 0.0941  loss_mask_0: 0.2117  loss_dice_0: 0.2231  loss_ce_1: 0.1294  loss_mask_1: 0.2098  loss_dice_1: 0.2177  loss_ce_2: 0.1293  loss_mask_2: 0.2211  loss_dice_2: 0.2188  loss_ce_3: 0.1293  loss_mask_3: 0.2206  loss_dice_3: 0.2253  loss_ce_4: 0.1297  loss_mask_4: 0.2137  loss_dice_4: 0.2182  loss_ce_5: 0.1295  loss_mask_5: 0.213  loss_dice_5: 0.2235  loss_ce_6: 0.1292  loss_mask_6: 0.2009  loss_dice_6: 0.2249  loss_ce_7: 0.1295  loss_mask_7: 0.2176  loss_dice_7: 0.222  loss_ce_8: 0.1295  loss_mask_8: 0.22  loss_dice_8: 0.2186  time: 0.4844  data_time: 0.0017  lr: 0.0001  max_mem: 1568M
[07/11 11:30:57] d2.utils.events INFO:  eta: 4:14:35  iter: 1739  total_loss: 6.021  loss_ce: 0.1425  loss_mask: 0.2619  loss_dice: 0.2182  loss_ce_0: 0.08859  loss_mask_0: 0.266  loss_dice_0: 0.221  loss_ce_1: 0.1404  loss_mask_1: 0.2527  loss_dice_1: 0.2138  loss_ce_2: 0.1399  loss_mask_2: 0.2578  loss_dice_2: 0.2167  loss_ce_3: 0.1397  loss_mask_3: 0.2529  loss_dice_3: 0.2167  loss_ce_4: 0.1408  loss_mask_4: 0.2714  loss_dice_4: 0.2256  loss_ce_5: 0.1414  loss_mask_5: 0.2605  loss_dice_5: 0.2188  loss_ce_6: 0.1402  loss_mask_6: 0.2587  loss_dice_6: 0.2109  loss_ce_7: 0.1412  loss_mask_7: 0.2522  loss_dice_7: 0.2199  loss_ce_8: 0.1412  loss_mask_8: 0.2608  loss_dice_8: 0.2248  time: 0.4853  data_time: 0.0034  lr: 0.0001  max_mem: 1568M
[07/11 11:31:08] d2.utils.events INFO:  eta: 4:14:55  iter: 1759  total_loss: 5.716  loss_ce: 0.1153  loss_mask: 0.2291  loss_dice: 0.1973  loss_ce_0: 0.08705  loss_mask_0: 0.2334  loss_dice_0: 0.1935  loss_ce_1: 0.116  loss_mask_1: 0.2354  loss_dice_1: 0.2083  loss_ce_2: 0.1163  loss_mask_2: 0.2343  loss_dice_2: 0.202  loss_ce_3: 0.1157  loss_mask_3: 0.2282  loss_dice_3: 0.1953  loss_ce_4: 0.1156  loss_mask_4: 0.23  loss_dice_4: 0.1896  loss_ce_5: 0.1158  loss_mask_5: 0.2315  loss_dice_5: 0.1915  loss_ce_6: 0.1152  loss_mask_6: 0.2367  loss_dice_6: 0.1985  loss_ce_7: 0.1155  loss_mask_7: 0.2288  loss_dice_7: 0.1898  loss_ce_8: 0.1151  loss_mask_8: 0.2288  loss_dice_8: 0.1981  time: 0.4862  data_time: 0.0017  lr: 0.0001  max_mem: 1568M
[07/11 11:31:19] d2.utils.events INFO:  eta: 4:15:03  iter: 1779  total_loss: 5.212  loss_ce: 0.1099  loss_mask: 0.2253  loss_dice: 0.1677  loss_ce_0: 0.08642  loss_mask_0: 0.2295  loss_dice_0: 0.1682  loss_ce_1: 0.1105  loss_mask_1: 0.2208  loss_dice_1: 0.1691  loss_ce_2: 0.1108  loss_mask_2: 0.2294  loss_dice_2: 0.1687  loss_ce_3: 0.1103  loss_mask_3: 0.2348  loss_dice_3: 0.1673  loss_ce_4: 0.1101  loss_mask_4: 0.2393  loss_dice_4: 0.1736  loss_ce_5: 0.1104  loss_mask_5: 0.2297  loss_dice_5: 0.1658  loss_ce_6: 0.1098  loss_mask_6: 0.2223  loss_dice_6: 0.1703  loss_ce_7: 0.1101  loss_mask_7: 0.226  loss_dice_7: 0.1687  loss_ce_8: 0.1099  loss_mask_8: 0.2318  loss_dice_8: 0.1673  time: 0.4870  data_time: 0.0017  lr: 0.0001  max_mem: 1568M
[07/11 11:31:31] d2.utils.events INFO:  eta: 4:15:48  iter: 1799  total_loss: 5.727  loss_ce: 0.1288  loss_mask: 0.1886  loss_dice: 0.2002  loss_ce_0: 0.09746  loss_mask_0: 0.1922  loss_dice_0: 0.202  loss_ce_1: 0.1295  loss_mask_1: 0.1844  loss_dice_1: 0.1996  loss_ce_2: 0.129  loss_mask_2: 0.1849  loss_dice_2: 0.1949  loss_ce_3: 0.1296  loss_mask_3: 0.182  loss_dice_3: 0.2009  loss_ce_4: 0.1291  loss_mask_4: 0.189  loss_dice_4: 0.1962  loss_ce_5: 0.1282  loss_mask_5: 0.1787  loss_dice_5: 0.1999  loss_ce_6: 0.1282  loss_mask_6: 0.173  loss_dice_6: 0.1975  loss_ce_7: 0.1281  loss_mask_7: 0.1934  loss_dice_7: 0.1988  loss_ce_8: 0.1278  loss_mask_8: 0.1893  loss_dice_8: 0.2047  time: 0.4878  data_time: 0.0019  lr: 0.0001  max_mem: 1568M
[07/11 11:31:42] d2.utils.events INFO:  eta: 4:16:07  iter: 1819  total_loss: 5.325  loss_ce: 0.1288  loss_mask: 0.2183  loss_dice: 0.1521  loss_ce_0: 0.0906  loss_mask_0: 0.2095  loss_dice_0: 0.1549  loss_ce_1: 0.1327  loss_mask_1: 0.2115  loss_dice_1: 0.1523  loss_ce_2: 0.129  loss_mask_2: 0.2102  loss_dice_2: 0.1484  loss_ce_3: 0.1291  loss_mask_3: 0.2175  loss_dice_3: 0.1512  loss_ce_4: 0.1307  loss_mask_4: 0.2056  loss_dice_4: 0.1553  loss_ce_5: 0.1289  loss_mask_5: 0.2131  loss_dice_5: 0.1521  loss_ce_6: 0.129  loss_mask_6: 0.212  loss_dice_6: 0.1469  loss_ce_7: 0.1303  loss_mask_7: 0.2137  loss_dice_7: 0.1491  loss_ce_8: 0.129  loss_mask_8: 0.2103  loss_dice_8: 0.149  time: 0.4886  data_time: 0.0019  lr: 0.0001  max_mem: 1568M
[07/11 11:31:53] d2.utils.events INFO:  eta: 4:16:28  iter: 1839  total_loss: 5.517  loss_ce: 0.1289  loss_mask: 0.2196  loss_dice: 0.1746  loss_ce_0: 0.08544  loss_mask_0: 0.2245  loss_dice_0: 0.1809  loss_ce_1: 0.1291  loss_mask_1: 0.2237  loss_dice_1: 0.1779  loss_ce_2: 0.1291  loss_mask_2: 0.2221  loss_dice_2: 0.1865  loss_ce_3: 0.1292  loss_mask_3: 0.2161  loss_dice_3: 0.1827  loss_ce_4: 0.1291  loss_mask_4: 0.2036  loss_dice_4: 0.1744  loss_ce_5: 0.1291  loss_mask_5: 0.2141  loss_dice_5: 0.1876  loss_ce_6: 0.1291  loss_mask_6: 0.231  loss_dice_6: 0.185  loss_ce_7: 0.1292  loss_mask_7: 0.2241  loss_dice_7: 0.1877  loss_ce_8: 0.1291  loss_mask_8: 0.2224  loss_dice_8: 0.1806  time: 0.4893  data_time: 0.0019  lr: 0.0001  max_mem: 1568M
[07/11 11:32:04] d2.utils.events INFO:  eta: 4:16:47  iter: 1859  total_loss: 5.736  loss_ce: 0.1349  loss_mask: 0.2557  loss_dice: 0.1835  loss_ce_0: 0.09468  loss_mask_0: 0.2522  loss_dice_0: 0.1871  loss_ce_1: 0.1364  loss_mask_1: 0.2586  loss_dice_1: 0.1732  loss_ce_2: 0.1355  loss_mask_2: 0.2561  loss_dice_2: 0.1827  loss_ce_3: 0.136  loss_mask_3: 0.2499  loss_dice_3: 0.1762  loss_ce_4: 0.1355  loss_mask_4: 0.2515  loss_dice_4: 0.1807  loss_ce_5: 0.1353  loss_mask_5: 0.2592  loss_dice_5: 0.1793  loss_ce_6: 0.1355  loss_mask_6: 0.2526  loss_dice_6: 0.1814  loss_ce_7: 0.1352  loss_mask_7: 0.2538  loss_dice_7: 0.1834  loss_ce_8: 0.1352  loss_mask_8: 0.256  loss_dice_8: 0.1771  time: 0.4899  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:32:15] d2.utils.events INFO:  eta: 4:17:11  iter: 1879  total_loss: 6.004  loss_ce: 0.1385  loss_mask: 0.1806  loss_dice: 0.2195  loss_ce_0: 0.08594  loss_mask_0: 0.1862  loss_dice_0: 0.2324  loss_ce_1: 0.1417  loss_mask_1: 0.1884  loss_dice_1: 0.225  loss_ce_2: 0.1425  loss_mask_2: 0.1853  loss_dice_2: 0.2264  loss_ce_3: 0.1427  loss_mask_3: 0.1884  loss_dice_3: 0.2306  loss_ce_4: 0.1408  loss_mask_4: 0.1875  loss_dice_4: 0.2344  loss_ce_5: 0.1422  loss_mask_5: 0.1876  loss_dice_5: 0.2236  loss_ce_6: 0.1422  loss_mask_6: 0.1814  loss_dice_6: 0.221  loss_ce_7: 0.14  loss_mask_7: 0.1878  loss_dice_7: 0.2285  loss_ce_8: 0.1406  loss_mask_8: 0.1905  loss_dice_8: 0.2262  time: 0.4906  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:32:26] d2.utils.events INFO:  eta: 4:17:34  iter: 1899  total_loss: 6.009  loss_ce: 0.1287  loss_mask: 0.2377  loss_dice: 0.2261  loss_ce_0: 0.0852  loss_mask_0: 0.2429  loss_dice_0: 0.2237  loss_ce_1: 0.1276  loss_mask_1: 0.2319  loss_dice_1: 0.2187  loss_ce_2: 0.1272  loss_mask_2: 0.2394  loss_dice_2: 0.2338  loss_ce_3: 0.1274  loss_mask_3: 0.2321  loss_dice_3: 0.2246  loss_ce_4: 0.1282  loss_mask_4: 0.2499  loss_dice_4: 0.2273  loss_ce_5: 0.128  loss_mask_5: 0.2433  loss_dice_5: 0.233  loss_ce_6: 0.1278  loss_mask_6: 0.2407  loss_dice_6: 0.2316  loss_ce_7: 0.128  loss_mask_7: 0.2395  loss_dice_7: 0.2279  loss_ce_8: 0.1284  loss_mask_8: 0.2347  loss_dice_8: 0.2248  time: 0.4913  data_time: 0.0021  lr: 0.0001  max_mem: 1568M
[07/11 11:32:37] d2.utils.events INFO:  eta: 4:17:51  iter: 1919  total_loss: 5.279  loss_ce: 0.08689  loss_mask: 0.2309  loss_dice: 0.1732  loss_ce_0: 0.08159  loss_mask_0: 0.2462  loss_dice_0: 0.1724  loss_ce_1: 0.08361  loss_mask_1: 0.2405  loss_dice_1: 0.1796  loss_ce_2: 0.08472  loss_mask_2: 0.2382  loss_dice_2: 0.1673  loss_ce_3: 0.08412  loss_mask_3: 0.2427  loss_dice_3: 0.1807  loss_ce_4: 0.08452  loss_mask_4: 0.2333  loss_dice_4: 0.1687  loss_ce_5: 0.08492  loss_mask_5: 0.236  loss_dice_5: 0.1699  loss_ce_6: 0.08426  loss_mask_6: 0.2407  loss_dice_6: 0.1758  loss_ce_7: 0.08476  loss_mask_7: 0.2387  loss_dice_7: 0.1773  loss_ce_8: 0.08521  loss_mask_8: 0.2336  loss_dice_8: 0.1778  time: 0.4920  data_time: 0.0020  lr: 0.0001  max_mem: 1568M
[07/11 11:32:49] d2.utils.events INFO:  eta: 4:18:16  iter: 1939  total_loss: 4.96  loss_ce: 0.1628  loss_mask: 0.174  loss_dice: 0.166  loss_ce_0: 0.09595  loss_mask_0: 0.1656  loss_dice_0: 0.1608  loss_ce_1: 0.1592  loss_mask_1: 0.1649  loss_dice_1: 0.1658  loss_ce_2: 0.1599  loss_mask_2: 0.1576  loss_dice_2: 0.1611  loss_ce_3: 0.1594  loss_mask_3: 0.1673  loss_dice_3: 0.1645  loss_ce_4: 0.1597  loss_mask_4: 0.1673  loss_dice_4: 0.1656  loss_ce_5: 0.1601  loss_mask_5: 0.1557  loss_dice_5: 0.1632  loss_ce_6: 0.1605  loss_mask_6: 0.1719  loss_dice_6: 0.1594  loss_ce_7: 0.161  loss_mask_7: 0.1635  loss_dice_7: 0.1683  loss_ce_8: 0.1619  loss_mask_8: 0.1588  loss_dice_8: 0.1612  time: 0.4927  data_time: 0.0019  lr: 0.0001  max_mem: 1568M
[07/11 11:33:00] d2.utils.events INFO:  eta: 4:18:37  iter: 1959  total_loss: 5.039  loss_ce: 0.1271  loss_mask: 0.182  loss_dice: 0.1774  loss_ce_0: 0.09141  loss_mask_0: 0.1749  loss_dice_0: 0.1756  loss_ce_1: 0.1259  loss_mask_1: 0.1746  loss_dice_1: 0.1763  loss_ce_2: 0.1257  loss_mask_2: 0.1869  loss_dice_2: 0.1852  loss_ce_3: 0.1257  loss_mask_3: 0.1869  loss_dice_3: 0.1831  loss_ce_4: 0.1257  loss_mask_4: 0.1876  loss_dice_4: 0.1805  loss_ce_5: 0.1257  loss_mask_5: 0.1778  loss_dice_5: 0.1756  loss_ce_6: 0.1257  loss_mask_6: 0.1735  loss_dice_6: 0.173  loss_ce_7: 0.1258  loss_mask_7: 0.1857  loss_dice_7: 0.1856  loss_ce_8: 0.1257  loss_mask_8: 0.1795  loss_dice_8: 0.1802  time: 0.4934  data_time: 0.0018  lr: 0.0001  max_mem: 1568M
[07/11 11:33:11] d2.utils.events INFO:  eta: 4:19:12  iter: 1979  total_loss: 5.541  loss_ce: 0.1276  loss_mask: 0.2173  loss_dice: 0.1662  loss_ce_0: 0.08548  loss_mask_0: 0.2185  loss_dice_0: 0.1653  loss_ce_1: 0.1288  loss_mask_1: 0.2212  loss_dice_1: 0.1641  loss_ce_2: 0.1274  loss_mask_2: 0.222  loss_dice_2: 0.1619  loss_ce_3: 0.1275  loss_mask_3: 0.208  loss_dice_3: 0.1657  loss_ce_4: 0.1274  loss_mask_4: 0.2246  loss_dice_4: 0.1732  loss_ce_5: 0.1274  loss_mask_5: 0.2206  loss_dice_5: 0.1699  loss_ce_6: 0.1275  loss_mask_6: 0.2161  loss_dice_6: 0.1662  loss_ce_7: 0.1274  loss_mask_7: 0.2226  loss_dice_7: 0.1628  loss_ce_8: 0.1275  loss_mask_8: 0.2249  loss_dice_8: 0.1617  time: 0.4942  data_time: 0.0028  lr: 0.0001  max_mem: 1568M
[07/11 11:33:22] d2.utils.events INFO:  eta: 4:18:45  iter: 1999  total_loss: 5.417  loss_ce: 0.1329  loss_mask: 0.1662  loss_dice: 0.2001  loss_ce_0: 0.08198  loss_mask_0: 0.169  loss_dice_0: 0.2005  loss_ce_1: 0.1314  loss_mask_1: 0.1662  loss_dice_1: 0.2052  loss_ce_2: 0.1313  loss_mask_2: 0.1696  loss_dice_2: 0.1895  loss_ce_3: 0.131  loss_mask_3: 0.1629  loss_dice_3: 0.2044  loss_ce_4: 0.1309  loss_mask_4: 0.1709  loss_dice_4: 0.2  loss_ce_5: 0.1318  loss_mask_5: 0.1627  loss_dice_5: 0.2012  loss_ce_6: 0.1316  loss_mask_6: 0.1609  loss_dice_6: 0.2042  loss_ce_7: 0.1318  loss_mask_7: 0.1799  loss_dice_7: 0.2032  loss_ce_8: 0.1322  loss_mask_8: 0.1715  loss_dice_8: 0.2027  time: 0.4948  data_time: 0.0027  lr: 0.0001  max_mem: 1568M
[07/11 11:33:34] d2.utils.events INFO:  eta: 4:18:36  iter: 2019  total_loss: 5.686  loss_ce: 0.1272  loss_mask: 0.2322  loss_dice: 0.201  loss_ce_0: 0.08183  loss_mask_0: 0.2159  loss_dice_0: 0.1965  loss_ce_1: 0.1261  loss_mask_1: 0.2331  loss_dice_1: 0.2132  loss_ce_2: 0.1262  loss_mask_2: 0.2365  loss_dice_2: 0.2005  loss_ce_3: 0.126  loss_mask_3: 0.2262  loss_dice_3: 0.2031  loss_ce_4: 0.1259  loss_mask_4: 0.2268  loss_dice_4: 0.1948  loss_ce_5: 0.1265  loss_mask_5: 0.2074  loss_dice_5: 0.2005  loss_ce_6: 0.1261  loss_mask_6: 0.2223  loss_dice_6: 0.2123  loss_ce_7: 0.126  loss_mask_7: 0.2308  loss_dice_7: 0.199  loss_ce_8: 0.1265  loss_mask_8: 0.2265  loss_dice_8: 0.2015  time: 0.4955  data_time: 0.0020  lr: 0.0001  max_mem: 1568M
[07/11 11:33:45] d2.utils.events INFO:  eta: 4:18:22  iter: 2039  total_loss: 5.802  loss_ce: 0.09425  loss_mask: 0.2101  loss_dice: 0.1846  loss_ce_0: 0.07868  loss_mask_0: 0.2189  loss_dice_0: 0.1851  loss_ce_1: 0.09199  loss_mask_1: 0.2143  loss_dice_1: 0.1924  loss_ce_2: 0.09276  loss_mask_2: 0.2195  loss_dice_2: 0.183  loss_ce_3: 0.09218  loss_mask_3: 0.2119  loss_dice_3: 0.1865  loss_ce_4: 0.09226  loss_mask_4: 0.2059  loss_dice_4: 0.1819  loss_ce_5: 0.09276  loss_mask_5: 0.22  loss_dice_5: 0.186  loss_ce_6: 0.09233  loss_mask_6: 0.208  loss_dice_6: 0.1885  loss_ce_7: 0.09262  loss_mask_7: 0.2221  loss_dice_7: 0.1869  loss_ce_8: 0.09278  loss_mask_8: 0.2174  loss_dice_8: 0.1898  time: 0.4961  data_time: 0.0017  lr: 0.0001  max_mem: 1568M
[07/11 11:33:56] d2.utils.events INFO:  eta: 4:18:11  iter: 2059  total_loss: 5.363  loss_ce: 0.1788  loss_mask: 0.2058  loss_dice: 0.1855  loss_ce_0: 0.09172  loss_mask_0: 0.1963  loss_dice_0: 0.185  loss_ce_1: 0.1781  loss_mask_1: 0.1993  loss_dice_1: 0.1893  loss_ce_2: 0.1782  loss_mask_2: 0.208  loss_dice_2: 0.184  loss_ce_3: 0.1782  loss_mask_3: 0.2047  loss_dice_3: 0.1814  loss_ce_4: 0.1782  loss_mask_4: 0.2057  loss_dice_4: 0.1861  loss_ce_5: 0.1781  loss_mask_5: 0.2102  loss_dice_5: 0.191  loss_ce_6: 0.1789  loss_mask_6: 0.1982  loss_dice_6: 0.1919  loss_ce_7: 0.1787  loss_mask_7: 0.2049  loss_dice_7: 0.1868  loss_ce_8: 0.1791  loss_mask_8: 0.2064  loss_dice_8: 0.1889  time: 0.4967  data_time: 0.0020  lr: 0.0001  max_mem: 1568M
[07/11 11:34:07] d2.utils.events INFO:  eta: 4:17:50  iter: 2079  total_loss: 5.062  loss_ce: 0.09838  loss_mask: 0.1606  loss_dice: 0.1913  loss_ce_0: 0.07509  loss_mask_0: 0.167  loss_dice_0: 0.2007  loss_ce_1: 0.1006  loss_mask_1: 0.1637  loss_dice_1: 0.1999  loss_ce_2: 0.1  loss_mask_2: 0.161  loss_dice_2: 0.1926  loss_ce_3: 0.1002  loss_mask_3: 0.1595  loss_dice_3: 0.189  loss_ce_4: 0.1006  loss_mask_4: 0.1674  loss_dice_4: 0.1888  loss_ce_5: 0.1  loss_mask_5: 0.1616  loss_dice_5: 0.198  loss_ce_6: 0.0997  loss_mask_6: 0.1559  loss_dice_6: 0.1932  loss_ce_7: 0.09967  loss_mask_7: 0.1664  loss_dice_7: 0.1935  loss_ce_8: 0.09916  loss_mask_8: 0.1679  loss_dice_8: 0.1976  time: 0.4973  data_time: 0.0017  lr: 0.0001  max_mem: 1568M
[07/11 11:34:19] d2.utils.events INFO:  eta: 4:17:49  iter: 2099  total_loss: 5.125  loss_ce: 0.08197  loss_mask: 0.225  loss_dice: 0.168  loss_ce_0: 0.07245  loss_mask_0: 0.2191  loss_dice_0: 0.1604  loss_ce_1: 0.08106  loss_mask_1: 0.2203  loss_dice_1: 0.1608  loss_ce_2: 0.08111  loss_mask_2: 0.2163  loss_dice_2: 0.1643  loss_ce_3: 0.08107  loss_mask_3: 0.2194  loss_dice_3: 0.1632  loss_ce_4: 0.08133  loss_mask_4: 0.221  loss_dice_4: 0.1649  loss_ce_5: 0.08153  loss_mask_5: 0.2244  loss_dice_5: 0.1716  loss_ce_6: 0.08127  loss_mask_6: 0.2221  loss_dice_6: 0.1656  loss_ce_7: 0.0815  loss_mask_7: 0.2255  loss_dice_7: 0.1693  loss_ce_8: 0.08161  loss_mask_8: 0.2204  loss_dice_8: 0.157  time: 0.4980  data_time: 0.0016  lr: 0.0001  max_mem: 1568M
[07/11 11:34:30] d2.utils.events INFO:  eta: 4:17:28  iter: 2119  total_loss: 5.5  loss_ce: 0.07951  loss_mask: 0.2065  loss_dice: 0.184  loss_ce_0: 0.07008  loss_mask_0: 0.2099  loss_dice_0: 0.1824  loss_ce_1: 0.0795  loss_mask_1: 0.2107  loss_dice_1: 0.1953  loss_ce_2: 0.07952  loss_mask_2: 0.219  loss_dice_2: 0.1871  loss_ce_3: 0.07929  loss_mask_3: 0.1981  loss_dice_3: 0.1872  loss_ce_4: 0.07967  loss_mask_4: 0.2069  loss_dice_4: 0.1815  loss_ce_5: 0.07962  loss_mask_5: 0.2093  loss_dice_5: 0.1937  loss_ce_6: 0.07924  loss_mask_6: 0.2064  loss_dice_6: 0.1796  loss_ce_7: 0.07943  loss_mask_7: 0.212  loss_dice_7: 0.1929  loss_ce_8: 0.07932  loss_mask_8: 0.2083  loss_dice_8: 0.1919  time: 0.4985  data_time: 0.0020  lr: 0.0001  max_mem: 1568M
[07/11 11:34:41] d2.utils.events INFO:  eta: 4:17:27  iter: 2139  total_loss: 6.114  loss_ce: 0.1417  loss_mask: 0.1864  loss_dice: 0.2279  loss_ce_0: 0.09555  loss_mask_0: 0.189  loss_dice_0: 0.2192  loss_ce_1: 0.1347  loss_mask_1: 0.187  loss_dice_1: 0.2272  loss_ce_2: 0.1369  loss_mask_2: 0.1865  loss_dice_2: 0.2325  loss_ce_3: 0.1362  loss_mask_3: 0.1935  loss_dice_3: 0.2315  loss_ce_4: 0.1355  loss_mask_4: 0.1834  loss_dice_4: 0.2259  loss_ce_5: 0.138  loss_mask_5: 0.1848  loss_dice_5: 0.2374  loss_ce_6: 0.1376  loss_mask_6: 0.1902  loss_dice_6: 0.2355  loss_ce_7: 0.1383  loss_mask_7: 0.1844  loss_dice_7: 0.2311  loss_ce_8: 0.139  loss_mask_8: 0.1894  loss_dice_8: 0.2253  time: 0.4991  data_time: 0.0020  lr: 0.0001  max_mem: 1568M
[07/11 11:34:52] d2.utils.events INFO:  eta: 4:17:06  iter: 2159  total_loss: 6.706  loss_ce: 0.1328  loss_mask: 0.1374  loss_dice: 0.3487  loss_ce_0: 0.08159  loss_mask_0: 0.1465  loss_dice_0: 0.3454  loss_ce_1: 0.1342  loss_mask_1: 0.1458  loss_dice_1: 0.3222  loss_ce_2: 0.1334  loss_mask_2: 0.1464  loss_dice_2: 0.3559  loss_ce_3: 0.1335  loss_mask_3: 0.1428  loss_dice_3: 0.3269  loss_ce_4: 0.1337  loss_mask_4: 0.1454  loss_dice_4: 0.3472  loss_ce_5: 0.133  loss_mask_5: 0.1512  loss_dice_5: 0.3437  loss_ce_6: 0.1331  loss_mask_6: 0.1369  loss_dice_6: 0.3553  loss_ce_7: 0.1331  loss_mask_7: 0.1445  loss_dice_7: 0.3543  loss_ce_8: 0.1331  loss_mask_8: 0.1516  loss_dice_8: 0.3415  time: 0.4996  data_time: 0.0019  lr: 0.0001  max_mem: 1568M
[07/11 11:35:03] d2.utils.events INFO:  eta: 4:16:45  iter: 2179  total_loss: 5.649  loss_ce: 0.1531  loss_mask: 0.2144  loss_dice: 0.1444  loss_ce_0: 0.07502  loss_mask_0: 0.2109  loss_dice_0: 0.1489  loss_ce_1: 0.153  loss_mask_1: 0.2089  loss_dice_1: 0.1481  loss_ce_2: 0.1533  loss_mask_2: 0.2067  loss_dice_2: 0.1486  loss_ce_3: 0.1532  loss_mask_3: 0.2091  loss_dice_3: 0.1457  loss_ce_4: 0.1544  loss_mask_4: 0.2116  loss_dice_4: 0.1435  loss_ce_5: 0.1542  loss_mask_5: 0.2107  loss_dice_5: 0.1458  loss_ce_6: 0.1546  loss_mask_6: 0.21  loss_dice_6: 0.1442  loss_ce_7: 0.1534  loss_mask_7: 0.2132  loss_dice_7: 0.1525  loss_ce_8: 0.154  loss_mask_8: 0.2112  loss_dice_8: 0.1471  time: 0.5001  data_time: 0.0019  lr: 0.0001  max_mem: 1568M
[07/11 11:35:14] d2.utils.events INFO:  eta: 4:16:40  iter: 2199  total_loss: 5.287  loss_ce: 0.1326  loss_mask: 0.175  loss_dice: 0.217  loss_ce_0: 0.07484  loss_mask_0: 0.1558  loss_dice_0: 0.2059  loss_ce_1: 0.1297  loss_mask_1: 0.1612  loss_dice_1: 0.2083  loss_ce_2: 0.1299  loss_mask_2: 0.1526  loss_dice_2: 0.2128  loss_ce_3: 0.1302  loss_mask_3: 0.1595  loss_dice_3: 0.2109  loss_ce_4: 0.1308  loss_mask_4: 0.154  loss_dice_4: 0.2138  loss_ce_5: 0.1317  loss_mask_5: 0.1716  loss_dice_5: 0.2108  loss_ce_6: 0.1318  loss_mask_6: 0.1572  loss_dice_6: 0.2014  loss_ce_7: 0.132  loss_mask_7: 0.1638  loss_dice_7: 0.2028  loss_ce_8: 0.1321  loss_mask_8: 0.1604  loss_dice_8: 0.2119  time: 0.5006  data_time: 0.0022  lr: 0.0001  max_mem: 1568M
[07/11 11:35:25] d2.utils.events INFO:  eta: 4:16:17  iter: 2219  total_loss: 4.974  loss_ce: 0.1295  loss_mask: 0.2145  loss_dice: 0.1526  loss_ce_0: 0.08061  loss_mask_0: 0.2152  loss_dice_0: 0.1609  loss_ce_1: 0.1319  loss_mask_1: 0.2146  loss_dice_1: 0.1595  loss_ce_2: 0.13  loss_mask_2: 0.2234  loss_dice_2: 0.1556  loss_ce_3: 0.13  loss_mask_3: 0.2155  loss_dice_3: 0.1662  loss_ce_4: 0.1309  loss_mask_4: 0.2243  loss_dice_4: 0.1627  loss_ce_5: 0.1298  loss_mask_5: 0.2127  loss_dice_5: 0.1632  loss_ce_6: 0.1299  loss_mask_6: 0.2191  loss_dice_6: 0.163  loss_ce_7: 0.1308  loss_mask_7: 0.2193  loss_dice_7: 0.163  loss_ce_8: 0.1297  loss_mask_8: 0.2169  loss_dice_8: 0.1561  time: 0.5011  data_time: 0.0018  lr: 0.0001  max_mem: 1568M
[07/11 11:35:37] d2.utils.events INFO:  eta: 4:16:05  iter: 2239  total_loss: 5.509  loss_ce: 0.1255  loss_mask: 0.1735  loss_dice: 0.1719  loss_ce_0: 0.08366  loss_mask_0: 0.1697  loss_dice_0: 0.1635  loss_ce_1: 0.124  loss_mask_1: 0.1644  loss_dice_1: 0.1726  loss_ce_2: 0.1242  loss_mask_2: 0.1831  loss_dice_2: 0.1732  loss_ce_3: 0.1242  loss_mask_3: 0.1733  loss_dice_3: 0.1699  loss_ce_4: 0.124  loss_mask_4: 0.1712  loss_dice_4: 0.1763  loss_ce_5: 0.124  loss_mask_5: 0.1631  loss_dice_5: 0.1661  loss_ce_6: 0.1249  loss_mask_6: 0.1832  loss_dice_6: 0.1743  loss_ce_7: 0.1248  loss_mask_7: 0.1698  loss_dice_7: 0.1718  loss_ce_8: 0.1251  loss_mask_8: 0.175  loss_dice_8: 0.1773  time: 0.5016  data_time: 0.0019  lr: 0.0001  max_mem: 1568M
[07/11 11:35:48] d2.utils.events INFO:  eta: 4:15:54  iter: 2259  total_loss: 5.384  loss_ce: 0.1281  loss_mask: 0.2118  loss_dice: 0.1841  loss_ce_0: 0.07945  loss_mask_0: 0.2028  loss_dice_0: 0.1817  loss_ce_1: 0.1282  loss_mask_1: 0.1999  loss_dice_1: 0.1789  loss_ce_2: 0.1281  loss_mask_2: 0.2029  loss_dice_2: 0.1831  loss_ce_3: 0.1281  loss_mask_3: 0.2117  loss_dice_3: 0.1875  loss_ce_4: 0.1282  loss_mask_4: 0.2087  loss_dice_4: 0.1824  loss_ce_5: 0.1281  loss_mask_5: 0.2013  loss_dice_5: 0.1856  loss_ce_6: 0.1282  loss_mask_6: 0.2017  loss_dice_6: 0.185  loss_ce_7: 0.1281  loss_mask_7: 0.2028  loss_dice_7: 0.1916  loss_ce_8: 0.1281  loss_mask_8: 0.1972  loss_dice_8: 0.1865  time: 0.5021  data_time: 0.0018  lr: 0.0001  max_mem: 1568M
[07/11 11:35:59] d2.utils.events INFO:  eta: 4:15:39  iter: 2279  total_loss: 5.435  loss_ce: 0.1434  loss_mask: 0.234  loss_dice: 0.1455  loss_ce_0: 0.0794  loss_mask_0: 0.2386  loss_dice_0: 0.1424  loss_ce_1: 0.14  loss_mask_1: 0.2297  loss_dice_1: 0.1389  loss_ce_2: 0.1409  loss_mask_2: 0.2293  loss_dice_2: 0.1334  loss_ce_3: 0.1414  loss_mask_3: 0.2436  loss_dice_3: 0.1367  loss_ce_4: 0.1405  loss_mask_4: 0.2335  loss_dice_4: 0.1428  loss_ce_5: 0.1421  loss_mask_5: 0.2353  loss_dice_5: 0.1351  loss_ce_6: 0.1423  loss_mask_6: 0.2449  loss_dice_6: 0.1406  loss_ce_7: 0.1416  loss_mask_7: 0.2406  loss_dice_7: 0.1423  loss_ce_8: 0.1427  loss_mask_8: 0.2451  loss_dice_8: 0.1322  time: 0.5025  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:36:10] d2.utils.events INFO:  eta: 4:15:21  iter: 2299  total_loss: 6.065  loss_ce: 0.1189  loss_mask: 0.2167  loss_dice: 0.2262  loss_ce_0: 0.07825  loss_mask_0: 0.2013  loss_dice_0: 0.2271  loss_ce_1: 0.1139  loss_mask_1: 0.2135  loss_dice_1: 0.2202  loss_ce_2: 0.1144  loss_mask_2: 0.219  loss_dice_2: 0.2245  loss_ce_3: 0.1144  loss_mask_3: 0.2143  loss_dice_3: 0.2234  loss_ce_4: 0.1147  loss_mask_4: 0.21  loss_dice_4: 0.2138  loss_ce_5: 0.1158  loss_mask_5: 0.2082  loss_dice_5: 0.2244  loss_ce_6: 0.1159  loss_mask_6: 0.2105  loss_dice_6: 0.2302  loss_ce_7: 0.1166  loss_mask_7: 0.2158  loss_dice_7: 0.2255  loss_ce_8: 0.1168  loss_mask_8: 0.2134  loss_dice_8: 0.223  time: 0.5029  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:36:21] d2.utils.events INFO:  eta: 4:15:16  iter: 2319  total_loss: 5.371  loss_ce: 0.07773  loss_mask: 0.2136  loss_dice: 0.1622  loss_ce_0: 0.07404  loss_mask_0: 0.2198  loss_dice_0: 0.1576  loss_ce_1: 0.07353  loss_mask_1: 0.2163  loss_dice_1: 0.1558  loss_ce_2: 0.07457  loss_mask_2: 0.2289  loss_dice_2: 0.1631  loss_ce_3: 0.07433  loss_mask_3: 0.2045  loss_dice_3: 0.1586  loss_ce_4: 0.07386  loss_mask_4: 0.2201  loss_dice_4: 0.168  loss_ce_5: 0.07464  loss_mask_5: 0.2228  loss_dice_5: 0.1659  loss_ce_6: 0.07452  loss_mask_6: 0.2176  loss_dice_6: 0.1625  loss_ce_7: 0.07484  loss_mask_7: 0.2209  loss_dice_7: 0.1619  loss_ce_8: 0.07499  loss_mask_8: 0.2191  loss_dice_8: 0.1697  time: 0.5033  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:36:32] d2.utils.events INFO:  eta: 4:15:13  iter: 2339  total_loss: 5.361  loss_ce: 0.09518  loss_mask: 0.2046  loss_dice: 0.1849  loss_ce_0: 0.07102  loss_mask_0: 0.2044  loss_dice_0: 0.1703  loss_ce_1: 0.09829  loss_mask_1: 0.2095  loss_dice_1: 0.1744  loss_ce_2: 0.09839  loss_mask_2: 0.2016  loss_dice_2: 0.1647  loss_ce_3: 0.0976  loss_mask_3: 0.2091  loss_dice_3: 0.1671  loss_ce_4: 0.09805  loss_mask_4: 0.2082  loss_dice_4: 0.1717  loss_ce_5: 0.09754  loss_mask_5: 0.2156  loss_dice_5: 0.1776  loss_ce_6: 0.09703  loss_mask_6: 0.2104  loss_dice_6: 0.1803  loss_ce_7: 0.0967  loss_mask_7: 0.2084  loss_dice_7: 0.175  loss_ce_8: 0.09646  loss_mask_8: 0.2195  loss_dice_8: 0.1733  time: 0.5038  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:36:43] d2.utils.events INFO:  eta: 4:15:02  iter: 2359  total_loss: 5.693  loss_ce: 0.1504  loss_mask: 0.2407  loss_dice: 0.1742  loss_ce_0: 0.08722  loss_mask_0: 0.2436  loss_dice_0: 0.1739  loss_ce_1: 0.1469  loss_mask_1: 0.2371  loss_dice_1: 0.1777  loss_ce_2: 0.1476  loss_mask_2: 0.249  loss_dice_2: 0.1812  loss_ce_3: 0.1478  loss_mask_3: 0.2426  loss_dice_3: 0.1898  loss_ce_4: 0.147  loss_mask_4: 0.2404  loss_dice_4: 0.1901  loss_ce_5: 0.1477  loss_mask_5: 0.2441  loss_dice_5: 0.1839  loss_ce_6: 0.1483  loss_mask_6: 0.2457  loss_dice_6: 0.1825  loss_ce_7: 0.1486  loss_mask_7: 0.2485  loss_dice_7: 0.1824  loss_ce_8: 0.1487  loss_mask_8: 0.2507  loss_dice_8: 0.1845  time: 0.5042  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:36:54] d2.utils.events INFO:  eta: 4:14:54  iter: 2379  total_loss: 5.284  loss_ce: 0.117  loss_mask: 0.2479  loss_dice: 0.1914  loss_ce_0: 0.07029  loss_mask_0: 0.251  loss_dice_0: 0.1912  loss_ce_1: 0.1192  loss_mask_1: 0.2412  loss_dice_1: 0.1882  loss_ce_2: 0.1195  loss_mask_2: 0.2526  loss_dice_2: 0.1907  loss_ce_3: 0.1194  loss_mask_3: 0.2542  loss_dice_3: 0.1924  loss_ce_4: 0.1191  loss_mask_4: 0.2579  loss_dice_4: 0.2015  loss_ce_5: 0.1195  loss_mask_5: 0.2471  loss_dice_5: 0.1977  loss_ce_6: 0.1192  loss_mask_6: 0.2424  loss_dice_6: 0.1918  loss_ce_7: 0.1175  loss_mask_7: 0.2447  loss_dice_7: 0.1962  loss_ce_8: 0.1185  loss_mask_8: 0.2476  loss_dice_8: 0.1893  time: 0.5046  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:37:05] d2.utils.events INFO:  eta: 4:14:43  iter: 2399  total_loss: 5.629  loss_ce: 0.1395  loss_mask: 0.2204  loss_dice: 0.183  loss_ce_0: 0.08515  loss_mask_0: 0.2196  loss_dice_0: 0.1902  loss_ce_1: 0.1415  loss_mask_1: 0.2206  loss_dice_1: 0.1841  loss_ce_2: 0.1408  loss_mask_2: 0.228  loss_dice_2: 0.1916  loss_ce_3: 0.141  loss_mask_3: 0.2172  loss_dice_3: 0.1892  loss_ce_4: 0.1411  loss_mask_4: 0.2187  loss_dice_4: 0.1914  loss_ce_5: 0.1404  loss_mask_5: 0.2215  loss_dice_5: 0.1922  loss_ce_6: 0.1405  loss_mask_6: 0.2253  loss_dice_6: 0.1956  loss_ce_7: 0.1405  loss_mask_7: 0.2205  loss_dice_7: 0.1875  loss_ce_8: 0.1398  loss_mask_8: 0.2306  loss_dice_8: 0.1878  time: 0.5050  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:37:16] d2.utils.events INFO:  eta: 4:14:41  iter: 2419  total_loss: 5.175  loss_ce: 0.1279  loss_mask: 0.2363  loss_dice: 0.1849  loss_ce_0: 0.07715  loss_mask_0: 0.2306  loss_dice_0: 0.1818  loss_ce_1: 0.1284  loss_mask_1: 0.2312  loss_dice_1: 0.1835  loss_ce_2: 0.128  loss_mask_2: 0.2342  loss_dice_2: 0.1841  loss_ce_3: 0.1282  loss_mask_3: 0.2302  loss_dice_3: 0.1896  loss_ce_4: 0.1281  loss_mask_4: 0.2331  loss_dice_4: 0.1781  loss_ce_5: 0.128  loss_mask_5: 0.2353  loss_dice_5: 0.1844  loss_ce_6: 0.128  loss_mask_6: 0.2382  loss_dice_6: 0.1811  loss_ce_7: 0.1281  loss_mask_7: 0.225  loss_dice_7: 0.1837  loss_ce_8: 0.128  loss_mask_8: 0.235  loss_dice_8: 0.1797  time: 0.5055  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:37:28] d2.utils.events INFO:  eta: 4:14:31  iter: 2439  total_loss: 5.647  loss_ce: 0.1374  loss_mask: 0.1964  loss_dice: 0.1614  loss_ce_0: 0.0738  loss_mask_0: 0.2027  loss_dice_0: 0.166  loss_ce_1: 0.1361  loss_mask_1: 0.2119  loss_dice_1: 0.1674  loss_ce_2: 0.1377  loss_mask_2: 0.1985  loss_dice_2: 0.1665  loss_ce_3: 0.1379  loss_mask_3: 0.2012  loss_dice_3: 0.1679  loss_ce_4: 0.136  loss_mask_4: 0.202  loss_dice_4: 0.1562  loss_ce_5: 0.1376  loss_mask_5: 0.2137  loss_dice_5: 0.1662  loss_ce_6: 0.1375  loss_mask_6: 0.1939  loss_dice_6: 0.1598  loss_ce_7: 0.1354  loss_mask_7: 0.2022  loss_dice_7: 0.1603  loss_ce_8: 0.1375  loss_mask_8: 0.1981  loss_dice_8: 0.1616  time: 0.5059  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:37:39] d2.utils.events INFO:  eta: 4:14:10  iter: 2459  total_loss: 5.81  loss_ce: 0.1078  loss_mask: 0.2305  loss_dice: 0.193  loss_ce_0: 0.07219  loss_mask_0: 0.2266  loss_dice_0: 0.1899  loss_ce_1: 0.1032  loss_mask_1: 0.2302  loss_dice_1: 0.1955  loss_ce_2: 0.1037  loss_mask_2: 0.2301  loss_dice_2: 0.1988  loss_ce_3: 0.1037  loss_mask_3: 0.2294  loss_dice_3: 0.1939  loss_ce_4: 0.1039  loss_mask_4: 0.2281  loss_dice_4: 0.1926  loss_ce_5: 0.1051  loss_mask_5: 0.2151  loss_dice_5: 0.1884  loss_ce_6: 0.1053  loss_mask_6: 0.2229  loss_dice_6: 0.1861  loss_ce_7: 0.1062  loss_mask_7: 0.2204  loss_dice_7: 0.1903  loss_ce_8: 0.106  loss_mask_8: 0.2315  loss_dice_8: 0.1932  time: 0.5063  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:37:50] d2.utils.events INFO:  eta: 4:14:24  iter: 2479  total_loss: 5.336  loss_ce: 0.1402  loss_mask: 0.1743  loss_dice: 0.1731  loss_ce_0: 0.08146  loss_mask_0: 0.1708  loss_dice_0: 0.1647  loss_ce_1: 0.1441  loss_mask_1: 0.1871  loss_dice_1: 0.1626  loss_ce_2: 0.1438  loss_mask_2: 0.1745  loss_dice_2: 0.1641  loss_ce_3: 0.1438  loss_mask_3: 0.1878  loss_dice_3: 0.168  loss_ce_4: 0.144  loss_mask_4: 0.1845  loss_dice_4: 0.1587  loss_ce_5: 0.1434  loss_mask_5: 0.1809  loss_dice_5: 0.1588  loss_ce_6: 0.1434  loss_mask_6: 0.1807  loss_dice_6: 0.165  loss_ce_7: 0.1431  loss_mask_7: 0.1788  loss_dice_7: 0.1654  loss_ce_8: 0.143  loss_mask_8: 0.1755  loss_dice_8: 0.1742  time: 0.5068  data_time: 0.0033  lr: 0.0001  max_mem: 1568M
[07/11 11:38:01] d2.utils.events INFO:  eta: 4:14:13  iter: 2499  total_loss: 5.764  loss_ce: 0.16  loss_mask: 0.1806  loss_dice: 0.1602  loss_ce_0: 0.07638  loss_mask_0: 0.1753  loss_dice_0: 0.1559  loss_ce_1: 0.1479  loss_mask_1: 0.1766  loss_dice_1: 0.1604  loss_ce_2: 0.1529  loss_mask_2: 0.1773  loss_dice_2: 0.1679  loss_ce_3: 0.1551  loss_mask_3: 0.176  loss_dice_3: 0.1606  loss_ce_4: 0.1586  loss_mask_4: 0.1813  loss_dice_4: 0.1527  loss_ce_5: 0.1652  loss_mask_5: 0.1764  loss_dice_5: 0.1634  loss_ce_6: 0.1652  loss_mask_6: 0.1773  loss_dice_6: 0.1637  loss_ce_7: 0.1623  loss_mask_7: 0.1759  loss_dice_7: 0.1579  loss_ce_8: 0.1642  loss_mask_8: 0.186  loss_dice_8: 0.1605  time: 0.5071  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:38:12] d2.utils.events INFO:  eta: 4:13:57  iter: 2519  total_loss: 5.625  loss_ce: 0.1291  loss_mask: 0.1878  loss_dice: 0.1948  loss_ce_0: 0.07664  loss_mask_0: 0.1927  loss_dice_0: 0.203  loss_ce_1: 0.1384  loss_mask_1: 0.1885  loss_dice_1: 0.1987  loss_ce_2: 0.1365  loss_mask_2: 0.1913  loss_dice_2: 0.201  loss_ce_3: 0.138  loss_mask_3: 0.1802  loss_dice_3: 0.1923  loss_ce_4: 0.1402  loss_mask_4: 0.1895  loss_dice_4: 0.1915  loss_ce_5: 0.1384  loss_mask_5: 0.1919  loss_dice_5: 0.2009  loss_ce_6: 0.1379  loss_mask_6: 0.1852  loss_dice_6: 0.1901  loss_ce_7: 0.1332  loss_mask_7: 0.1806  loss_dice_7: 0.1872  loss_ce_8: 0.1337  loss_mask_8: 0.1896  loss_dice_8: 0.1945  time: 0.5076  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:38:24] d2.utils.events INFO:  eta: 4:13:45  iter: 2539  total_loss: 6.394  loss_ce: 0.1299  loss_mask: 0.2166  loss_dice: 0.2413  loss_ce_0: 0.07622  loss_mask_0: 0.2281  loss_dice_0: 0.242  loss_ce_1: 0.1285  loss_mask_1: 0.2167  loss_dice_1: 0.2417  loss_ce_2: 0.129  loss_mask_2: 0.2046  loss_dice_2: 0.2382  loss_ce_3: 0.1279  loss_mask_3: 0.2146  loss_dice_3: 0.2435  loss_ce_4: 0.1272  loss_mask_4: 0.2211  loss_dice_4: 0.2468  loss_ce_5: 0.1292  loss_mask_5: 0.2166  loss_dice_5: 0.2327  loss_ce_6: 0.1278  loss_mask_6: 0.2192  loss_dice_6: 0.2432  loss_ce_7: 0.1283  loss_mask_7: 0.2209  loss_dice_7: 0.2484  loss_ce_8: 0.1274  loss_mask_8: 0.2162  loss_dice_8: 0.2361  time: 0.5080  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:38:35] d2.utils.events INFO:  eta: 4:13:25  iter: 2559  total_loss: 6.073  loss_ce: 0.104  loss_mask: 0.188  loss_dice: 0.1657  loss_ce_0: 0.07407  loss_mask_0: 0.1903  loss_dice_0: 0.1837  loss_ce_1: 0.1001  loss_mask_1: 0.1869  loss_dice_1: 0.1742  loss_ce_2: 0.1064  loss_mask_2: 0.1851  loss_dice_2: 0.1776  loss_ce_3: 0.1051  loss_mask_3: 0.1874  loss_dice_3: 0.1724  loss_ce_4: 0.1033  loss_mask_4: 0.1842  loss_dice_4: 0.1794  loss_ce_5: 0.1006  loss_mask_5: 0.1851  loss_dice_5: 0.1732  loss_ce_6: 0.1003  loss_mask_6: 0.1853  loss_dice_6: 0.1745  loss_ce_7: 0.1015  loss_mask_7: 0.1874  loss_dice_7: 0.1761  loss_ce_8: 0.1013  loss_mask_8: 0.1844  loss_dice_8: 0.1717  time: 0.5084  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:38:46] d2.utils.events INFO:  eta: 4:13:18  iter: 2579  total_loss: 5.383  loss_ce: 0.07266  loss_mask: 0.1961  loss_dice: 0.1844  loss_ce_0: 0.06926  loss_mask_0: 0.2152  loss_dice_0: 0.1968  loss_ce_1: 0.07343  loss_mask_1: 0.2143  loss_dice_1: 0.1885  loss_ce_2: 0.07271  loss_mask_2: 0.2123  loss_dice_2: 0.1911  loss_ce_3: 0.07227  loss_mask_3: 0.2039  loss_dice_3: 0.1868  loss_ce_4: 0.07259  loss_mask_4: 0.2099  loss_dice_4: 0.1864  loss_ce_5: 0.07271  loss_mask_5: 0.2002  loss_dice_5: 0.1958  loss_ce_6: 0.07159  loss_mask_6: 0.214  loss_dice_6: 0.194  loss_ce_7: 0.07219  loss_mask_7: 0.2229  loss_dice_7: 0.1857  loss_ce_8: 0.07189  loss_mask_8: 0.2021  loss_dice_8: 0.1885  time: 0.5087  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:38:57] d2.utils.events INFO:  eta: 4:13:01  iter: 2599  total_loss: 5.602  loss_ce: 0.08777  loss_mask: 0.2278  loss_dice: 0.1866  loss_ce_0: 0.06704  loss_mask_0: 0.2233  loss_dice_0: 0.1915  loss_ce_1: 0.09795  loss_mask_1: 0.2297  loss_dice_1: 0.1899  loss_ce_2: 0.09487  loss_mask_2: 0.2354  loss_dice_2: 0.2042  loss_ce_3: 0.09519  loss_mask_3: 0.2244  loss_dice_3: 0.1855  loss_ce_4: 0.09447  loss_mask_4: 0.2262  loss_dice_4: 0.19  loss_ce_5: 0.09398  loss_mask_5: 0.2274  loss_dice_5: 0.1884  loss_ce_6: 0.09114  loss_mask_6: 0.2278  loss_dice_6: 0.189  loss_ce_7: 0.09034  loss_mask_7: 0.2308  loss_dice_7: 0.1914  loss_ce_8: 0.08973  loss_mask_8: 0.2265  loss_dice_8: 0.1944  time: 0.5090  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:39:08] d2.utils.events INFO:  eta: 4:12:50  iter: 2619  total_loss: 5.308  loss_ce: 0.1369  loss_mask: 0.2298  loss_dice: 0.175  loss_ce_0: 0.08523  loss_mask_0: 0.2285  loss_dice_0: 0.1787  loss_ce_1: 0.1269  loss_mask_1: 0.237  loss_dice_1: 0.1736  loss_ce_2: 0.129  loss_mask_2: 0.2374  loss_dice_2: 0.182  loss_ce_3: 0.1282  loss_mask_3: 0.2344  loss_dice_3: 0.1782  loss_ce_4: 0.1294  loss_mask_4: 0.2318  loss_dice_4: 0.1731  loss_ce_5: 0.1305  loss_mask_5: 0.2277  loss_dice_5: 0.1719  loss_ce_6: 0.1315  loss_mask_6: 0.2336  loss_dice_6: 0.1773  loss_ce_7: 0.1328  loss_mask_7: 0.2302  loss_dice_7: 0.1728  loss_ce_8: 0.1331  loss_mask_8: 0.2322  loss_dice_8: 0.1796  time: 0.5094  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:39:19] d2.utils.events INFO:  eta: 4:12:51  iter: 2639  total_loss: 4.957  loss_ce: 0.1287  loss_mask: 0.177  loss_dice: 0.1514  loss_ce_0: 0.06989  loss_mask_0: 0.1775  loss_dice_0: 0.1658  loss_ce_1: 0.138  loss_mask_1: 0.1722  loss_dice_1: 0.1615  loss_ce_2: 0.1355  loss_mask_2: 0.1736  loss_dice_2: 0.1531  loss_ce_3: 0.1369  loss_mask_3: 0.1684  loss_dice_3: 0.1589  loss_ce_4: 0.1356  loss_mask_4: 0.1737  loss_dice_4: 0.1595  loss_ce_5: 0.1341  loss_mask_5: 0.1737  loss_dice_5: 0.1545  loss_ce_6: 0.1332  loss_mask_6: 0.1686  loss_dice_6: 0.1573  loss_ce_7: 0.1318  loss_mask_7: 0.1583  loss_dice_7: 0.1562  loss_ce_8: 0.1316  loss_mask_8: 0.1739  loss_dice_8: 0.1568  time: 0.5097  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:39:30] d2.utils.events INFO:  eta: 4:12:40  iter: 2659  total_loss: 5.744  loss_ce: 0.1269  loss_mask: 0.204  loss_dice: 0.2146  loss_ce_0: 0.06997  loss_mask_0: 0.195  loss_dice_0: 0.2008  loss_ce_1: 0.1289  loss_mask_1: 0.1981  loss_dice_1: 0.1945  loss_ce_2: 0.1287  loss_mask_2: 0.1949  loss_dice_2: 0.2042  loss_ce_3: 0.1292  loss_mask_3: 0.1976  loss_dice_3: 0.2129  loss_ce_4: 0.1286  loss_mask_4: 0.1941  loss_dice_4: 0.1982  loss_ce_5: 0.127  loss_mask_5: 0.1903  loss_dice_5: 0.203  loss_ce_6: 0.1268  loss_mask_6: 0.2066  loss_dice_6: 0.1988  loss_ce_7: 0.1268  loss_mask_7: 0.1996  loss_dice_7: 0.2111  loss_ce_8: 0.1268  loss_mask_8: 0.197  loss_dice_8: 0.1954  time: 0.5101  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:39:41] d2.utils.events INFO:  eta: 4:12:18  iter: 2679  total_loss: 5.514  loss_ce: 0.109  loss_mask: 0.2229  loss_dice: 0.1587  loss_ce_0: 0.06887  loss_mask_0: 0.2203  loss_dice_0: 0.1697  loss_ce_1: 0.1017  loss_mask_1: 0.2331  loss_dice_1: 0.1645  loss_ce_2: 0.1029  loss_mask_2: 0.222  loss_dice_2: 0.1706  loss_ce_3: 0.1024  loss_mask_3: 0.2262  loss_dice_3: 0.1649  loss_ce_4: 0.1035  loss_mask_4: 0.2301  loss_dice_4: 0.1692  loss_ce_5: 0.1052  loss_mask_5: 0.2272  loss_dice_5: 0.1671  loss_ce_6: 0.1058  loss_mask_6: 0.2079  loss_dice_6: 0.1684  loss_ce_7: 0.1064  loss_mask_7: 0.2212  loss_dice_7: 0.1706  loss_ce_8: 0.1068  loss_mask_8: 0.213  loss_dice_8: 0.1659  time: 0.5104  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:39:53] d2.utils.events INFO:  eta: 4:12:05  iter: 2699  total_loss: 5.516  loss_ce: 0.1027  loss_mask: 0.2299  loss_dice: 0.1728  loss_ce_0: 0.06723  loss_mask_0: 0.2385  loss_dice_0: 0.1771  loss_ce_1: 0.1049  loss_mask_1: 0.2439  loss_dice_1: 0.1709  loss_ce_2: 0.1048  loss_mask_2: 0.2277  loss_dice_2: 0.1666  loss_ce_3: 0.1044  loss_mask_3: 0.2452  loss_dice_3: 0.1668  loss_ce_4: 0.1036  loss_mask_4: 0.2478  loss_dice_4: 0.1703  loss_ce_5: 0.1024  loss_mask_5: 0.2386  loss_dice_5: 0.1695  loss_ce_6: 0.1025  loss_mask_6: 0.2452  loss_dice_6: 0.17  loss_ce_7: 0.1023  loss_mask_7: 0.2359  loss_dice_7: 0.1709  loss_ce_8: 0.1025  loss_mask_8: 0.2349  loss_dice_8: 0.1704  time: 0.5108  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:40:04] d2.utils.events INFO:  eta: 4:12:01  iter: 2719  total_loss: 6.033  loss_ce: 0.1611  loss_mask: 0.1365  loss_dice: 0.2254  loss_ce_0: 0.08423  loss_mask_0: 0.1542  loss_dice_0: 0.2494  loss_ce_1: 0.1583  loss_mask_1: 0.1597  loss_dice_1: 0.2484  loss_ce_2: 0.1587  loss_mask_2: 0.151  loss_dice_2: 0.2347  loss_ce_3: 0.1589  loss_mask_3: 0.1446  loss_dice_3: 0.2351  loss_ce_4: 0.1589  loss_mask_4: 0.1491  loss_dice_4: 0.2268  loss_ce_5: 0.1601  loss_mask_5: 0.1478  loss_dice_5: 0.2328  loss_ce_6: 0.1603  loss_mask_6: 0.145  loss_dice_6: 0.2334  loss_ce_7: 0.1608  loss_mask_7: 0.1486  loss_dice_7: 0.2404  loss_ce_8: 0.1606  loss_mask_8: 0.1594  loss_dice_8: 0.2402  time: 0.5111  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:40:15] d2.utils.events INFO:  eta: 4:11:41  iter: 2739  total_loss: 5.549  loss_ce: 0.1114  loss_mask: 0.1932  loss_dice: 0.1799  loss_ce_0: 0.06641  loss_mask_0: 0.191  loss_dice_0: 0.1853  loss_ce_1: 0.117  loss_mask_1: 0.1915  loss_dice_1: 0.1766  loss_ce_2: 0.1156  loss_mask_2: 0.1975  loss_dice_2: 0.1825  loss_ce_3: 0.1159  loss_mask_3: 0.1988  loss_dice_3: 0.1818  loss_ce_4: 0.1152  loss_mask_4: 0.1926  loss_dice_4: 0.1862  loss_ce_5: 0.1138  loss_mask_5: 0.1884  loss_dice_5: 0.188  loss_ce_6: 0.1134  loss_mask_6: 0.1963  loss_dice_6: 0.1774  loss_ce_7: 0.1129  loss_mask_7: 0.2016  loss_dice_7: 0.1785  loss_ce_8: 0.1129  loss_mask_8: 0.2023  loss_dice_8: 0.1835  time: 0.5115  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:40:26] d2.utils.events INFO:  eta: 4:11:22  iter: 2759  total_loss: 5.395  loss_ce: 0.1289  loss_mask: 0.2026  loss_dice: 0.174  loss_ce_0: 0.07451  loss_mask_0: 0.2173  loss_dice_0: 0.1738  loss_ce_1: 0.1293  loss_mask_1: 0.2133  loss_dice_1: 0.1766  loss_ce_2: 0.1292  loss_mask_2: 0.2134  loss_dice_2: 0.1701  loss_ce_3: 0.1292  loss_mask_3: 0.2158  loss_dice_3: 0.1698  loss_ce_4: 0.1291  loss_mask_4: 0.2101  loss_dice_4: 0.1654  loss_ce_5: 0.129  loss_mask_5: 0.2235  loss_dice_5: 0.1685  loss_ce_6: 0.1291  loss_mask_6: 0.2142  loss_dice_6: 0.1707  loss_ce_7: 0.129  loss_mask_7: 0.2218  loss_dice_7: 0.167  loss_ce_8: 0.1291  loss_mask_8: 0.2143  loss_dice_8: 0.1693  time: 0.5118  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:40:37] d2.utils.events INFO:  eta: 4:11:21  iter: 2779  total_loss: 5.244  loss_ce: 0.1271  loss_mask: 0.2284  loss_dice: 0.1559  loss_ce_0: 0.0741  loss_mask_0: 0.2295  loss_dice_0: 0.1594  loss_ce_1: 0.1273  loss_mask_1: 0.2193  loss_dice_1: 0.1512  loss_ce_2: 0.127  loss_mask_2: 0.2244  loss_dice_2: 0.1543  loss_ce_3: 0.127  loss_mask_3: 0.2271  loss_dice_3: 0.152  loss_ce_4: 0.1269  loss_mask_4: 0.2248  loss_dice_4: 0.1461  loss_ce_5: 0.1269  loss_mask_5: 0.2084  loss_dice_5: 0.1557  loss_ce_6: 0.1269  loss_mask_6: 0.2216  loss_dice_6: 0.1559  loss_ce_7: 0.1269  loss_mask_7: 0.2158  loss_dice_7: 0.1488  loss_ce_8: 0.127  loss_mask_8: 0.2223  loss_dice_8: 0.1545  time: 0.5121  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:40:48] d2.utils.events INFO:  eta: 4:11:10  iter: 2799  total_loss: 5.485  loss_ce: 0.1529  loss_mask: 0.2016  loss_dice: 0.1857  loss_ce_0: 0.07185  loss_mask_0: 0.2183  loss_dice_0: 0.187  loss_ce_1: 0.1565  loss_mask_1: 0.2087  loss_dice_1: 0.1909  loss_ce_2: 0.155  loss_mask_2: 0.205  loss_dice_2: 0.1884  loss_ce_3: 0.1554  loss_mask_3: 0.2002  loss_dice_3: 0.1827  loss_ce_4: 0.155  loss_mask_4: 0.2085  loss_dice_4: 0.1898  loss_ce_5: 0.1547  loss_mask_5: 0.2103  loss_dice_5: 0.1856  loss_ce_6: 0.1549  loss_mask_6: 0.2166  loss_dice_6: 0.1879  loss_ce_7: 0.1545  loss_mask_7: 0.2091  loss_dice_7: 0.183  loss_ce_8: 0.1545  loss_mask_8: 0.2047  loss_dice_8: 0.179  time: 0.5125  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:41:00] d2.utils.events INFO:  eta: 4:10:59  iter: 2819  total_loss: 4.873  loss_ce: 0.1075  loss_mask: 0.1923  loss_dice: 0.1701  loss_ce_0: 0.07411  loss_mask_0: 0.1895  loss_dice_0: 0.1704  loss_ce_1: 0.1071  loss_mask_1: 0.1731  loss_dice_1: 0.1769  loss_ce_2: 0.1078  loss_mask_2: 0.1812  loss_dice_2: 0.1714  loss_ce_3: 0.1076  loss_mask_3: 0.1873  loss_dice_3: 0.1696  loss_ce_4: 0.1071  loss_mask_4: 0.1772  loss_dice_4: 0.1735  loss_ce_5: 0.107  loss_mask_5: 0.1856  loss_dice_5: 0.1718  loss_ce_6: 0.1072  loss_mask_6: 0.1879  loss_dice_6: 0.1709  loss_ce_7: 0.1071  loss_mask_7: 0.1888  loss_dice_7: 0.1727  loss_ce_8: 0.1069  loss_mask_8: 0.1784  loss_dice_8: 0.1751  time: 0.5128  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:41:11] d2.utils.events INFO:  eta: 4:10:54  iter: 2839  total_loss: 5.669  loss_ce: 0.1646  loss_mask: 0.2315  loss_dice: 0.1648  loss_ce_0: 0.07657  loss_mask_0: 0.2369  loss_dice_0: 0.1604  loss_ce_1: 0.1634  loss_mask_1: 0.2432  loss_dice_1: 0.1727  loss_ce_2: 0.1631  loss_mask_2: 0.2364  loss_dice_2: 0.1641  loss_ce_3: 0.1631  loss_mask_3: 0.2451  loss_dice_3: 0.1661  loss_ce_4: 0.1629  loss_mask_4: 0.2487  loss_dice_4: 0.1617  loss_ce_5: 0.1636  loss_mask_5: 0.2363  loss_dice_5: 0.1698  loss_ce_6: 0.1641  loss_mask_6: 0.2416  loss_dice_6: 0.1657  loss_ce_7: 0.1639  loss_mask_7: 0.231  loss_dice_7: 0.1658  loss_ce_8: 0.1643  loss_mask_8: 0.2347  loss_dice_8: 0.1596  time: 0.5131  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:41:22] d2.utils.events INFO:  eta: 4:10:48  iter: 2859  total_loss: 5.041  loss_ce: 0.1298  loss_mask: 0.2054  loss_dice: 0.1638  loss_ce_0: 0.07346  loss_mask_0: 0.185  loss_dice_0: 0.1669  loss_ce_1: 0.1299  loss_mask_1: 0.1909  loss_dice_1: 0.1652  loss_ce_2: 0.1297  loss_mask_2: 0.1998  loss_dice_2: 0.1706  loss_ce_3: 0.1298  loss_mask_3: 0.1973  loss_dice_3: 0.1643  loss_ce_4: 0.1306  loss_mask_4: 0.1942  loss_dice_4: 0.1676  loss_ce_5: 0.1297  loss_mask_5: 0.1886  loss_dice_5: 0.1587  loss_ce_6: 0.1298  loss_mask_6: 0.1889  loss_dice_6: 0.161  loss_ce_7: 0.1307  loss_mask_7: 0.1918  loss_dice_7: 0.163  loss_ce_8: 0.1298  loss_mask_8: 0.1999  loss_dice_8: 0.1699  time: 0.5134  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:41:33] d2.utils.events INFO:  eta: 4:10:21  iter: 2879  total_loss: 5.218  loss_ce: 0.1308  loss_mask: 0.2063  loss_dice: 0.1903  loss_ce_0: 0.0759  loss_mask_0: 0.2166  loss_dice_0: 0.1987  loss_ce_1: 0.1288  loss_mask_1: 0.219  loss_dice_1: 0.195  loss_ce_2: 0.1296  loss_mask_2: 0.2109  loss_dice_2: 0.1958  loss_ce_3: 0.1288  loss_mask_3: 0.2021  loss_dice_3: 0.1918  loss_ce_4: 0.1294  loss_mask_4: 0.201  loss_dice_4: 0.1921  loss_ce_5: 0.13  loss_mask_5: 0.2074  loss_dice_5: 0.1872  loss_ce_6: 0.1302  loss_mask_6: 0.2111  loss_dice_6: 0.1876  loss_ce_7: 0.13  loss_mask_7: 0.2092  loss_dice_7: 0.1919  loss_ce_8: 0.1307  loss_mask_8: 0.2088  loss_dice_8: 0.1876  time: 0.5137  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:41:44] d2.utils.events INFO:  eta: 4:10:05  iter: 2899  total_loss: 5.366  loss_ce: 0.0829  loss_mask: 0.2115  loss_dice: 0.2071  loss_ce_0: 0.0714  loss_mask_0: 0.2065  loss_dice_0: 0.2041  loss_ce_1: 0.07468  loss_mask_1: 0.2135  loss_dice_1: 0.2123  loss_ce_2: 0.07697  loss_mask_2: 0.2156  loss_dice_2: 0.2071  loss_ce_3: 0.07592  loss_mask_3: 0.2089  loss_dice_3: 0.2035  loss_ce_4: 0.07374  loss_mask_4: 0.2119  loss_dice_4: 0.2043  loss_ce_5: 0.07731  loss_mask_5: 0.2063  loss_dice_5: 0.2053  loss_ce_6: 0.0789  loss_mask_6: 0.2008  loss_dice_6: 0.2102  loss_ce_7: 0.07536  loss_mask_7: 0.2087  loss_dice_7: 0.2136  loss_ce_8: 0.0792  loss_mask_8: 0.2096  loss_dice_8: 0.2093  time: 0.5140  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:41:55] d2.utils.events INFO:  eta: 4:09:59  iter: 2919  total_loss: 5.984  loss_ce: 0.1367  loss_mask: 0.2288  loss_dice: 0.1942  loss_ce_0: 0.07258  loss_mask_0: 0.2363  loss_dice_0: 0.1991  loss_ce_1: 0.1365  loss_mask_1: 0.2413  loss_dice_1: 0.1967  loss_ce_2: 0.1363  loss_mask_2: 0.2284  loss_dice_2: 0.1864  loss_ce_3: 0.1365  loss_mask_3: 0.2294  loss_dice_3: 0.1995  loss_ce_4: 0.1356  loss_mask_4: 0.244  loss_dice_4: 0.1976  loss_ce_5: 0.1362  loss_mask_5: 0.2313  loss_dice_5: 0.1995  loss_ce_6: 0.1369  loss_mask_6: 0.2348  loss_dice_6: 0.1966  loss_ce_7: 0.1356  loss_mask_7: 0.233  loss_dice_7: 0.1935  loss_ce_8: 0.1368  loss_mask_8: 0.2271  loss_dice_8: 0.1965  time: 0.5142  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:42:06] d2.utils.events INFO:  eta: 4:09:52  iter: 2939  total_loss: 5.452  loss_ce: 0.09169  loss_mask: 0.2165  loss_dice: 0.2028  loss_ce_0: 0.06511  loss_mask_0: 0.2114  loss_dice_0: 0.2023  loss_ce_1: 0.09585  loss_mask_1: 0.2201  loss_dice_1: 0.201  loss_ce_2: 0.0956  loss_mask_2: 0.2176  loss_dice_2: 0.2006  loss_ce_3: 0.09586  loss_mask_3: 0.2115  loss_dice_3: 0.1953  loss_ce_4: 0.09925  loss_mask_4: 0.216  loss_dice_4: 0.1958  loss_ce_5: 0.09602  loss_mask_5: 0.2207  loss_dice_5: 0.2005  loss_ce_6: 0.09383  loss_mask_6: 0.2174  loss_dice_6: 0.1989  loss_ce_7: 0.09902  loss_mask_7: 0.2151  loss_dice_7: 0.1974  loss_ce_8: 0.09312  loss_mask_8: 0.217  loss_dice_8: 0.2005  time: 0.5146  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:42:18] d2.utils.events INFO:  eta: 4:09:31  iter: 2959  total_loss: 5.128  loss_ce: 0.106  loss_mask: 0.2044  loss_dice: 0.1632  loss_ce_0: 0.06463  loss_mask_0: 0.2024  loss_dice_0: 0.1676  loss_ce_1: 0.1124  loss_mask_1: 0.1969  loss_dice_1: 0.161  loss_ce_2: 0.1113  loss_mask_2: 0.2097  loss_dice_2: 0.1637  loss_ce_3: 0.1118  loss_mask_3: 0.1958  loss_dice_3: 0.1574  loss_ce_4: 0.1151  loss_mask_4: 0.1999  loss_dice_4: 0.1623  loss_ce_5: 0.1117  loss_mask_5: 0.197  loss_dice_5: 0.1682  loss_ce_6: 0.1094  loss_mask_6: 0.209  loss_dice_6: 0.1631  loss_ce_7: 0.1145  loss_mask_7: 0.1913  loss_dice_7: 0.1653  loss_ce_8: 0.108  loss_mask_8: 0.1979  loss_dice_8: 0.1675  time: 0.5148  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:42:29] d2.utils.events INFO:  eta: 4:09:09  iter: 2979  total_loss: 5.355  loss_ce: 0.1206  loss_mask: 0.2122  loss_dice: 0.1866  loss_ce_0: 0.06539  loss_mask_0: 0.2109  loss_dice_0: 0.1794  loss_ce_1: 0.124  loss_mask_1: 0.2126  loss_dice_1: 0.1782  loss_ce_2: 0.1229  loss_mask_2: 0.2198  loss_dice_2: 0.1898  loss_ce_3: 0.1231  loss_mask_3: 0.2118  loss_dice_3: 0.1832  loss_ce_4: 0.1243  loss_mask_4: 0.2174  loss_dice_4: 0.1817  loss_ce_5: 0.1229  loss_mask_5: 0.1988  loss_dice_5: 0.1789  loss_ce_6: 0.1223  loss_mask_6: 0.218  loss_dice_6: 0.1774  loss_ce_7: 0.1233  loss_mask_7: 0.2134  loss_dice_7: 0.1802  loss_ce_8: 0.122  loss_mask_8: 0.2184  loss_dice_8: 0.176  time: 0.5151  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:42:40] d2.utils.events INFO:  eta: 4:09:03  iter: 2999  total_loss: 4.824  loss_ce: 0.1143  loss_mask: 0.2063  loss_dice: 0.1516  loss_ce_0: 0.06506  loss_mask_0: 0.2075  loss_dice_0: 0.1477  loss_ce_1: 0.1152  loss_mask_1: 0.2007  loss_dice_1: 0.1541  loss_ce_2: 0.1146  loss_mask_2: 0.2035  loss_dice_2: 0.1559  loss_ce_3: 0.1147  loss_mask_3: 0.2141  loss_dice_3: 0.1494  loss_ce_4: 0.1152  loss_mask_4: 0.2178  loss_dice_4: 0.1534  loss_ce_5: 0.1147  loss_mask_5: 0.2172  loss_dice_5: 0.1539  loss_ce_6: 0.1146  loss_mask_6: 0.2102  loss_dice_6: 0.1532  loss_ce_7: 0.1146  loss_mask_7: 0.2062  loss_dice_7: 0.1503  loss_ce_8: 0.1149  loss_mask_8: 0.2054  loss_dice_8: 0.1543  time: 0.5154  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:42:51] d2.utils.events INFO:  eta: 4:08:55  iter: 3019  total_loss: 5.251  loss_ce: 0.1441  loss_mask: 0.1382  loss_dice: 0.234  loss_ce_0: 0.08076  loss_mask_0: 0.1519  loss_dice_0: 0.2291  loss_ce_1: 0.1412  loss_mask_1: 0.1366  loss_dice_1: 0.227  loss_ce_2: 0.1417  loss_mask_2: 0.1474  loss_dice_2: 0.2253  loss_ce_3: 0.1417  loss_mask_3: 0.1386  loss_dice_3: 0.2193  loss_ce_4: 0.1424  loss_mask_4: 0.1378  loss_dice_4: 0.2398  loss_ce_5: 0.1429  loss_mask_5: 0.1359  loss_dice_5: 0.2328  loss_ce_6: 0.1429  loss_mask_6: 0.1336  loss_dice_6: 0.2213  loss_ce_7: 0.1437  loss_mask_7: 0.1456  loss_dice_7: 0.2352  loss_ce_8: 0.1433  loss_mask_8: 0.1513  loss_dice_8: 0.2358  time: 0.5157  data_time: 0.0016  lr: 0.0001  max_mem: 1568M
[07/11 11:43:02] d2.utils.events INFO:  eta: 4:08:43  iter: 3039  total_loss: 5.665  loss_ce: 0.1118  loss_mask: 0.236  loss_dice: 0.2137  loss_ce_0: 0.06461  loss_mask_0: 0.2385  loss_dice_0: 0.2167  loss_ce_1: 0.1139  loss_mask_1: 0.2289  loss_dice_1: 0.2116  loss_ce_2: 0.1133  loss_mask_2: 0.2267  loss_dice_2: 0.2122  loss_ce_3: 0.1137  loss_mask_3: 0.2165  loss_dice_3: 0.2104  loss_ce_4: 0.1133  loss_mask_4: 0.2326  loss_dice_4: 0.2035  loss_ce_5: 0.1126  loss_mask_5: 0.221  loss_dice_5: 0.2099  loss_ce_6: 0.1126  loss_mask_6: 0.2233  loss_dice_6: 0.2079  loss_ce_7: 0.1122  loss_mask_7: 0.2369  loss_dice_7: 0.2127  loss_ce_8: 0.1125  loss_mask_8: 0.2241  loss_dice_8: 0.2108  time: 0.5159  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:43:13] d2.utils.events INFO:  eta: 4:08:30  iter: 3059  total_loss: 5.507  loss_ce: 0.1303  loss_mask: 0.2009  loss_dice: 0.1826  loss_ce_0: 0.07308  loss_mask_0: 0.2115  loss_dice_0: 0.1806  loss_ce_1: 0.13  loss_mask_1: 0.2169  loss_dice_1: 0.1827  loss_ce_2: 0.1301  loss_mask_2: 0.2117  loss_dice_2: 0.1885  loss_ce_3: 0.1303  loss_mask_3: 0.1997  loss_dice_3: 0.1788  loss_ce_4: 0.13  loss_mask_4: 0.2148  loss_dice_4: 0.1777  loss_ce_5: 0.1302  loss_mask_5: 0.2034  loss_dice_5: 0.1753  loss_ce_6: 0.1303  loss_mask_6: 0.2142  loss_dice_6: 0.1813  loss_ce_7: 0.1301  loss_mask_7: 0.2264  loss_dice_7: 0.1849  loss_ce_8: 0.1303  loss_mask_8: 0.2108  loss_dice_8: 0.1853  time: 0.5162  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:43:24] d2.utils.events INFO:  eta: 4:08:21  iter: 3079  total_loss: 5.038  loss_ce: 0.09687  loss_mask: 0.1993  loss_dice: 0.1845  loss_ce_0: 0.06177  loss_mask_0: 0.2015  loss_dice_0: 0.1839  loss_ce_1: 0.09843  loss_mask_1: 0.2077  loss_dice_1: 0.1814  loss_ce_2: 0.09817  loss_mask_2: 0.1936  loss_dice_2: 0.19  loss_ce_3: 0.0982  loss_mask_3: 0.2022  loss_dice_3: 0.178  loss_ce_4: 0.09857  loss_mask_4: 0.1961  loss_dice_4: 0.1758  loss_ce_5: 0.09784  loss_mask_5: 0.2064  loss_dice_5: 0.1869  loss_ce_6: 0.09733  loss_mask_6: 0.1946  loss_dice_6: 0.1861  loss_ce_7: 0.09803  loss_mask_7: 0.1957  loss_dice_7: 0.1759  loss_ce_8: 0.09738  loss_mask_8: 0.2024  loss_dice_8: 0.1809  time: 0.5165  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:43:36] d2.utils.events INFO:  eta: 4:08:10  iter: 3099  total_loss: 4.866  loss_ce: 0.092  loss_mask: 0.1945  loss_dice: 0.1703  loss_ce_0: 0.0602  loss_mask_0: 0.1911  loss_dice_0: 0.1691  loss_ce_1: 0.09226  loss_mask_1: 0.1897  loss_dice_1: 0.1751  loss_ce_2: 0.09229  loss_mask_2: 0.187  loss_dice_2: 0.1779  loss_ce_3: 0.09225  loss_mask_3: 0.1907  loss_dice_3: 0.1722  loss_ce_4: 0.09233  loss_mask_4: 0.1933  loss_dice_4: 0.1714  loss_ce_5: 0.09234  loss_mask_5: 0.2026  loss_dice_5: 0.1766  loss_ce_6: 0.09206  loss_mask_6: 0.1896  loss_dice_6: 0.1689  loss_ce_7: 0.09213  loss_mask_7: 0.1988  loss_dice_7: 0.182  loss_ce_8: 0.09193  loss_mask_8: 0.1885  loss_dice_8: 0.1766  time: 0.5167  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:43:47] d2.utils.events INFO:  eta: 4:08:19  iter: 3119  total_loss: 5.021  loss_ce: 0.09101  loss_mask: 0.2456  loss_dice: 0.1396  loss_ce_0: 0.05971  loss_mask_0: 0.2516  loss_dice_0: 0.1366  loss_ce_1: 0.09124  loss_mask_1: 0.2475  loss_dice_1: 0.1333  loss_ce_2: 0.09106  loss_mask_2: 0.2504  loss_dice_2: 0.1383  loss_ce_3: 0.09105  loss_mask_3: 0.2473  loss_dice_3: 0.1405  loss_ce_4: 0.09113  loss_mask_4: 0.2442  loss_dice_4: 0.1405  loss_ce_5: 0.09135  loss_mask_5: 0.2573  loss_dice_5: 0.1356  loss_ce_6: 0.091  loss_mask_6: 0.2492  loss_dice_6: 0.1401  loss_ce_7: 0.09114  loss_mask_7: 0.25  loss_dice_7: 0.1355  loss_ce_8: 0.09098  loss_mask_8: 0.2483  loss_dice_8: 0.1392  time: 0.5170  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:43:58] d2.utils.events INFO:  eta: 4:08:03  iter: 3139  total_loss: 5.257  loss_ce: 0.1466  loss_mask: 0.1892  loss_dice: 0.1661  loss_ce_0: 0.08448  loss_mask_0: 0.1906  loss_dice_0: 0.1653  loss_ce_1: 0.1411  loss_mask_1: 0.1928  loss_dice_1: 0.167  loss_ce_2: 0.1423  loss_mask_2: 0.1963  loss_dice_2: 0.1669  loss_ce_3: 0.1417  loss_mask_3: 0.1803  loss_dice_3: 0.1642  loss_ce_4: 0.1423  loss_mask_4: 0.1861  loss_dice_4: 0.1732  loss_ce_5: 0.1428  loss_mask_5: 0.1883  loss_dice_5: 0.1695  loss_ce_6: 0.1437  loss_mask_6: 0.1911  loss_dice_6: 0.1665  loss_ce_7: 0.1435  loss_mask_7: 0.181  loss_dice_7: 0.1759  loss_ce_8: 0.1448  loss_mask_8: 0.1822  loss_dice_8: 0.1736  time: 0.5173  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:44:09] d2.utils.events INFO:  eta: 4:08:01  iter: 3159  total_loss: 4.798  loss_ce: 0.1246  loss_mask: 0.202  loss_dice: 0.1489  loss_ce_0: 0.07581  loss_mask_0: 0.2081  loss_dice_0: 0.1507  loss_ce_1: 0.1168  loss_mask_1: 0.2052  loss_dice_1: 0.1592  loss_ce_2: 0.1188  loss_mask_2: 0.2166  loss_dice_2: 0.1551  loss_ce_3: 0.1178  loss_mask_3: 0.207  loss_dice_3: 0.1567  loss_ce_4: 0.1185  loss_mask_4: 0.2063  loss_dice_4: 0.1574  loss_ce_5: 0.1198  loss_mask_5: 0.2072  loss_dice_5: 0.1591  loss_ce_6: 0.1204  loss_mask_6: 0.2112  loss_dice_6: 0.158  loss_ce_7: 0.1201  loss_mask_7: 0.2136  loss_dice_7: 0.1576  loss_ce_8: 0.122  loss_mask_8: 0.2003  loss_dice_8: 0.1491  time: 0.5175  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:44:20] d2.utils.events INFO:  eta: 4:07:54  iter: 3179  total_loss: 5.705  loss_ce: 0.1531  loss_mask: 0.2347  loss_dice: 0.1667  loss_ce_0: 0.06937  loss_mask_0: 0.2254  loss_dice_0: 0.1824  loss_ce_1: 0.1526  loss_mask_1: 0.2319  loss_dice_1: 0.1772  loss_ce_2: 0.1521  loss_mask_2: 0.2283  loss_dice_2: 0.1692  loss_ce_3: 0.1522  loss_mask_3: 0.2392  loss_dice_3: 0.1761  loss_ce_4: 0.1519  loss_mask_4: 0.2262  loss_dice_4: 0.171  loss_ce_5: 0.1526  loss_mask_5: 0.236  loss_dice_5: 0.1822  loss_ce_6: 0.1534  loss_mask_6: 0.2314  loss_dice_6: 0.1715  loss_ce_7: 0.1523  loss_mask_7: 0.2328  loss_dice_7: 0.1703  loss_ce_8: 0.1544  loss_mask_8: 0.2311  loss_dice_8: 0.1763  time: 0.5178  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:44:31] d2.utils.events INFO:  eta: 4:07:40  iter: 3199  total_loss: 5.118  loss_ce: 0.1241  loss_mask: 0.1849  loss_dice: 0.1841  loss_ce_0: 0.07224  loss_mask_0: 0.1759  loss_dice_0: 0.1856  loss_ce_1: 0.1284  loss_mask_1: 0.1774  loss_dice_1: 0.1811  loss_ce_2: 0.1278  loss_mask_2: 0.1786  loss_dice_2: 0.1803  loss_ce_3: 0.1284  loss_mask_3: 0.1807  loss_dice_3: 0.189  loss_ce_4: 0.1284  loss_mask_4: 0.1742  loss_dice_4: 0.1823  loss_ce_5: 0.1278  loss_mask_5: 0.18  loss_dice_5: 0.19  loss_ce_6: 0.1266  loss_mask_6: 0.1879  loss_dice_6: 0.1789  loss_ce_7: 0.1278  loss_mask_7: 0.1878  loss_dice_7: 0.1829  loss_ce_8: 0.1246  loss_mask_8: 0.1896  loss_dice_8: 0.1861  time: 0.5180  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:44:43] d2.utils.events INFO:  eta: 4:07:25  iter: 3219  total_loss: 5.987  loss_ce: 0.14  loss_mask: 0.2318  loss_dice: 0.2205  loss_ce_0: 0.0719  loss_mask_0: 0.2302  loss_dice_0: 0.2283  loss_ce_1: 0.14  loss_mask_1: 0.2126  loss_dice_1: 0.2183  loss_ce_2: 0.1402  loss_mask_2: 0.2336  loss_dice_2: 0.2235  loss_ce_3: 0.14  loss_mask_3: 0.2289  loss_dice_3: 0.2203  loss_ce_4: 0.1396  loss_mask_4: 0.2313  loss_dice_4: 0.217  loss_ce_5: 0.1403  loss_mask_5: 0.2248  loss_dice_5: 0.2177  loss_ce_6: 0.1398  loss_mask_6: 0.236  loss_dice_6: 0.2213  loss_ce_7: 0.1391  loss_mask_7: 0.2234  loss_dice_7: 0.2199  loss_ce_8: 0.1402  loss_mask_8: 0.2248  loss_dice_8: 0.23  time: 0.5182  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:44:54] d2.utils.events INFO:  eta: 4:07:18  iter: 3239  total_loss: 5.545  loss_ce: 0.115  loss_mask: 0.2215  loss_dice: 0.1875  loss_ce_0: 0.06974  loss_mask_0: 0.2199  loss_dice_0: 0.1977  loss_ce_1: 0.1113  loss_mask_1: 0.2247  loss_dice_1: 0.1878  loss_ce_2: 0.1127  loss_mask_2: 0.2207  loss_dice_2: 0.193  loss_ce_3: 0.1117  loss_mask_3: 0.2191  loss_dice_3: 0.1873  loss_ce_4: 0.1119  loss_mask_4: 0.2143  loss_dice_4: 0.1896  loss_ce_5: 0.1132  loss_mask_5: 0.2246  loss_dice_5: 0.1907  loss_ce_6: 0.1132  loss_mask_6: 0.2226  loss_dice_6: 0.1944  loss_ce_7: 0.1129  loss_mask_7: 0.2222  loss_dice_7: 0.1861  loss_ce_8: 0.1141  loss_mask_8: 0.2234  loss_dice_8: 0.1906  time: 0.5185  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:45:05] d2.utils.events INFO:  eta: 4:07:12  iter: 3259  total_loss: 5.76  loss_ce: 0.09697  loss_mask: 0.2339  loss_dice: 0.1893  loss_ce_0: 0.06626  loss_mask_0: 0.2375  loss_dice_0: 0.1941  loss_ce_1: 0.09582  loss_mask_1: 0.237  loss_dice_1: 0.1904  loss_ce_2: 0.09651  loss_mask_2: 0.2342  loss_dice_2: 0.1913  loss_ce_3: 0.09621  loss_mask_3: 0.2345  loss_dice_3: 0.1975  loss_ce_4: 0.09599  loss_mask_4: 0.2321  loss_dice_4: 0.1908  loss_ce_5: 0.09615  loss_mask_5: 0.2391  loss_dice_5: 0.1928  loss_ce_6: 0.09594  loss_mask_6: 0.2335  loss_dice_6: 0.1943  loss_ce_7: 0.09594  loss_mask_7: 0.2391  loss_dice_7: 0.1954  loss_ce_8: 0.09599  loss_mask_8: 0.2309  loss_dice_8: 0.1904  time: 0.5187  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:45:16] d2.utils.events INFO:  eta: 4:07:09  iter: 3279  total_loss: 5.112  loss_ce: 0.1311  loss_mask: 0.1538  loss_dice: 0.1795  loss_ce_0: 0.07524  loss_mask_0: 0.1525  loss_dice_0: 0.184  loss_ce_1: 0.1275  loss_mask_1: 0.1542  loss_dice_1: 0.183  loss_ce_2: 0.1286  loss_mask_2: 0.1595  loss_dice_2: 0.1768  loss_ce_3: 0.1275  loss_mask_3: 0.158  loss_dice_3: 0.186  loss_ce_4: 0.1281  loss_mask_4: 0.1571  loss_dice_4: 0.1831  loss_ce_5: 0.129  loss_mask_5: 0.1602  loss_dice_5: 0.1801  loss_ce_6: 0.1293  loss_mask_6: 0.1507  loss_dice_6: 0.1834  loss_ce_7: 0.13  loss_mask_7: 0.1575  loss_dice_7: 0.1801  loss_ce_8: 0.1308  loss_mask_8: 0.1572  loss_dice_8: 0.185  time: 0.5189  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:45:27] d2.utils.events INFO:  eta: 4:06:58  iter: 3299  total_loss: 5.005  loss_ce: 0.13  loss_mask: 0.2072  loss_dice: 0.1733  loss_ce_0: 0.07109  loss_mask_0: 0.2118  loss_dice_0: 0.1746  loss_ce_1: 0.1301  loss_mask_1: 0.2148  loss_dice_1: 0.1788  loss_ce_2: 0.1301  loss_mask_2: 0.2034  loss_dice_2: 0.177  loss_ce_3: 0.1304  loss_mask_3: 0.2091  loss_dice_3: 0.1821  loss_ce_4: 0.1302  loss_mask_4: 0.2113  loss_dice_4: 0.174  loss_ce_5: 0.1302  loss_mask_5: 0.2192  loss_dice_5: 0.1836  loss_ce_6: 0.1306  loss_mask_6: 0.2068  loss_dice_6: 0.1738  loss_ce_7: 0.1301  loss_mask_7: 0.2071  loss_dice_7: 0.1657  loss_ce_8: 0.1299  loss_mask_8: 0.2131  loss_dice_8: 0.1786  time: 0.5191  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:45:38] d2.utils.events INFO:  eta: 4:06:47  iter: 3319  total_loss: 5.349  loss_ce: 0.138  loss_mask: 0.1645  loss_dice: 0.1792  loss_ce_0: 0.07102  loss_mask_0: 0.1703  loss_dice_0: 0.1766  loss_ce_1: 0.1372  loss_mask_1: 0.1738  loss_dice_1: 0.1777  loss_ce_2: 0.1368  loss_mask_2: 0.1609  loss_dice_2: 0.1858  loss_ce_3: 0.1367  loss_mask_3: 0.1759  loss_dice_3: 0.1665  loss_ce_4: 0.1372  loss_mask_4: 0.161  loss_dice_4: 0.1781  loss_ce_5: 0.1374  loss_mask_5: 0.1703  loss_dice_5: 0.1724  loss_ce_6: 0.1355  loss_mask_6: 0.1737  loss_dice_6: 0.1764  loss_ce_7: 0.1376  loss_mask_7: 0.171  loss_dice_7: 0.1849  loss_ce_8: 0.1386  loss_mask_8: 0.1708  loss_dice_8: 0.1703  time: 0.5193  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:45:49] d2.utils.events INFO:  eta: 4:06:35  iter: 3339  total_loss: 4.882  loss_ce: 0.1302  loss_mask: 0.1951  loss_dice: 0.1424  loss_ce_0: 0.07043  loss_mask_0: 0.1905  loss_dice_0: 0.1411  loss_ce_1: 0.128  loss_mask_1: 0.1872  loss_dice_1: 0.1429  loss_ce_2: 0.1281  loss_mask_2: 0.1834  loss_dice_2: 0.1446  loss_ce_3: 0.1279  loss_mask_3: 0.1841  loss_dice_3: 0.1374  loss_ce_4: 0.1283  loss_mask_4: 0.1887  loss_dice_4: 0.136  loss_ce_5: 0.1287  loss_mask_5: 0.1956  loss_dice_5: 0.142  loss_ce_6: 0.1265  loss_mask_6: 0.1991  loss_dice_6: 0.1459  loss_ce_7: 0.1292  loss_mask_7: 0.1861  loss_dice_7: 0.1426  loss_ce_8: 0.1303  loss_mask_8: 0.1871  loss_dice_8: 0.1394  time: 0.5195  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:46:00] d2.utils.events INFO:  eta: 4:06:25  iter: 3359  total_loss: 4.801  loss_ce: 0.1087  loss_mask: 0.1566  loss_dice: 0.1653  loss_ce_0: 0.068  loss_mask_0: 0.1602  loss_dice_0: 0.1683  loss_ce_1: 0.1066  loss_mask_1: 0.1517  loss_dice_1: 0.1706  loss_ce_2: 0.1073  loss_mask_2: 0.1526  loss_dice_2: 0.1632  loss_ce_3: 0.1068  loss_mask_3: 0.1614  loss_dice_3: 0.1726  loss_ce_4: 0.1072  loss_mask_4: 0.1554  loss_dice_4: 0.1652  loss_ce_5: 0.1075  loss_mask_5: 0.1603  loss_dice_5: 0.1751  loss_ce_6: 0.1065  loss_mask_6: 0.1569  loss_dice_6: 0.165  loss_ce_7: 0.1077  loss_mask_7: 0.1532  loss_dice_7: 0.1654  loss_ce_8: 0.108  loss_mask_8: 0.1578  loss_dice_8: 0.1754  time: 0.5197  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:46:12] d2.utils.events INFO:  eta: 4:06:13  iter: 3379  total_loss: 5.686  loss_ce: 0.1273  loss_mask: 0.2213  loss_dice: 0.1947  loss_ce_0: 0.07066  loss_mask_0: 0.2159  loss_dice_0: 0.1909  loss_ce_1: 0.1279  loss_mask_1: 0.2075  loss_dice_1: 0.1961  loss_ce_2: 0.1274  loss_mask_2: 0.2133  loss_dice_2: 0.1953  loss_ce_3: 0.1276  loss_mask_3: 0.2224  loss_dice_3: 0.2019  loss_ce_4: 0.1274  loss_mask_4: 0.2075  loss_dice_4: 0.1933  loss_ce_5: 0.1271  loss_mask_5: 0.2183  loss_dice_5: 0.2025  loss_ce_6: 0.128  loss_mask_6: 0.2236  loss_dice_6: 0.1901  loss_ce_7: 0.1272  loss_mask_7: 0.2163  loss_dice_7: 0.1953  loss_ce_8: 0.1271  loss_mask_8: 0.2176  loss_dice_8: 0.1957  time: 0.5200  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:46:23] d2.utils.events INFO:  eta: 4:06:05  iter: 3399  total_loss: 5.954  loss_ce: 0.1264  loss_mask: 0.2038  loss_dice: 0.2322  loss_ce_0: 0.06818  loss_mask_0: 0.2215  loss_dice_0: 0.2469  loss_ce_1: 0.1272  loss_mask_1: 0.2003  loss_dice_1: 0.2199  loss_ce_2: 0.1273  loss_mask_2: 0.2044  loss_dice_2: 0.2383  loss_ce_3: 0.1273  loss_mask_3: 0.2137  loss_dice_3: 0.2327  loss_ce_4: 0.1274  loss_mask_4: 0.2108  loss_dice_4: 0.2321  loss_ce_5: 0.1272  loss_mask_5: 0.2083  loss_dice_5: 0.235  loss_ce_6: 0.1272  loss_mask_6: 0.2128  loss_dice_6: 0.2439  loss_ce_7: 0.1272  loss_mask_7: 0.2096  loss_dice_7: 0.237  loss_ce_8: 0.1269  loss_mask_8: 0.1974  loss_dice_8: 0.228  time: 0.5202  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:46:34] d2.utils.events INFO:  eta: 4:06:01  iter: 3419  total_loss: 4.872  loss_ce: 0.1315  loss_mask: 0.2259  loss_dice: 0.1368  loss_ce_0: 0.07296  loss_mask_0: 0.216  loss_dice_0: 0.1358  loss_ce_1: 0.1313  loss_mask_1: 0.2192  loss_dice_1: 0.1363  loss_ce_2: 0.1314  loss_mask_2: 0.2284  loss_dice_2: 0.1439  loss_ce_3: 0.1314  loss_mask_3: 0.2377  loss_dice_3: 0.1414  loss_ce_4: 0.131  loss_mask_4: 0.2268  loss_dice_4: 0.146  loss_ce_5: 0.1311  loss_mask_5: 0.2248  loss_dice_5: 0.138  loss_ce_6: 0.131  loss_mask_6: 0.228  loss_dice_6: 0.1322  loss_ce_7: 0.1313  loss_mask_7: 0.2302  loss_dice_7: 0.142  loss_ce_8: 0.1314  loss_mask_8: 0.2209  loss_dice_8: 0.1402  time: 0.5205  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:46:47] d2.utils.events INFO:  eta: 4:06:08  iter: 3439  total_loss: 5.207  loss_ce: 0.1377  loss_mask: 0.1699  loss_dice: 0.1755  loss_ce_0: 0.07035  loss_mask_0: 0.1785  loss_dice_0: 0.1738  loss_ce_1: 0.1405  loss_mask_1: 0.1762  loss_dice_1: 0.1774  loss_ce_2: 0.1399  loss_mask_2: 0.1698  loss_dice_2: 0.1813  loss_ce_3: 0.1401  loss_mask_3: 0.1762  loss_dice_3: 0.1868  loss_ce_4: 0.1401  loss_mask_4: 0.1761  loss_dice_4: 0.1798  loss_ce_5: 0.14  loss_mask_5: 0.1881  loss_dice_5: 0.1725  loss_ce_6: 0.1402  loss_mask_6: 0.1757  loss_dice_6: 0.1808  loss_ce_7: 0.139  loss_mask_7: 0.1852  loss_dice_7: 0.18  loss_ce_8: 0.1385  loss_mask_8: 0.1789  loss_dice_8: 0.1796  time: 0.5211  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:46:59] d2.utils.events INFO:  eta: 4:06:13  iter: 3459  total_loss: 4.981  loss_ce: 0.1224  loss_mask: 0.2211  loss_dice: 0.1584  loss_ce_0: 0.07025  loss_mask_0: 0.2203  loss_dice_0: 0.156  loss_ce_1: 0.122  loss_mask_1: 0.2221  loss_dice_1: 0.1579  loss_ce_2: 0.1221  loss_mask_2: 0.2287  loss_dice_2: 0.16  loss_ce_3: 0.1222  loss_mask_3: 0.2224  loss_dice_3: 0.1534  loss_ce_4: 0.122  loss_mask_4: 0.229  loss_dice_4: 0.1572  loss_ce_5: 0.122  loss_mask_5: 0.2267  loss_dice_5: 0.1591  loss_ce_6: 0.122  loss_mask_6: 0.2144  loss_dice_6: 0.1516  loss_ce_7: 0.1221  loss_mask_7: 0.2172  loss_dice_7: 0.1603  loss_ce_8: 0.1219  loss_mask_8: 0.2223  loss_dice_8: 0.1577  time: 0.5218  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:47:12] d2.utils.events INFO:  eta: 4:06:15  iter: 3479  total_loss: 5.795  loss_ce: 0.1349  loss_mask: 0.1946  loss_dice: 0.2202  loss_ce_0: 0.07105  loss_mask_0: 0.1875  loss_dice_0: 0.2335  loss_ce_1: 0.1335  loss_mask_1: 0.1926  loss_dice_1: 0.2189  loss_ce_2: 0.1338  loss_mask_2: 0.1962  loss_dice_2: 0.2265  loss_ce_3: 0.1335  loss_mask_3: 0.1992  loss_dice_3: 0.2276  loss_ce_4: 0.134  loss_mask_4: 0.2034  loss_dice_4: 0.2321  loss_ce_5: 0.1342  loss_mask_5: 0.1951  loss_dice_5: 0.2244  loss_ce_6: 0.1341  loss_mask_6: 0.1965  loss_dice_6: 0.2193  loss_ce_7: 0.1343  loss_mask_7: 0.1971  loss_dice_7: 0.2249  loss_ce_8: 0.1349  loss_mask_8: 0.1896  loss_dice_8: 0.2237  time: 0.5224  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:47:25] d2.utils.events INFO:  eta: 4:06:31  iter: 3499  total_loss: 5.617  loss_ce: 0.1223  loss_mask: 0.2195  loss_dice: 0.2263  loss_ce_0: 0.06993  loss_mask_0: 0.201  loss_dice_0: 0.2186  loss_ce_1: 0.1192  loss_mask_1: 0.2143  loss_dice_1: 0.2144  loss_ce_2: 0.1199  loss_mask_2: 0.2147  loss_dice_2: 0.2206  loss_ce_3: 0.1194  loss_mask_3: 0.2126  loss_dice_3: 0.2189  loss_ce_4: 0.12  loss_mask_4: 0.2102  loss_dice_4: 0.2157  loss_ce_5: 0.1203  loss_mask_5: 0.2164  loss_dice_5: 0.2267  loss_ce_6: 0.1203  loss_mask_6: 0.2154  loss_dice_6: 0.2104  loss_ce_7: 0.121  loss_mask_7: 0.2191  loss_dice_7: 0.2201  loss_ce_8: 0.1217  loss_mask_8: 0.2197  loss_dice_8: 0.2226  time: 0.5230  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:47:37] d2.utils.events INFO:  eta: 4:06:53  iter: 3519  total_loss: 5.315  loss_ce: 0.1319  loss_mask: 0.2325  loss_dice: 0.169  loss_ce_0: 0.07082  loss_mask_0: 0.2267  loss_dice_0: 0.1702  loss_ce_1: 0.1306  loss_mask_1: 0.226  loss_dice_1: 0.1731  loss_ce_2: 0.1305  loss_mask_2: 0.2415  loss_dice_2: 0.171  loss_ce_3: 0.1305  loss_mask_3: 0.2411  loss_dice_3: 0.1692  loss_ce_4: 0.1313  loss_mask_4: 0.2362  loss_dice_4: 0.1723  loss_ce_5: 0.1312  loss_mask_5: 0.2347  loss_dice_5: 0.1742  loss_ce_6: 0.1317  loss_mask_6: 0.2351  loss_dice_6: 0.1734  loss_ce_7: 0.1315  loss_mask_7: 0.2411  loss_dice_7: 0.171  loss_ce_8: 0.132  loss_mask_8: 0.2348  loss_dice_8: 0.1709  time: 0.5237  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:47:50] d2.utils.events INFO:  eta: 4:07:04  iter: 3539  total_loss: 5.266  loss_ce: 0.127  loss_mask: 0.2406  loss_dice: 0.165  loss_ce_0: 0.0694  loss_mask_0: 0.253  loss_dice_0: 0.1654  loss_ce_1: 0.1282  loss_mask_1: 0.2352  loss_dice_1: 0.1519  loss_ce_2: 0.1277  loss_mask_2: 0.2546  loss_dice_2: 0.1583  loss_ce_3: 0.1277  loss_mask_3: 0.247  loss_dice_3: 0.1574  loss_ce_4: 0.1276  loss_mask_4: 0.2561  loss_dice_4: 0.1602  loss_ce_5: 0.1276  loss_mask_5: 0.2518  loss_dice_5: 0.1549  loss_ce_6: 0.1277  loss_mask_6: 0.2533  loss_dice_6: 0.1534  loss_ce_7: 0.1277  loss_mask_7: 0.2547  loss_dice_7: 0.1597  loss_ce_8: 0.1273  loss_mask_8: 0.2435  loss_dice_8: 0.1538  time: 0.5243  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:48:03] d2.utils.events INFO:  eta: 4:07:14  iter: 3559  total_loss: 5.617  loss_ce: 0.1171  loss_mask: 0.204  loss_dice: 0.1695  loss_ce_0: 0.068  loss_mask_0: 0.2087  loss_dice_0: 0.1792  loss_ce_1: 0.1162  loss_mask_1: 0.2121  loss_dice_1: 0.1659  loss_ce_2: 0.1161  loss_mask_2: 0.1928  loss_dice_2: 0.1731  loss_ce_3: 0.1163  loss_mask_3: 0.2083  loss_dice_3: 0.1654  loss_ce_4: 0.1166  loss_mask_4: 0.2067  loss_dice_4: 0.1749  loss_ce_5: 0.1162  loss_mask_5: 0.2123  loss_dice_5: 0.1649  loss_ce_6: 0.1165  loss_mask_6: 0.2015  loss_dice_6: 0.1674  loss_ce_7: 0.1169  loss_mask_7: 0.2145  loss_dice_7: 0.1733  loss_ce_8: 0.1162  loss_mask_8: 0.2037  loss_dice_8: 0.1679  time: 0.5249  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:48:15] d2.utils.events INFO:  eta: 4:07:30  iter: 3579  total_loss: 5.369  loss_ce: 0.09052  loss_mask: 0.2163  loss_dice: 0.1852  loss_ce_0: 0.06332  loss_mask_0: 0.2088  loss_dice_0: 0.1879  loss_ce_1: 0.08811  loss_mask_1: 0.2169  loss_dice_1: 0.1835  loss_ce_2: 0.08898  loss_mask_2: 0.2069  loss_dice_2: 0.1847  loss_ce_3: 0.08835  loss_mask_3: 0.2174  loss_dice_3: 0.1965  loss_ce_4: 0.08877  loss_mask_4: 0.2119  loss_dice_4: 0.1814  loss_ce_5: 0.08919  loss_mask_5: 0.2119  loss_dice_5: 0.1838  loss_ce_6: 0.08875  loss_mask_6: 0.2091  loss_dice_6: 0.1839  loss_ce_7: 0.08947  loss_mask_7: 0.212  loss_dice_7: 0.1896  loss_ce_8: 0.08966  loss_mask_8: 0.216  loss_dice_8: 0.1823  time: 0.5255  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:48:28] d2.utils.events INFO:  eta: 4:07:44  iter: 3599  total_loss: 4.991  loss_ce: 0.06967  loss_mask: 0.2249  loss_dice: 0.1473  loss_ce_0: 0.05899  loss_mask_0: 0.2118  loss_dice_0: 0.1349  loss_ce_1: 0.06706  loss_mask_1: 0.2119  loss_dice_1: 0.1416  loss_ce_2: 0.06781  loss_mask_2: 0.211  loss_dice_2: 0.1412  loss_ce_3: 0.06681  loss_mask_3: 0.2118  loss_dice_3: 0.1406  loss_ce_4: 0.06756  loss_mask_4: 0.2134  loss_dice_4: 0.1394  loss_ce_5: 0.0679  loss_mask_5: 0.2185  loss_dice_5: 0.1382  loss_ce_6: 0.06732  loss_mask_6: 0.2185  loss_dice_6: 0.1391  loss_ce_7: 0.06857  loss_mask_7: 0.2101  loss_dice_7: 0.1362  loss_ce_8: 0.06849  loss_mask_8: 0.2139  loss_dice_8: 0.1376  time: 0.5261  data_time: 0.0016  lr: 0.0001  max_mem: 1568M
[07/11 11:48:41] d2.utils.events INFO:  eta: 4:08:22  iter: 3619  total_loss: 5.369  loss_ce: 0.1784  loss_mask: 0.2259  loss_dice: 0.1544  loss_ce_0: 0.08446  loss_mask_0: 0.2382  loss_dice_0: 0.1531  loss_ce_1: 0.1746  loss_mask_1: 0.2253  loss_dice_1: 0.1527  loss_ce_2: 0.1748  loss_mask_2: 0.2192  loss_dice_2: 0.1562  loss_ce_3: 0.174  loss_mask_3: 0.2353  loss_dice_3: 0.1565  loss_ce_4: 0.1753  loss_mask_4: 0.2308  loss_dice_4: 0.1533  loss_ce_5: 0.176  loss_mask_5: 0.2207  loss_dice_5: 0.1514  loss_ce_6: 0.1759  loss_mask_6: 0.2167  loss_dice_6: 0.1576  loss_ce_7: 0.1776  loss_mask_7: 0.2262  loss_dice_7: 0.1546  loss_ce_8: 0.1783  loss_mask_8: 0.2224  loss_dice_8: 0.1573  time: 0.5267  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:48:54] d2.utils.events INFO:  eta: 4:08:51  iter: 3639  total_loss: 5.506  loss_ce: 0.1038  loss_mask: 0.2073  loss_dice: 0.1718  loss_ce_0: 0.0583  loss_mask_0: 0.2087  loss_dice_0: 0.1613  loss_ce_1: 0.1076  loss_mask_1: 0.1958  loss_dice_1: 0.1732  loss_ce_2: 0.1071  loss_mask_2: 0.214  loss_dice_2: 0.168  loss_ce_3: 0.1082  loss_mask_3: 0.2121  loss_dice_3: 0.1739  loss_ce_4: 0.1073  loss_mask_4: 0.222  loss_dice_4: 0.1695  loss_ce_5: 0.1066  loss_mask_5: 0.2072  loss_dice_5: 0.1628  loss_ce_6: 0.107  loss_mask_6: 0.2096  loss_dice_6: 0.1679  loss_ce_7: 0.1053  loss_mask_7: 0.2216  loss_dice_7: 0.1675  loss_ce_8: 0.1048  loss_mask_8: 0.2112  loss_dice_8: 0.1711  time: 0.5273  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:49:06] d2.utils.events INFO:  eta: 4:09:17  iter: 3659  total_loss: 5.23  loss_ce: 0.1094  loss_mask: 0.1771  loss_dice: 0.2318  loss_ce_0: 0.05899  loss_mask_0: 0.1632  loss_dice_0: 0.2293  loss_ce_1: 0.1116  loss_mask_1: 0.1526  loss_dice_1: 0.2289  loss_ce_2: 0.1109  loss_mask_2: 0.16  loss_dice_2: 0.2235  loss_ce_3: 0.1121  loss_mask_3: 0.1766  loss_dice_3: 0.226  loss_ce_4: 0.1115  loss_mask_4: 0.1636  loss_dice_4: 0.2292  loss_ce_5: 0.1111  loss_mask_5: 0.164  loss_dice_5: 0.2352  loss_ce_6: 0.1116  loss_mask_6: 0.1667  loss_dice_6: 0.2324  loss_ce_7: 0.1103  loss_mask_7: 0.1664  loss_dice_7: 0.2279  loss_ce_8: 0.1102  loss_mask_8: 0.1656  loss_dice_8: 0.2364  time: 0.5279  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:49:19] d2.utils.events INFO:  eta: 4:10:01  iter: 3679  total_loss: 4.874  loss_ce: 0.1392  loss_mask: 0.2105  loss_dice: 0.1548  loss_ce_0: 0.07952  loss_mask_0: 0.2033  loss_dice_0: 0.1478  loss_ce_1: 0.1372  loss_mask_1: 0.2154  loss_dice_1: 0.151  loss_ce_2: 0.138  loss_mask_2: 0.2088  loss_dice_2: 0.1526  loss_ce_3: 0.137  loss_mask_3: 0.2047  loss_dice_3: 0.1526  loss_ce_4: 0.1374  loss_mask_4: 0.2139  loss_dice_4: 0.152  loss_ce_5: 0.1376  loss_mask_5: 0.1985  loss_dice_5: 0.1479  loss_ce_6: 0.1375  loss_mask_6: 0.2163  loss_dice_6: 0.1454  loss_ce_7: 0.1383  loss_mask_7: 0.222  loss_dice_7: 0.1514  loss_ce_8: 0.1385  loss_mask_8: 0.2073  loss_dice_8: 0.1494  time: 0.5285  data_time: 0.0016  lr: 0.0001  max_mem: 1568M
[07/11 11:49:32] d2.utils.events INFO:  eta: 4:10:19  iter: 3699  total_loss: 5.419  loss_ce: 0.1268  loss_mask: 0.1963  loss_dice: 0.1845  loss_ce_0: 0.06937  loss_mask_0: 0.1829  loss_dice_0: 0.1856  loss_ce_1: 0.1264  loss_mask_1: 0.1809  loss_dice_1: 0.1934  loss_ce_2: 0.1263  loss_mask_2: 0.1928  loss_dice_2: 0.1877  loss_ce_3: 0.1265  loss_mask_3: 0.1962  loss_dice_3: 0.1883  loss_ce_4: 0.1263  loss_mask_4: 0.1813  loss_dice_4: 0.1914  loss_ce_5: 0.1263  loss_mask_5: 0.1842  loss_dice_5: 0.1833  loss_ce_6: 0.1262  loss_mask_6: 0.1866  loss_dice_6: 0.1918  loss_ce_7: 0.1265  loss_mask_7: 0.1969  loss_dice_7: 0.192  loss_ce_8: 0.127  loss_mask_8: 0.2004  loss_dice_8: 0.189  time: 0.5290  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:49:44] d2.utils.events INFO:  eta: 4:10:52  iter: 3719  total_loss: 4.932  loss_ce: 0.1276  loss_mask: 0.1692  loss_dice: 0.1686  loss_ce_0: 0.06924  loss_mask_0: 0.1782  loss_dice_0: 0.1789  loss_ce_1: 0.1278  loss_mask_1: 0.1748  loss_dice_1: 0.1776  loss_ce_2: 0.1277  loss_mask_2: 0.1666  loss_dice_2: 0.1711  loss_ce_3: 0.1279  loss_mask_3: 0.1649  loss_dice_3: 0.1704  loss_ce_4: 0.1278  loss_mask_4: 0.1746  loss_dice_4: 0.1731  loss_ce_5: 0.1277  loss_mask_5: 0.1686  loss_dice_5: 0.1701  loss_ce_6: 0.1278  loss_mask_6: 0.1783  loss_dice_6: 0.1669  loss_ce_7: 0.1276  loss_mask_7: 0.1689  loss_dice_7: 0.1697  loss_ce_8: 0.1276  loss_mask_8: 0.1781  loss_dice_8: 0.17  time: 0.5296  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:49:57] d2.utils.events INFO:  eta: 4:11:03  iter: 3739  total_loss: 5.075  loss_ce: 0.1066  loss_mask: 0.1745  loss_dice: 0.1772  loss_ce_0: 0.06914  loss_mask_0: 0.1678  loss_dice_0: 0.1763  loss_ce_1: 0.1046  loss_mask_1: 0.1709  loss_dice_1: 0.1813  loss_ce_2: 0.1056  loss_mask_2: 0.1643  loss_dice_2: 0.1749  loss_ce_3: 0.1044  loss_mask_3: 0.1713  loss_dice_3: 0.1812  loss_ce_4: 0.1046  loss_mask_4: 0.1745  loss_dice_4: 0.1805  loss_ce_5: 0.1048  loss_mask_5: 0.1649  loss_dice_5: 0.1831  loss_ce_6: 0.1049  loss_mask_6: 0.1702  loss_dice_6: 0.1793  loss_ce_7: 0.1059  loss_mask_7: 0.1691  loss_dice_7: 0.1814  loss_ce_8: 0.1062  loss_mask_8: 0.1713  loss_dice_8: 0.1767  time: 0.5302  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:50:10] d2.utils.events INFO:  eta: 4:12:05  iter: 3759  total_loss: 4.649  loss_ce: 0.1404  loss_mask: 0.171  loss_dice: 0.1403  loss_ce_0: 0.06976  loss_mask_0: 0.1785  loss_dice_0: 0.1489  loss_ce_1: 0.1394  loss_mask_1: 0.1688  loss_dice_1: 0.1396  loss_ce_2: 0.1392  loss_mask_2: 0.1848  loss_dice_2: 0.1453  loss_ce_3: 0.1383  loss_mask_3: 0.1782  loss_dice_3: 0.1452  loss_ce_4: 0.1388  loss_mask_4: 0.1815  loss_dice_4: 0.1432  loss_ce_5: 0.139  loss_mask_5: 0.1822  loss_dice_5: 0.1533  loss_ce_6: 0.1394  loss_mask_6: 0.1785  loss_dice_6: 0.1453  loss_ce_7: 0.1403  loss_mask_7: 0.1798  loss_dice_7: 0.1424  loss_ce_8: 0.1413  loss_mask_8: 0.1872  loss_dice_8: 0.1432  time: 0.5307  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:50:22] d2.utils.events INFO:  eta: 4:12:52  iter: 3779  total_loss: 5.242  loss_ce: 0.1242  loss_mask: 0.2457  loss_dice: 0.1864  loss_ce_0: 0.06887  loss_mask_0: 0.2351  loss_dice_0: 0.1823  loss_ce_1: 0.1214  loss_mask_1: 0.2423  loss_dice_1: 0.1818  loss_ce_2: 0.1219  loss_mask_2: 0.2279  loss_dice_2: 0.1865  loss_ce_3: 0.1201  loss_mask_3: 0.2326  loss_dice_3: 0.1785  loss_ce_4: 0.121  loss_mask_4: 0.2429  loss_dice_4: 0.1908  loss_ce_5: 0.1212  loss_mask_5: 0.2526  loss_dice_5: 0.1852  loss_ce_6: 0.1218  loss_mask_6: 0.236  loss_dice_6: 0.1823  loss_ce_7: 0.1232  loss_mask_7: 0.2438  loss_dice_7: 0.1843  loss_ce_8: 0.1245  loss_mask_8: 0.2286  loss_dice_8: 0.1869  time: 0.5313  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:50:35] d2.utils.events INFO:  eta: 4:13:22  iter: 3799  total_loss: 5.123  loss_ce: 0.08097  loss_mask: 0.2367  loss_dice: 0.1543  loss_ce_0: 0.06323  loss_mask_0: 0.2345  loss_dice_0: 0.1518  loss_ce_1: 0.07769  loss_mask_1: 0.235  loss_dice_1: 0.1543  loss_ce_2: 0.07877  loss_mask_2: 0.2372  loss_dice_2: 0.1468  loss_ce_3: 0.07659  loss_mask_3: 0.2315  loss_dice_3: 0.1484  loss_ce_4: 0.0778  loss_mask_4: 0.2386  loss_dice_4: 0.148  loss_ce_5: 0.07762  loss_mask_5: 0.236  loss_dice_5: 0.1491  loss_ce_6: 0.07835  loss_mask_6: 0.2332  loss_dice_6: 0.1536  loss_ce_7: 0.08026  loss_mask_7: 0.2379  loss_dice_7: 0.1552  loss_ce_8: 0.08094  loss_mask_8: 0.2417  loss_dice_8: 0.1532  time: 0.5318  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:50:48] d2.utils.events INFO:  eta: 4:14:11  iter: 3819  total_loss: 4.902  loss_ce: 0.06636  loss_mask: 0.1802  loss_dice: 0.1349  loss_ce_0: 0.05924  loss_mask_0: 0.1856  loss_dice_0: 0.1375  loss_ce_1: 0.06641  loss_mask_1: 0.178  loss_dice_1: 0.1392  loss_ce_2: 0.06655  loss_mask_2: 0.1858  loss_dice_2: 0.1381  loss_ce_3: 0.06477  loss_mask_3: 0.1856  loss_dice_3: 0.1348  loss_ce_4: 0.06571  loss_mask_4: 0.1797  loss_dice_4: 0.143  loss_ce_5: 0.06481  loss_mask_5: 0.1866  loss_dice_5: 0.1412  loss_ce_6: 0.06536  loss_mask_6: 0.1849  loss_dice_6: 0.1387  loss_ce_7: 0.06728  loss_mask_7: 0.1805  loss_dice_7: 0.1404  loss_ce_8: 0.06726  loss_mask_8: 0.1795  loss_dice_8: 0.1366  time: 0.5323  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:51:01] d2.utils.events INFO:  eta: 4:14:42  iter: 3839  total_loss: 4.93  loss_ce: 0.137  loss_mask: 0.1753  loss_dice: 0.1552  loss_ce_0: 0.07008  loss_mask_0: 0.1781  loss_dice_0: 0.1541  loss_ce_1: 0.1382  loss_mask_1: 0.1818  loss_dice_1: 0.1564  loss_ce_2: 0.1375  loss_mask_2: 0.1771  loss_dice_2: 0.1567  loss_ce_3: 0.1368  loss_mask_3: 0.176  loss_dice_3: 0.1611  loss_ce_4: 0.1374  loss_mask_4: 0.1731  loss_dice_4: 0.1523  loss_ce_5: 0.1365  loss_mask_5: 0.1717  loss_dice_5: 0.158  loss_ce_6: 0.1373  loss_mask_6: 0.1723  loss_dice_6: 0.1596  loss_ce_7: 0.1384  loss_mask_7: 0.1783  loss_dice_7: 0.1671  loss_ce_8: 0.1383  loss_mask_8: 0.1784  loss_dice_8: 0.1567  time: 0.5329  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:51:13] d2.utils.events INFO:  eta: 4:16:05  iter: 3859  total_loss: 5.802  loss_ce: 0.09375  loss_mask: 0.2069  loss_dice: 0.212  loss_ce_0: 0.05385  loss_mask_0: 0.213  loss_dice_0: 0.2065  loss_ce_1: 0.09375  loss_mask_1: 0.2172  loss_dice_1: 0.2103  loss_ce_2: 0.09491  loss_mask_2: 0.2119  loss_dice_2: 0.2043  loss_ce_3: 0.09658  loss_mask_3: 0.2174  loss_dice_3: 0.2106  loss_ce_4: 0.09513  loss_mask_4: 0.2089  loss_dice_4: 0.2116  loss_ce_5: 0.09704  loss_mask_5: 0.2083  loss_dice_5: 0.2069  loss_ce_6: 0.09562  loss_mask_6: 0.2234  loss_dice_6: 0.2031  loss_ce_7: 0.0922  loss_mask_7: 0.2166  loss_dice_7: 0.2077  loss_ce_8: 0.0924  loss_mask_8: 0.22  loss_dice_8: 0.205  time: 0.5334  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:51:26] d2.utils.events INFO:  eta: 4:17:01  iter: 3879  total_loss: 5.56  loss_ce: 0.09946  loss_mask: 0.215  loss_dice: 0.2267  loss_ce_0: 0.05445  loss_mask_0: 0.2221  loss_dice_0: 0.2215  loss_ce_1: 0.0998  loss_mask_1: 0.2111  loss_dice_1: 0.23  loss_ce_2: 0.1006  loss_mask_2: 0.2179  loss_dice_2: 0.2239  loss_ce_3: 0.1021  loss_mask_3: 0.2094  loss_dice_3: 0.2192  loss_ce_4: 0.101  loss_mask_4: 0.2072  loss_dice_4: 0.2087  loss_ce_5: 0.1026  loss_mask_5: 0.2108  loss_dice_5: 0.2258  loss_ce_6: 0.1015  loss_mask_6: 0.2144  loss_dice_6: 0.229  loss_ce_7: 0.09837  loss_mask_7: 0.2149  loss_dice_7: 0.2192  loss_ce_8: 0.09868  loss_mask_8: 0.2176  loss_dice_8: 0.2203  time: 0.5339  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:51:39] d2.utils.events INFO:  eta: 4:17:21  iter: 3899  total_loss: 5.634  loss_ce: 0.1624  loss_mask: 0.1976  loss_dice: 0.2295  loss_ce_0: 0.08588  loss_mask_0: 0.1982  loss_dice_0: 0.2152  loss_ce_1: 0.1615  loss_mask_1: 0.1972  loss_dice_1: 0.2222  loss_ce_2: 0.1614  loss_mask_2: 0.1959  loss_dice_2: 0.214  loss_ce_3: 0.1607  loss_mask_3: 0.2012  loss_dice_3: 0.2294  loss_ce_4: 0.1613  loss_mask_4: 0.201  loss_dice_4: 0.2169  loss_ce_5: 0.1611  loss_mask_5: 0.1837  loss_dice_5: 0.2228  loss_ce_6: 0.1617  loss_mask_6: 0.1836  loss_dice_6: 0.232  loss_ce_7: 0.1624  loss_mask_7: 0.1901  loss_dice_7: 0.2218  loss_ce_8: 0.1624  loss_mask_8: 0.1903  loss_dice_8: 0.2179  time: 0.5344  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:51:51] d2.utils.events INFO:  eta: 4:18:07  iter: 3919  total_loss: 5.828  loss_ce: 0.1281  loss_mask: 0.2181  loss_dice: 0.1896  loss_ce_0: 0.06922  loss_mask_0: 0.2214  loss_dice_0: 0.1924  loss_ce_1: 0.1269  loss_mask_1: 0.2234  loss_dice_1: 0.1851  loss_ce_2: 0.127  loss_mask_2: 0.2319  loss_dice_2: 0.1961  loss_ce_3: 0.1276  loss_mask_3: 0.2219  loss_dice_3: 0.1958  loss_ce_4: 0.1269  loss_mask_4: 0.2197  loss_dice_4: 0.1908  loss_ce_5: 0.1274  loss_mask_5: 0.2361  loss_dice_5: 0.1977  loss_ce_6: 0.1273  loss_mask_6: 0.2161  loss_dice_6: 0.1922  loss_ce_7: 0.1267  loss_mask_7: 0.2239  loss_dice_7: 0.1886  loss_ce_8: 0.1266  loss_mask_8: 0.2218  loss_dice_8: 0.1917  time: 0.5349  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:52:04] d2.utils.events INFO:  eta: 4:18:42  iter: 3939  total_loss: 5.421  loss_ce: 0.1328  loss_mask: 0.2146  loss_dice: 0.2156  loss_ce_0: 0.06143  loss_mask_0: 0.2063  loss_dice_0: 0.2173  loss_ce_1: 0.1347  loss_mask_1: 0.2235  loss_dice_1: 0.2102  loss_ce_2: 0.1339  loss_mask_2: 0.2153  loss_dice_2: 0.2054  loss_ce_3: 0.1345  loss_mask_3: 0.2135  loss_dice_3: 0.2173  loss_ce_4: 0.1346  loss_mask_4: 0.2167  loss_dice_4: 0.2186  loss_ce_5: 0.1349  loss_mask_5: 0.2308  loss_dice_5: 0.2058  loss_ce_6: 0.1343  loss_mask_6: 0.217  loss_dice_6: 0.2057  loss_ce_7: 0.134  loss_mask_7: 0.2165  loss_dice_7: 0.2135  loss_ce_8: 0.1343  loss_mask_8: 0.2214  loss_dice_8: 0.2204  time: 0.5354  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:52:17] d2.utils.events INFO:  eta: 4:19:37  iter: 3959  total_loss: 5.152  loss_ce: 0.1291  loss_mask: 0.238  loss_dice: 0.1549  loss_ce_0: 0.07428  loss_mask_0: 0.2356  loss_dice_0: 0.1494  loss_ce_1: 0.1296  loss_mask_1: 0.2393  loss_dice_1: 0.1533  loss_ce_2: 0.1296  loss_mask_2: 0.2373  loss_dice_2: 0.1489  loss_ce_3: 0.1296  loss_mask_3: 0.2397  loss_dice_3: 0.1568  loss_ce_4: 0.1295  loss_mask_4: 0.2412  loss_dice_4: 0.1535  loss_ce_5: 0.1296  loss_mask_5: 0.2397  loss_dice_5: 0.1532  loss_ce_6: 0.1295  loss_mask_6: 0.2276  loss_dice_6: 0.1474  loss_ce_7: 0.1293  loss_mask_7: 0.232  loss_dice_7: 0.1592  loss_ce_8: 0.129  loss_mask_8: 0.2424  loss_dice_8: 0.1591  time: 0.5359  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:52:29] d2.utils.events INFO:  eta: 4:20:35  iter: 3979  total_loss: 5.193  loss_ce: 0.1388  loss_mask: 0.1814  loss_dice: 0.1932  loss_ce_0: 0.06643  loss_mask_0: 0.172  loss_dice_0: 0.1949  loss_ce_1: 0.1387  loss_mask_1: 0.1714  loss_dice_1: 0.1961  loss_ce_2: 0.1383  loss_mask_2: 0.1816  loss_dice_2: 0.1938  loss_ce_3: 0.1384  loss_mask_3: 0.1797  loss_dice_3: 0.1834  loss_ce_4: 0.1384  loss_mask_4: 0.1832  loss_dice_4: 0.1977  loss_ce_5: 0.1383  loss_mask_5: 0.1729  loss_dice_5: 0.1939  loss_ce_6: 0.1385  loss_mask_6: 0.1776  loss_dice_6: 0.1812  loss_ce_7: 0.1386  loss_mask_7: 0.1857  loss_dice_7: 0.1917  loss_ce_8: 0.1389  loss_mask_8: 0.1787  loss_dice_8: 0.1953  time: 0.5364  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:52:42] d2.utils.events INFO:  eta: 4:21:17  iter: 3999  total_loss: 5.284  loss_ce: 0.1208  loss_mask: 0.2197  loss_dice: 0.1746  loss_ce_0: 0.06494  loss_mask_0: 0.2183  loss_dice_0: 0.1817  loss_ce_1: 0.1174  loss_mask_1: 0.2194  loss_dice_1: 0.1724  loss_ce_2: 0.1172  loss_mask_2: 0.2235  loss_dice_2: 0.1798  loss_ce_3: 0.1167  loss_mask_3: 0.223  loss_dice_3: 0.1842  loss_ce_4: 0.1172  loss_mask_4: 0.2148  loss_dice_4: 0.1809  loss_ce_5: 0.117  loss_mask_5: 0.2091  loss_dice_5: 0.1761  loss_ce_6: 0.1184  loss_mask_6: 0.2327  loss_dice_6: 0.1792  loss_ce_7: 0.119  loss_mask_7: 0.2153  loss_dice_7: 0.177  loss_ce_8: 0.1197  loss_mask_8: 0.2162  loss_dice_8: 0.1722  time: 0.5369  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:52:55] d2.utils.events INFO:  eta: 4:22:25  iter: 4019  total_loss: 5.16  loss_ce: 0.09442  loss_mask: 0.2066  loss_dice: 0.1716  loss_ce_0: 0.06192  loss_mask_0: 0.2038  loss_dice_0: 0.1696  loss_ce_1: 0.08987  loss_mask_1: 0.2052  loss_dice_1: 0.1605  loss_ce_2: 0.09019  loss_mask_2: 0.2035  loss_dice_2: 0.165  loss_ce_3: 0.08932  loss_mask_3: 0.2085  loss_dice_3: 0.1692  loss_ce_4: 0.08939  loss_mask_4: 0.1982  loss_dice_4: 0.1649  loss_ce_5: 0.08982  loss_mask_5: 0.1975  loss_dice_5: 0.1635  loss_ce_6: 0.09122  loss_mask_6: 0.201  loss_dice_6: 0.174  loss_ce_7: 0.09164  loss_mask_7: 0.1947  loss_dice_7: 0.1583  loss_ce_8: 0.09225  loss_mask_8: 0.2106  loss_dice_8: 0.173  time: 0.5373  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:53:07] d2.utils.events INFO:  eta: 4:23:10  iter: 4039  total_loss: 5.63  loss_ce: 0.09995  loss_mask: 0.2285  loss_dice: 0.1964  loss_ce_0: 0.06022  loss_mask_0: 0.2318  loss_dice_0: 0.1959  loss_ce_1: 0.1017  loss_mask_1: 0.2359  loss_dice_1: 0.1957  loss_ce_2: 0.1018  loss_mask_2: 0.2313  loss_dice_2: 0.1961  loss_ce_3: 0.1022  loss_mask_3: 0.236  loss_dice_3: 0.1934  loss_ce_4: 0.1017  loss_mask_4: 0.238  loss_dice_4: 0.1966  loss_ce_5: 0.1013  loss_mask_5: 0.2353  loss_dice_5: 0.2007  loss_ce_6: 0.1005  loss_mask_6: 0.2262  loss_dice_6: 0.1928  loss_ce_7: 0.1012  loss_mask_7: 0.2279  loss_dice_7: 0.192  loss_ce_8: 0.1001  loss_mask_8: 0.2373  loss_dice_8: 0.2017  time: 0.5378  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:53:20] d2.utils.events INFO:  eta: 4:23:57  iter: 4059  total_loss: 5.625  loss_ce: 0.1028  loss_mask: 0.2321  loss_dice: 0.2173  loss_ce_0: 0.05999  loss_mask_0: 0.2253  loss_dice_0: 0.2216  loss_ce_1: 0.1044  loss_mask_1: 0.2211  loss_dice_1: 0.2184  loss_ce_2: 0.1043  loss_mask_2: 0.2354  loss_dice_2: 0.2197  loss_ce_3: 0.1048  loss_mask_3: 0.2335  loss_dice_3: 0.218  loss_ce_4: 0.1048  loss_mask_4: 0.2431  loss_dice_4: 0.2158  loss_ce_5: 0.1045  loss_mask_5: 0.2393  loss_dice_5: 0.2135  loss_ce_6: 0.1038  loss_mask_6: 0.2298  loss_dice_6: 0.212  loss_ce_7: 0.1041  loss_mask_7: 0.2328  loss_dice_7: 0.2136  loss_ce_8: 0.1031  loss_mask_8: 0.2264  loss_dice_8: 0.2132  time: 0.5383  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:53:33] d2.utils.events INFO:  eta: 4:24:46  iter: 4079  total_loss: 5.078  loss_ce: 0.09119  loss_mask: 0.2167  loss_dice: 0.1735  loss_ce_0: 0.05804  loss_mask_0: 0.2209  loss_dice_0: 0.1643  loss_ce_1: 0.08975  loss_mask_1: 0.2238  loss_dice_1: 0.1746  loss_ce_2: 0.09006  loss_mask_2: 0.2064  loss_dice_2: 0.1738  loss_ce_3: 0.08977  loss_mask_3: 0.2318  loss_dice_3: 0.1761  loss_ce_4: 0.09002  loss_mask_4: 0.2177  loss_dice_4: 0.1704  loss_ce_5: 0.09045  loss_mask_5: 0.2355  loss_dice_5: 0.1712  loss_ce_6: 0.09052  loss_mask_6: 0.217  loss_dice_6: 0.1711  loss_ce_7: 0.09062  loss_mask_7: 0.2251  loss_dice_7: 0.1733  loss_ce_8: 0.09081  loss_mask_8: 0.2185  loss_dice_8: 0.173  time: 0.5388  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:53:45] d2.utils.events INFO:  eta: 4:25:22  iter: 4099  total_loss: 4.873  loss_ce: 0.08004  loss_mask: 0.2064  loss_dice: 0.159  loss_ce_0: 0.05533  loss_mask_0: 0.2118  loss_dice_0: 0.1658  loss_ce_1: 0.07808  loss_mask_1: 0.2146  loss_dice_1: 0.1613  loss_ce_2: 0.07874  loss_mask_2: 0.2128  loss_dice_2: 0.1609  loss_ce_3: 0.07823  loss_mask_3: 0.2192  loss_dice_3: 0.1622  loss_ce_4: 0.07848  loss_mask_4: 0.2018  loss_dice_4: 0.165  loss_ce_5: 0.07887  loss_mask_5: 0.2089  loss_dice_5: 0.1667  loss_ce_6: 0.07891  loss_mask_6: 0.214  loss_dice_6: 0.1673  loss_ce_7: 0.07884  loss_mask_7: 0.2167  loss_dice_7: 0.1687  loss_ce_8: 0.07942  loss_mask_8: 0.2099  loss_dice_8: 0.1621  time: 0.5392  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:53:58] d2.utils.events INFO:  eta: 4:25:38  iter: 4119  total_loss: 5.18  loss_ce: 0.07492  loss_mask: 0.2403  loss_dice: 0.1942  loss_ce_0: 0.05269  loss_mask_0: 0.2327  loss_dice_0: 0.1839  loss_ce_1: 0.07471  loss_mask_1: 0.2318  loss_dice_1: 0.1907  loss_ce_2: 0.07491  loss_mask_2: 0.2362  loss_dice_2: 0.1856  loss_ce_3: 0.07448  loss_mask_3: 0.2331  loss_dice_3: 0.1835  loss_ce_4: 0.07463  loss_mask_4: 0.2351  loss_dice_4: 0.1822  loss_ce_5: 0.07459  loss_mask_5: 0.2275  loss_dice_5: 0.191  loss_ce_6: 0.07441  loss_mask_6: 0.2225  loss_dice_6: 0.1824  loss_ce_7: 0.07483  loss_mask_7: 0.2322  loss_dice_7: 0.1855  loss_ce_8: 0.07454  loss_mask_8: 0.2232  loss_dice_8: 0.1853  time: 0.5397  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:54:11] d2.utils.events INFO:  eta: 4:25:49  iter: 4139  total_loss: 5.259  loss_ce: 0.2161  loss_mask: 0.2135  loss_dice: 0.1213  loss_ce_0: 0.0912  loss_mask_0: 0.2173  loss_dice_0: 0.1226  loss_ce_1: 0.2166  loss_mask_1: 0.1994  loss_dice_1: 0.1234  loss_ce_2: 0.2162  loss_mask_2: 0.2117  loss_dice_2: 0.123  loss_ce_3: 0.2168  loss_mask_3: 0.227  loss_dice_3: 0.1227  loss_ce_4: 0.2167  loss_mask_4: 0.2135  loss_dice_4: 0.1222  loss_ce_5: 0.2165  loss_mask_5: 0.2026  loss_dice_5: 0.1202  loss_ce_6: 0.2167  loss_mask_6: 0.2103  loss_dice_6: 0.1254  loss_ce_7: 0.2163  loss_mask_7: 0.2116  loss_dice_7: 0.116  loss_ce_8: 0.2169  loss_mask_8: 0.219  loss_dice_8: 0.1225  time: 0.5402  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:54:23] d2.utils.events INFO:  eta: 4:26:00  iter: 4159  total_loss: 5.677  loss_ce: 0.1315  loss_mask: 0.2345  loss_dice: 0.1919  loss_ce_0: 0.07013  loss_mask_0: 0.2285  loss_dice_0: 0.1939  loss_ce_1: 0.1305  loss_mask_1: 0.2279  loss_dice_1: 0.1941  loss_ce_2: 0.1307  loss_mask_2: 0.2299  loss_dice_2: 0.1917  loss_ce_3: 0.1305  loss_mask_3: 0.2189  loss_dice_3: 0.1977  loss_ce_4: 0.1305  loss_mask_4: 0.2295  loss_dice_4: 0.188  loss_ce_5: 0.1308  loss_mask_5: 0.2288  loss_dice_5: 0.1941  loss_ce_6: 0.131  loss_mask_6: 0.2314  loss_dice_6: 0.1887  loss_ce_7: 0.131  loss_mask_7: 0.2303  loss_dice_7: 0.1839  loss_ce_8: 0.1311  loss_mask_8: 0.2178  loss_dice_8: 0.1784  time: 0.5406  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:54:36] d2.utils.events INFO:  eta: 4:26:12  iter: 4179  total_loss: 5.548  loss_ce: 0.1021  loss_mask: 0.1794  loss_dice: 0.175  loss_ce_0: 0.05417  loss_mask_0: 0.1856  loss_dice_0: 0.1848  loss_ce_1: 0.104  loss_mask_1: 0.1767  loss_dice_1: 0.1807  loss_ce_2: 0.1037  loss_mask_2: 0.1793  loss_dice_2: 0.1757  loss_ce_3: 0.1045  loss_mask_3: 0.1754  loss_dice_3: 0.1834  loss_ce_4: 0.1047  loss_mask_4: 0.1753  loss_dice_4: 0.1733  loss_ce_5: 0.1041  loss_mask_5: 0.1818  loss_dice_5: 0.1786  loss_ce_6: 0.1036  loss_mask_6: 0.1797  loss_dice_6: 0.1741  loss_ce_7: 0.1036  loss_mask_7: 0.1849  loss_dice_7: 0.1809  loss_ce_8: 0.1035  loss_mask_8: 0.1784  loss_dice_8: 0.1793  time: 0.5410  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:54:49] d2.utils.events INFO:  eta: 4:26:41  iter: 4199  total_loss: 5.914  loss_ce: 0.104  loss_mask: 0.1956  loss_dice: 0.231  loss_ce_0: 0.05483  loss_mask_0: 0.1969  loss_dice_0: 0.225  loss_ce_1: 0.1054  loss_mask_1: 0.1975  loss_dice_1: 0.2239  loss_ce_2: 0.105  loss_mask_2: 0.2028  loss_dice_2: 0.2257  loss_ce_3: 0.1057  loss_mask_3: 0.1999  loss_dice_3: 0.2324  loss_ce_4: 0.1058  loss_mask_4: 0.2022  loss_dice_4: 0.2155  loss_ce_5: 0.1056  loss_mask_5: 0.1963  loss_dice_5: 0.2358  loss_ce_6: 0.1052  loss_mask_6: 0.1957  loss_dice_6: 0.2176  loss_ce_7: 0.1051  loss_mask_7: 0.2007  loss_dice_7: 0.2297  loss_ce_8: 0.1052  loss_mask_8: 0.2005  loss_dice_8: 0.2231  time: 0.5415  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:55:02] d2.utils.events INFO:  eta: 4:27:16  iter: 4219  total_loss: 5.429  loss_ce: 0.09859  loss_mask: 0.2096  loss_dice: 0.1582  loss_ce_0: 0.05461  loss_mask_0: 0.2119  loss_dice_0: 0.1636  loss_ce_1: 0.09824  loss_mask_1: 0.2122  loss_dice_1: 0.1627  loss_ce_2: 0.09806  loss_mask_2: 0.2092  loss_dice_2: 0.1606  loss_ce_3: 0.0982  loss_mask_3: 0.2179  loss_dice_3: 0.1583  loss_ce_4: 0.09873  loss_mask_4: 0.2033  loss_dice_4: 0.1545  loss_ce_5: 0.09859  loss_mask_5: 0.2068  loss_dice_5: 0.1604  loss_ce_6: 0.09852  loss_mask_6: 0.2075  loss_dice_6: 0.1613  loss_ce_7: 0.09852  loss_mask_7: 0.2162  loss_dice_7: 0.1582  loss_ce_8: 0.09897  loss_mask_8: 0.2095  loss_dice_8: 0.158  time: 0.5419  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:55:14] d2.utils.events INFO:  eta: 4:27:35  iter: 4239  total_loss: 4.91  loss_ce: 0.08212  loss_mask: 0.2102  loss_dice: 0.1624  loss_ce_0: 0.0525  loss_mask_0: 0.2141  loss_dice_0: 0.161  loss_ce_1: 0.0808  loss_mask_1: 0.2101  loss_dice_1: 0.161  loss_ce_2: 0.08105  loss_mask_2: 0.2086  loss_dice_2: 0.1607  loss_ce_3: 0.08072  loss_mask_3: 0.21  loss_dice_3: 0.1616  loss_ce_4: 0.08101  loss_mask_4: 0.204  loss_dice_4: 0.162  loss_ce_5: 0.08102  loss_mask_5: 0.2156  loss_dice_5: 0.1643  loss_ce_6: 0.08119  loss_mask_6: 0.1982  loss_dice_6: 0.1591  loss_ce_7: 0.08128  loss_mask_7: 0.2139  loss_dice_7: 0.1649  loss_ce_8: 0.08164  loss_mask_8: 0.2117  loss_dice_8: 0.1588  time: 0.5424  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:55:27] d2.utils.events INFO:  eta: 4:27:42  iter: 4259  total_loss: 5.489  loss_ce: 0.07185  loss_mask: 0.1375  loss_dice: 0.2194  loss_ce_0: 0.05043  loss_mask_0: 0.1471  loss_dice_0: 0.2089  loss_ce_1: 0.07012  loss_mask_1: 0.1405  loss_dice_1: 0.2144  loss_ce_2: 0.07054  loss_mask_2: 0.1345  loss_dice_2: 0.2094  loss_ce_3: 0.07008  loss_mask_3: 0.1331  loss_dice_3: 0.2222  loss_ce_4: 0.07028  loss_mask_4: 0.1407  loss_dice_4: 0.211  loss_ce_5: 0.07066  loss_mask_5: 0.1346  loss_dice_5: 0.2159  loss_ce_6: 0.07067  loss_mask_6: 0.1354  loss_dice_6: 0.2195  loss_ce_7: 0.07104  loss_mask_7: 0.1386  loss_dice_7: 0.2161  loss_ce_8: 0.0711  loss_mask_8: 0.1321  loss_dice_8: 0.2133  time: 0.5428  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:55:40] d2.utils.events INFO:  eta: 4:27:59  iter: 4279  total_loss: 5.343  loss_ce: 0.05943  loss_mask: 0.2127  loss_dice: 0.165  loss_ce_0: 0.04748  loss_mask_0: 0.215  loss_dice_0: 0.1635  loss_ce_1: 0.05728  loss_mask_1: 0.2264  loss_dice_1: 0.1621  loss_ce_2: 0.05783  loss_mask_2: 0.2222  loss_dice_2: 0.1645  loss_ce_3: 0.05729  loss_mask_3: 0.2133  loss_dice_3: 0.168  loss_ce_4: 0.05751  loss_mask_4: 0.2178  loss_dice_4: 0.1665  loss_ce_5: 0.05779  loss_mask_5: 0.2279  loss_dice_5: 0.1654  loss_ce_6: 0.05806  loss_mask_6: 0.2212  loss_dice_6: 0.1651  loss_ce_7: 0.05842  loss_mask_7: 0.2143  loss_dice_7: 0.1675  loss_ce_8: 0.05853  loss_mask_8: 0.2242  loss_dice_8: 0.1719  time: 0.5432  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:55:52] d2.utils.events INFO:  eta: 4:28:07  iter: 4299  total_loss: 4.765  loss_ce: 0.0575  loss_mask: 0.1935  loss_dice: 0.1822  loss_ce_0: 0.04499  loss_mask_0: 0.1827  loss_dice_0: 0.1834  loss_ce_1: 0.05728  loss_mask_1: 0.1966  loss_dice_1: 0.1848  loss_ce_2: 0.05759  loss_mask_2: 0.1806  loss_dice_2: 0.1811  loss_ce_3: 0.05746  loss_mask_3: 0.1955  loss_dice_3: 0.183  loss_ce_4: 0.05734  loss_mask_4: 0.1804  loss_dice_4: 0.1893  loss_ce_5: 0.05735  loss_mask_5: 0.18  loss_dice_5: 0.1779  loss_ce_6: 0.05717  loss_mask_6: 0.1891  loss_dice_6: 0.1779  loss_ce_7: 0.0574  loss_mask_7: 0.1826  loss_dice_7: 0.1731  loss_ce_8: 0.05717  loss_mask_8: 0.1845  loss_dice_8: 0.1788  time: 0.5437  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:56:05] d2.utils.events INFO:  eta: 4:28:26  iter: 4319  total_loss: 5.546  loss_ce: 0.05065  loss_mask: 0.1921  loss_dice: 0.1995  loss_ce_0: 0.04239  loss_mask_0: 0.2096  loss_dice_0: 0.2024  loss_ce_1: 0.05007  loss_mask_1: 0.2  loss_dice_1: 0.2025  loss_ce_2: 0.05028  loss_mask_2: 0.1889  loss_dice_2: 0.2  loss_ce_3: 0.05003  loss_mask_3: 0.1919  loss_dice_3: 0.1974  loss_ce_4: 0.0501  loss_mask_4: 0.1931  loss_dice_4: 0.1975  loss_ce_5: 0.05013  loss_mask_5: 0.2077  loss_dice_5: 0.1958  loss_ce_6: 0.05007  loss_mask_6: 0.2038  loss_dice_6: 0.1953  loss_ce_7: 0.05034  loss_mask_7: 0.1966  loss_dice_7: 0.196  loss_ce_8: 0.05016  loss_mask_8: 0.1963  loss_dice_8: 0.2052  time: 0.5441  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:56:18] d2.utils.events INFO:  eta: 4:28:45  iter: 4339  total_loss: 6.04  loss_ce: 0.06137  loss_mask: 0.1893  loss_dice: 0.2071  loss_ce_0: 0.04238  loss_mask_0: 0.1915  loss_dice_0: 0.2271  loss_ce_1: 0.06253  loss_mask_1: 0.1848  loss_dice_1: 0.2117  loss_ce_2: 0.06246  loss_mask_2: 0.1743  loss_dice_2: 0.2097  loss_ce_3: 0.0625  loss_mask_3: 0.1926  loss_dice_3: 0.2118  loss_ce_4: 0.06261  loss_mask_4: 0.1923  loss_dice_4: 0.2124  loss_ce_5: 0.06226  loss_mask_5: 0.1889  loss_dice_5: 0.2106  loss_ce_6: 0.06188  loss_mask_6: 0.189  loss_dice_6: 0.2145  loss_ce_7: 0.06187  loss_mask_7: 0.1859  loss_dice_7: 0.2224  loss_ce_8: 0.06147  loss_mask_8: 0.1825  loss_dice_8: 0.2162  time: 0.5445  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:56:31] d2.utils.events INFO:  eta: 4:28:58  iter: 4359  total_loss: 5.394  loss_ce: 0.1369  loss_mask: 0.1652  loss_dice: 0.1567  loss_ce_0: 0.07275  loss_mask_0: 0.1589  loss_dice_0: 0.159  loss_ce_1: 0.1358  loss_mask_1: 0.1762  loss_dice_1: 0.1634  loss_ce_2: 0.1357  loss_mask_2: 0.16  loss_dice_2: 0.1544  loss_ce_3: 0.1358  loss_mask_3: 0.1684  loss_dice_3: 0.1587  loss_ce_4: 0.1355  loss_mask_4: 0.1573  loss_dice_4: 0.1592  loss_ce_5: 0.1357  loss_mask_5: 0.1611  loss_dice_5: 0.1556  loss_ce_6: 0.1363  loss_mask_6: 0.1693  loss_dice_6: 0.1561  loss_ce_7: 0.1362  loss_mask_7: 0.1616  loss_dice_7: 0.158  loss_ce_8: 0.1367  loss_mask_8: 0.1662  loss_dice_8: 0.1509  time: 0.5449  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:56:43] d2.utils.events INFO:  eta: 4:29:00  iter: 4379  total_loss: 5.356  loss_ce: 0.09396  loss_mask: 0.2288  loss_dice: 0.1922  loss_ce_0: 0.04761  loss_mask_0: 0.2287  loss_dice_0: 0.1879  loss_ce_1: 0.09755  loss_mask_1: 0.2394  loss_dice_1: 0.2006  loss_ce_2: 0.09732  loss_mask_2: 0.2399  loss_dice_2: 0.1875  loss_ce_3: 0.09727  loss_mask_3: 0.2364  loss_dice_3: 0.1925  loss_ce_4: 0.0988  loss_mask_4: 0.2357  loss_dice_4: 0.1923  loss_ce_5: 0.09733  loss_mask_5: 0.2482  loss_dice_5: 0.1983  loss_ce_6: 0.09587  loss_mask_6: 0.2371  loss_dice_6: 0.1774  loss_ce_7: 0.0959  loss_mask_7: 0.2388  loss_dice_7: 0.1998  loss_ce_8: 0.09522  loss_mask_8: 0.2347  loss_dice_8: 0.1901  time: 0.5453  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:56:56] d2.utils.events INFO:  eta: 4:29:10  iter: 4399  total_loss: 4.942  loss_ce: 0.08858  loss_mask: 0.1812  loss_dice: 0.191  loss_ce_0: 0.04788  loss_mask_0: 0.189  loss_dice_0: 0.1793  loss_ce_1: 0.08935  loss_mask_1: 0.1924  loss_dice_1: 0.1884  loss_ce_2: 0.08919  loss_mask_2: 0.186  loss_dice_2: 0.1781  loss_ce_3: 0.08935  loss_mask_3: 0.1899  loss_dice_3: 0.193  loss_ce_4: 0.08988  loss_mask_4: 0.1838  loss_dice_4: 0.1919  loss_ce_5: 0.08982  loss_mask_5: 0.1884  loss_dice_5: 0.1821  loss_ce_6: 0.08944  loss_mask_6: 0.1862  loss_dice_6: 0.1874  loss_ce_7: 0.0892  loss_mask_7: 0.1899  loss_dice_7: 0.1871  loss_ce_8: 0.08931  loss_mask_8: 0.1821  loss_dice_8: 0.1865  time: 0.5457  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:57:09] d2.utils.events INFO:  eta: 4:29:21  iter: 4419  total_loss: 5.203  loss_ce: 0.08183  loss_mask: 0.2311  loss_dice: 0.1695  loss_ce_0: 0.04793  loss_mask_0: 0.2249  loss_dice_0: 0.1706  loss_ce_1: 0.08125  loss_mask_1: 0.2213  loss_dice_1: 0.1639  loss_ce_2: 0.08095  loss_mask_2: 0.2233  loss_dice_2: 0.1676  loss_ce_3: 0.08126  loss_mask_3: 0.2255  loss_dice_3: 0.1713  loss_ce_4: 0.08101  loss_mask_4: 0.221  loss_dice_4: 0.1686  loss_ce_5: 0.08134  loss_mask_5: 0.2246  loss_dice_5: 0.1669  loss_ce_6: 0.0818  loss_mask_6: 0.2236  loss_dice_6: 0.1646  loss_ce_7: 0.08141  loss_mask_7: 0.2255  loss_dice_7: 0.1677  loss_ce_8: 0.08199  loss_mask_8: 0.22  loss_dice_8: 0.1618  time: 0.5461  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:57:21] d2.utils.events INFO:  eta: 4:29:13  iter: 4439  total_loss: 5.05  loss_ce: 0.1835  loss_mask: 0.1695  loss_dice: 0.1616  loss_ce_0: 0.09233  loss_mask_0: 0.1817  loss_dice_0: 0.1605  loss_ce_1: 0.181  loss_mask_1: 0.1703  loss_dice_1: 0.1623  loss_ce_2: 0.1814  loss_mask_2: 0.164  loss_dice_2: 0.161  loss_ce_3: 0.1814  loss_mask_3: 0.1738  loss_dice_3: 0.1618  loss_ce_4: 0.1818  loss_mask_4: 0.1802  loss_dice_4: 0.1677  loss_ce_5: 0.1818  loss_mask_5: 0.1772  loss_dice_5: 0.1665  loss_ce_6: 0.1823  loss_mask_6: 0.1835  loss_dice_6: 0.1607  loss_ce_7: 0.1826  loss_mask_7: 0.1871  loss_dice_7: 0.1652  loss_ce_8: 0.183  loss_mask_8: 0.1759  loss_dice_8: 0.1644  time: 0.5465  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:57:34] d2.utils.events INFO:  eta: 4:29:00  iter: 4459  total_loss: 5.002  loss_ce: 0.09388  loss_mask: 0.1864  loss_dice: 0.1632  loss_ce_0: 0.05019  loss_mask_0: 0.1867  loss_dice_0: 0.1577  loss_ce_1: 0.09733  loss_mask_1: 0.1871  loss_dice_1: 0.1636  loss_ce_2: 0.09681  loss_mask_2: 0.1809  loss_dice_2: 0.1616  loss_ce_3: 0.09702  loss_mask_3: 0.1911  loss_dice_3: 0.1589  loss_ce_4: 0.09673  loss_mask_4: 0.1852  loss_dice_4: 0.1588  loss_ce_5: 0.0963  loss_mask_5: 0.1877  loss_dice_5: 0.1583  loss_ce_6: 0.09547  loss_mask_6: 0.1824  loss_dice_6: 0.1629  loss_ce_7: 0.09535  loss_mask_7: 0.1759  loss_dice_7: 0.1583  loss_ce_8: 0.09464  loss_mask_8: 0.1869  loss_dice_8: 0.1568  time: 0.5469  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:57:47] d2.utils.events INFO:  eta: 4:28:47  iter: 4479  total_loss: 5.413  loss_ce: 0.0845  loss_mask: 0.2152  loss_dice: 0.2078  loss_ce_0: 0.04972  loss_mask_0: 0.2332  loss_dice_0: 0.2129  loss_ce_1: 0.08368  loss_mask_1: 0.2239  loss_dice_1: 0.1993  loss_ce_2: 0.08364  loss_mask_2: 0.2217  loss_dice_2: 0.2006  loss_ce_3: 0.08382  loss_mask_3: 0.2281  loss_dice_3: 0.194  loss_ce_4: 0.08393  loss_mask_4: 0.2237  loss_dice_4: 0.196  loss_ce_5: 0.0842  loss_mask_5: 0.2261  loss_dice_5: 0.1873  loss_ce_6: 0.08437  loss_mask_6: 0.2269  loss_dice_6: 0.204  loss_ce_7: 0.08427  loss_mask_7: 0.2225  loss_dice_7: 0.1991  loss_ce_8: 0.08454  loss_mask_8: 0.2252  loss_dice_8: 0.2061  time: 0.5473  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:57:59] d2.utils.events INFO:  eta: 4:28:39  iter: 4499  total_loss: 5.425  loss_ce: 0.0882  loss_mask: 0.1718  loss_dice: 0.2257  loss_ce_0: 0.05045  loss_mask_0: 0.1681  loss_dice_0: 0.2293  loss_ce_1: 0.08878  loss_mask_1: 0.1649  loss_dice_1: 0.2141  loss_ce_2: 0.08871  loss_mask_2: 0.1762  loss_dice_2: 0.2225  loss_ce_3: 0.0887  loss_mask_3: 0.1742  loss_dice_3: 0.2275  loss_ce_4: 0.08856  loss_mask_4: 0.1745  loss_dice_4: 0.2222  loss_ce_5: 0.08851  loss_mask_5: 0.1734  loss_dice_5: 0.2192  loss_ce_6: 0.08865  loss_mask_6: 0.1736  loss_dice_6: 0.2333  loss_ce_7: 0.08827  loss_mask_7: 0.1747  loss_dice_7: 0.2209  loss_ce_8: 0.08841  loss_mask_8: 0.1724  loss_dice_8: 0.2215  time: 0.5477  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:58:13] d2.utils.events INFO:  eta: 4:28:44  iter: 4519  total_loss: 4.946  loss_ce: 0.07823  loss_mask: 0.1744  loss_dice: 0.2062  loss_ce_0: 0.04919  loss_mask_0: 0.1668  loss_dice_0: 0.1994  loss_ce_1: 0.07691  loss_mask_1: 0.1608  loss_dice_1: 0.2062  loss_ce_2: 0.07693  loss_mask_2: 0.1549  loss_dice_2: 0.1942  loss_ce_3: 0.07707  loss_mask_3: 0.1684  loss_dice_3: 0.2071  loss_ce_4: 0.0782  loss_mask_4: 0.1695  loss_dice_4: 0.2005  loss_ce_5: 0.07758  loss_mask_5: 0.1554  loss_dice_5: 0.2004  loss_ce_6: 0.0776  loss_mask_6: 0.1769  loss_dice_6: 0.2062  loss_ce_7: 0.07868  loss_mask_7: 0.1596  loss_dice_7: 0.1973  loss_ce_8: 0.07832  loss_mask_8: 0.1661  loss_dice_8: 0.2066  time: 0.5482  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:58:25] d2.utils.events INFO:  eta: 4:28:35  iter: 4539  total_loss: 5.044  loss_ce: 0.1675  loss_mask: 0.1933  loss_dice: 0.156  loss_ce_0: 0.09161  loss_mask_0: 0.203  loss_dice_0: 0.1595  loss_ce_1: 0.1676  loss_mask_1: 0.1901  loss_dice_1: 0.1512  loss_ce_2: 0.1571  loss_mask_2: 0.1964  loss_dice_2: 0.1559  loss_ce_3: 0.1417  loss_mask_3: 0.204  loss_dice_3: 0.1555  loss_ce_4: 0.1519  loss_mask_4: 0.1903  loss_dice_4: 0.1526  loss_ce_5: 0.156  loss_mask_5: 0.1955  loss_dice_5: 0.1617  loss_ce_6: 0.1597  loss_mask_6: 0.204  loss_dice_6: 0.1639  loss_ce_7: 0.1555  loss_mask_7: 0.1986  loss_dice_7: 0.1557  loss_ce_8: 0.1613  loss_mask_8: 0.2026  loss_dice_8: 0.1612  time: 0.5485  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 11:58:38] d2.utils.events INFO:  eta: 4:28:22  iter: 4559  total_loss: 5.286  loss_ce: 0.1192  loss_mask: 0.2135  loss_dice: 0.1732  loss_ce_0: 0.05236  loss_mask_0: 0.2016  loss_dice_0: 0.1637  loss_ce_1: 0.1158  loss_mask_1: 0.2061  loss_dice_1: 0.1705  loss_ce_2: 0.1245  loss_mask_2: 0.2024  loss_dice_2: 0.1628  loss_ce_3: 0.1299  loss_mask_3: 0.2078  loss_dice_3: 0.1731  loss_ce_4: 0.1304  loss_mask_4: 0.1972  loss_dice_4: 0.1653  loss_ce_5: 0.1307  loss_mask_5: 0.2066  loss_dice_5: 0.1669  loss_ce_6: 0.1272  loss_mask_6: 0.2047  loss_dice_6: 0.1698  loss_ce_7: 0.1304  loss_mask_7: 0.2085  loss_dice_7: 0.1653  loss_ce_8: 0.1261  loss_mask_8: 0.2059  loss_dice_8: 0.1721  time: 0.5489  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:58:50] d2.utils.events INFO:  eta: 4:28:09  iter: 4579  total_loss: 4.528  loss_ce: 0.1291  loss_mask: 0.1868  loss_dice: 0.1378  loss_ce_0: 0.06885  loss_mask_0: 0.1888  loss_dice_0: 0.1462  loss_ce_1: 0.1267  loss_mask_1: 0.1824  loss_dice_1: 0.147  loss_ce_2: 0.1277  loss_mask_2: 0.1893  loss_dice_2: 0.1519  loss_ce_3: 0.1296  loss_mask_3: 0.1959  loss_dice_3: 0.1468  loss_ce_4: 0.1265  loss_mask_4: 0.1905  loss_dice_4: 0.1436  loss_ce_5: 0.1296  loss_mask_5: 0.1856  loss_dice_5: 0.1445  loss_ce_6: 0.1295  loss_mask_6: 0.192  loss_dice_6: 0.1418  loss_ce_7: 0.1263  loss_mask_7: 0.1922  loss_dice_7: 0.1382  loss_ce_8: 0.1291  loss_mask_8: 0.1905  loss_dice_8: 0.143  time: 0.5492  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:59:03] d2.utils.events INFO:  eta: 4:27:54  iter: 4599  total_loss: 5.172  loss_ce: 0.1309  loss_mask: 0.1953  loss_dice: 0.1572  loss_ce_0: 0.0572  loss_mask_0: 0.2036  loss_dice_0: 0.1631  loss_ce_1: 0.1306  loss_mask_1: 0.1984  loss_dice_1: 0.1658  loss_ce_2: 0.1304  loss_mask_2: 0.2021  loss_dice_2: 0.169  loss_ce_3: 0.1308  loss_mask_3: 0.2007  loss_dice_3: 0.1624  loss_ce_4: 0.1302  loss_mask_4: 0.1999  loss_dice_4: 0.1621  loss_ce_5: 0.1305  loss_mask_5: 0.2008  loss_dice_5: 0.1614  loss_ce_6: 0.1301  loss_mask_6: 0.1956  loss_dice_6: 0.164  loss_ce_7: 0.1301  loss_mask_7: 0.1942  loss_dice_7: 0.1599  loss_ce_8: 0.1309  loss_mask_8: 0.1923  loss_dice_8: 0.1598  time: 0.5496  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:59:16] d2.utils.events INFO:  eta: 4:27:41  iter: 4619  total_loss: 5.395  loss_ce: 0.1059  loss_mask: 0.2271  loss_dice: 0.2138  loss_ce_0: 0.057  loss_mask_0: 0.2319  loss_dice_0: 0.2123  loss_ce_1: 0.1085  loss_mask_1: 0.2335  loss_dice_1: 0.2084  loss_ce_2: 0.1046  loss_mask_2: 0.2302  loss_dice_2: 0.2115  loss_ce_3: 0.1036  loss_mask_3: 0.2309  loss_dice_3: 0.2112  loss_ce_4: 0.1041  loss_mask_4: 0.2195  loss_dice_4: 0.2127  loss_ce_5: 0.1053  loss_mask_5: 0.2399  loss_dice_5: 0.2159  loss_ce_6: 0.1061  loss_mask_6: 0.2212  loss_dice_6: 0.2071  loss_ce_7: 0.1056  loss_mask_7: 0.2261  loss_dice_7: 0.2048  loss_ce_8: 0.1059  loss_mask_8: 0.2213  loss_dice_8: 0.214  time: 0.5500  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:59:28] d2.utils.events INFO:  eta: 4:27:32  iter: 4639  total_loss: 4.696  loss_ce: 0.07924  loss_mask: 0.1753  loss_dice: 0.1685  loss_ce_0: 0.05498  loss_mask_0: 0.1856  loss_dice_0: 0.1727  loss_ce_1: 0.07996  loss_mask_1: 0.1819  loss_dice_1: 0.1697  loss_ce_2: 0.0764  loss_mask_2: 0.1879  loss_dice_2: 0.1615  loss_ce_3: 0.0743  loss_mask_3: 0.1852  loss_dice_3: 0.1717  loss_ce_4: 0.07555  loss_mask_4: 0.1858  loss_dice_4: 0.1702  loss_ce_5: 0.07664  loss_mask_5: 0.1812  loss_dice_5: 0.1677  loss_ce_6: 0.07788  loss_mask_6: 0.179  loss_dice_6: 0.1657  loss_ce_7: 0.07711  loss_mask_7: 0.1868  loss_dice_7: 0.1634  loss_ce_8: 0.07806  loss_mask_8: 0.1781  loss_dice_8: 0.1651  time: 0.5503  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:59:41] d2.utils.events INFO:  eta: 4:27:20  iter: 4659  total_loss: 5.086  loss_ce: 0.07458  loss_mask: 0.2274  loss_dice: 0.1583  loss_ce_0: 0.05194  loss_mask_0: 0.2332  loss_dice_0: 0.1584  loss_ce_1: 0.07563  loss_mask_1: 0.2243  loss_dice_1: 0.15  loss_ce_2: 0.07475  loss_mask_2: 0.2335  loss_dice_2: 0.1603  loss_ce_3: 0.07422  loss_mask_3: 0.225  loss_dice_3: 0.1552  loss_ce_4: 0.07433  loss_mask_4: 0.2273  loss_dice_4: 0.1545  loss_ce_5: 0.07483  loss_mask_5: 0.2256  loss_dice_5: 0.1552  loss_ce_6: 0.07483  loss_mask_6: 0.2286  loss_dice_6: 0.1507  loss_ce_7: 0.07436  loss_mask_7: 0.2162  loss_dice_7: 0.1552  loss_ce_8: 0.07468  loss_mask_8: 0.2237  loss_dice_8: 0.1592  time: 0.5507  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 11:59:54] d2.utils.events INFO:  eta: 4:27:08  iter: 4679  total_loss: 5.767  loss_ce: 0.07589  loss_mask: 0.1672  loss_dice: 0.1656  loss_ce_0: 0.05153  loss_mask_0: 0.1698  loss_dice_0: 0.1741  loss_ce_1: 0.07738  loss_mask_1: 0.1603  loss_dice_1: 0.1634  loss_ce_2: 0.07761  loss_mask_2: 0.1599  loss_dice_2: 0.1711  loss_ce_3: 0.07652  loss_mask_3: 0.1783  loss_dice_3: 0.1629  loss_ce_4: 0.07631  loss_mask_4: 0.1618  loss_dice_4: 0.168  loss_ce_5: 0.078  loss_mask_5: 0.1722  loss_dice_5: 0.1719  loss_ce_6: 0.07632  loss_mask_6: 0.1669  loss_dice_6: 0.1657  loss_ce_7: 0.07601  loss_mask_7: 0.1671  loss_dice_7: 0.1729  loss_ce_8: 0.0777  loss_mask_8: 0.164  loss_dice_8: 0.1683  time: 0.5510  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:00:07] d2.utils.events INFO:  eta: 4:26:56  iter: 4699  total_loss: 4.832  loss_ce: 0.08654  loss_mask: 0.1959  loss_dice: 0.1635  loss_ce_0: 0.05136  loss_mask_0: 0.1831  loss_dice_0: 0.1668  loss_ce_1: 0.08837  loss_mask_1: 0.1904  loss_dice_1: 0.1634  loss_ce_2: 0.08818  loss_mask_2: 0.1929  loss_dice_2: 0.1586  loss_ce_3: 0.08912  loss_mask_3: 0.1893  loss_dice_3: 0.1687  loss_ce_4: 0.08856  loss_mask_4: 0.1938  loss_dice_4: 0.1569  loss_ce_5: 0.08866  loss_mask_5: 0.1932  loss_dice_5: 0.1554  loss_ce_6: 0.08811  loss_mask_6: 0.1879  loss_dice_6: 0.1704  loss_ce_7: 0.08797  loss_mask_7: 0.1832  loss_dice_7: 0.1596  loss_ce_8: 0.0878  loss_mask_8: 0.1937  loss_dice_8: 0.1683  time: 0.5514  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:00:19] d2.utils.events INFO:  eta: 4:26:43  iter: 4719  total_loss: 5.123  loss_ce: 0.1487  loss_mask: 0.1988  loss_dice: 0.1624  loss_ce_0: 0.08357  loss_mask_0: 0.2218  loss_dice_0: 0.1627  loss_ce_1: 0.1453  loss_mask_1: 0.2067  loss_dice_1: 0.1665  loss_ce_2: 0.1448  loss_mask_2: 0.2135  loss_dice_2: 0.1689  loss_ce_3: 0.1413  loss_mask_3: 0.2048  loss_dice_3: 0.1606  loss_ce_4: 0.143  loss_mask_4: 0.2091  loss_dice_4: 0.1693  loss_ce_5: 0.1439  loss_mask_5: 0.2083  loss_dice_5: 0.1612  loss_ce_6: 0.1445  loss_mask_6: 0.2077  loss_dice_6: 0.1654  loss_ce_7: 0.1443  loss_mask_7: 0.2071  loss_dice_7: 0.1624  loss_ce_8: 0.145  loss_mask_8: 0.2078  loss_dice_8: 0.1623  time: 0.5518  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:00:32] d2.utils.events INFO:  eta: 4:26:30  iter: 4739  total_loss: 4.903  loss_ce: 0.128  loss_mask: 0.1797  loss_dice: 0.1743  loss_ce_0: 0.06787  loss_mask_0: 0.1812  loss_dice_0: 0.1705  loss_ce_1: 0.1281  loss_mask_1: 0.1842  loss_dice_1: 0.1678  loss_ce_2: 0.1291  loss_mask_2: 0.1857  loss_dice_2: 0.1641  loss_ce_3: 0.1319  loss_mask_3: 0.1846  loss_dice_3: 0.1748  loss_ce_4: 0.131  loss_mask_4: 0.1888  loss_dice_4: 0.1668  loss_ce_5: 0.1301  loss_mask_5: 0.177  loss_dice_5: 0.1586  loss_ce_6: 0.1299  loss_mask_6: 0.1841  loss_dice_6: 0.1733  loss_ce_7: 0.13  loss_mask_7: 0.1665  loss_dice_7: 0.1721  loss_ce_8: 0.13  loss_mask_8: 0.1825  loss_dice_8: 0.1786  time: 0.5521  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 12:00:45] d2.utils.events INFO:  eta: 4:26:21  iter: 4759  total_loss: 4.316  loss_ce: 0.1265  loss_mask: 0.2036  loss_dice: 0.1305  loss_ce_0: 0.07381  loss_mask_0: 0.1981  loss_dice_0: 0.1265  loss_ce_1: 0.1253  loss_mask_1: 0.1992  loss_dice_1: 0.1264  loss_ce_2: 0.1251  loss_mask_2: 0.2061  loss_dice_2: 0.1314  loss_ce_3: 0.1226  loss_mask_3: 0.1992  loss_dice_3: 0.1277  loss_ce_4: 0.1236  loss_mask_4: 0.2022  loss_dice_4: 0.1282  loss_ce_5: 0.1241  loss_mask_5: 0.1962  loss_dice_5: 0.131  loss_ce_6: 0.1243  loss_mask_6: 0.1955  loss_dice_6: 0.126  loss_ce_7: 0.1245  loss_mask_7: 0.207  loss_dice_7: 0.1271  loss_ce_8: 0.1244  loss_mask_8: 0.1961  loss_dice_8: 0.1295  time: 0.5524  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:00:57] d2.utils.events INFO:  eta: 4:26:15  iter: 4779  total_loss: 5.314  loss_ce: 0.1048  loss_mask: 0.2406  loss_dice: 0.1532  loss_ce_0: 0.06829  loss_mask_0: 0.2272  loss_dice_0: 0.15  loss_ce_1: 0.1047  loss_mask_1: 0.2355  loss_dice_1: 0.1502  loss_ce_2: 0.1034  loss_mask_2: 0.239  loss_dice_2: 0.1502  loss_ce_3: 0.1014  loss_mask_3: 0.2298  loss_dice_3: 0.1493  loss_ce_4: 0.1023  loss_mask_4: 0.2308  loss_dice_4: 0.1475  loss_ce_5: 0.1032  loss_mask_5: 0.2305  loss_dice_5: 0.1474  loss_ce_6: 0.1028  loss_mask_6: 0.2402  loss_dice_6: 0.1514  loss_ce_7: 0.103  loss_mask_7: 0.2388  loss_dice_7: 0.1519  loss_ce_8: 0.1026  loss_mask_8: 0.2428  loss_dice_8: 0.1518  time: 0.5528  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:01:10] d2.utils.events INFO:  eta: 4:26:04  iter: 4799  total_loss: 5.913  loss_ce: 0.1142  loss_mask: 0.1836  loss_dice: 0.1787  loss_ce_0: 0.06708  loss_mask_0: 0.1869  loss_dice_0: 0.193  loss_ce_1: 0.1128  loss_mask_1: 0.1843  loss_dice_1: 0.1858  loss_ce_2: 0.1142  loss_mask_2: 0.1702  loss_dice_2: 0.1885  loss_ce_3: 0.1157  loss_mask_3: 0.1777  loss_dice_3: 0.1839  loss_ce_4: 0.1157  loss_mask_4: 0.1852  loss_dice_4: 0.1863  loss_ce_5: 0.1146  loss_mask_5: 0.1733  loss_dice_5: 0.1874  loss_ce_6: 0.1153  loss_mask_6: 0.1826  loss_dice_6: 0.189  loss_ce_7: 0.1158  loss_mask_7: 0.188  loss_dice_7: 0.1885  loss_ce_8: 0.116  loss_mask_8: 0.1737  loss_dice_8: 0.1874  time: 0.5531  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:01:23] d2.utils.events INFO:  eta: 4:25:54  iter: 4819  total_loss: 4.6  loss_ce: 0.1064  loss_mask: 0.2117  loss_dice: 0.1372  loss_ce_0: 0.06615  loss_mask_0: 0.2051  loss_dice_0: 0.1479  loss_ce_1: 0.1053  loss_mask_1: 0.207  loss_dice_1: 0.136  loss_ce_2: 0.1061  loss_mask_2: 0.2073  loss_dice_2: 0.1371  loss_ce_3: 0.1068  loss_mask_3: 0.2023  loss_dice_3: 0.1412  loss_ce_4: 0.1071  loss_mask_4: 0.2083  loss_dice_4: 0.1446  loss_ce_5: 0.1065  loss_mask_5: 0.2054  loss_dice_5: 0.1314  loss_ce_6: 0.1071  loss_mask_6: 0.1975  loss_dice_6: 0.1378  loss_ce_7: 0.1075  loss_mask_7: 0.2092  loss_dice_7: 0.1362  loss_ce_8: 0.1079  loss_mask_8: 0.2043  loss_dice_8: 0.1393  time: 0.5534  data_time: 0.0014  lr: 0.0001  max_mem: 1568M
[07/11 12:01:35] d2.utils.events INFO:  eta: 4:25:44  iter: 4839  total_loss: 5.32  loss_ce: 0.1419  loss_mask: 0.2295  loss_dice: 0.1721  loss_ce_0: 0.06979  loss_mask_0: 0.2371  loss_dice_0: 0.1797  loss_ce_1: 0.1415  loss_mask_1: 0.2321  loss_dice_1: 0.1713  loss_ce_2: 0.1397  loss_mask_2: 0.2241  loss_dice_2: 0.1761  loss_ce_3: 0.1386  loss_mask_3: 0.2312  loss_dice_3: 0.1761  loss_ce_4: 0.1393  loss_mask_4: 0.2213  loss_dice_4: 0.1798  loss_ce_5: 0.1405  loss_mask_5: 0.2345  loss_dice_5: 0.1668  loss_ce_6: 0.1405  loss_mask_6: 0.2297  loss_dice_6: 0.1739  loss_ce_7: 0.1404  loss_mask_7: 0.2315  loss_dice_7: 0.1658  loss_ce_8: 0.1408  loss_mask_8: 0.2326  loss_dice_8: 0.1757  time: 0.5538  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:01:48] d2.utils.events INFO:  eta: 4:25:39  iter: 4859  total_loss: 5.202  loss_ce: 0.1238  loss_mask: 0.2166  loss_dice: 0.18  loss_ce_0: 0.06793  loss_mask_0: 0.2261  loss_dice_0: 0.1741  loss_ce_1: 0.1218  loss_mask_1: 0.2167  loss_dice_1: 0.1788  loss_ce_2: 0.1202  loss_mask_2: 0.2256  loss_dice_2: 0.1781  loss_ce_3: 0.1183  loss_mask_3: 0.2274  loss_dice_3: 0.1711  loss_ce_4: 0.1193  loss_mask_4: 0.234  loss_dice_4: 0.1683  loss_ce_5: 0.1208  loss_mask_5: 0.2228  loss_dice_5: 0.1679  loss_ce_6: 0.1212  loss_mask_6: 0.2276  loss_dice_6: 0.1678  loss_ce_7: 0.1212  loss_mask_7: 0.2223  loss_dice_7: 0.1682  loss_ce_8: 0.1222  loss_mask_8: 0.2314  loss_dice_8: 0.1733  time: 0.5541  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:02:01] d2.utils.events INFO:  eta: 4:25:26  iter: 4879  total_loss: 4.944  loss_ce: 0.151  loss_mask: 0.1952  loss_dice: 0.1784  loss_ce_0: 0.06878  loss_mask_0: 0.1938  loss_dice_0: 0.1771  loss_ce_1: 0.15  loss_mask_1: 0.201  loss_dice_1: 0.1761  loss_ce_2: 0.1502  loss_mask_2: 0.1939  loss_dice_2: 0.1713  loss_ce_3: 0.1515  loss_mask_3: 0.1957  loss_dice_3: 0.1797  loss_ce_4: 0.1518  loss_mask_4: 0.1998  loss_dice_4: 0.1721  loss_ce_5: 0.151  loss_mask_5: 0.1958  loss_dice_5: 0.1733  loss_ce_6: 0.1514  loss_mask_6: 0.1958  loss_dice_6: 0.1728  loss_ce_7: 0.1517  loss_mask_7: 0.1875  loss_dice_7: 0.1801  loss_ce_8: 0.1519  loss_mask_8: 0.2003  loss_dice_8: 0.1871  time: 0.5544  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:02:14] d2.utils.events INFO:  eta: 4:25:09  iter: 4899  total_loss: 4.672  loss_ce: 0.1124  loss_mask: 0.1676  loss_dice: 0.1758  loss_ce_0: 0.06477  loss_mask_0: 0.1636  loss_dice_0: 0.1806  loss_ce_1: 0.1144  loss_mask_1: 0.1609  loss_dice_1: 0.1807  loss_ce_2: 0.1141  loss_mask_2: 0.1715  loss_dice_2: 0.1874  loss_ce_3: 0.1137  loss_mask_3: 0.1637  loss_dice_3: 0.1843  loss_ce_4: 0.1133  loss_mask_4: 0.1557  loss_dice_4: 0.1843  loss_ce_5: 0.1137  loss_mask_5: 0.1727  loss_dice_5: 0.1833  loss_ce_6: 0.1132  loss_mask_6: 0.1636  loss_dice_6: 0.1756  loss_ce_7: 0.1129  loss_mask_7: 0.16  loss_dice_7: 0.1815  loss_ce_8: 0.1124  loss_mask_8: 0.1624  loss_dice_8: 0.1788  time: 0.5548  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:02:26] d2.utils.events INFO:  eta: 4:25:01  iter: 4919  total_loss: 5.427  loss_ce: 0.163  loss_mask: 0.1938  loss_dice: 0.1797  loss_ce_0: 0.07257  loss_mask_0: 0.1952  loss_dice_0: 0.1773  loss_ce_1: 0.1631  loss_mask_1: 0.1908  loss_dice_1: 0.1837  loss_ce_2: 0.1638  loss_mask_2: 0.1988  loss_dice_2: 0.1899  loss_ce_3: 0.1651  loss_mask_3: 0.2086  loss_dice_3: 0.1834  loss_ce_4: 0.1631  loss_mask_4: 0.2031  loss_dice_4: 0.1906  loss_ce_5: 0.1638  loss_mask_5: 0.2039  loss_dice_5: 0.1879  loss_ce_6: 0.1643  loss_mask_6: 0.2082  loss_dice_6: 0.1935  loss_ce_7: 0.1632  loss_mask_7: 0.2065  loss_dice_7: 0.1861  loss_ce_8: 0.1638  loss_mask_8: 0.1999  loss_dice_8: 0.1875  time: 0.5551  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:02:39] d2.utils.events INFO:  eta: 4:24:50  iter: 4939  total_loss: 4.829  loss_ce: 0.1334  loss_mask: 0.2167  loss_dice: 0.1351  loss_ce_0: 0.06891  loss_mask_0: 0.2236  loss_dice_0: 0.1314  loss_ce_1: 0.1355  loss_mask_1: 0.218  loss_dice_1: 0.1375  loss_ce_2: 0.1353  loss_mask_2: 0.2173  loss_dice_2: 0.1347  loss_ce_3: 0.1357  loss_mask_3: 0.222  loss_dice_3: 0.1346  loss_ce_4: 0.1355  loss_mask_4: 0.2284  loss_dice_4: 0.136  loss_ce_5: 0.1354  loss_mask_5: 0.2182  loss_dice_5: 0.1253  loss_ce_6: 0.1351  loss_mask_6: 0.2241  loss_dice_6: 0.1295  loss_ce_7: 0.135  loss_mask_7: 0.2259  loss_dice_7: 0.1355  loss_ce_8: 0.1347  loss_mask_8: 0.2184  loss_dice_8: 0.1335  time: 0.5554  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:02:52] d2.utils.events INFO:  eta: 4:24:38  iter: 4959  total_loss: 4.879  loss_ce: 0.1279  loss_mask: 0.2039  loss_dice: 0.154  loss_ce_0: 0.06679  loss_mask_0: 0.1991  loss_dice_0: 0.1498  loss_ce_1: 0.1287  loss_mask_1: 0.2036  loss_dice_1: 0.1473  loss_ce_2: 0.1286  loss_mask_2: 0.2007  loss_dice_2: 0.1463  loss_ce_3: 0.1288  loss_mask_3: 0.205  loss_dice_3: 0.1513  loss_ce_4: 0.1288  loss_mask_4: 0.2047  loss_dice_4: 0.1507  loss_ce_5: 0.1287  loss_mask_5: 0.1997  loss_dice_5: 0.1488  loss_ce_6: 0.1285  loss_mask_6: 0.2028  loss_dice_6: 0.152  loss_ce_7: 0.1286  loss_mask_7: 0.1894  loss_dice_7: 0.1551  loss_ce_8: 0.1284  loss_mask_8: 0.209  loss_dice_8: 0.1565  time: 0.5557  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:03:04] d2.utils.events INFO:  eta: 4:24:27  iter: 4979  total_loss: 5.154  loss_ce: 0.1081  loss_mask: 0.1384  loss_dice: 0.2535  loss_ce_0: 0.06612  loss_mask_0: 0.1211  loss_dice_0: 0.2531  loss_ce_1: 0.1063  loss_mask_1: 0.1247  loss_dice_1: 0.2382  loss_ce_2: 0.1068  loss_mask_2: 0.1192  loss_dice_2: 0.2494  loss_ce_3: 0.1056  loss_mask_3: 0.1338  loss_dice_3: 0.2487  loss_ce_4: 0.1058  loss_mask_4: 0.1306  loss_dice_4: 0.2534  loss_ce_5: 0.106  loss_mask_5: 0.1248  loss_dice_5: 0.2449  loss_ce_6: 0.1064  loss_mask_6: 0.1346  loss_dice_6: 0.24  loss_ce_7: 0.1066  loss_mask_7: 0.1239  loss_dice_7: 0.227  loss_ce_8: 0.1067  loss_mask_8: 0.1247  loss_dice_8: 0.2479  time: 0.5561  data_time: 0.0015  lr: 0.0001  max_mem: 1568M
[07/11 12:03:17] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256to256_instance/model_0004999.pth
[07/11 12:03:17] d2.data.datasets.coco INFO: Loaded 754 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_val2017.json
[07/11 12:03:17] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 355          |   defect   | 399          |
|            |              |            |              |
|   total    | 754          |            |              |[0m
[07/11 12:03:17] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/11 12:03:17] d2.data.common INFO: Serializing 754 elements to byte tensors and concatenating them all ...
[07/11 12:03:17] d2.data.common INFO: Serialized dataset takes 2.49 MiB
[07/11 12:03:17] d2.evaluation.evaluator INFO: Start inference on 754 batches
[07/11 12:03:22] d2.evaluation.evaluator INFO: Inference done 11/754. Dataloading: 0.0005 s/iter. Inference: 0.1422 s/iter. Eval: 0.2645 s/iter. Total: 0.4071 s/iter. ETA=0:05:02
[07/11 12:03:27] d2.evaluation.evaluator INFO: Inference done 23/754. Dataloading: 0.0007 s/iter. Inference: 0.1441 s/iter. Eval: 0.2722 s/iter. Total: 0.4170 s/iter. ETA=0:05:04
[07/11 12:03:33] d2.evaluation.evaluator INFO: Inference done 36/754. Dataloading: 0.0007 s/iter. Inference: 0.1458 s/iter. Eval: 0.2684 s/iter. Total: 0.4150 s/iter. ETA=0:04:57
[07/11 12:03:38] d2.evaluation.evaluator INFO: Inference done 49/754. Dataloading: 0.0007 s/iter. Inference: 0.1471 s/iter. Eval: 0.2645 s/iter. Total: 0.4124 s/iter. ETA=0:04:50
[07/11 12:03:43] d2.evaluation.evaluator INFO: Inference done 62/754. Dataloading: 0.0007 s/iter. Inference: 0.1476 s/iter. Eval: 0.2633 s/iter. Total: 0.4118 s/iter. ETA=0:04:44
[07/11 12:03:49] d2.evaluation.evaluator INFO: Inference done 75/754. Dataloading: 0.0007 s/iter. Inference: 0.1475 s/iter. Eval: 0.2641 s/iter. Total: 0.4124 s/iter. ETA=0:04:40
[07/11 12:03:54] d2.evaluation.evaluator INFO: Inference done 88/754. Dataloading: 0.0007 s/iter. Inference: 0.1464 s/iter. Eval: 0.2634 s/iter. Total: 0.4106 s/iter. ETA=0:04:33
[07/11 12:03:59] d2.evaluation.evaluator INFO: Inference done 100/754. Dataloading: 0.0008 s/iter. Inference: 0.1472 s/iter. Eval: 0.2642 s/iter. Total: 0.4123 s/iter. ETA=0:04:29
[07/11 12:04:04] d2.evaluation.evaluator INFO: Inference done 113/754. Dataloading: 0.0008 s/iter. Inference: 0.1469 s/iter. Eval: 0.2640 s/iter. Total: 0.4117 s/iter. ETA=0:04:23
[07/11 12:04:10] d2.evaluation.evaluator INFO: Inference done 126/754. Dataloading: 0.0008 s/iter. Inference: 0.1473 s/iter. Eval: 0.2633 s/iter. Total: 0.4115 s/iter. ETA=0:04:18
[07/11 12:04:15] d2.evaluation.evaluator INFO: Inference done 138/754. Dataloading: 0.0008 s/iter. Inference: 0.1486 s/iter. Eval: 0.2630 s/iter. Total: 0.4125 s/iter. ETA=0:04:14
[07/11 12:04:20] d2.evaluation.evaluator INFO: Inference done 151/754. Dataloading: 0.0008 s/iter. Inference: 0.1483 s/iter. Eval: 0.2635 s/iter. Total: 0.4127 s/iter. ETA=0:04:08
[07/11 12:04:25] d2.evaluation.evaluator INFO: Inference done 163/754. Dataloading: 0.0008 s/iter. Inference: 0.1481 s/iter. Eval: 0.2641 s/iter. Total: 0.4131 s/iter. ETA=0:04:04
[07/11 12:04:30] d2.evaluation.evaluator INFO: Inference done 175/754. Dataloading: 0.0008 s/iter. Inference: 0.1480 s/iter. Eval: 0.2651 s/iter. Total: 0.4140 s/iter. ETA=0:03:59
[07/11 12:04:36] d2.evaluation.evaluator INFO: Inference done 188/754. Dataloading: 0.0008 s/iter. Inference: 0.1484 s/iter. Eval: 0.2649 s/iter. Total: 0.4141 s/iter. ETA=0:03:54
[07/11 12:04:41] d2.evaluation.evaluator INFO: Inference done 201/754. Dataloading: 0.0008 s/iter. Inference: 0.1484 s/iter. Eval: 0.2647 s/iter. Total: 0.4139 s/iter. ETA=0:03:48
[07/11 12:04:46] d2.evaluation.evaluator INFO: Inference done 214/754. Dataloading: 0.0008 s/iter. Inference: 0.1483 s/iter. Eval: 0.2645 s/iter. Total: 0.4136 s/iter. ETA=0:03:43
[07/11 12:04:52] d2.evaluation.evaluator INFO: Inference done 227/754. Dataloading: 0.0008 s/iter. Inference: 0.1482 s/iter. Eval: 0.2643 s/iter. Total: 0.4134 s/iter. ETA=0:03:37
[07/11 12:04:57] d2.evaluation.evaluator INFO: Inference done 240/754. Dataloading: 0.0008 s/iter. Inference: 0.1479 s/iter. Eval: 0.2642 s/iter. Total: 0.4130 s/iter. ETA=0:03:32
[07/11 12:05:02] d2.evaluation.evaluator INFO: Inference done 253/754. Dataloading: 0.0008 s/iter. Inference: 0.1479 s/iter. Eval: 0.2641 s/iter. Total: 0.4128 s/iter. ETA=0:03:26
[07/11 12:05:07] d2.evaluation.evaluator INFO: Inference done 266/754. Dataloading: 0.0008 s/iter. Inference: 0.1478 s/iter. Eval: 0.2641 s/iter. Total: 0.4127 s/iter. ETA=0:03:21
[07/11 12:05:13] d2.evaluation.evaluator INFO: Inference done 279/754. Dataloading: 0.0008 s/iter. Inference: 0.1478 s/iter. Eval: 0.2642 s/iter. Total: 0.4128 s/iter. ETA=0:03:16
[07/11 12:05:18] d2.evaluation.evaluator INFO: Inference done 292/754. Dataloading: 0.0007 s/iter. Inference: 0.1479 s/iter. Eval: 0.2638 s/iter. Total: 0.4126 s/iter. ETA=0:03:10
[07/11 12:05:23] d2.evaluation.evaluator INFO: Inference done 304/754. Dataloading: 0.0007 s/iter. Inference: 0.1480 s/iter. Eval: 0.2640 s/iter. Total: 0.4128 s/iter. ETA=0:03:05
[07/11 12:05:29] d2.evaluation.evaluator INFO: Inference done 317/754. Dataloading: 0.0007 s/iter. Inference: 0.1481 s/iter. Eval: 0.2640 s/iter. Total: 0.4129 s/iter. ETA=0:03:00
[07/11 12:05:34] d2.evaluation.evaluator INFO: Inference done 329/754. Dataloading: 0.0007 s/iter. Inference: 0.1481 s/iter. Eval: 0.2644 s/iter. Total: 0.4133 s/iter. ETA=0:02:55
[07/11 12:05:39] d2.evaluation.evaluator INFO: Inference done 342/754. Dataloading: 0.0007 s/iter. Inference: 0.1481 s/iter. Eval: 0.2641 s/iter. Total: 0.4130 s/iter. ETA=0:02:50
[07/11 12:05:44] d2.evaluation.evaluator INFO: Inference done 355/754. Dataloading: 0.0007 s/iter. Inference: 0.1482 s/iter. Eval: 0.2640 s/iter. Total: 0.4130 s/iter. ETA=0:02:44
[07/11 12:05:49] d2.evaluation.evaluator INFO: Inference done 367/754. Dataloading: 0.0007 s/iter. Inference: 0.1484 s/iter. Eval: 0.2640 s/iter. Total: 0.4132 s/iter. ETA=0:02:39
[07/11 12:05:55] d2.evaluation.evaluator INFO: Inference done 380/754. Dataloading: 0.0007 s/iter. Inference: 0.1483 s/iter. Eval: 0.2640 s/iter. Total: 0.4131 s/iter. ETA=0:02:34
[07/11 12:06:00] d2.evaluation.evaluator INFO: Inference done 393/754. Dataloading: 0.0007 s/iter. Inference: 0.1481 s/iter. Eval: 0.2639 s/iter. Total: 0.4128 s/iter. ETA=0:02:29
[07/11 12:06:05] d2.evaluation.evaluator INFO: Inference done 405/754. Dataloading: 0.0007 s/iter. Inference: 0.1481 s/iter. Eval: 0.2639 s/iter. Total: 0.4129 s/iter. ETA=0:02:24
[07/11 12:06:10] d2.evaluation.evaluator INFO: Inference done 417/754. Dataloading: 0.0007 s/iter. Inference: 0.1485 s/iter. Eval: 0.2641 s/iter. Total: 0.4134 s/iter. ETA=0:02:19
[07/11 12:06:15] d2.evaluation.evaluator INFO: Inference done 430/754. Dataloading: 0.0007 s/iter. Inference: 0.1485 s/iter. Eval: 0.2640 s/iter. Total: 0.4133 s/iter. ETA=0:02:13
[07/11 12:06:21] d2.evaluation.evaluator INFO: Inference done 443/754. Dataloading: 0.0007 s/iter. Inference: 0.1484 s/iter. Eval: 0.2638 s/iter. Total: 0.4130 s/iter. ETA=0:02:08
[07/11 12:06:26] d2.evaluation.evaluator INFO: Inference done 455/754. Dataloading: 0.0007 s/iter. Inference: 0.1485 s/iter. Eval: 0.2641 s/iter. Total: 0.4134 s/iter. ETA=0:02:03
[07/11 12:06:31] d2.evaluation.evaluator INFO: Inference done 468/754. Dataloading: 0.0007 s/iter. Inference: 0.1486 s/iter. Eval: 0.2639 s/iter. Total: 0.4133 s/iter. ETA=0:01:58
[07/11 12:06:36] d2.evaluation.evaluator INFO: Inference done 481/754. Dataloading: 0.0007 s/iter. Inference: 0.1485 s/iter. Eval: 0.2640 s/iter. Total: 0.4133 s/iter. ETA=0:01:52
[07/11 12:06:42] d2.evaluation.evaluator INFO: Inference done 493/754. Dataloading: 0.0007 s/iter. Inference: 0.1485 s/iter. Eval: 0.2642 s/iter. Total: 0.4135 s/iter. ETA=0:01:47
[07/11 12:06:47] d2.evaluation.evaluator INFO: Inference done 506/754. Dataloading: 0.0007 s/iter. Inference: 0.1483 s/iter. Eval: 0.2641 s/iter. Total: 0.4132 s/iter. ETA=0:01:42
[07/11 12:06:52] d2.evaluation.evaluator INFO: Inference done 519/754. Dataloading: 0.0007 s/iter. Inference: 0.1483 s/iter. Eval: 0.2641 s/iter. Total: 0.4132 s/iter. ETA=0:01:37
[07/11 12:06:57] d2.evaluation.evaluator INFO: Inference done 531/754. Dataloading: 0.0007 s/iter. Inference: 0.1485 s/iter. Eval: 0.2641 s/iter. Total: 0.4134 s/iter. ETA=0:01:32
[07/11 12:07:03] d2.evaluation.evaluator INFO: Inference done 544/754. Dataloading: 0.0007 s/iter. Inference: 0.1484 s/iter. Eval: 0.2640 s/iter. Total: 0.4133 s/iter. ETA=0:01:26
[07/11 12:07:08] d2.evaluation.evaluator INFO: Inference done 557/754. Dataloading: 0.0007 s/iter. Inference: 0.1484 s/iter. Eval: 0.2639 s/iter. Total: 0.4132 s/iter. ETA=0:01:21
[07/11 12:07:13] d2.evaluation.evaluator INFO: Inference done 570/754. Dataloading: 0.0007 s/iter. Inference: 0.1483 s/iter. Eval: 0.2638 s/iter. Total: 0.4130 s/iter. ETA=0:01:15
[07/11 12:07:18] d2.evaluation.evaluator INFO: Inference done 583/754. Dataloading: 0.0007 s/iter. Inference: 0.1483 s/iter. Eval: 0.2637 s/iter. Total: 0.4129 s/iter. ETA=0:01:10
[07/11 12:07:24] d2.evaluation.evaluator INFO: Inference done 596/754. Dataloading: 0.0007 s/iter. Inference: 0.1482 s/iter. Eval: 0.2636 s/iter. Total: 0.4126 s/iter. ETA=0:01:05
[07/11 12:07:29] d2.evaluation.evaluator INFO: Inference done 609/754. Dataloading: 0.0007 s/iter. Inference: 0.1482 s/iter. Eval: 0.2635 s/iter. Total: 0.4125 s/iter. ETA=0:00:59
[07/11 12:07:34] d2.evaluation.evaluator INFO: Inference done 622/754. Dataloading: 0.0008 s/iter. Inference: 0.1482 s/iter. Eval: 0.2635 s/iter. Total: 0.4126 s/iter. ETA=0:00:54
[07/11 12:07:39] d2.evaluation.evaluator INFO: Inference done 634/754. Dataloading: 0.0008 s/iter. Inference: 0.1482 s/iter. Eval: 0.2636 s/iter. Total: 0.4127 s/iter. ETA=0:00:49
[07/11 12:07:45] d2.evaluation.evaluator INFO: Inference done 647/754. Dataloading: 0.0008 s/iter. Inference: 0.1482 s/iter. Eval: 0.2636 s/iter. Total: 0.4126 s/iter. ETA=0:00:44
[07/11 12:07:50] d2.evaluation.evaluator INFO: Inference done 659/754. Dataloading: 0.0008 s/iter. Inference: 0.1483 s/iter. Eval: 0.2637 s/iter. Total: 0.4128 s/iter. ETA=0:00:39
[07/11 12:07:55] d2.evaluation.evaluator INFO: Inference done 671/754. Dataloading: 0.0008 s/iter. Inference: 0.1483 s/iter. Eval: 0.2637 s/iter. Total: 0.4129 s/iter. ETA=0:00:34
[07/11 12:08:00] d2.evaluation.evaluator INFO: Inference done 683/754. Dataloading: 0.0007 s/iter. Inference: 0.1484 s/iter. Eval: 0.2637 s/iter. Total: 0.4130 s/iter. ETA=0:00:29
[07/11 12:08:05] d2.evaluation.evaluator INFO: Inference done 695/754. Dataloading: 0.0007 s/iter. Inference: 0.1485 s/iter. Eval: 0.2638 s/iter. Total: 0.4131 s/iter. ETA=0:00:24
[07/11 12:08:10] d2.evaluation.evaluator INFO: Inference done 708/754. Dataloading: 0.0007 s/iter. Inference: 0.1484 s/iter. Eval: 0.2637 s/iter. Total: 0.4129 s/iter. ETA=0:00:18
[07/11 12:08:15] d2.evaluation.evaluator INFO: Inference done 720/754. Dataloading: 0.0007 s/iter. Inference: 0.1484 s/iter. Eval: 0.2638 s/iter. Total: 0.4130 s/iter. ETA=0:00:14
[07/11 12:08:20] d2.evaluation.evaluator INFO: Inference done 732/754. Dataloading: 0.0007 s/iter. Inference: 0.1485 s/iter. Eval: 0.2639 s/iter. Total: 0.4132 s/iter. ETA=0:00:09
[07/11 12:08:25] d2.evaluation.evaluator INFO: Inference done 744/754. Dataloading: 0.0007 s/iter. Inference: 0.1486 s/iter. Eval: 0.2638 s/iter. Total: 0.4132 s/iter. ETA=0:00:04
[07/11 12:08:29] d2.evaluation.evaluator INFO: Total inference time: 0:05:09.433357 (0.413129 s / iter per device, on 1 devices)
[07/11 12:08:29] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:51 (0.148464 s / iter per device, on 1 devices)
[07/11 12:08:30] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/11 12:08:30] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/11 12:08:31] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/11 12:08:33] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/11 12:08:33] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 12:08:33] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/11 12:08:41] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 65.902 | 65.902 | 65.902 |  nan  |  nan  | 65.902 |
[07/11 12:08:41] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 12:08:41] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 78.887 | defect     | 52.918 |
[07/11 12:08:41] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_new in csv format:
[07/11 12:08:41] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/11 12:08:41] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 12:08:41] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/11 12:08:41] d2.evaluation.testing INFO: copypaste: Task: segm
[07/11 12:08:41] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 12:08:41] d2.evaluation.testing INFO: copypaste: 65.9023,65.9023,65.9023,nan,nan,65.9023
[07/11 12:08:41] d2.utils.events INFO:  eta: 4:24:15  iter: 4999  total_loss: 5.387  loss_ce: 0.09055  loss_mask: 0.173  loss_dice: 0.2154  loss_ce_0: 0.06256  loss_mask_0: 0.182  loss_dice_0: 0.2191  loss_ce_1: 0.08773  loss_mask_1: 0.1825  loss_dice_1: 0.2283  loss_ce_2: 0.08811  loss_mask_2: 0.178  loss_dice_2: 0.2274  loss_ce_3: 0.08658  loss_mask_3: 0.1763  loss_dice_3: 0.2213  loss_ce_4: 0.08713  loss_mask_4: 0.1712  loss_dice_4: 0.2181  loss_ce_5: 0.08758  loss_mask_5: 0.179  loss_dice_5: 0.2207  loss_ce_6: 0.08803  loss_mask_6: 0.1673  loss_dice_6: 0.2179  loss_ce_7: 0.08827  loss_mask_7: 0.1728  loss_dice_7: 0.2186  loss_ce_8: 0.08865  loss_mask_8: 0.186  loss_dice_8: 0.2155  time: 0.5564  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:08:41] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256to256_instance/model_best.pth
[07/11 12:08:42] d2.engine.hooks INFO: Saved first model at 4.45740 @ 4999 steps
[07/11 12:08:54] d2.utils.events INFO:  eta: 4:24:03  iter: 5019  total_loss: 4.872  loss_ce: 0.1652  loss_mask: 0.1734  loss_dice: 0.1592  loss_ce_0: 0.07476  loss_mask_0: 0.164  loss_dice_0: 0.157  loss_ce_1: 0.1623  loss_mask_1: 0.1673  loss_dice_1: 0.1592  loss_ce_2: 0.1618  loss_mask_2: 0.1704  loss_dice_2: 0.1576  loss_ce_3: 0.1615  loss_mask_3: 0.18  loss_dice_3: 0.1652  loss_ce_4: 0.1622  loss_mask_4: 0.1703  loss_dice_4: 0.1628  loss_ce_5: 0.163  loss_mask_5: 0.1691  loss_dice_5: 0.1623  loss_ce_6: 0.1636  loss_mask_6: 0.1754  loss_dice_6: 0.1605  loss_ce_7: 0.1633  loss_mask_7: 0.1675  loss_dice_7: 0.1596  loss_ce_8: 0.1646  loss_mask_8: 0.1832  loss_dice_8: 0.1673  time: 0.5567  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:09:07] d2.utils.events INFO:  eta: 4:23:46  iter: 5039  total_loss: 5.579  loss_ce: 0.1251  loss_mask: 0.2172  loss_dice: 0.1891  loss_ce_0: 0.07079  loss_mask_0: 0.2196  loss_dice_0: 0.1875  loss_ce_1: 0.1243  loss_mask_1: 0.2243  loss_dice_1: 0.2063  loss_ce_2: 0.1244  loss_mask_2: 0.2214  loss_dice_2: 0.193  loss_ce_3: 0.1243  loss_mask_3: 0.226  loss_dice_3: 0.1915  loss_ce_4: 0.1242  loss_mask_4: 0.2213  loss_dice_4: 0.1904  loss_ce_5: 0.1248  loss_mask_5: 0.2229  loss_dice_5: 0.2018  loss_ce_6: 0.1249  loss_mask_6: 0.2245  loss_dice_6: 0.1963  loss_ce_7: 0.1246  loss_mask_7: 0.2284  loss_dice_7: 0.1884  loss_ce_8: 0.1251  loss_mask_8: 0.2189  loss_dice_8: 0.1966  time: 0.5569  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:09:18] d2.utils.events INFO:  eta: 4:23:26  iter: 5059  total_loss: 5.312  loss_ce: 0.1029  loss_mask: 0.1782  loss_dice: 0.2149  loss_ce_0: 0.06766  loss_mask_0: 0.1757  loss_dice_0: 0.227  loss_ce_1: 0.09707  loss_mask_1: 0.1858  loss_dice_1: 0.2292  loss_ce_2: 0.09808  loss_mask_2: 0.1768  loss_dice_2: 0.2204  loss_ce_3: 0.09555  loss_mask_3: 0.1818  loss_dice_3: 0.2252  loss_ce_4: 0.09625  loss_mask_4: 0.1684  loss_dice_4: 0.2162  loss_ce_5: 0.09737  loss_mask_5: 0.1789  loss_dice_5: 0.2157  loss_ce_6: 0.09815  loss_mask_6: 0.1736  loss_dice_6: 0.2189  loss_ce_7: 0.09814  loss_mask_7: 0.1771  loss_dice_7: 0.2178  loss_ce_8: 0.09968  loss_mask_8: 0.1801  loss_dice_8: 0.2145  time: 0.5570  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:09:30] d2.utils.events INFO:  eta: 4:22:58  iter: 5079  total_loss: 5.465  loss_ce: 0.1795  loss_mask: 0.2162  loss_dice: 0.1833  loss_ce_0: 0.07126  loss_mask_0: 0.2216  loss_dice_0: 0.1851  loss_ce_1: 0.1814  loss_mask_1: 0.226  loss_dice_1: 0.1984  loss_ce_2: 0.1801  loss_mask_2: 0.2258  loss_dice_2: 0.1854  loss_ce_3: 0.1826  loss_mask_3: 0.2164  loss_dice_3: 0.1887  loss_ce_4: 0.1828  loss_mask_4: 0.211  loss_dice_4: 0.1828  loss_ce_5: 0.182  loss_mask_5: 0.217  loss_dice_5: 0.1895  loss_ce_6: 0.1822  loss_mask_6: 0.208  loss_dice_6: 0.1841  loss_ce_7: 0.1823  loss_mask_7: 0.2062  loss_dice_7: 0.1805  loss_ce_8: 0.1824  loss_mask_8: 0.2168  loss_dice_8: 0.1864  time: 0.5571  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:09:41] d2.utils.events INFO:  eta: 4:22:26  iter: 5099  total_loss: 5.022  loss_ce: 0.09844  loss_mask: 0.2083  loss_dice: 0.1756  loss_ce_0: 0.06172  loss_mask_0: 0.1961  loss_dice_0: 0.1821  loss_ce_1: 0.1006  loss_mask_1: 0.2001  loss_dice_1: 0.1836  loss_ce_2: 0.1004  loss_mask_2: 0.2027  loss_dice_2: 0.1846  loss_ce_3: 0.1001  loss_mask_3: 0.1986  loss_dice_3: 0.1899  loss_ce_4: 0.1003  loss_mask_4: 0.2086  loss_dice_4: 0.1822  loss_ce_5: 0.0999  loss_mask_5: 0.1954  loss_dice_5: 0.187  loss_ce_6: 0.09942  loss_mask_6: 0.2047  loss_dice_6: 0.1805  loss_ce_7: 0.09942  loss_mask_7: 0.2015  loss_dice_7: 0.1885  loss_ce_8: 0.09837  loss_mask_8: 0.1986  loss_dice_8: 0.1857  time: 0.5572  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:09:53] d2.utils.events INFO:  eta: 4:21:47  iter: 5119  total_loss: 5.266  loss_ce: 0.1552  loss_mask: 0.21  loss_dice: 0.162  loss_ce_0: 0.07338  loss_mask_0: 0.1975  loss_dice_0: 0.1621  loss_ce_1: 0.153  loss_mask_1: 0.1989  loss_dice_1: 0.1603  loss_ce_2: 0.1532  loss_mask_2: 0.205  loss_dice_2: 0.164  loss_ce_3: 0.1529  loss_mask_3: 0.1928  loss_dice_3: 0.1676  loss_ce_4: 0.1531  loss_mask_4: 0.1966  loss_dice_4: 0.1637  loss_ce_5: 0.1528  loss_mask_5: 0.2061  loss_dice_5: 0.1655  loss_ce_6: 0.1536  loss_mask_6: 0.208  loss_dice_6: 0.1673  loss_ce_7: 0.154  loss_mask_7: 0.2054  loss_dice_7: 0.166  loss_ce_8: 0.1546  loss_mask_8: 0.1922  loss_dice_8: 0.1609  time: 0.5572  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:10:04] d2.utils.events INFO:  eta: 4:21:20  iter: 5139  total_loss: 4.617  loss_ce: 0.1133  loss_mask: 0.193  loss_dice: 0.1742  loss_ce_0: 0.06155  loss_mask_0: 0.1925  loss_dice_0: 0.1764  loss_ce_1: 0.1168  loss_mask_1: 0.1867  loss_dice_1: 0.1664  loss_ce_2: 0.116  loss_mask_2: 0.1901  loss_dice_2: 0.1718  loss_ce_3: 0.1168  loss_mask_3: 0.1893  loss_dice_3: 0.1714  loss_ce_4: 0.1167  loss_mask_4: 0.185  loss_dice_4: 0.1654  loss_ce_5: 0.1166  loss_mask_5: 0.1825  loss_dice_5: 0.1696  loss_ce_6: 0.1157  loss_mask_6: 0.1865  loss_dice_6: 0.1669  loss_ce_7: 0.1154  loss_mask_7: 0.1852  loss_dice_7: 0.1708  loss_ce_8: 0.1145  loss_mask_8: 0.1921  loss_dice_8: 0.1689  time: 0.5573  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:10:16] d2.utils.events INFO:  eta: 4:20:48  iter: 5159  total_loss: 4.812  loss_ce: 0.1154  loss_mask: 0.2064  loss_dice: 0.1409  loss_ce_0: 0.06157  loss_mask_0: 0.2039  loss_dice_0: 0.1366  loss_ce_1: 0.1169  loss_mask_1: 0.2104  loss_dice_1: 0.1387  loss_ce_2: 0.1163  loss_mask_2: 0.2064  loss_dice_2: 0.1339  loss_ce_3: 0.1169  loss_mask_3: 0.2068  loss_dice_3: 0.1426  loss_ce_4: 0.1171  loss_mask_4: 0.2143  loss_dice_4: 0.1349  loss_ce_5: 0.1171  loss_mask_5: 0.2078  loss_dice_5: 0.1421  loss_ce_6: 0.1166  loss_mask_6: 0.218  loss_dice_6: 0.1361  loss_ce_7: 0.1164  loss_mask_7: 0.2137  loss_dice_7: 0.1369  loss_ce_8: 0.1163  loss_mask_8: 0.2041  loss_dice_8: 0.1442  time: 0.5573  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:10:28] d2.utils.events INFO:  eta: 4:20:11  iter: 5179  total_loss: 4.78  loss_ce: 0.1053  loss_mask: 0.2064  loss_dice: 0.1704  loss_ce_0: 0.06019  loss_mask_0: 0.2012  loss_dice_0: 0.1598  loss_ce_1: 0.1034  loss_mask_1: 0.1985  loss_dice_1: 0.1691  loss_ce_2: 0.1034  loss_mask_2: 0.2023  loss_dice_2: 0.1654  loss_ce_3: 0.1033  loss_mask_3: 0.2017  loss_dice_3: 0.1708  loss_ce_4: 0.1038  loss_mask_4: 0.21  loss_dice_4: 0.1678  loss_ce_5: 0.1041  loss_mask_5: 0.2017  loss_dice_5: 0.1639  loss_ce_6: 0.1041  loss_mask_6: 0.2094  loss_dice_6: 0.1703  loss_ce_7: 0.1043  loss_mask_7: 0.2035  loss_dice_7: 0.166  loss_ce_8: 0.1049  loss_mask_8: 0.1964  loss_dice_8: 0.1684  time: 0.5574  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:10:39] d2.utils.events INFO:  eta: 4:19:07  iter: 5199  total_loss: 5.067  loss_ce: 0.08656  loss_mask: 0.1867  loss_dice: 0.1735  loss_ce_0: 0.05689  loss_mask_0: 0.2031  loss_dice_0: 0.1758  loss_ce_1: 0.08301  loss_mask_1: 0.1843  loss_dice_1: 0.1664  loss_ce_2: 0.08352  loss_mask_2: 0.1861  loss_dice_2: 0.1701  loss_ce_3: 0.08271  loss_mask_3: 0.1946  loss_dice_3: 0.1678  loss_ce_4: 0.0833  loss_mask_4: 0.1925  loss_dice_4: 0.1778  loss_ce_5: 0.08364  loss_mask_5: 0.1925  loss_dice_5: 0.1709  loss_ce_6: 0.08397  loss_mask_6: 0.2008  loss_dice_6: 0.1813  loss_ce_7: 0.08433  loss_mask_7: 0.2004  loss_dice_7: 0.1708  loss_ce_8: 0.08519  loss_mask_8: 0.1882  loss_dice_8: 0.176  time: 0.5575  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:10:51] d2.utils.events INFO:  eta: 4:18:26  iter: 5219  total_loss: 4.64  loss_ce: 0.06753  loss_mask: 0.2007  loss_dice: 0.1652  loss_ce_0: 0.05242  loss_mask_0: 0.1909  loss_dice_0: 0.1639  loss_ce_1: 0.06436  loss_mask_1: 0.1897  loss_dice_1: 0.1571  loss_ce_2: 0.0649  loss_mask_2: 0.1816  loss_dice_2: 0.1565  loss_ce_3: 0.06404  loss_mask_3: 0.1972  loss_dice_3: 0.162  loss_ce_4: 0.06441  loss_mask_4: 0.1906  loss_dice_4: 0.1518  loss_ce_5: 0.06453  loss_mask_5: 0.1841  loss_dice_5: 0.1599  loss_ce_6: 0.06506  loss_mask_6: 0.1856  loss_dice_6: 0.1555  loss_ce_7: 0.06546  loss_mask_7: 0.189  loss_dice_7: 0.1589  loss_ce_8: 0.06589  loss_mask_8: 0.1829  loss_dice_8: 0.1613  time: 0.5576  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:11:02] d2.utils.events INFO:  eta: 4:17:42  iter: 5239  total_loss: 5.391  loss_ce: 0.05674  loss_mask: 0.2041  loss_dice: 0.1721  loss_ce_0: 0.0487  loss_mask_0: 0.1944  loss_dice_0: 0.1628  loss_ce_1: 0.05499  loss_mask_1: 0.206  loss_dice_1: 0.167  loss_ce_2: 0.05537  loss_mask_2: 0.2073  loss_dice_2: 0.1733  loss_ce_3: 0.05465  loss_mask_3: 0.1959  loss_dice_3: 0.1692  loss_ce_4: 0.05486  loss_mask_4: 0.1931  loss_dice_4: 0.1605  loss_ce_5: 0.05477  loss_mask_5: 0.2011  loss_dice_5: 0.1648  loss_ce_6: 0.05498  loss_mask_6: 0.2054  loss_dice_6: 0.1669  loss_ce_7: 0.05561  loss_mask_7: 0.1979  loss_dice_7: 0.1727  loss_ce_8: 0.05547  loss_mask_8: 0.2014  loss_dice_8: 0.1683  time: 0.5577  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:11:14] d2.utils.events INFO:  eta: 4:17:09  iter: 5259  total_loss: 5.597  loss_ce: 0.05297  loss_mask: 0.1973  loss_dice: 0.1513  loss_ce_0: 0.04616  loss_mask_0: 0.1906  loss_dice_0: 0.1553  loss_ce_1: 0.0527  loss_mask_1: 0.2006  loss_dice_1: 0.1567  loss_ce_2: 0.05282  loss_mask_2: 0.192  loss_dice_2: 0.1502  loss_ce_3: 0.05249  loss_mask_3: 0.1946  loss_dice_3: 0.1519  loss_ce_4: 0.05249  loss_mask_4: 0.1916  loss_dice_4: 0.1493  loss_ce_5: 0.05246  loss_mask_5: 0.1898  loss_dice_5: 0.1603  loss_ce_6: 0.05233  loss_mask_6: 0.1893  loss_dice_6: 0.1635  loss_ce_7: 0.05279  loss_mask_7: 0.2012  loss_dice_7: 0.1523  loss_ce_8: 0.05232  loss_mask_8: 0.1849  loss_dice_8: 0.1599  time: 0.5578  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:11:25] d2.utils.events INFO:  eta: 4:16:26  iter: 5279  total_loss: 5.415  loss_ce: 0.1386  loss_mask: 0.1799  loss_dice: 0.2079  loss_ce_0: 0.07061  loss_mask_0: 0.1868  loss_dice_0: 0.2104  loss_ce_1: 0.1373  loss_mask_1: 0.1817  loss_dice_1: 0.2094  loss_ce_2: 0.1375  loss_mask_2: 0.1793  loss_dice_2: 0.2177  loss_ce_3: 0.1375  loss_mask_3: 0.1772  loss_dice_3: 0.212  loss_ce_4: 0.1377  loss_mask_4: 0.1859  loss_dice_4: 0.2104  loss_ce_5: 0.1376  loss_mask_5: 0.1854  loss_dice_5: 0.2038  loss_ce_6: 0.1378  loss_mask_6: 0.1857  loss_dice_6: 0.2133  loss_ce_7: 0.1381  loss_mask_7: 0.1798  loss_dice_7: 0.2112  loss_ce_8: 0.1384  loss_mask_8: 0.1797  loss_dice_8: 0.2091  time: 0.5578  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:11:37] d2.utils.events INFO:  eta: 4:15:44  iter: 5299  total_loss: 5.543  loss_ce: 0.1333  loss_mask: 0.169  loss_dice: 0.2027  loss_ce_0: 0.06988  loss_mask_0: 0.1642  loss_dice_0: 0.2103  loss_ce_1: 0.1324  loss_mask_1: 0.1648  loss_dice_1: 0.2035  loss_ce_2: 0.1324  loss_mask_2: 0.1592  loss_dice_2: 0.2047  loss_ce_3: 0.1322  loss_mask_3: 0.1626  loss_dice_3: 0.2034  loss_ce_4: 0.1329  loss_mask_4: 0.1626  loss_dice_4: 0.2058  loss_ce_5: 0.1324  loss_mask_5: 0.1597  loss_dice_5: 0.2012  loss_ce_6: 0.1326  loss_mask_6: 0.167  loss_dice_6: 0.2115  loss_ce_7: 0.1331  loss_mask_7: 0.1688  loss_dice_7: 0.2119  loss_ce_8: 0.1331  loss_mask_8: 0.1608  loss_dice_8: 0.2019  time: 0.5579  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:11:49] d2.utils.events INFO:  eta: 4:15:02  iter: 5319  total_loss: 6.176  loss_ce: 0.096  loss_mask: 0.1592  loss_dice: 0.2542  loss_ce_0: 0.04948  loss_mask_0: 0.1434  loss_dice_0: 0.2641  loss_ce_1: 0.1005  loss_mask_1: 0.1443  loss_dice_1: 0.242  loss_ce_2: 0.09975  loss_mask_2: 0.1548  loss_dice_2: 0.2509  loss_ce_3: 0.1008  loss_mask_3: 0.1494  loss_dice_3: 0.2565  loss_ce_4: 0.1001  loss_mask_4: 0.1377  loss_dice_4: 0.2533  loss_ce_5: 0.1006  loss_mask_5: 0.1565  loss_dice_5: 0.2459  loss_ce_6: 0.09892  loss_mask_6: 0.1482  loss_dice_6: 0.2493  loss_ce_7: 0.09881  loss_mask_7: 0.144  loss_dice_7: 0.2483  loss_ce_8: 0.09762  loss_mask_8: 0.1528  loss_dice_8: 0.2472  time: 0.5580  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:12:00] d2.utils.events INFO:  eta: 4:14:24  iter: 5339  total_loss: 5.351  loss_ce: 0.1357  loss_mask: 0.1678  loss_dice: 0.1743  loss_ce_0: 0.06974  loss_mask_0: 0.1691  loss_dice_0: 0.1767  loss_ce_1: 0.1366  loss_mask_1: 0.1687  loss_dice_1: 0.1794  loss_ce_2: 0.1368  loss_mask_2: 0.1708  loss_dice_2: 0.1739  loss_ce_3: 0.1369  loss_mask_3: 0.1675  loss_dice_3: 0.175  loss_ce_4: 0.1365  loss_mask_4: 0.1626  loss_dice_4: 0.1748  loss_ce_5: 0.1364  loss_mask_5: 0.1682  loss_dice_5: 0.1727  loss_ce_6: 0.1362  loss_mask_6: 0.1666  loss_dice_6: 0.1767  loss_ce_7: 0.1363  loss_mask_7: 0.1758  loss_dice_7: 0.1812  loss_ce_8: 0.1358  loss_mask_8: 0.1627  loss_dice_8: 0.1783  time: 0.5581  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:12:12] d2.utils.events INFO:  eta: 4:13:50  iter: 5359  total_loss: 5.415  loss_ce: 0.08459  loss_mask: 0.2194  loss_dice: 0.2288  loss_ce_0: 0.04909  loss_mask_0: 0.225  loss_dice_0: 0.2223  loss_ce_1: 0.08467  loss_mask_1: 0.2175  loss_dice_1: 0.2235  loss_ce_2: 0.08463  loss_mask_2: 0.2209  loss_dice_2: 0.2295  loss_ce_3: 0.08451  loss_mask_3: 0.2177  loss_dice_3: 0.2262  loss_ce_4: 0.08458  loss_mask_4: 0.2184  loss_dice_4: 0.2302  loss_ce_5: 0.08443  loss_mask_5: 0.2216  loss_dice_5: 0.2255  loss_ce_6: 0.08456  loss_mask_6: 0.2265  loss_dice_6: 0.2248  loss_ce_7: 0.08426  loss_mask_7: 0.2219  loss_dice_7: 0.2267  loss_ce_8: 0.08466  loss_mask_8: 0.2229  loss_dice_8: 0.2332  time: 0.5581  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:12:23] d2.utils.events INFO:  eta: 4:13:14  iter: 5379  total_loss: 5.278  loss_ce: 0.1754  loss_mask: 0.1717  loss_dice: 0.1907  loss_ce_0: 0.08781  loss_mask_0: 0.1738  loss_dice_0: 0.187  loss_ce_1: 0.1721  loss_mask_1: 0.1809  loss_dice_1: 0.1855  loss_ce_2: 0.1725  loss_mask_2: 0.1692  loss_dice_2: 0.1863  loss_ce_3: 0.1728  loss_mask_3: 0.176  loss_dice_3: 0.1792  loss_ce_4: 0.1724  loss_mask_4: 0.1767  loss_dice_4: 0.186  loss_ce_5: 0.1734  loss_mask_5: 0.174  loss_dice_5: 0.1873  loss_ce_6: 0.1738  loss_mask_6: 0.1763  loss_dice_6: 0.1808  loss_ce_7: 0.1739  loss_mask_7: 0.1804  loss_dice_7: 0.1897  loss_ce_8: 0.1747  loss_mask_8: 0.1742  loss_dice_8: 0.1854  time: 0.5582  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:12:35] d2.utils.events INFO:  eta: 4:12:32  iter: 5399  total_loss: 5.302  loss_ce: 0.09201  loss_mask: 0.1935  loss_dice: 0.1945  loss_ce_0: 0.05056  loss_mask_0: 0.2013  loss_dice_0: 0.1892  loss_ce_1: 0.09371  loss_mask_1: 0.195  loss_dice_1: 0.1971  loss_ce_2: 0.09337  loss_mask_2: 0.1991  loss_dice_2: 0.1997  loss_ce_3: 0.09353  loss_mask_3: 0.1986  loss_dice_3: 0.1927  loss_ce_4: 0.09342  loss_mask_4: 0.1857  loss_dice_4: 0.1922  loss_ce_5: 0.09337  loss_mask_5: 0.1948  loss_dice_5: 0.188  loss_ce_6: 0.09308  loss_mask_6: 0.1999  loss_dice_6: 0.1927  loss_ce_7: 0.09258  loss_mask_7: 0.1975  loss_dice_7: 0.1905  loss_ce_8: 0.09244  loss_mask_8: 0.197  loss_dice_8: 0.1882  time: 0.5583  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:12:46] d2.utils.events INFO:  eta: 4:11:43  iter: 5419  total_loss: 4.631  loss_ce: 0.0821  loss_mask: 0.1718  loss_dice: 0.1554  loss_ce_0: 0.04931  loss_mask_0: 0.1708  loss_dice_0: 0.1605  loss_ce_1: 0.08121  loss_mask_1: 0.1663  loss_dice_1: 0.1546  loss_ce_2: 0.08128  loss_mask_2: 0.1694  loss_dice_2: 0.1573  loss_ce_3: 0.08116  loss_mask_3: 0.1636  loss_dice_3: 0.1566  loss_ce_4: 0.08126  loss_mask_4: 0.1695  loss_dice_4: 0.1609  loss_ce_5: 0.08143  loss_mask_5: 0.1636  loss_dice_5: 0.1576  loss_ce_6: 0.08159  loss_mask_6: 0.1654  loss_dice_6: 0.1528  loss_ce_7: 0.08149  loss_mask_7: 0.1637  loss_dice_7: 0.1519  loss_ce_8: 0.08188  loss_mask_8: 0.1664  loss_dice_8: 0.1528  time: 0.5583  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:12:56] d2.utils.events INFO:  eta: 4:10:58  iter: 5439  total_loss: 4.918  loss_ce: 0.1313  loss_mask: 0.1883  loss_dice: 0.1505  loss_ce_0: 0.06845  loss_mask_0: 0.1897  loss_dice_0: 0.1562  loss_ce_1: 0.1307  loss_mask_1: 0.1882  loss_dice_1: 0.1523  loss_ce_2: 0.1308  loss_mask_2: 0.1915  loss_dice_2: 0.1572  loss_ce_3: 0.1307  loss_mask_3: 0.1939  loss_dice_3: 0.1525  loss_ce_4: 0.1308  loss_mask_4: 0.2015  loss_dice_4: 0.1511  loss_ce_5: 0.1309  loss_mask_5: 0.1919  loss_dice_5: 0.1576  loss_ce_6: 0.131  loss_mask_6: 0.1898  loss_dice_6: 0.1551  loss_ce_7: 0.1311  loss_mask_7: 0.196  loss_dice_7: 0.1583  loss_ce_8: 0.1311  loss_mask_8: 0.1971  loss_dice_8: 0.1551  time: 0.5581  data_time: 0.0022  lr: 0.0001  max_mem: 2811M
[07/11 12:13:07] d2.utils.events INFO:  eta: 4:10:27  iter: 5459  total_loss: 5.58  loss_ce: 0.1299  loss_mask: 0.2268  loss_dice: 0.1694  loss_ce_0: 0.06798  loss_mask_0: 0.2277  loss_dice_0: 0.1688  loss_ce_1: 0.1296  loss_mask_1: 0.2181  loss_dice_1: 0.1588  loss_ce_2: 0.1296  loss_mask_2: 0.2278  loss_dice_2: 0.1661  loss_ce_3: 0.1295  loss_mask_3: 0.2263  loss_dice_3: 0.1627  loss_ce_4: 0.1298  loss_mask_4: 0.2169  loss_dice_4: 0.1628  loss_ce_5: 0.1292  loss_mask_5: 0.2277  loss_dice_5: 0.1664  loss_ce_6: 0.1297  loss_mask_6: 0.2255  loss_dice_6: 0.1667  loss_ce_7: 0.1298  loss_mask_7: 0.22  loss_dice_7: 0.168  loss_ce_8: 0.1293  loss_mask_8: 0.2138  loss_dice_8: 0.1671  time: 0.5580  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:13:17] d2.utils.events INFO:  eta: 4:09:30  iter: 5479  total_loss: 5.647  loss_ce: 0.1278  loss_mask: 0.2169  loss_dice: 0.177  loss_ce_0: 0.0672  loss_mask_0: 0.2093  loss_dice_0: 0.1891  loss_ce_1: 0.1278  loss_mask_1: 0.2229  loss_dice_1: 0.1914  loss_ce_2: 0.1278  loss_mask_2: 0.2174  loss_dice_2: 0.1859  loss_ce_3: 0.1278  loss_mask_3: 0.22  loss_dice_3: 0.1839  loss_ce_4: 0.1278  loss_mask_4: 0.2155  loss_dice_4: 0.1903  loss_ce_5: 0.1279  loss_mask_5: 0.2203  loss_dice_5: 0.1941  loss_ce_6: 0.1277  loss_mask_6: 0.2215  loss_dice_6: 0.1889  loss_ce_7: 0.1278  loss_mask_7: 0.2175  loss_dice_7: 0.1831  loss_ce_8: 0.1277  loss_mask_8: 0.2199  loss_dice_8: 0.1933  time: 0.5579  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:13:28] d2.utils.events INFO:  eta: 4:08:35  iter: 5499  total_loss: 4.794  loss_ce: 0.1169  loss_mask: 0.2054  loss_dice: 0.166  loss_ce_0: 0.05616  loss_mask_0: 0.2049  loss_dice_0: 0.1686  loss_ce_1: 0.1203  loss_mask_1: 0.2066  loss_dice_1: 0.1671  loss_ce_2: 0.1195  loss_mask_2: 0.2033  loss_dice_2: 0.1619  loss_ce_3: 0.1203  loss_mask_3: 0.1992  loss_dice_3: 0.1657  loss_ce_4: 0.1203  loss_mask_4: 0.21  loss_dice_4: 0.1649  loss_ce_5: 0.1204  loss_mask_5: 0.2087  loss_dice_5: 0.1683  loss_ce_6: 0.1193  loss_mask_6: 0.2016  loss_dice_6: 0.1689  loss_ce_7: 0.1189  loss_mask_7: 0.1929  loss_dice_7: 0.1633  loss_ce_8: 0.1182  loss_mask_8: 0.2015  loss_dice_8: 0.1656  time: 0.5577  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:13:38] d2.utils.events INFO:  eta: 4:07:45  iter: 5519  total_loss: 5.405  loss_ce: 0.1176  loss_mask: 0.1683  loss_dice: 0.2182  loss_ce_0: 0.05741  loss_mask_0: 0.1704  loss_dice_0: 0.2061  loss_ce_1: 0.1182  loss_mask_1: 0.1685  loss_dice_1: 0.2097  loss_ce_2: 0.1193  loss_mask_2: 0.1681  loss_dice_2: 0.2139  loss_ce_3: 0.1184  loss_mask_3: 0.1724  loss_dice_3: 0.2148  loss_ce_4: 0.1187  loss_mask_4: 0.1673  loss_dice_4: 0.2071  loss_ce_5: 0.1188  loss_mask_5: 0.1627  loss_dice_5: 0.2155  loss_ce_6: 0.1182  loss_mask_6: 0.1683  loss_dice_6: 0.206  loss_ce_7: 0.1181  loss_mask_7: 0.1694  loss_dice_7: 0.2166  loss_ce_8: 0.1181  loss_mask_8: 0.1645  loss_dice_8: 0.2057  time: 0.5576  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:13:48] d2.utils.events INFO:  eta: 4:06:52  iter: 5539  total_loss: 4.966  loss_ce: 0.1397  loss_mask: 0.1445  loss_dice: 0.1827  loss_ce_0: 0.07499  loss_mask_0: 0.1413  loss_dice_0: 0.1777  loss_ce_1: 0.1326  loss_mask_1: 0.1486  loss_dice_1: 0.1831  loss_ce_2: 0.1318  loss_mask_2: 0.1522  loss_dice_2: 0.1811  loss_ce_3: 0.1345  loss_mask_3: 0.1516  loss_dice_3: 0.1762  loss_ce_4: 0.137  loss_mask_4: 0.1397  loss_dice_4: 0.1809  loss_ce_5: 0.138  loss_mask_5: 0.1553  loss_dice_5: 0.1857  loss_ce_6: 0.1383  loss_mask_6: 0.1369  loss_dice_6: 0.1779  loss_ce_7: 0.139  loss_mask_7: 0.1466  loss_dice_7: 0.1831  loss_ce_8: 0.1396  loss_mask_8: 0.1364  loss_dice_8: 0.179  time: 0.5574  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:13:59] d2.utils.events INFO:  eta: 4:05:43  iter: 5559  total_loss: 4.583  loss_ce: 0.1278  loss_mask: 0.157  loss_dice: 0.1546  loss_ce_0: 0.06649  loss_mask_0: 0.1542  loss_dice_0: 0.1427  loss_ce_1: 0.13  loss_mask_1: 0.1537  loss_dice_1: 0.1445  loss_ce_2: 0.13  loss_mask_2: 0.1586  loss_dice_2: 0.146  loss_ce_3: 0.1288  loss_mask_3: 0.1509  loss_dice_3: 0.1477  loss_ce_4: 0.1286  loss_mask_4: 0.1528  loss_dice_4: 0.1443  loss_ce_5: 0.1282  loss_mask_5: 0.1496  loss_dice_5: 0.1432  loss_ce_6: 0.128  loss_mask_6: 0.1541  loss_dice_6: 0.1485  loss_ce_7: 0.128  loss_mask_7: 0.1538  loss_dice_7: 0.1472  loss_ce_8: 0.1279  loss_mask_8: 0.1511  loss_dice_8: 0.1509  time: 0.5573  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:14:09] d2.utils.events INFO:  eta: 4:04:31  iter: 5579  total_loss: 4.925  loss_ce: 0.1221  loss_mask: 0.2186  loss_dice: 0.1388  loss_ce_0: 0.0602  loss_mask_0: 0.2218  loss_dice_0: 0.1361  loss_ce_1: 0.1226  loss_mask_1: 0.2251  loss_dice_1: 0.1364  loss_ce_2: 0.1252  loss_mask_2: 0.2267  loss_dice_2: 0.1334  loss_ce_3: 0.128  loss_mask_3: 0.2211  loss_dice_3: 0.1354  loss_ce_4: 0.1219  loss_mask_4: 0.2131  loss_dice_4: 0.127  loss_ce_5: 0.1223  loss_mask_5: 0.2218  loss_dice_5: 0.135  loss_ce_6: 0.1224  loss_mask_6: 0.226  loss_dice_6: 0.1345  loss_ce_7: 0.1221  loss_mask_7: 0.2117  loss_dice_7: 0.1345  loss_ce_8: 0.1225  loss_mask_8: 0.2151  loss_dice_8: 0.1352  time: 0.5571  data_time: 0.0021  lr: 0.0001  max_mem: 2811M
[07/11 12:14:19] d2.utils.events INFO:  eta: 4:03:41  iter: 5599  total_loss: 4.501  loss_ce: 0.1107  loss_mask: 0.2076  loss_dice: 0.1348  loss_ce_0: 0.05924  loss_mask_0: 0.2099  loss_dice_0: 0.1383  loss_ce_1: 0.1089  loss_mask_1: 0.2107  loss_dice_1: 0.129  loss_ce_2: 0.1099  loss_mask_2: 0.2031  loss_dice_2: 0.1355  loss_ce_3: 0.1056  loss_mask_3: 0.2006  loss_dice_3: 0.1418  loss_ce_4: 0.1077  loss_mask_4: 0.2102  loss_dice_4: 0.1381  loss_ce_5: 0.1085  loss_mask_5: 0.2054  loss_dice_5: 0.1352  loss_ce_6: 0.1091  loss_mask_6: 0.2036  loss_dice_6: 0.139  loss_ce_7: 0.1094  loss_mask_7: 0.2111  loss_dice_7: 0.1378  loss_ce_8: 0.1102  loss_mask_8: 0.2068  loss_dice_8: 0.1436  time: 0.5569  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:14:29] d2.utils.events INFO:  eta: 4:02:39  iter: 5619  total_loss: 5.317  loss_ce: 0.1657  loss_mask: 0.2039  loss_dice: 0.1439  loss_ce_0: 0.07638  loss_mask_0: 0.2095  loss_dice_0: 0.1411  loss_ce_1: 0.1651  loss_mask_1: 0.1892  loss_dice_1: 0.1401  loss_ce_2: 0.17  loss_mask_2: 0.2147  loss_dice_2: 0.147  loss_ce_3: 0.1673  loss_mask_3: 0.2038  loss_dice_3: 0.1467  loss_ce_4: 0.1657  loss_mask_4: 0.2038  loss_dice_4: 0.1445  loss_ce_5: 0.1668  loss_mask_5: 0.2047  loss_dice_5: 0.1396  loss_ce_6: 0.1666  loss_mask_6: 0.1797  loss_dice_6: 0.1422  loss_ce_7: 0.1656  loss_mask_7: 0.1995  loss_dice_7: 0.1441  loss_ce_8: 0.1669  loss_mask_8: 0.2038  loss_dice_8: 0.1465  time: 0.5568  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:14:40] d2.utils.events INFO:  eta: 4:01:15  iter: 5639  total_loss: 4.687  loss_ce: 0.09566  loss_mask: 0.2235  loss_dice: 0.1189  loss_ce_0: 0.05681  loss_mask_0: 0.2163  loss_dice_0: 0.1186  loss_ce_1: 0.09694  loss_mask_1: 0.2129  loss_dice_1: 0.1219  loss_ce_2: 0.09451  loss_mask_2: 0.2099  loss_dice_2: 0.1164  loss_ce_3: 0.09569  loss_mask_3: 0.2123  loss_dice_3: 0.119  loss_ce_4: 0.09534  loss_mask_4: 0.2048  loss_dice_4: 0.1138  loss_ce_5: 0.09533  loss_mask_5: 0.2177  loss_dice_5: 0.1146  loss_ce_6: 0.09533  loss_mask_6: 0.2163  loss_dice_6: 0.1193  loss_ce_7: 0.09529  loss_mask_7: 0.2286  loss_dice_7: 0.1183  loss_ce_8: 0.09479  loss_mask_8: 0.223  loss_dice_8: 0.1174  time: 0.5566  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:14:50] d2.utils.events INFO:  eta: 4:00:02  iter: 5659  total_loss: 5.118  loss_ce: 0.1036  loss_mask: 0.2026  loss_dice: 0.1553  loss_ce_0: 0.05686  loss_mask_0: 0.1896  loss_dice_0: 0.1532  loss_ce_1: 0.1089  loss_mask_1: 0.205  loss_dice_1: 0.1603  loss_ce_2: 0.1092  loss_mask_2: 0.1977  loss_dice_2: 0.1554  loss_ce_3: 0.1077  loss_mask_3: 0.2047  loss_dice_3: 0.1571  loss_ce_4: 0.1063  loss_mask_4: 0.2122  loss_dice_4: 0.1528  loss_ce_5: 0.1057  loss_mask_5: 0.212  loss_dice_5: 0.1539  loss_ce_6: 0.1049  loss_mask_6: 0.1932  loss_dice_6: 0.1577  loss_ce_7: 0.1046  loss_mask_7: 0.1999  loss_dice_7: 0.152  loss_ce_8: 0.1037  loss_mask_8: 0.1995  loss_dice_8: 0.1531  time: 0.5565  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:15:00] d2.utils.events INFO:  eta: 3:58:46  iter: 5679  total_loss: 4.721  loss_ce: 0.1027  loss_mask: 0.1802  loss_dice: 0.1218  loss_ce_0: 0.05655  loss_mask_0: 0.1939  loss_dice_0: 0.1224  loss_ce_1: 0.1048  loss_mask_1: 0.188  loss_dice_1: 0.1316  loss_ce_2: 0.1052  loss_mask_2: 0.1784  loss_dice_2: 0.1311  loss_ce_3: 0.1043  loss_mask_3: 0.1805  loss_dice_3: 0.1286  loss_ce_4: 0.1038  loss_mask_4: 0.1887  loss_dice_4: 0.1335  loss_ce_5: 0.1037  loss_mask_5: 0.188  loss_dice_5: 0.1277  loss_ce_6: 0.1033  loss_mask_6: 0.1884  loss_dice_6: 0.1315  loss_ce_7: 0.1031  loss_mask_7: 0.1905  loss_dice_7: 0.1254  loss_ce_8: 0.1029  loss_mask_8: 0.1789  loss_dice_8: 0.1245  time: 0.5563  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:15:11] d2.utils.events INFO:  eta: 3:58:10  iter: 5699  total_loss: 5.188  loss_ce: 0.1723  loss_mask: 0.1823  loss_dice: 0.1602  loss_ce_0: 0.07989  loss_mask_0: 0.1855  loss_dice_0: 0.1547  loss_ce_1: 0.1742  loss_mask_1: 0.1839  loss_dice_1: 0.1602  loss_ce_2: 0.1738  loss_mask_2: 0.1849  loss_dice_2: 0.1561  loss_ce_3: 0.1738  loss_mask_3: 0.1764  loss_dice_3: 0.1599  loss_ce_4: 0.1735  loss_mask_4: 0.1833  loss_dice_4: 0.1587  loss_ce_5: 0.1731  loss_mask_5: 0.1729  loss_dice_5: 0.1565  loss_ce_6: 0.1729  loss_mask_6: 0.1831  loss_dice_6: 0.155  loss_ce_7: 0.1732  loss_mask_7: 0.1853  loss_dice_7: 0.1575  loss_ce_8: 0.1726  loss_mask_8: 0.178  loss_dice_8: 0.1575  time: 0.5562  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:15:21] d2.utils.events INFO:  eta: 3:57:12  iter: 5719  total_loss: 5.5  loss_ce: 0.09878  loss_mask: 0.259  loss_dice: 0.163  loss_ce_0: 0.05527  loss_mask_0: 0.2488  loss_dice_0: 0.1571  loss_ce_1: 0.101  loss_mask_1: 0.2565  loss_dice_1: 0.1626  loss_ce_2: 0.1015  loss_mask_2: 0.2447  loss_dice_2: 0.159  loss_ce_3: 0.1007  loss_mask_3: 0.2424  loss_dice_3: 0.1658  loss_ce_4: 0.1001  loss_mask_4: 0.2459  loss_dice_4: 0.1638  loss_ce_5: 0.09975  loss_mask_5: 0.2598  loss_dice_5: 0.1632  loss_ce_6: 0.09953  loss_mask_6: 0.2454  loss_dice_6: 0.1631  loss_ce_7: 0.0993  loss_mask_7: 0.2477  loss_dice_7: 0.156  loss_ce_8: 0.09893  loss_mask_8: 0.2446  loss_dice_8: 0.164  time: 0.5561  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:15:32] d2.utils.events INFO:  eta: 3:56:18  iter: 5739  total_loss: 5.256  loss_ce: 0.1524  loss_mask: 0.1835  loss_dice: 0.1672  loss_ce_0: 0.07781  loss_mask_0: 0.1928  loss_dice_0: 0.1638  loss_ce_1: 0.1479  loss_mask_1: 0.1951  loss_dice_1: 0.1766  loss_ce_2: 0.1477  loss_mask_2: 0.1899  loss_dice_2: 0.1728  loss_ce_3: 0.1489  loss_mask_3: 0.183  loss_dice_3: 0.1634  loss_ce_4: 0.1496  loss_mask_4: 0.1933  loss_dice_4: 0.1735  loss_ce_5: 0.15  loss_mask_5: 0.1803  loss_dice_5: 0.161  loss_ce_6: 0.1507  loss_mask_6: 0.1893  loss_dice_6: 0.1683  loss_ce_7: 0.1511  loss_mask_7: 0.1885  loss_dice_7: 0.1687  loss_ce_8: 0.1516  loss_mask_8: 0.1834  loss_dice_8: 0.1752  time: 0.5559  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:15:42] d2.utils.events INFO:  eta: 3:55:12  iter: 5759  total_loss: 4.582  loss_ce: 0.126  loss_mask: 0.1692  loss_dice: 0.1389  loss_ce_0: 0.06611  loss_mask_0: 0.1808  loss_dice_0: 0.1403  loss_ce_1: 0.1249  loss_mask_1: 0.1756  loss_dice_1: 0.134  loss_ce_2: 0.1251  loss_mask_2: 0.1833  loss_dice_2: 0.1353  loss_ce_3: 0.124  loss_mask_3: 0.1885  loss_dice_3: 0.1405  loss_ce_4: 0.124  loss_mask_4: 0.1726  loss_dice_4: 0.1368  loss_ce_5: 0.1241  loss_mask_5: 0.1827  loss_dice_5: 0.1374  loss_ce_6: 0.1255  loss_mask_6: 0.1762  loss_dice_6: 0.1458  loss_ce_7: 0.1254  loss_mask_7: 0.1806  loss_dice_7: 0.1428  loss_ce_8: 0.1256  loss_mask_8: 0.1849  loss_dice_8: 0.1391  time: 0.5558  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:15:52] d2.utils.events INFO:  eta: 3:53:50  iter: 5779  total_loss: 5.78  loss_ce: 0.135  loss_mask: 0.2229  loss_dice: 0.2209  loss_ce_0: 0.06176  loss_mask_0: 0.2152  loss_dice_0: 0.2033  loss_ce_1: 0.1377  loss_mask_1: 0.2181  loss_dice_1: 0.2017  loss_ce_2: 0.1372  loss_mask_2: 0.2361  loss_dice_2: 0.2052  loss_ce_3: 0.1369  loss_mask_3: 0.2094  loss_dice_3: 0.2248  loss_ce_4: 0.1374  loss_mask_4: 0.2201  loss_dice_4: 0.2179  loss_ce_5: 0.1378  loss_mask_5: 0.2304  loss_dice_5: 0.2156  loss_ce_6: 0.1369  loss_mask_6: 0.2269  loss_dice_6: 0.2134  loss_ce_7: 0.1368  loss_mask_7: 0.2248  loss_dice_7: 0.2029  loss_ce_8: 0.1366  loss_mask_8: 0.2205  loss_dice_8: 0.219  time: 0.5557  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:16:03] d2.utils.events INFO:  eta: 3:52:55  iter: 5799  total_loss: 5.128  loss_ce: 0.1262  loss_mask: 0.1935  loss_dice: 0.1501  loss_ce_0: 0.06954  loss_mask_0: 0.1884  loss_dice_0: 0.152  loss_ce_1: 0.128  loss_mask_1: 0.1902  loss_dice_1: 0.1569  loss_ce_2: 0.1287  loss_mask_2: 0.1814  loss_dice_2: 0.1506  loss_ce_3: 0.1284  loss_mask_3: 0.1888  loss_dice_3: 0.1627  loss_ce_4: 0.127  loss_mask_4: 0.199  loss_dice_4: 0.1602  loss_ce_5: 0.1267  loss_mask_5: 0.1906  loss_dice_5: 0.159  loss_ce_6: 0.1264  loss_mask_6: 0.1889  loss_dice_6: 0.1599  loss_ce_7: 0.1264  loss_mask_7: 0.1866  loss_dice_7: 0.149  loss_ce_8: 0.1256  loss_mask_8: 0.195  loss_dice_8: 0.1502  time: 0.5555  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:16:13] d2.utils.events INFO:  eta: 3:51:29  iter: 5819  total_loss: 5.169  loss_ce: 0.1259  loss_mask: 0.1546  loss_dice: 0.2003  loss_ce_0: 0.06798  loss_mask_0: 0.1607  loss_dice_0: 0.195  loss_ce_1: 0.1275  loss_mask_1: 0.1577  loss_dice_1: 0.1993  loss_ce_2: 0.1278  loss_mask_2: 0.1576  loss_dice_2: 0.1952  loss_ce_3: 0.1275  loss_mask_3: 0.163  loss_dice_3: 0.1965  loss_ce_4: 0.1265  loss_mask_4: 0.1513  loss_dice_4: 0.1924  loss_ce_5: 0.1264  loss_mask_5: 0.1502  loss_dice_5: 0.187  loss_ce_6: 0.1263  loss_mask_6: 0.1565  loss_dice_6: 0.1931  loss_ce_7: 0.1262  loss_mask_7: 0.1585  loss_dice_7: 0.1942  loss_ce_8: 0.1257  loss_mask_8: 0.1598  loss_dice_8: 0.1942  time: 0.5554  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:16:24] d2.utils.events INFO:  eta: 3:50:08  iter: 5839  total_loss: 4.74  loss_ce: 0.1286  loss_mask: 0.1563  loss_dice: 0.1844  loss_ce_0: 0.06488  loss_mask_0: 0.1623  loss_dice_0: 0.1796  loss_ce_1: 0.1279  loss_mask_1: 0.1638  loss_dice_1: 0.1845  loss_ce_2: 0.1279  loss_mask_2: 0.1593  loss_dice_2: 0.1924  loss_ce_3: 0.1279  loss_mask_3: 0.1676  loss_dice_3: 0.189  loss_ce_4: 0.1283  loss_mask_4: 0.159  loss_dice_4: 0.1849  loss_ce_5: 0.1282  loss_mask_5: 0.1627  loss_dice_5: 0.1865  loss_ce_6: 0.1283  loss_mask_6: 0.1648  loss_dice_6: 0.1849  loss_ce_7: 0.1283  loss_mask_7: 0.1627  loss_dice_7: 0.1865  loss_ce_8: 0.1285  loss_mask_8: 0.1577  loss_dice_8: 0.1824  time: 0.5553  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:16:34] d2.utils.events INFO:  eta: 3:48:55  iter: 5859  total_loss: 4.352  loss_ce: 0.128  loss_mask: 0.1638  loss_dice: 0.1286  loss_ce_0: 0.06488  loss_mask_0: 0.1571  loss_dice_0: 0.1327  loss_ce_1: 0.1284  loss_mask_1: 0.1674  loss_dice_1: 0.1294  loss_ce_2: 0.1284  loss_mask_2: 0.1572  loss_dice_2: 0.1279  loss_ce_3: 0.1284  loss_mask_3: 0.1655  loss_dice_3: 0.1313  loss_ce_4: 0.1283  loss_mask_4: 0.1555  loss_dice_4: 0.1274  loss_ce_5: 0.1283  loss_mask_5: 0.1561  loss_dice_5: 0.1287  loss_ce_6: 0.1281  loss_mask_6: 0.1604  loss_dice_6: 0.1277  loss_ce_7: 0.1281  loss_mask_7: 0.1558  loss_dice_7: 0.1303  loss_ce_8: 0.128  loss_mask_8: 0.1575  loss_dice_8: 0.1332  time: 0.5552  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:16:45] d2.utils.events INFO:  eta: 3:48:02  iter: 5879  total_loss: 4.585  loss_ce: 0.1145  loss_mask: 0.1889  loss_dice: 0.143  loss_ce_0: 0.06277  loss_mask_0: 0.1855  loss_dice_0: 0.1414  loss_ce_1: 0.1117  loss_mask_1: 0.1844  loss_dice_1: 0.1414  loss_ce_2: 0.1113  loss_mask_2: 0.1811  loss_dice_2: 0.1426  loss_ce_3: 0.1117  loss_mask_3: 0.1827  loss_dice_3: 0.1446  loss_ce_4: 0.1121  loss_mask_4: 0.1843  loss_dice_4: 0.145  loss_ce_5: 0.1123  loss_mask_5: 0.1853  loss_dice_5: 0.142  loss_ce_6: 0.1131  loss_mask_6: 0.1832  loss_dice_6: 0.1423  loss_ce_7: 0.113  loss_mask_7: 0.1819  loss_dice_7: 0.1388  loss_ce_8: 0.1136  loss_mask_8: 0.1799  loss_dice_8: 0.1457  time: 0.5551  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:16:55] d2.utils.events INFO:  eta: 3:46:25  iter: 5899  total_loss: 5.159  loss_ce: 0.09921  loss_mask: 0.2434  loss_dice: 0.1651  loss_ce_0: 0.06002  loss_mask_0: 0.2402  loss_dice_0: 0.1643  loss_ce_1: 0.09718  loss_mask_1: 0.2393  loss_dice_1: 0.1647  loss_ce_2: 0.09685  loss_mask_2: 0.2391  loss_dice_2: 0.1664  loss_ce_3: 0.09708  loss_mask_3: 0.2332  loss_dice_3: 0.1633  loss_ce_4: 0.09697  loss_mask_4: 0.2419  loss_dice_4: 0.1696  loss_ce_5: 0.09699  loss_mask_5: 0.2341  loss_dice_5: 0.1695  loss_ce_6: 0.09747  loss_mask_6: 0.2436  loss_dice_6: 0.175  loss_ce_7: 0.09758  loss_mask_7: 0.2371  loss_dice_7: 0.1651  loss_ce_8: 0.09769  loss_mask_8: 0.2297  loss_dice_8: 0.1691  time: 0.5549  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:17:05] d2.utils.events INFO:  eta: 3:45:04  iter: 5919  total_loss: 4.921  loss_ce: 0.1292  loss_mask: 0.2253  loss_dice: 0.137  loss_ce_0: 0.06621  loss_mask_0: 0.2264  loss_dice_0: 0.146  loss_ce_1: 0.1291  loss_mask_1: 0.2181  loss_dice_1: 0.1377  loss_ce_2: 0.129  loss_mask_2: 0.2281  loss_dice_2: 0.1384  loss_ce_3: 0.1291  loss_mask_3: 0.2254  loss_dice_3: 0.1429  loss_ce_4: 0.1291  loss_mask_4: 0.2236  loss_dice_4: 0.1336  loss_ce_5: 0.1292  loss_mask_5: 0.232  loss_dice_5: 0.1402  loss_ce_6: 0.1293  loss_mask_6: 0.2273  loss_dice_6: 0.1432  loss_ce_7: 0.1291  loss_mask_7: 0.2233  loss_dice_7: 0.1337  loss_ce_8: 0.1292  loss_mask_8: 0.225  loss_dice_8: 0.1402  time: 0.5548  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:17:15] d2.utils.events INFO:  eta: 3:44:01  iter: 5939  total_loss: 4.881  loss_ce: 0.1399  loss_mask: 0.207  loss_dice: 0.1747  loss_ce_0: 0.07208  loss_mask_0: 0.209  loss_dice_0: 0.176  loss_ce_1: 0.1364  loss_mask_1: 0.2057  loss_dice_1: 0.172  loss_ce_2: 0.1366  loss_mask_2: 0.2045  loss_dice_2: 0.1749  loss_ce_3: 0.1369  loss_mask_3: 0.2152  loss_dice_3: 0.1758  loss_ce_4: 0.1372  loss_mask_4: 0.2096  loss_dice_4: 0.1715  loss_ce_5: 0.1372  loss_mask_5: 0.2099  loss_dice_5: 0.1683  loss_ce_6: 0.1382  loss_mask_6: 0.209  loss_dice_6: 0.1742  loss_ce_7: 0.1385  loss_mask_7: 0.209  loss_dice_7: 0.1744  loss_ce_8: 0.139  loss_mask_8: 0.2054  loss_dice_8: 0.1789  time: 0.5546  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:17:27] d2.utils.events INFO:  eta: 3:42:51  iter: 5959  total_loss: 5.154  loss_ce: 0.1251  loss_mask: 0.2082  loss_dice: 0.1748  loss_ce_0: 0.06142  loss_mask_0: 0.2  loss_dice_0: 0.1782  loss_ce_1: 0.127  loss_mask_1: 0.1946  loss_dice_1: 0.1771  loss_ce_2: 0.127  loss_mask_2: 0.192  loss_dice_2: 0.1778  loss_ce_3: 0.127  loss_mask_3: 0.2053  loss_dice_3: 0.1659  loss_ce_4: 0.1269  loss_mask_4: 0.2039  loss_dice_4: 0.1748  loss_ce_5: 0.127  loss_mask_5: 0.2078  loss_dice_5: 0.1689  loss_ce_6: 0.1269  loss_mask_6: 0.2054  loss_dice_6: 0.1852  loss_ce_7: 0.1267  loss_mask_7: 0.198  loss_dice_7: 0.1828  loss_ce_8: 0.1263  loss_mask_8: 0.2031  loss_dice_8: 0.1815  time: 0.5547  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:17:38] d2.utils.events INFO:  eta: 3:41:44  iter: 5979  total_loss: 5.105  loss_ce: 0.1083  loss_mask: 0.1484  loss_dice: 0.2037  loss_ce_0: 0.05928  loss_mask_0: 0.1515  loss_dice_0: 0.196  loss_ce_1: 0.105  loss_mask_1: 0.1556  loss_dice_1: 0.1968  loss_ce_2: 0.1049  loss_mask_2: 0.152  loss_dice_2: 0.2135  loss_ce_3: 0.1052  loss_mask_3: 0.1499  loss_dice_3: 0.191  loss_ce_4: 0.1059  loss_mask_4: 0.1489  loss_dice_4: 0.1942  loss_ce_5: 0.1063  loss_mask_5: 0.1518  loss_dice_5: 0.1977  loss_ce_6: 0.1069  loss_mask_6: 0.155  loss_dice_6: 0.1979  loss_ce_7: 0.1071  loss_mask_7: 0.1477  loss_dice_7: 0.199  loss_ce_8: 0.1081  loss_mask_8: 0.1564  loss_dice_8: 0.2045  time: 0.5548  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:17:50] d2.utils.events INFO:  eta: 3:40:34  iter: 5999  total_loss: 5.287  loss_ce: 0.1616  loss_mask: 0.184  loss_dice: 0.2113  loss_ce_0: 0.07558  loss_mask_0: 0.1786  loss_dice_0: 0.2282  loss_ce_1: 0.161  loss_mask_1: 0.1793  loss_dice_1: 0.2242  loss_ce_2: 0.1612  loss_mask_2: 0.1797  loss_dice_2: 0.2133  loss_ce_3: 0.1614  loss_mask_3: 0.1959  loss_dice_3: 0.2086  loss_ce_4: 0.1621  loss_mask_4: 0.1745  loss_dice_4: 0.2215  loss_ce_5: 0.1623  loss_mask_5: 0.1791  loss_dice_5: 0.2311  loss_ce_6: 0.1618  loss_mask_6: 0.185  loss_dice_6: 0.2072  loss_ce_7: 0.1621  loss_mask_7: 0.1816  loss_dice_7: 0.2161  loss_ce_8: 0.1621  loss_mask_8: 0.1652  loss_dice_8: 0.2243  time: 0.5549  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:18:02] d2.utils.events INFO:  eta: 3:39:15  iter: 6019  total_loss: 4.933  loss_ce: 0.1122  loss_mask: 0.1964  loss_dice: 0.1606  loss_ce_0: 0.05904  loss_mask_0: 0.1986  loss_dice_0: 0.1578  loss_ce_1: 0.1155  loss_mask_1: 0.2015  loss_dice_1: 0.1656  loss_ce_2: 0.1155  loss_mask_2: 0.197  loss_dice_2: 0.1555  loss_ce_3: 0.1153  loss_mask_3: 0.2139  loss_dice_3: 0.1605  loss_ce_4: 0.1145  loss_mask_4: 0.1982  loss_dice_4: 0.163  loss_ce_5: 0.1145  loss_mask_5: 0.2068  loss_dice_5: 0.1634  loss_ce_6: 0.1138  loss_mask_6: 0.2095  loss_dice_6: 0.1624  loss_ce_7: 0.1133  loss_mask_7: 0.2009  loss_dice_7: 0.1608  loss_ce_8: 0.1127  loss_mask_8: 0.1998  loss_dice_8: 0.167  time: 0.5550  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:18:13] d2.utils.events INFO:  eta: 3:38:42  iter: 6039  total_loss: 4.772  loss_ce: 0.1075  loss_mask: 0.2056  loss_dice: 0.1584  loss_ce_0: 0.05821  loss_mask_0: 0.2105  loss_dice_0: 0.1625  loss_ce_1: 0.1082  loss_mask_1: 0.2034  loss_dice_1: 0.1573  loss_ce_2: 0.1082  loss_mask_2: 0.1959  loss_dice_2: 0.1616  loss_ce_3: 0.1082  loss_mask_3: 0.2024  loss_dice_3: 0.1626  loss_ce_4: 0.108  loss_mask_4: 0.2077  loss_dice_4: 0.1652  loss_ce_5: 0.1082  loss_mask_5: 0.1994  loss_dice_5: 0.1653  loss_ce_6: 0.1078  loss_mask_6: 0.2063  loss_dice_6: 0.1571  loss_ce_7: 0.1078  loss_mask_7: 0.2068  loss_dice_7: 0.1569  loss_ce_8: 0.1077  loss_mask_8: 0.2086  loss_dice_8: 0.156  time: 0.5551  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:18:25] d2.utils.events INFO:  eta: 3:38:10  iter: 6059  total_loss: 4.717  loss_ce: 0.09655  loss_mask: 0.2279  loss_dice: 0.1466  loss_ce_0: 0.05625  loss_mask_0: 0.2237  loss_dice_0: 0.1466  loss_ce_1: 0.09418  loss_mask_1: 0.2285  loss_dice_1: 0.1477  loss_ce_2: 0.094  loss_mask_2: 0.2317  loss_dice_2: 0.1453  loss_ce_3: 0.09424  loss_mask_3: 0.2349  loss_dice_3: 0.1513  loss_ce_4: 0.09475  loss_mask_4: 0.2272  loss_dice_4: 0.1442  loss_ce_5: 0.09502  loss_mask_5: 0.2346  loss_dice_5: 0.1523  loss_ce_6: 0.09528  loss_mask_6: 0.2307  loss_dice_6: 0.1518  loss_ce_7: 0.0956  loss_mask_7: 0.2379  loss_dice_7: 0.1491  loss_ce_8: 0.09608  loss_mask_8: 0.2204  loss_dice_8: 0.1473  time: 0.5551  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:18:36] d2.utils.events INFO:  eta: 3:37:15  iter: 6079  total_loss: 4.776  loss_ce: 0.17  loss_mask: 0.1844  loss_dice: 0.1408  loss_ce_0: 0.07874  loss_mask_0: 0.1918  loss_dice_0: 0.1423  loss_ce_1: 0.1696  loss_mask_1: 0.1951  loss_dice_1: 0.1476  loss_ce_2: 0.1693  loss_mask_2: 0.1839  loss_dice_2: 0.1418  loss_ce_3: 0.1693  loss_mask_3: 0.1836  loss_dice_3: 0.1525  loss_ce_4: 0.1698  loss_mask_4: 0.191  loss_dice_4: 0.142  loss_ce_5: 0.1697  loss_mask_5: 0.1755  loss_dice_5: 0.1408  loss_ce_6: 0.17  loss_mask_6: 0.1831  loss_dice_6: 0.1411  loss_ce_7: 0.1698  loss_mask_7: 0.1885  loss_dice_7: 0.1437  loss_ce_8: 0.1704  loss_mask_8: 0.1826  loss_dice_8: 0.1447  time: 0.5552  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:18:48] d2.utils.events INFO:  eta: 3:36:05  iter: 6099  total_loss: 4.602  loss_ce: 0.09952  loss_mask: 0.1898  loss_dice: 0.1478  loss_ce_0: 0.05551  loss_mask_0: 0.2017  loss_dice_0: 0.1557  loss_ce_1: 0.1014  loss_mask_1: 0.1878  loss_dice_1: 0.1495  loss_ce_2: 0.1018  loss_mask_2: 0.1886  loss_dice_2: 0.1607  loss_ce_3: 0.1016  loss_mask_3: 0.1939  loss_dice_3: 0.1475  loss_ce_4: 0.101  loss_mask_4: 0.194  loss_dice_4: 0.1559  loss_ce_5: 0.1013  loss_mask_5: 0.1932  loss_dice_5: 0.1534  loss_ce_6: 0.1003  loss_mask_6: 0.1972  loss_dice_6: 0.1547  loss_ce_7: 0.1003  loss_mask_7: 0.1988  loss_dice_7: 0.1511  loss_ce_8: 0.1  loss_mask_8: 0.1919  loss_dice_8: 0.1463  time: 0.5553  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:19:00] d2.utils.events INFO:  eta: 3:35:09  iter: 6119  total_loss: 4.893  loss_ce: 0.08843  loss_mask: 0.2039  loss_dice: 0.1514  loss_ce_0: 0.05329  loss_mask_0: 0.2144  loss_dice_0: 0.1464  loss_ce_1: 0.0874  loss_mask_1: 0.1986  loss_dice_1: 0.146  loss_ce_2: 0.08703  loss_mask_2: 0.2053  loss_dice_2: 0.1418  loss_ce_3: 0.08719  loss_mask_3: 0.2024  loss_dice_3: 0.1428  loss_ce_4: 0.08756  loss_mask_4: 0.2049  loss_dice_4: 0.1514  loss_ce_5: 0.08758  loss_mask_5: 0.2041  loss_dice_5: 0.1446  loss_ce_6: 0.08785  loss_mask_6: 0.2103  loss_dice_6: 0.1454  loss_ce_7: 0.08805  loss_mask_7: 0.2014  loss_dice_7: 0.1404  loss_ce_8: 0.08823  loss_mask_8: 0.2076  loss_dice_8: 0.1495  time: 0.5554  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:19:11] d2.utils.events INFO:  eta: 3:34:39  iter: 6139  total_loss: 5.291  loss_ce: 0.07478  loss_mask: 0.2229  loss_dice: 0.1775  loss_ce_0: 0.0501  loss_mask_0: 0.2146  loss_dice_0: 0.1828  loss_ce_1: 0.07388  loss_mask_1: 0.2254  loss_dice_1: 0.1798  loss_ce_2: 0.07381  loss_mask_2: 0.2287  loss_dice_2: 0.178  loss_ce_3: 0.07366  loss_mask_3: 0.2284  loss_dice_3: 0.1756  loss_ce_4: 0.07388  loss_mask_4: 0.2271  loss_dice_4: 0.1766  loss_ce_5: 0.07386  loss_mask_5: 0.2272  loss_dice_5: 0.1822  loss_ce_6: 0.07425  loss_mask_6: 0.2251  loss_dice_6: 0.1877  loss_ce_7: 0.07445  loss_mask_7: 0.2179  loss_dice_7: 0.1795  loss_ce_8: 0.07444  loss_mask_8: 0.2225  loss_dice_8: 0.1804  time: 0.5555  data_time: 0.0020  lr: 0.0001  max_mem: 2811M
[07/11 12:19:23] d2.utils.events INFO:  eta: 3:33:56  iter: 6159  total_loss: 4.725  loss_ce: 0.0725  loss_mask: 0.214  loss_dice: 0.1674  loss_ce_0: 0.04853  loss_mask_0: 0.2069  loss_dice_0: 0.1646  loss_ce_1: 0.07225  loss_mask_1: 0.2099  loss_dice_1: 0.1673  loss_ce_2: 0.07199  loss_mask_2: 0.2071  loss_dice_2: 0.1726  loss_ce_3: 0.07196  loss_mask_3: 0.2014  loss_dice_3: 0.179  loss_ce_4: 0.07191  loss_mask_4: 0.2086  loss_dice_4: 0.1715  loss_ce_5: 0.0718  loss_mask_5: 0.2075  loss_dice_5: 0.1686  loss_ce_6: 0.07201  loss_mask_6: 0.2071  loss_dice_6: 0.1654  loss_ce_7: 0.07218  loss_mask_7: 0.2053  loss_dice_7: 0.1685  loss_ce_8: 0.07204  loss_mask_8: 0.2041  loss_dice_8: 0.1717  time: 0.5556  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:19:35] d2.utils.events INFO:  eta: 3:32:58  iter: 6179  total_loss: 4.888  loss_ce: 0.06421  loss_mask: 0.1542  loss_dice: 0.2013  loss_ce_0: 0.04538  loss_mask_0: 0.1484  loss_dice_0: 0.2032  loss_ce_1: 0.06392  loss_mask_1: 0.1506  loss_dice_1: 0.1986  loss_ce_2: 0.06366  loss_mask_2: 0.1393  loss_dice_2: 0.2011  loss_ce_3: 0.06385  loss_mask_3: 0.1482  loss_dice_3: 0.1982  loss_ce_4: 0.06367  loss_mask_4: 0.151  loss_dice_4: 0.2045  loss_ce_5: 0.06381  loss_mask_5: 0.1455  loss_dice_5: 0.1987  loss_ce_6: 0.06381  loss_mask_6: 0.1468  loss_dice_6: 0.2028  loss_ce_7: 0.06405  loss_mask_7: 0.1497  loss_dice_7: 0.1963  loss_ce_8: 0.06389  loss_mask_8: 0.1534  loss_dice_8: 0.1956  time: 0.5556  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:19:46] d2.utils.events INFO:  eta: 3:32:17  iter: 6199  total_loss: 5.266  loss_ce: 0.06095  loss_mask: 0.1643  loss_dice: 0.1609  loss_ce_0: 0.04401  loss_mask_0: 0.1679  loss_dice_0: 0.1595  loss_ce_1: 0.06089  loss_mask_1: 0.1677  loss_dice_1: 0.1563  loss_ce_2: 0.06027  loss_mask_2: 0.1695  loss_dice_2: 0.1613  loss_ce_3: 0.06056  loss_mask_3: 0.1723  loss_dice_3: 0.1583  loss_ce_4: 0.0613  loss_mask_4: 0.1546  loss_dice_4: 0.1546  loss_ce_5: 0.06064  loss_mask_5: 0.1693  loss_dice_5: 0.1593  loss_ce_6: 0.06049  loss_mask_6: 0.1684  loss_dice_6: 0.1541  loss_ce_7: 0.06141  loss_mask_7: 0.172  loss_dice_7: 0.1585  loss_ce_8: 0.06072  loss_mask_8: 0.1615  loss_dice_8: 0.1588  time: 0.5557  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:19:58] d2.utils.events INFO:  eta: 3:31:34  iter: 6219  total_loss: 4.792  loss_ce: 0.07321  loss_mask: 0.2209  loss_dice: 0.1569  loss_ce_0: 0.04441  loss_mask_0: 0.2179  loss_dice_0: 0.1611  loss_ce_1: 0.07439  loss_mask_1: 0.2151  loss_dice_1: 0.1564  loss_ce_2: 0.07475  loss_mask_2: 0.2141  loss_dice_2: 0.1554  loss_ce_3: 0.07441  loss_mask_3: 0.2187  loss_dice_3: 0.1527  loss_ce_4: 0.07426  loss_mask_4: 0.2202  loss_dice_4: 0.1562  loss_ce_5: 0.07419  loss_mask_5: 0.2181  loss_dice_5: 0.1591  loss_ce_6: 0.07389  loss_mask_6: 0.2213  loss_dice_6: 0.1649  loss_ce_7: 0.07365  loss_mask_7: 0.2153  loss_dice_7: 0.16  loss_ce_8: 0.07336  loss_mask_8: 0.2211  loss_dice_8: 0.1597  time: 0.5558  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:20:09] d2.utils.events INFO:  eta: 3:31:18  iter: 6239  total_loss: 4.553  loss_ce: 0.1372  loss_mask: 0.1656  loss_dice: 0.1473  loss_ce_0: 0.0696  loss_mask_0: 0.1761  loss_dice_0: 0.1541  loss_ce_1: 0.1364  loss_mask_1: 0.1655  loss_dice_1: 0.1346  loss_ce_2: 0.1367  loss_mask_2: 0.1688  loss_dice_2: 0.1495  loss_ce_3: 0.1368  loss_mask_3: 0.1634  loss_dice_3: 0.1376  loss_ce_4: 0.1368  loss_mask_4: 0.1655  loss_dice_4: 0.1433  loss_ce_5: 0.1368  loss_mask_5: 0.1669  loss_dice_5: 0.1477  loss_ce_6: 0.137  loss_mask_6: 0.1732  loss_dice_6: 0.1467  loss_ce_7: 0.137  loss_mask_7: 0.1652  loss_dice_7: 0.1449  loss_ce_8: 0.137  loss_mask_8: 0.1652  loss_dice_8: 0.1545  time: 0.5558  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:20:21] d2.utils.events INFO:  eta: 3:30:55  iter: 6259  total_loss: 4.747  loss_ce: 0.1331  loss_mask: 0.1936  loss_dice: 0.1282  loss_ce_0: 0.06867  loss_mask_0: 0.1903  loss_dice_0: 0.1338  loss_ce_1: 0.1325  loss_mask_1: 0.1868  loss_dice_1: 0.1322  loss_ce_2: 0.1325  loss_mask_2: 0.188  loss_dice_2: 0.1317  loss_ce_3: 0.1327  loss_mask_3: 0.1827  loss_dice_3: 0.1311  loss_ce_4: 0.1327  loss_mask_4: 0.1915  loss_dice_4: 0.1313  loss_ce_5: 0.1326  loss_mask_5: 0.191  loss_dice_5: 0.1311  loss_ce_6: 0.1329  loss_mask_6: 0.1857  loss_dice_6: 0.1292  loss_ce_7: 0.1329  loss_mask_7: 0.1902  loss_dice_7: 0.1317  loss_ce_8: 0.1329  loss_mask_8: 0.1982  loss_dice_8: 0.1378  time: 0.5559  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:20:33] d2.utils.events INFO:  eta: 3:30:08  iter: 6279  total_loss: 5.456  loss_ce: 0.1003  loss_mask: 0.1811  loss_dice: 0.1866  loss_ce_0: 0.04996  loss_mask_0: 0.1802  loss_dice_0: 0.1887  loss_ce_1: 0.1028  loss_mask_1: 0.1871  loss_dice_1: 0.1939  loss_ce_2: 0.1028  loss_mask_2: 0.1744  loss_dice_2: 0.188  loss_ce_3: 0.1027  loss_mask_3: 0.1851  loss_dice_3: 0.1944  loss_ce_4: 0.1026  loss_mask_4: 0.1863  loss_dice_4: 0.1794  loss_ce_5: 0.1028  loss_mask_5: 0.1791  loss_dice_5: 0.2003  loss_ce_6: 0.1017  loss_mask_6: 0.1876  loss_dice_6: 0.1891  loss_ce_7: 0.1014  loss_mask_7: 0.1849  loss_dice_7: 0.1893  loss_ce_8: 0.101  loss_mask_8: 0.1829  loss_dice_8: 0.1944  time: 0.5560  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:20:45] d2.utils.events INFO:  eta: 3:29:23  iter: 6299  total_loss: 5.191  loss_ce: 0.13  loss_mask: 0.2151  loss_dice: 0.1667  loss_ce_0: 0.06769  loss_mask_0: 0.2116  loss_dice_0: 0.1662  loss_ce_1: 0.1298  loss_mask_1: 0.2224  loss_dice_1: 0.1719  loss_ce_2: 0.1297  loss_mask_2: 0.2162  loss_dice_2: 0.1682  loss_ce_3: 0.1296  loss_mask_3: 0.2221  loss_dice_3: 0.168  loss_ce_4: 0.1296  loss_mask_4: 0.2178  loss_dice_4: 0.1653  loss_ce_5: 0.1295  loss_mask_5: 0.2241  loss_dice_5: 0.1609  loss_ce_6: 0.1298  loss_mask_6: 0.2128  loss_dice_6: 0.1728  loss_ce_7: 0.1298  loss_mask_7: 0.2212  loss_dice_7: 0.1658  loss_ce_8: 0.1298  loss_mask_8: 0.222  loss_dice_8: 0.1683  time: 0.5561  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:20:56] d2.utils.events INFO:  eta: 3:28:32  iter: 6319  total_loss: 5.009  loss_ce: 0.1105  loss_mask: 0.227  loss_dice: 0.1646  loss_ce_0: 0.05305  loss_mask_0: 0.2228  loss_dice_0: 0.1622  loss_ce_1: 0.1136  loss_mask_1: 0.2207  loss_dice_1: 0.1627  loss_ce_2: 0.1136  loss_mask_2: 0.2242  loss_dice_2: 0.1651  loss_ce_3: 0.1135  loss_mask_3: 0.218  loss_dice_3: 0.1623  loss_ce_4: 0.1135  loss_mask_4: 0.2297  loss_dice_4: 0.1646  loss_ce_5: 0.1135  loss_mask_5: 0.2193  loss_dice_5: 0.1652  loss_ce_6: 0.1123  loss_mask_6: 0.2263  loss_dice_6: 0.1603  loss_ce_7: 0.1119  loss_mask_7: 0.2148  loss_dice_7: 0.1654  loss_ce_8: 0.1116  loss_mask_8: 0.2188  loss_dice_8: 0.1735  time: 0.5562  data_time: 0.0021  lr: 0.0001  max_mem: 2811M
[07/11 12:21:08] d2.utils.events INFO:  eta: 3:27:59  iter: 6339  total_loss: 4.974  loss_ce: 0.1287  loss_mask: 0.2024  loss_dice: 0.1648  loss_ce_0: 0.06645  loss_mask_0: 0.2  loss_dice_0: 0.1581  loss_ce_1: 0.1287  loss_mask_1: 0.1872  loss_dice_1: 0.1535  loss_ce_2: 0.1288  loss_mask_2: 0.2056  loss_dice_2: 0.1576  loss_ce_3: 0.1289  loss_mask_3: 0.2082  loss_dice_3: 0.1657  loss_ce_4: 0.1288  loss_mask_4: 0.1922  loss_dice_4: 0.1566  loss_ce_5: 0.1289  loss_mask_5: 0.1935  loss_dice_5: 0.1561  loss_ce_6: 0.1288  loss_mask_6: 0.197  loss_dice_6: 0.1612  loss_ce_7: 0.1288  loss_mask_7: 0.209  loss_dice_7: 0.1603  loss_ce_8: 0.1286  loss_mask_8: 0.1919  loss_dice_8: 0.1649  time: 0.5563  data_time: 0.0021  lr: 0.0001  max_mem: 2811M
[07/11 12:21:20] d2.utils.events INFO:  eta: 3:26:51  iter: 6359  total_loss: 4.999  loss_ce: 0.1286  loss_mask: 0.224  loss_dice: 0.1367  loss_ce_0: 0.07371  loss_mask_0: 0.2227  loss_dice_0: 0.135  loss_ce_1: 0.1277  loss_mask_1: 0.2199  loss_dice_1: 0.134  loss_ce_2: 0.128  loss_mask_2: 0.2211  loss_dice_2: 0.1347  loss_ce_3: 0.128  loss_mask_3: 0.2111  loss_dice_3: 0.1329  loss_ce_4: 0.1279  loss_mask_4: 0.2305  loss_dice_4: 0.1399  loss_ce_5: 0.128  loss_mask_5: 0.2211  loss_dice_5: 0.1387  loss_ce_6: 0.1285  loss_mask_6: 0.2273  loss_dice_6: 0.1339  loss_ce_7: 0.1285  loss_mask_7: 0.2207  loss_dice_7: 0.1362  loss_ce_8: 0.1284  loss_mask_8: 0.2273  loss_dice_8: 0.1337  time: 0.5564  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:21:32] d2.utils.events INFO:  eta: 3:25:57  iter: 6379  total_loss: 4.936  loss_ce: 0.1345  loss_mask: 0.157  loss_dice: 0.1661  loss_ce_0: 0.06117  loss_mask_0: 0.1588  loss_dice_0: 0.1743  loss_ce_1: 0.1386  loss_mask_1: 0.1535  loss_dice_1: 0.1743  loss_ce_2: 0.1386  loss_mask_2: 0.1602  loss_dice_2: 0.171  loss_ce_3: 0.1386  loss_mask_3: 0.1577  loss_dice_3: 0.1654  loss_ce_4: 0.1391  loss_mask_4: 0.1554  loss_dice_4: 0.167  loss_ce_5: 0.1389  loss_mask_5: 0.1557  loss_dice_5: 0.1658  loss_ce_6: 0.137  loss_mask_6: 0.1554  loss_dice_6: 0.1673  loss_ce_7: 0.1365  loss_mask_7: 0.1597  loss_dice_7: 0.1629  loss_ce_8: 0.1357  loss_mask_8: 0.1608  loss_dice_8: 0.1703  time: 0.5565  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:21:43] d2.utils.events INFO:  eta: 3:25:25  iter: 6399  total_loss: 4.64  loss_ce: 0.1209  loss_mask: 0.1833  loss_dice: 0.1536  loss_ce_0: 0.06748  loss_mask_0: 0.1918  loss_dice_0: 0.1516  loss_ce_1: 0.1214  loss_mask_1: 0.1823  loss_dice_1: 0.1546  loss_ce_2: 0.1214  loss_mask_2: 0.1809  loss_dice_2: 0.1616  loss_ce_3: 0.1213  loss_mask_3: 0.1843  loss_dice_3: 0.16  loss_ce_4: 0.1209  loss_mask_4: 0.1848  loss_dice_4: 0.1543  loss_ce_5: 0.1205  loss_mask_5: 0.1808  loss_dice_5: 0.1515  loss_ce_6: 0.1208  loss_mask_6: 0.1848  loss_dice_6: 0.1551  loss_ce_7: 0.121  loss_mask_7: 0.1903  loss_dice_7: 0.1514  loss_ce_8: 0.1205  loss_mask_8: 0.1874  loss_dice_8: 0.1523  time: 0.5566  data_time: 0.0021  lr: 0.0001  max_mem: 2811M
[07/11 12:21:55] d2.utils.events INFO:  eta: 3:25:04  iter: 6419  total_loss: 4.936  loss_ce: 0.129  loss_mask: 0.2119  loss_dice: 0.1453  loss_ce_0: 0.06523  loss_mask_0: 0.2151  loss_dice_0: 0.1483  loss_ce_1: 0.1291  loss_mask_1: 0.2157  loss_dice_1: 0.1503  loss_ce_2: 0.1291  loss_mask_2: 0.2118  loss_dice_2: 0.148  loss_ce_3: 0.1292  loss_mask_3: 0.2064  loss_dice_3: 0.1461  loss_ce_4: 0.1291  loss_mask_4: 0.1991  loss_dice_4: 0.1441  loss_ce_5: 0.1292  loss_mask_5: 0.2094  loss_dice_5: 0.1488  loss_ce_6: 0.129  loss_mask_6: 0.2132  loss_dice_6: 0.1446  loss_ce_7: 0.1291  loss_mask_7: 0.2094  loss_dice_7: 0.147  loss_ce_8: 0.129  loss_mask_8: 0.2083  loss_dice_8: 0.1521  time: 0.5566  data_time: 0.0031  lr: 0.0001  max_mem: 2811M
[07/11 12:22:06] d2.utils.events INFO:  eta: 3:25:12  iter: 6439  total_loss: 5.154  loss_ce: 0.1313  loss_mask: 0.1926  loss_dice: 0.172  loss_ce_0: 0.06514  loss_mask_0: 0.195  loss_dice_0: 0.1626  loss_ce_1: 0.1323  loss_mask_1: 0.1929  loss_dice_1: 0.1665  loss_ce_2: 0.1321  loss_mask_2: 0.1901  loss_dice_2: 0.1704  loss_ce_3: 0.132  loss_mask_3: 0.1955  loss_dice_3: 0.1789  loss_ce_4: 0.1322  loss_mask_4: 0.1888  loss_dice_4: 0.1669  loss_ce_5: 0.1323  loss_mask_5: 0.1954  loss_dice_5: 0.1712  loss_ce_6: 0.1322  loss_mask_6: 0.1978  loss_dice_6: 0.1603  loss_ce_7: 0.1323  loss_mask_7: 0.199  loss_dice_7: 0.1627  loss_ce_8: 0.1315  loss_mask_8: 0.1897  loss_dice_8: 0.1595  time: 0.5567  data_time: 0.0024  lr: 0.0001  max_mem: 2811M
[07/11 12:22:18] d2.utils.events INFO:  eta: 3:25:12  iter: 6459  total_loss: 4.905  loss_ce: 0.1167  loss_mask: 0.1688  loss_dice: 0.1911  loss_ce_0: 0.0631  loss_mask_0: 0.1699  loss_dice_0: 0.1876  loss_ce_1: 0.1135  loss_mask_1: 0.1809  loss_dice_1: 0.1897  loss_ce_2: 0.1137  loss_mask_2: 0.1744  loss_dice_2: 0.1909  loss_ce_3: 0.1136  loss_mask_3: 0.1726  loss_dice_3: 0.1952  loss_ce_4: 0.1134  loss_mask_4: 0.1674  loss_dice_4: 0.1943  loss_ce_5: 0.1136  loss_mask_5: 0.1744  loss_dice_5: 0.1893  loss_ce_6: 0.115  loss_mask_6: 0.171  loss_dice_6: 0.1924  loss_ce_7: 0.1148  loss_mask_7: 0.1746  loss_dice_7: 0.1865  loss_ce_8: 0.1159  loss_mask_8: 0.1798  loss_dice_8: 0.1948  time: 0.5567  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:22:29] d2.utils.events INFO:  eta: 3:25:21  iter: 6479  total_loss: 5.09  loss_ce: 0.1565  loss_mask: 0.2155  loss_dice: 0.1426  loss_ce_0: 0.07072  loss_mask_0: 0.2161  loss_dice_0: 0.1356  loss_ce_1: 0.1566  loss_mask_1: 0.2129  loss_dice_1: 0.1381  loss_ce_2: 0.1562  loss_mask_2: 0.2095  loss_dice_2: 0.1402  loss_ce_3: 0.1564  loss_mask_3: 0.2132  loss_dice_3: 0.1367  loss_ce_4: 0.1577  loss_mask_4: 0.2109  loss_dice_4: 0.135  loss_ce_5: 0.1578  loss_mask_5: 0.2083  loss_dice_5: 0.1359  loss_ce_6: 0.1568  loss_mask_6: 0.2137  loss_dice_6: 0.137  loss_ce_7: 0.157  loss_mask_7: 0.2191  loss_dice_7: 0.1364  loss_ce_8: 0.1574  loss_mask_8: 0.2064  loss_dice_8: 0.139  time: 0.5568  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:22:41] d2.utils.events INFO:  eta: 3:26:02  iter: 6499  total_loss: 4.937  loss_ce: 0.1273  loss_mask: 0.2012  loss_dice: 0.1372  loss_ce_0: 0.06537  loss_mask_0: 0.1944  loss_dice_0: 0.1325  loss_ce_1: 0.1269  loss_mask_1: 0.2029  loss_dice_1: 0.1321  loss_ce_2: 0.127  loss_mask_2: 0.1998  loss_dice_2: 0.1332  loss_ce_3: 0.1269  loss_mask_3: 0.203  loss_dice_3: 0.1333  loss_ce_4: 0.1269  loss_mask_4: 0.1955  loss_dice_4: 0.1333  loss_ce_5: 0.1269  loss_mask_5: 0.2068  loss_dice_5: 0.1356  loss_ce_6: 0.1271  loss_mask_6: 0.2019  loss_dice_6: 0.133  loss_ce_7: 0.127  loss_mask_7: 0.2084  loss_dice_7: 0.1415  loss_ce_8: 0.1272  loss_mask_8: 0.2043  loss_dice_8: 0.1318  time: 0.5569  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:22:52] d2.utils.events INFO:  eta: 3:26:38  iter: 6519  total_loss: 4.557  loss_ce: 0.1283  loss_mask: 0.1951  loss_dice: 0.1342  loss_ce_0: 0.06549  loss_mask_0: 0.197  loss_dice_0: 0.135  loss_ce_1: 0.1282  loss_mask_1: 0.2076  loss_dice_1: 0.1347  loss_ce_2: 0.129  loss_mask_2: 0.1923  loss_dice_2: 0.1309  loss_ce_3: 0.1288  loss_mask_3: 0.1941  loss_dice_3: 0.1276  loss_ce_4: 0.1288  loss_mask_4: 0.1876  loss_dice_4: 0.1338  loss_ce_5: 0.1291  loss_mask_5: 0.1883  loss_dice_5: 0.1295  loss_ce_6: 0.1294  loss_mask_6: 0.1977  loss_dice_6: 0.1318  loss_ce_7: 0.1292  loss_mask_7: 0.1913  loss_dice_7: 0.1347  loss_ce_8: 0.1286  loss_mask_8: 0.1966  loss_dice_8: 0.1317  time: 0.5569  data_time: 0.0021  lr: 0.0001  max_mem: 2811M
[07/11 12:23:04] d2.utils.events INFO:  eta: 3:27:25  iter: 6539  total_loss: 5.024  loss_ce: 0.1263  loss_mask: 0.1425  loss_dice: 0.1745  loss_ce_0: 0.06246  loss_mask_0: 0.1351  loss_dice_0: 0.1843  loss_ce_1: 0.1264  loss_mask_1: 0.1408  loss_dice_1: 0.182  loss_ce_2: 0.1263  loss_mask_2: 0.1431  loss_dice_2: 0.1757  loss_ce_3: 0.1264  loss_mask_3: 0.1419  loss_dice_3: 0.1782  loss_ce_4: 0.1265  loss_mask_4: 0.1452  loss_dice_4: 0.1806  loss_ce_5: 0.1263  loss_mask_5: 0.1405  loss_dice_5: 0.1813  loss_ce_6: 0.1263  loss_mask_6: 0.1445  loss_dice_6: 0.1783  loss_ce_7: 0.1266  loss_mask_7: 0.1465  loss_dice_7: 0.1849  loss_ce_8: 0.1263  loss_mask_8: 0.1453  loss_dice_8: 0.1844  time: 0.5570  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:23:16] d2.utils.events INFO:  eta: 3:28:16  iter: 6559  total_loss: 4.724  loss_ce: 0.1099  loss_mask: 0.2025  loss_dice: 0.1769  loss_ce_0: 0.06008  loss_mask_0: 0.2076  loss_dice_0: 0.175  loss_ce_1: 0.1067  loss_mask_1: 0.2019  loss_dice_1: 0.1637  loss_ce_2: 0.1068  loss_mask_2: 0.2046  loss_dice_2: 0.1751  loss_ce_3: 0.1069  loss_mask_3: 0.2027  loss_dice_3: 0.162  loss_ce_4: 0.1074  loss_mask_4: 0.2066  loss_dice_4: 0.1717  loss_ce_5: 0.1076  loss_mask_5: 0.2017  loss_dice_5: 0.1765  loss_ce_6: 0.1083  loss_mask_6: 0.2058  loss_dice_6: 0.1705  loss_ce_7: 0.1087  loss_mask_7: 0.1992  loss_dice_7: 0.1777  loss_ce_8: 0.1096  loss_mask_8: 0.1999  loss_dice_8: 0.1732  time: 0.5570  data_time: 0.0024  lr: 0.0001  max_mem: 2811M
[07/11 12:23:27] d2.utils.events INFO:  eta: 3:28:57  iter: 6579  total_loss: 4.846  loss_ce: 0.09947  loss_mask: 0.2096  loss_dice: 0.1634  loss_ce_0: 0.05762  loss_mask_0: 0.2095  loss_dice_0: 0.1669  loss_ce_1: 0.09941  loss_mask_1: 0.1996  loss_dice_1: 0.162  loss_ce_2: 0.09959  loss_mask_2: 0.1986  loss_dice_2: 0.1642  loss_ce_3: 0.09949  loss_mask_3: 0.1969  loss_dice_3: 0.163  loss_ce_4: 0.0989  loss_mask_4: 0.2054  loss_dice_4: 0.1629  loss_ce_5: 0.09883  loss_mask_5: 0.1937  loss_dice_5: 0.1635  loss_ce_6: 0.09914  loss_mask_6: 0.1984  loss_dice_6: 0.1649  loss_ce_7: 0.09917  loss_mask_7: 0.2057  loss_dice_7: 0.1625  loss_ce_8: 0.09902  loss_mask_8: 0.203  loss_dice_8: 0.1606  time: 0.5570  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:23:38] d2.utils.events INFO:  eta: 3:30:08  iter: 6599  total_loss: 5.145  loss_ce: 0.0961  loss_mask: 0.2255  loss_dice: 0.192  loss_ce_0: 0.05656  loss_mask_0: 0.2335  loss_dice_0: 0.199  loss_ce_1: 0.09623  loss_mask_1: 0.236  loss_dice_1: 0.1873  loss_ce_2: 0.09611  loss_mask_2: 0.2359  loss_dice_2: 0.1989  loss_ce_3: 0.09602  loss_mask_3: 0.2419  loss_dice_3: 0.1972  loss_ce_4: 0.09551  loss_mask_4: 0.2385  loss_dice_4: 0.1952  loss_ce_5: 0.09552  loss_mask_5: 0.2233  loss_dice_5: 0.1984  loss_ce_6: 0.0957  loss_mask_6: 0.2271  loss_dice_6: 0.1963  loss_ce_7: 0.09581  loss_mask_7: 0.2383  loss_dice_7: 0.1915  loss_ce_8: 0.09563  loss_mask_8: 0.2319  loss_dice_8: 0.1909  time: 0.5571  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:23:50] d2.utils.events INFO:  eta: 3:30:52  iter: 6619  total_loss: 4.633  loss_ce: 0.1684  loss_mask: 0.1902  loss_dice: 0.1603  loss_ce_0: 0.07709  loss_mask_0: 0.1896  loss_dice_0: 0.1566  loss_ce_1: 0.1679  loss_mask_1: 0.1952  loss_dice_1: 0.157  loss_ce_2: 0.1678  loss_mask_2: 0.1844  loss_dice_2: 0.1542  loss_ce_3: 0.1677  loss_mask_3: 0.187  loss_dice_3: 0.155  loss_ce_4: 0.1684  loss_mask_4: 0.1991  loss_dice_4: 0.1573  loss_ce_5: 0.1682  loss_mask_5: 0.1808  loss_dice_5: 0.1481  loss_ce_6: 0.1685  loss_mask_6: 0.1947  loss_dice_6: 0.1613  loss_ce_7: 0.1683  loss_mask_7: 0.182  loss_dice_7: 0.1502  loss_ce_8: 0.1688  loss_mask_8: 0.1981  loss_dice_8: 0.1628  time: 0.5571  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:24:01] d2.utils.events INFO:  eta: 3:31:18  iter: 6639  total_loss: 4.85  loss_ce: 0.1188  loss_mask: 0.2118  loss_dice: 0.1596  loss_ce_0: 0.05883  loss_mask_0: 0.2055  loss_dice_0: 0.1577  loss_ce_1: 0.1232  loss_mask_1: 0.212  loss_dice_1: 0.1615  loss_ce_2: 0.1228  loss_mask_2: 0.2111  loss_dice_2: 0.1584  loss_ce_3: 0.1229  loss_mask_3: 0.2068  loss_dice_3: 0.1597  loss_ce_4: 0.123  loss_mask_4: 0.2109  loss_dice_4: 0.1636  loss_ce_5: 0.1228  loss_mask_5: 0.2083  loss_dice_5: 0.1644  loss_ce_6: 0.1214  loss_mask_6: 0.2105  loss_dice_6: 0.1578  loss_ce_7: 0.121  loss_mask_7: 0.2103  loss_dice_7: 0.1609  loss_ce_8: 0.12  loss_mask_8: 0.2136  loss_dice_8: 0.1554  time: 0.5572  data_time: 0.0026  lr: 0.0001  max_mem: 2811M
[07/11 12:24:13] d2.utils.events INFO:  eta: 3:32:18  iter: 6659  total_loss: 5.226  loss_ce: 0.1166  loss_mask: 0.2092  loss_dice: 0.1808  loss_ce_0: 0.05858  loss_mask_0: 0.208  loss_dice_0: 0.1843  loss_ce_1: 0.1186  loss_mask_1: 0.2123  loss_dice_1: 0.1794  loss_ce_2: 0.1182  loss_mask_2: 0.2023  loss_dice_2: 0.1814  loss_ce_3: 0.1184  loss_mask_3: 0.1991  loss_dice_3: 0.1857  loss_ce_4: 0.1187  loss_mask_4: 0.2129  loss_dice_4: 0.1886  loss_ce_5: 0.1187  loss_mask_5: 0.2041  loss_dice_5: 0.1805  loss_ce_6: 0.1181  loss_mask_6: 0.2101  loss_dice_6: 0.179  loss_ce_7: 0.1176  loss_mask_7: 0.216  loss_dice_7: 0.1737  loss_ce_8: 0.1175  loss_mask_8: 0.1941  loss_dice_8: 0.179  time: 0.5572  data_time: 0.0022  lr: 0.0001  max_mem: 2811M
[07/11 12:24:24] d2.utils.events INFO:  eta: 3:33:37  iter: 6679  total_loss: 5.057  loss_ce: 0.1077  loss_mask: 0.2182  loss_dice: 0.1677  loss_ce_0: 0.05747  loss_mask_0: 0.2136  loss_dice_0: 0.1769  loss_ce_1: 0.1057  loss_mask_1: 0.2143  loss_dice_1: 0.164  loss_ce_2: 0.1057  loss_mask_2: 0.2197  loss_dice_2: 0.1647  loss_ce_3: 0.1058  loss_mask_3: 0.2123  loss_dice_3: 0.1637  loss_ce_4: 0.1064  loss_mask_4: 0.2123  loss_dice_4: 0.1595  loss_ce_5: 0.1064  loss_mask_5: 0.2198  loss_dice_5: 0.1694  loss_ce_6: 0.1069  loss_mask_6: 0.2085  loss_dice_6: 0.1601  loss_ce_7: 0.1071  loss_mask_7: 0.2217  loss_dice_7: 0.1695  loss_ce_8: 0.1078  loss_mask_8: 0.218  loss_dice_8: 0.1629  time: 0.5573  data_time: 0.0028  lr: 0.0001  max_mem: 2811M
[07/11 12:24:35] d2.utils.events INFO:  eta: 3:33:57  iter: 6699  total_loss: 4.884  loss_ce: 0.1603  loss_mask: 0.1841  loss_dice: 0.1391  loss_ce_0: 0.07599  loss_mask_0: 0.1729  loss_dice_0: 0.1459  loss_ce_1: 0.1603  loss_mask_1: 0.187  loss_dice_1: 0.1454  loss_ce_2: 0.16  loss_mask_2: 0.1771  loss_dice_2: 0.1411  loss_ce_3: 0.1602  loss_mask_3: 0.1779  loss_dice_3: 0.1436  loss_ce_4: 0.1606  loss_mask_4: 0.1738  loss_dice_4: 0.142  loss_ce_5: 0.161  loss_mask_5: 0.1827  loss_dice_5: 0.1471  loss_ce_6: 0.1605  loss_mask_6: 0.1807  loss_dice_6: 0.1371  loss_ce_7: 0.1604  loss_mask_7: 0.1789  loss_dice_7: 0.1479  loss_ce_8: 0.1607  loss_mask_8: 0.174  loss_dice_8: 0.1399  time: 0.5573  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:24:47] d2.utils.events INFO:  eta: 3:34:15  iter: 6719  total_loss: 4.437  loss_ce: 0.1304  loss_mask: 0.1834  loss_dice: 0.1332  loss_ce_0: 0.07052  loss_mask_0: 0.1896  loss_dice_0: 0.1277  loss_ce_1: 0.1281  loss_mask_1: 0.1883  loss_dice_1: 0.1323  loss_ce_2: 0.1282  loss_mask_2: 0.1919  loss_dice_2: 0.1273  loss_ce_3: 0.1282  loss_mask_3: 0.1843  loss_dice_3: 0.1241  loss_ce_4: 0.1283  loss_mask_4: 0.194  loss_dice_4: 0.1275  loss_ce_5: 0.1285  loss_mask_5: 0.184  loss_dice_5: 0.1308  loss_ce_6: 0.1289  loss_mask_6: 0.1866  loss_dice_6: 0.129  loss_ce_7: 0.1291  loss_mask_7: 0.1893  loss_dice_7: 0.1331  loss_ce_8: 0.1294  loss_mask_8: 0.1899  loss_dice_8: 0.13  time: 0.5573  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:24:58] d2.utils.events INFO:  eta: 3:34:40  iter: 6739  total_loss: 4.568  loss_ce: 0.1285  loss_mask: 0.2241  loss_dice: 0.1374  loss_ce_0: 0.06519  loss_mask_0: 0.2186  loss_dice_0: 0.1366  loss_ce_1: 0.1287  loss_mask_1: 0.2236  loss_dice_1: 0.1372  loss_ce_2: 0.1285  loss_mask_2: 0.2238  loss_dice_2: 0.1368  loss_ce_3: 0.1286  loss_mask_3: 0.2128  loss_dice_3: 0.1332  loss_ce_4: 0.1287  loss_mask_4: 0.2193  loss_dice_4: 0.1345  loss_ce_5: 0.1286  loss_mask_5: 0.2092  loss_dice_5: 0.1323  loss_ce_6: 0.1286  loss_mask_6: 0.2221  loss_dice_6: 0.1374  loss_ce_7: 0.1286  loss_mask_7: 0.2177  loss_dice_7: 0.1341  loss_ce_8: 0.1286  loss_mask_8: 0.2207  loss_dice_8: 0.1401  time: 0.5574  data_time: 0.0024  lr: 0.0001  max_mem: 2811M
[07/11 12:25:10] d2.utils.events INFO:  eta: 3:34:49  iter: 6759  total_loss: 4.765  loss_ce: 0.1323  loss_mask: 0.2291  loss_dice: 0.1301  loss_ce_0: 0.06386  loss_mask_0: 0.2211  loss_dice_0: 0.1424  loss_ce_1: 0.1312  loss_mask_1: 0.211  loss_dice_1: 0.1276  loss_ce_2: 0.131  loss_mask_2: 0.215  loss_dice_2: 0.1281  loss_ce_3: 0.131  loss_mask_3: 0.2212  loss_dice_3: 0.1313  loss_ce_4: 0.1315  loss_mask_4: 0.2248  loss_dice_4: 0.1329  loss_ce_5: 0.1318  loss_mask_5: 0.223  loss_dice_5: 0.1314  loss_ce_6: 0.1318  loss_mask_6: 0.2076  loss_dice_6: 0.1304  loss_ce_7: 0.1317  loss_mask_7: 0.2195  loss_dice_7: 0.1287  loss_ce_8: 0.1326  loss_mask_8: 0.2109  loss_dice_8: 0.132  time: 0.5574  data_time: 0.0022  lr: 0.0001  max_mem: 2811M
[07/11 12:25:21] d2.utils.events INFO:  eta: 3:34:57  iter: 6779  total_loss: 4.508  loss_ce: 0.1326  loss_mask: 0.1763  loss_dice: 0.11  loss_ce_0: 0.06826  loss_mask_0: 0.1824  loss_dice_0: 0.1097  loss_ce_1: 0.1375  loss_mask_1: 0.1767  loss_dice_1: 0.1112  loss_ce_2: 0.137  loss_mask_2: 0.1759  loss_dice_2: 0.1111  loss_ce_3: 0.1368  loss_mask_3: 0.174  loss_dice_3: 0.1104  loss_ce_4: 0.1357  loss_mask_4: 0.1855  loss_dice_4: 0.1082  loss_ce_5: 0.1355  loss_mask_5: 0.1839  loss_dice_5: 0.1122  loss_ce_6: 0.1344  loss_mask_6: 0.1805  loss_dice_6: 0.1084  loss_ce_7: 0.1347  loss_mask_7: 0.1802  loss_dice_7: 0.1116  loss_ce_8: 0.1329  loss_mask_8: 0.18  loss_dice_8: 0.1109  time: 0.5574  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:25:32] d2.utils.events INFO:  eta: 3:34:54  iter: 6799  total_loss: 4.79  loss_ce: 0.1203  loss_mask: 0.2091  loss_dice: 0.1236  loss_ce_0: 0.06258  loss_mask_0: 0.2113  loss_dice_0: 0.1274  loss_ce_1: 0.1213  loss_mask_1: 0.2055  loss_dice_1: 0.1284  loss_ce_2: 0.1216  loss_mask_2: 0.207  loss_dice_2: 0.1226  loss_ce_3: 0.1214  loss_mask_3: 0.2128  loss_dice_3: 0.1244  loss_ce_4: 0.1207  loss_mask_4: 0.2058  loss_dice_4: 0.1241  loss_ce_5: 0.1207  loss_mask_5: 0.206  loss_dice_5: 0.1299  loss_ce_6: 0.1208  loss_mask_6: 0.2047  loss_dice_6: 0.1236  loss_ce_7: 0.1203  loss_mask_7: 0.2101  loss_dice_7: 0.1257  loss_ce_8: 0.1202  loss_mask_8: 0.2081  loss_dice_8: 0.1244  time: 0.5575  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:25:44] d2.utils.events INFO:  eta: 3:34:58  iter: 6819  total_loss: 4.757  loss_ce: 0.1187  loss_mask: 0.2161  loss_dice: 0.1497  loss_ce_0: 0.06219  loss_mask_0: 0.2205  loss_dice_0: 0.1541  loss_ce_1: 0.119  loss_mask_1: 0.2144  loss_dice_1: 0.1497  loss_ce_2: 0.1192  loss_mask_2: 0.2229  loss_dice_2: 0.1482  loss_ce_3: 0.1192  loss_mask_3: 0.221  loss_dice_3: 0.1452  loss_ce_4: 0.1188  loss_mask_4: 0.2164  loss_dice_4: 0.1579  loss_ce_5: 0.1189  loss_mask_5: 0.2162  loss_dice_5: 0.1516  loss_ce_6: 0.1189  loss_mask_6: 0.2157  loss_dice_6: 0.1432  loss_ce_7: 0.1186  loss_mask_7: 0.2159  loss_dice_7: 0.1497  loss_ce_8: 0.1185  loss_mask_8: 0.2148  loss_dice_8: 0.1455  time: 0.5575  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:25:55] d2.utils.events INFO:  eta: 3:35:15  iter: 6839  total_loss: 4.744  loss_ce: 0.1115  loss_mask: 0.2022  loss_dice: 0.1686  loss_ce_0: 0.06066  loss_mask_0: 0.2058  loss_dice_0: 0.168  loss_ce_1: 0.1101  loss_mask_1: 0.2092  loss_dice_1: 0.1646  loss_ce_2: 0.1102  loss_mask_2: 0.2036  loss_dice_2: 0.1727  loss_ce_3: 0.1102  loss_mask_3: 0.2091  loss_dice_3: 0.1669  loss_ce_4: 0.1103  loss_mask_4: 0.2025  loss_dice_4: 0.1679  loss_ce_5: 0.1103  loss_mask_5: 0.2027  loss_dice_5: 0.1744  loss_ce_6: 0.1107  loss_mask_6: 0.2003  loss_dice_6: 0.1722  loss_ce_7: 0.1107  loss_mask_7: 0.2092  loss_dice_7: 0.1667  loss_ce_8: 0.111  loss_mask_8: 0.191  loss_dice_8: 0.1707  time: 0.5575  data_time: 0.0028  lr: 0.0001  max_mem: 2811M
[07/11 12:26:07] d2.utils.events INFO:  eta: 3:35:13  iter: 6859  total_loss: 4.39  loss_ce: 0.09585  loss_mask: 0.1483  loss_dice: 0.1425  loss_ce_0: 0.05725  loss_mask_0: 0.1545  loss_dice_0: 0.1482  loss_ce_1: 0.09235  loss_mask_1: 0.1541  loss_dice_1: 0.153  loss_ce_2: 0.09301  loss_mask_2: 0.1597  loss_dice_2: 0.1488  loss_ce_3: 0.09288  loss_mask_3: 0.1427  loss_dice_3: 0.1458  loss_ce_4: 0.09282  loss_mask_4: 0.1558  loss_dice_4: 0.1475  loss_ce_5: 0.09336  loss_mask_5: 0.1552  loss_dice_5: 0.143  loss_ce_6: 0.09386  loss_mask_6: 0.1506  loss_dice_6: 0.1402  loss_ce_7: 0.09398  loss_mask_7: 0.1615  loss_dice_7: 0.1457  loss_ce_8: 0.09487  loss_mask_8: 0.1542  loss_dice_8: 0.1404  time: 0.5576  data_time: 0.0020  lr: 0.0001  max_mem: 2811M
[07/11 12:26:18] d2.utils.events INFO:  eta: 3:35:51  iter: 6879  total_loss: 4.961  loss_ce: 0.1763  loss_mask: 0.143  loss_dice: 0.1522  loss_ce_0: 0.07792  loss_mask_0: 0.1323  loss_dice_0: 0.1555  loss_ce_1: 0.1753  loss_mask_1: 0.1438  loss_dice_1: 0.1657  loss_ce_2: 0.1748  loss_mask_2: 0.1474  loss_dice_2: 0.1609  loss_ce_3: 0.1749  loss_mask_3: 0.1491  loss_dice_3: 0.1676  loss_ce_4: 0.1761  loss_mask_4: 0.1385  loss_dice_4: 0.1579  loss_ce_5: 0.176  loss_mask_5: 0.1563  loss_dice_5: 0.1627  loss_ce_6: 0.1761  loss_mask_6: 0.1433  loss_dice_6: 0.147  loss_ce_7: 0.1759  loss_mask_7: 0.1442  loss_dice_7: 0.1646  loss_ce_8: 0.1768  loss_mask_8: 0.1426  loss_dice_8: 0.1602  time: 0.5576  data_time: 0.0027  lr: 0.0001  max_mem: 2811M
[07/11 12:26:30] d2.utils.events INFO:  eta: 3:35:53  iter: 6899  total_loss: 4.836  loss_ce: 0.1365  loss_mask: 0.1936  loss_dice: 0.1443  loss_ce_0: 0.07211  loss_mask_0: 0.1894  loss_dice_0: 0.1389  loss_ce_1: 0.1287  loss_mask_1: 0.1978  loss_dice_1: 0.1469  loss_ce_2: 0.1298  loss_mask_2: 0.1936  loss_dice_2: 0.1404  loss_ce_3: 0.1296  loss_mask_3: 0.1912  loss_dice_3: 0.1453  loss_ce_4: 0.1306  loss_mask_4: 0.1896  loss_dice_4: 0.1426  loss_ce_5: 0.1306  loss_mask_5: 0.1877  loss_dice_5: 0.1459  loss_ce_6: 0.1318  loss_mask_6: 0.201  loss_dice_6: 0.1435  loss_ce_7: 0.1329  loss_mask_7: 0.1944  loss_dice_7: 0.1527  loss_ce_8: 0.1348  loss_mask_8: 0.1915  loss_dice_8: 0.1538  time: 0.5576  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:26:41] d2.utils.events INFO:  eta: 3:36:16  iter: 6919  total_loss: 4.4  loss_ce: 0.1293  loss_mask: 0.1826  loss_dice: 0.1373  loss_ce_0: 0.06518  loss_mask_0: 0.1798  loss_dice_0: 0.1398  loss_ce_1: 0.1291  loss_mask_1: 0.1838  loss_dice_1: 0.1361  loss_ce_2: 0.129  loss_mask_2: 0.1878  loss_dice_2: 0.1437  loss_ce_3: 0.1291  loss_mask_3: 0.1778  loss_dice_3: 0.1314  loss_ce_4: 0.1291  loss_mask_4: 0.1771  loss_dice_4: 0.1361  loss_ce_5: 0.129  loss_mask_5: 0.1787  loss_dice_5: 0.1389  loss_ce_6: 0.1288  loss_mask_6: 0.1817  loss_dice_6: 0.1338  loss_ce_7: 0.1287  loss_mask_7: 0.1824  loss_dice_7: 0.1333  loss_ce_8: 0.1292  loss_mask_8: 0.1828  loss_dice_8: 0.1371  time: 0.5577  data_time: 0.0024  lr: 0.0001  max_mem: 2811M
[07/11 12:26:52] d2.utils.events INFO:  eta: 3:36:19  iter: 6939  total_loss: 4.731  loss_ce: 0.128  loss_mask: 0.2111  loss_dice: 0.1474  loss_ce_0: 0.06518  loss_mask_0: 0.2127  loss_dice_0: 0.1385  loss_ce_1: 0.128  loss_mask_1: 0.214  loss_dice_1: 0.1378  loss_ce_2: 0.1281  loss_mask_2: 0.2191  loss_dice_2: 0.1473  loss_ce_3: 0.1281  loss_mask_3: 0.2069  loss_dice_3: 0.139  loss_ce_4: 0.1282  loss_mask_4: 0.2112  loss_dice_4: 0.1445  loss_ce_5: 0.1281  loss_mask_5: 0.2112  loss_dice_5: 0.1398  loss_ce_6: 0.1281  loss_mask_6: 0.2119  loss_dice_6: 0.1363  loss_ce_7: 0.1282  loss_mask_7: 0.2095  loss_dice_7: 0.136  loss_ce_8: 0.1281  loss_mask_8: 0.2049  loss_dice_8: 0.1329  time: 0.5577  data_time: 0.0021  lr: 0.0001  max_mem: 2811M
[07/11 12:27:04] d2.utils.events INFO:  eta: 3:36:20  iter: 6959  total_loss: 5.128  loss_ce: 0.1192  loss_mask: 0.2036  loss_dice: 0.1801  loss_ce_0: 0.06549  loss_mask_0: 0.2118  loss_dice_0: 0.1776  loss_ce_1: 0.1206  loss_mask_1: 0.2013  loss_dice_1: 0.1726  loss_ce_2: 0.1208  loss_mask_2: 0.2094  loss_dice_2: 0.1749  loss_ce_3: 0.1205  loss_mask_3: 0.2063  loss_dice_3: 0.1644  loss_ce_4: 0.1197  loss_mask_4: 0.2046  loss_dice_4: 0.1828  loss_ce_5: 0.1197  loss_mask_5: 0.2012  loss_dice_5: 0.1805  loss_ce_6: 0.1196  loss_mask_6: 0.2027  loss_dice_6: 0.1724  loss_ce_7: 0.1199  loss_mask_7: 0.2178  loss_dice_7: 0.175  loss_ce_8: 0.1187  loss_mask_8: 0.2167  loss_dice_8: 0.1783  time: 0.5578  data_time: 0.0029  lr: 0.0001  max_mem: 2811M
[07/11 12:27:15] d2.utils.events INFO:  eta: 3:36:10  iter: 6979  total_loss: 4.737  loss_ce: 0.1229  loss_mask: 0.2075  loss_dice: 0.1662  loss_ce_0: 0.0651  loss_mask_0: 0.2086  loss_dice_0: 0.1644  loss_ce_1: 0.1251  loss_mask_1: 0.2109  loss_dice_1: 0.1651  loss_ce_2: 0.1249  loss_mask_2: 0.2008  loss_dice_2: 0.1652  loss_ce_3: 0.125  loss_mask_3: 0.1983  loss_dice_3: 0.1656  loss_ce_4: 0.1245  loss_mask_4: 0.2071  loss_dice_4: 0.1693  loss_ce_5: 0.1246  loss_mask_5: 0.2049  loss_dice_5: 0.1663  loss_ce_6: 0.1241  loss_mask_6: 0.2009  loss_dice_6: 0.1671  loss_ce_7: 0.1242  loss_mask_7: 0.1995  loss_dice_7: 0.1635  loss_ce_8: 0.1231  loss_mask_8: 0.2118  loss_dice_8: 0.1724  time: 0.5578  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:27:27] d2.utils.events INFO:  eta: 3:36:00  iter: 6999  total_loss: 5.276  loss_ce: 0.1347  loss_mask: 0.2234  loss_dice: 0.173  loss_ce_0: 0.06634  loss_mask_0: 0.2212  loss_dice_0: 0.1809  loss_ce_1: 0.1361  loss_mask_1: 0.2255  loss_dice_1: 0.1826  loss_ce_2: 0.1362  loss_mask_2: 0.2318  loss_dice_2: 0.1762  loss_ce_3: 0.1362  loss_mask_3: 0.2231  loss_dice_3: 0.175  loss_ce_4: 0.136  loss_mask_4: 0.2364  loss_dice_4: 0.1697  loss_ce_5: 0.136  loss_mask_5: 0.2179  loss_dice_5: 0.1801  loss_ce_6: 0.1359  loss_mask_6: 0.228  loss_dice_6: 0.1732  loss_ce_7: 0.1358  loss_mask_7: 0.2299  loss_dice_7: 0.1778  loss_ce_8: 0.1351  loss_mask_8: 0.2241  loss_dice_8: 0.1778  time: 0.5578  data_time: 0.0023  lr: 0.0001  max_mem: 2811M
[07/11 12:27:38] d2.utils.events INFO:  eta: 3:35:55  iter: 7019  total_loss: 5.192  loss_ce: 0.1299  loss_mask: 0.1897  loss_dice: 0.1624  loss_ce_0: 0.06558  loss_mask_0: 0.1864  loss_dice_0: 0.1651  loss_ce_1: 0.1313  loss_mask_1: 0.1983  loss_dice_1: 0.1674  loss_ce_2: 0.1311  loss_mask_2: 0.1992  loss_dice_2: 0.1766  loss_ce_3: 0.131  loss_mask_3: 0.1959  loss_dice_3: 0.1681  loss_ce_4: 0.1309  loss_mask_4: 0.2014  loss_dice_4: 0.1611  loss_ce_5: 0.1308  loss_mask_5: 0.1974  loss_dice_5: 0.1769  loss_ce_6: 0.1305  loss_mask_6: 0.1968  loss_dice_6: 0.1723  loss_ce_7: 0.1305  loss_mask_7: 0.1984  loss_dice_7: 0.1747  loss_ce_8: 0.1301  loss_mask_8: 0.2013  loss_dice_8: 0.1798  time: 0.5579  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:27:50] d2.utils.events INFO:  eta: 3:35:53  iter: 7039  total_loss: 5.044  loss_ce: 0.1463  loss_mask: 0.2071  loss_dice: 0.1314  loss_ce_0: 0.06959  loss_mask_0: 0.2179  loss_dice_0: 0.136  loss_ce_1: 0.1446  loss_mask_1: 0.216  loss_dice_1: 0.1307  loss_ce_2: 0.1444  loss_mask_2: 0.2123  loss_dice_2: 0.1343  loss_ce_3: 0.1444  loss_mask_3: 0.2117  loss_dice_3: 0.1337  loss_ce_4: 0.1456  loss_mask_4: 0.2188  loss_dice_4: 0.1352  loss_ce_5: 0.1458  loss_mask_5: 0.2197  loss_dice_5: 0.1369  loss_ce_6: 0.1457  loss_mask_6: 0.2061  loss_dice_6: 0.134  loss_ce_7: 0.1458  loss_mask_7: 0.2151  loss_dice_7: 0.1347  loss_ce_8: 0.1468  loss_mask_8: 0.2174  loss_dice_8: 0.1351  time: 0.5579  data_time: 0.0023  lr: 0.0001  max_mem: 2811M
[07/11 12:28:01] d2.utils.events INFO:  eta: 3:35:48  iter: 7059  total_loss: 4.717  loss_ce: 0.1123  loss_mask: 0.1685  loss_dice: 0.171  loss_ce_0: 0.06123  loss_mask_0: 0.1723  loss_dice_0: 0.1693  loss_ce_1: 0.1147  loss_mask_1: 0.1715  loss_dice_1: 0.166  loss_ce_2: 0.1145  loss_mask_2: 0.174  loss_dice_2: 0.1701  loss_ce_3: 0.1146  loss_mask_3: 0.186  loss_dice_3: 0.1641  loss_ce_4: 0.1137  loss_mask_4: 0.1732  loss_dice_4: 0.1659  loss_ce_5: 0.1137  loss_mask_5: 0.1718  loss_dice_5: 0.1661  loss_ce_6: 0.1134  loss_mask_6: 0.1763  loss_dice_6: 0.1747  loss_ce_7: 0.1133  loss_mask_7: 0.1831  loss_dice_7: 0.1699  loss_ce_8: 0.1123  loss_mask_8: 0.176  loss_dice_8: 0.1707  time: 0.5580  data_time: 0.0021  lr: 0.0001  max_mem: 2811M
[07/11 12:28:13] d2.utils.events INFO:  eta: 3:35:30  iter: 7079  total_loss: 5.596  loss_ce: 0.1052  loss_mask: 0.1837  loss_dice: 0.2192  loss_ce_0: 0.0592  loss_mask_0: 0.1971  loss_dice_0: 0.2168  loss_ce_1: 0.1042  loss_mask_1: 0.1966  loss_dice_1: 0.2239  loss_ce_2: 0.1044  loss_mask_2: 0.1733  loss_dice_2: 0.2127  loss_ce_3: 0.1044  loss_mask_3: 0.173  loss_dice_3: 0.2151  loss_ce_4: 0.1042  loss_mask_4: 0.1773  loss_dice_4: 0.2136  loss_ce_5: 0.1044  loss_mask_5: 0.18  loss_dice_5: 0.2177  loss_ce_6: 0.1044  loss_mask_6: 0.1756  loss_dice_6: 0.2168  loss_ce_7: 0.105  loss_mask_7: 0.1881  loss_dice_7: 0.2198  loss_ce_8: 0.105  loss_mask_8: 0.1881  loss_dice_8: 0.2146  time: 0.5580  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:28:24] d2.utils.events INFO:  eta: 3:35:33  iter: 7099  total_loss: 4.904  loss_ce: 0.09144  loss_mask: 0.1908  loss_dice: 0.1661  loss_ce_0: 0.05565  loss_mask_0: 0.2009  loss_dice_0: 0.1663  loss_ce_1: 0.08872  loss_mask_1: 0.1925  loss_dice_1: 0.162  loss_ce_2: 0.08933  loss_mask_2: 0.187  loss_dice_2: 0.1579  loss_ce_3: 0.08928  loss_mask_3: 0.1998  loss_dice_3: 0.1654  loss_ce_4: 0.0891  loss_mask_4: 0.19  loss_dice_4: 0.1568  loss_ce_5: 0.08942  loss_mask_5: 0.1966  loss_dice_5: 0.1679  loss_ce_6: 0.08988  loss_mask_6: 0.1953  loss_dice_6: 0.1691  loss_ce_7: 0.09033  loss_mask_7: 0.1939  loss_dice_7: 0.1657  loss_ce_8: 0.09078  loss_mask_8: 0.198  loss_dice_8: 0.1674  time: 0.5581  data_time: 0.0028  lr: 0.0001  max_mem: 2811M
[07/11 12:28:36] d2.utils.events INFO:  eta: 3:35:46  iter: 7119  total_loss: 4.661  loss_ce: 0.08432  loss_mask: 0.1767  loss_dice: 0.164  loss_ce_0: 0.05334  loss_mask_0: 0.1652  loss_dice_0: 0.175  loss_ce_1: 0.08215  loss_mask_1: 0.1745  loss_dice_1: 0.1667  loss_ce_2: 0.08273  loss_mask_2: 0.1803  loss_dice_2: 0.1657  loss_ce_3: 0.08258  loss_mask_3: 0.1688  loss_dice_3: 0.1726  loss_ce_4: 0.08225  loss_mask_4: 0.1771  loss_dice_4: 0.17  loss_ce_5: 0.08249  loss_mask_5: 0.1714  loss_dice_5: 0.1701  loss_ce_6: 0.08285  loss_mask_6: 0.1666  loss_dice_6: 0.164  loss_ce_7: 0.08321  loss_mask_7: 0.1759  loss_dice_7: 0.1678  loss_ce_8: 0.08344  loss_mask_8: 0.1703  loss_dice_8: 0.1763  time: 0.5581  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:28:47] d2.utils.events INFO:  eta: 3:35:35  iter: 7139  total_loss: 5.254  loss_ce: 0.07383  loss_mask: 0.2307  loss_dice: 0.1726  loss_ce_0: 0.04933  loss_mask_0: 0.2321  loss_dice_0: 0.1763  loss_ce_1: 0.07315  loss_mask_1: 0.2257  loss_dice_1: 0.1713  loss_ce_2: 0.07347  loss_mask_2: 0.2244  loss_dice_2: 0.176  loss_ce_3: 0.07334  loss_mask_3: 0.2317  loss_dice_3: 0.1731  loss_ce_4: 0.07305  loss_mask_4: 0.2405  loss_dice_4: 0.1731  loss_ce_5: 0.07303  loss_mask_5: 0.2268  loss_dice_5: 0.1732  loss_ce_6: 0.07321  loss_mask_6: 0.2337  loss_dice_6: 0.1735  loss_ce_7: 0.07347  loss_mask_7: 0.2362  loss_dice_7: 0.1806  loss_ce_8: 0.07327  loss_mask_8: 0.2378  loss_dice_8: 0.1784  time: 0.5582  data_time: 0.0022  lr: 0.0001  max_mem: 2811M
[07/11 12:28:59] d2.utils.events INFO:  eta: 3:35:25  iter: 7159  total_loss: 4.812  loss_ce: 0.06931  loss_mask: 0.1827  loss_dice: 0.1647  loss_ce_0: 0.04733  loss_mask_0: 0.1669  loss_dice_0: 0.1602  loss_ce_1: 0.06846  loss_mask_1: 0.1725  loss_dice_1: 0.1629  loss_ce_2: 0.06879  loss_mask_2: 0.1893  loss_dice_2: 0.1665  loss_ce_3: 0.06877  loss_mask_3: 0.1851  loss_dice_3: 0.1694  loss_ce_4: 0.0686  loss_mask_4: 0.1793  loss_dice_4: 0.1635  loss_ce_5: 0.0685  loss_mask_5: 0.1735  loss_dice_5: 0.1678  loss_ce_6: 0.06864  loss_mask_6: 0.1757  loss_dice_6: 0.1639  loss_ce_7: 0.069  loss_mask_7: 0.1833  loss_dice_7: 0.1641  loss_ce_8: 0.06872  loss_mask_8: 0.18  loss_dice_8: 0.167  time: 0.5582  data_time: 0.0028  lr: 0.0001  max_mem: 2811M
[07/11 12:29:10] d2.utils.events INFO:  eta: 3:35:14  iter: 7179  total_loss: 4.589  loss_ce: 0.06704  loss_mask: 0.2122  loss_dice: 0.1341  loss_ce_0: 0.04546  loss_mask_0: 0.2071  loss_dice_0: 0.1346  loss_ce_1: 0.0668  loss_mask_1: 0.2037  loss_dice_1: 0.1313  loss_ce_2: 0.0669  loss_mask_2: 0.2055  loss_dice_2: 0.1358  loss_ce_3: 0.06695  loss_mask_3: 0.2093  loss_dice_3: 0.128  loss_ce_4: 0.06684  loss_mask_4: 0.2087  loss_dice_4: 0.1341  loss_ce_5: 0.0668  loss_mask_5: 0.2132  loss_dice_5: 0.1308  loss_ce_6: 0.06677  loss_mask_6: 0.2113  loss_dice_6: 0.1372  loss_ce_7: 0.06704  loss_mask_7: 0.2061  loss_dice_7: 0.1369  loss_ce_8: 0.06668  loss_mask_8: 0.2086  loss_dice_8: 0.1336  time: 0.5582  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:29:22] d2.utils.events INFO:  eta: 3:35:17  iter: 7199  total_loss: 4.489  loss_ce: 0.07596  loss_mask: 0.1798  loss_dice: 0.1449  loss_ce_0: 0.04577  loss_mask_0: 0.181  loss_dice_0: 0.1526  loss_ce_1: 0.07721  loss_mask_1: 0.1811  loss_dice_1: 0.1444  loss_ce_2: 0.07701  loss_mask_2: 0.1781  loss_dice_2: 0.1476  loss_ce_3: 0.07711  loss_mask_3: 0.1832  loss_dice_3: 0.1504  loss_ce_4: 0.07723  loss_mask_4: 0.1842  loss_dice_4: 0.15  loss_ce_5: 0.07703  loss_mask_5: 0.1838  loss_dice_5: 0.1515  loss_ce_6: 0.07657  loss_mask_6: 0.1889  loss_dice_6: 0.1482  loss_ce_7: 0.07663  loss_mask_7: 0.1829  loss_dice_7: 0.1417  loss_ce_8: 0.07623  loss_mask_8: 0.1856  loss_dice_8: 0.1467  time: 0.5583  data_time: 0.0031  lr: 0.0001  max_mem: 2811M
[07/11 12:29:33] d2.utils.events INFO:  eta: 3:35:06  iter: 7219  total_loss: 4.942  loss_ce: 0.1629  loss_mask: 0.2094  loss_dice: 0.1404  loss_ce_0: 0.0842  loss_mask_0: 0.2028  loss_dice_0: 0.1383  loss_ce_1: 0.1562  loss_mask_1: 0.2097  loss_dice_1: 0.1415  loss_ce_2: 0.1576  loss_mask_2: 0.201  loss_dice_2: 0.143  loss_ce_3: 0.1569  loss_mask_3: 0.2075  loss_dice_3: 0.1418  loss_ce_4: 0.157  loss_mask_4: 0.2078  loss_dice_4: 0.1369  loss_ce_5: 0.1574  loss_mask_5: 0.2086  loss_dice_5: 0.1414  loss_ce_6: 0.1592  loss_mask_6: 0.2087  loss_dice_6: 0.1392  loss_ce_7: 0.1596  loss_mask_7: 0.206  loss_dice_7: 0.1354  loss_ce_8: 0.1613  loss_mask_8: 0.2192  loss_dice_8: 0.1392  time: 0.5583  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:29:45] d2.utils.events INFO:  eta: 3:34:55  iter: 7239  total_loss: 5.261  loss_ce: 0.1114  loss_mask: 0.2133  loss_dice: 0.1815  loss_ce_0: 0.05333  loss_mask_0: 0.2164  loss_dice_0: 0.1876  loss_ce_1: 0.1157  loss_mask_1: 0.2202  loss_dice_1: 0.1882  loss_ce_2: 0.1143  loss_mask_2: 0.2141  loss_dice_2: 0.1784  loss_ce_3: 0.1151  loss_mask_3: 0.2176  loss_dice_3: 0.1863  loss_ce_4: 0.1159  loss_mask_4: 0.2199  loss_dice_4: 0.1757  loss_ce_5: 0.1157  loss_mask_5: 0.2122  loss_dice_5: 0.1832  loss_ce_6: 0.1144  loss_mask_6: 0.2097  loss_dice_6: 0.1809  loss_ce_7: 0.1138  loss_mask_7: 0.2133  loss_dice_7: 0.1827  loss_ce_8: 0.1129  loss_mask_8: 0.2181  loss_dice_8: 0.1793  time: 0.5584  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:29:57] d2.utils.events INFO:  eta: 3:34:38  iter: 7259  total_loss: 4.683  loss_ce: 0.1202  loss_mask: 0.1978  loss_dice: 0.158  loss_ce_0: 0.05583  loss_mask_0: 0.1993  loss_dice_0: 0.1585  loss_ce_1: 0.1237  loss_mask_1: 0.1918  loss_dice_1: 0.1641  loss_ce_2: 0.1221  loss_mask_2: 0.1935  loss_dice_2: 0.1545  loss_ce_3: 0.1233  loss_mask_3: 0.2001  loss_dice_3: 0.1554  loss_ce_4: 0.1243  loss_mask_4: 0.19  loss_dice_4: 0.1554  loss_ce_5: 0.124  loss_mask_5: 0.1845  loss_dice_5: 0.1546  loss_ce_6: 0.1228  loss_mask_6: 0.2  loss_dice_6: 0.1598  loss_ce_7: 0.1223  loss_mask_7: 0.2023  loss_dice_7: 0.1538  loss_ce_8: 0.1214  loss_mask_8: 0.1985  loss_dice_8: 0.1557  time: 0.5585  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:30:08] d2.utils.events INFO:  eta: 3:34:32  iter: 7279  total_loss: 5.237  loss_ce: 0.1163  loss_mask: 0.1777  loss_dice: 0.1885  loss_ce_0: 0.05611  loss_mask_0: 0.1822  loss_dice_0: 0.1863  loss_ce_1: 0.117  loss_mask_1: 0.1845  loss_dice_1: 0.1926  loss_ce_2: 0.116  loss_mask_2: 0.1825  loss_dice_2: 0.1894  loss_ce_3: 0.1167  loss_mask_3: 0.1773  loss_dice_3: 0.1835  loss_ce_4: 0.1178  loss_mask_4: 0.1835  loss_dice_4: 0.1948  loss_ce_5: 0.1177  loss_mask_5: 0.1952  loss_dice_5: 0.1924  loss_ce_6: 0.1171  loss_mask_6: 0.1756  loss_dice_6: 0.1848  loss_ce_7: 0.117  loss_mask_7: 0.1849  loss_dice_7: 0.1906  loss_ce_8: 0.1171  loss_mask_8: 0.1823  loss_dice_8: 0.1904  time: 0.5585  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:30:20] d2.utils.events INFO:  eta: 3:34:21  iter: 7299  total_loss: 4.719  loss_ce: 0.1163  loss_mask: 0.1807  loss_dice: 0.1521  loss_ce_0: 0.05766  loss_mask_0: 0.1801  loss_dice_0: 0.1583  loss_ce_1: 0.1185  loss_mask_1: 0.1871  loss_dice_1: 0.1562  loss_ce_2: 0.118  loss_mask_2: 0.1944  loss_dice_2: 0.1602  loss_ce_3: 0.1178  loss_mask_3: 0.1789  loss_dice_3: 0.1496  loss_ce_4: 0.1178  loss_mask_4: 0.1754  loss_dice_4: 0.1608  loss_ce_5: 0.1177  loss_mask_5: 0.1879  loss_dice_5: 0.1577  loss_ce_6: 0.1171  loss_mask_6: 0.1795  loss_dice_6: 0.1604  loss_ce_7: 0.117  loss_mask_7: 0.1767  loss_dice_7: 0.1554  loss_ce_8: 0.1167  loss_mask_8: 0.1744  loss_dice_8: 0.1613  time: 0.5585  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:30:31] d2.utils.events INFO:  eta: 3:34:09  iter: 7319  total_loss: 5.278  loss_ce: 0.113  loss_mask: 0.2034  loss_dice: 0.1852  loss_ce_0: 0.05732  loss_mask_0: 0.2034  loss_dice_0: 0.1881  loss_ce_1: 0.1133  loss_mask_1: 0.1984  loss_dice_1: 0.192  loss_ce_2: 0.1134  loss_mask_2: 0.2083  loss_dice_2: 0.193  loss_ce_3: 0.1131  loss_mask_3: 0.1957  loss_dice_3: 0.1895  loss_ce_4: 0.1132  loss_mask_4: 0.2034  loss_dice_4: 0.1805  loss_ce_5: 0.1133  loss_mask_5: 0.2097  loss_dice_5: 0.1881  loss_ce_6: 0.1131  loss_mask_6: 0.2015  loss_dice_6: 0.1864  loss_ce_7: 0.1129  loss_mask_7: 0.2007  loss_dice_7: 0.1829  loss_ce_8: 0.1132  loss_mask_8: 0.2021  loss_dice_8: 0.1942  time: 0.5586  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:30:43] d2.utils.events INFO:  eta: 3:33:58  iter: 7339  total_loss: 4.851  loss_ce: 0.09988  loss_mask: 0.1778  loss_dice: 0.1888  loss_ce_0: 0.05538  loss_mask_0: 0.1754  loss_dice_0: 0.1987  loss_ce_1: 0.09839  loss_mask_1: 0.1774  loss_dice_1: 0.185  loss_ce_2: 0.099  loss_mask_2: 0.1883  loss_dice_2: 0.196  loss_ce_3: 0.09868  loss_mask_3: 0.1803  loss_dice_3: 0.1948  loss_ce_4: 0.09827  loss_mask_4: 0.1831  loss_dice_4: 0.1949  loss_ce_5: 0.09846  loss_mask_5: 0.1809  loss_dice_5: 0.1842  loss_ce_6: 0.0988  loss_mask_6: 0.173  loss_dice_6: 0.1904  loss_ce_7: 0.09898  loss_mask_7: 0.1824  loss_dice_7: 0.1835  loss_ce_8: 0.09944  loss_mask_8: 0.1722  loss_dice_8: 0.1922  time: 0.5587  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:30:54] d2.utils.events INFO:  eta: 3:33:51  iter: 7359  total_loss: 5.059  loss_ce: 0.1623  loss_mask: 0.1774  loss_dice: 0.189  loss_ce_0: 0.077  loss_mask_0: 0.1743  loss_dice_0: 0.2041  loss_ce_1: 0.1613  loss_mask_1: 0.1785  loss_dice_1: 0.1952  loss_ce_2: 0.161  loss_mask_2: 0.1783  loss_dice_2: 0.1921  loss_ce_3: 0.1612  loss_mask_3: 0.1785  loss_dice_3: 0.2046  loss_ce_4: 0.162  loss_mask_4: 0.1697  loss_dice_4: 0.2098  loss_ce_5: 0.162  loss_mask_5: 0.1727  loss_dice_5: 0.2001  loss_ce_6: 0.162  loss_mask_6: 0.1736  loss_dice_6: 0.1983  loss_ce_7: 0.1621  loss_mask_7: 0.1743  loss_dice_7: 0.1963  loss_ce_8: 0.1625  loss_mask_8: 0.1787  loss_dice_8: 0.2024  time: 0.5587  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:31:06] d2.utils.events INFO:  eta: 3:33:46  iter: 7379  total_loss: 4.92  loss_ce: 0.1415  loss_mask: 0.1845  loss_dice: 0.1751  loss_ce_0: 0.07304  loss_mask_0: 0.1769  loss_dice_0: 0.1728  loss_ce_1: 0.136  loss_mask_1: 0.1688  loss_dice_1: 0.1703  loss_ce_2: 0.1369  loss_mask_2: 0.1773  loss_dice_2: 0.1726  loss_ce_3: 0.1365  loss_mask_3: 0.1738  loss_dice_3: 0.1718  loss_ce_4: 0.137  loss_mask_4: 0.1702  loss_dice_4: 0.1749  loss_ce_5: 0.1371  loss_mask_5: 0.1778  loss_dice_5: 0.1782  loss_ce_6: 0.1385  loss_mask_6: 0.179  loss_dice_6: 0.1722  loss_ce_7: 0.1389  loss_mask_7: 0.1779  loss_dice_7: 0.1709  loss_ce_8: 0.1407  loss_mask_8: 0.1732  loss_dice_8: 0.1701  time: 0.5587  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:31:17] d2.utils.events INFO:  eta: 3:33:34  iter: 7399  total_loss: 4.99  loss_ce: 0.1264  loss_mask: 0.1698  loss_dice: 0.1909  loss_ce_0: 0.06568  loss_mask_0: 0.1745  loss_dice_0: 0.1902  loss_ce_1: 0.1249  loss_mask_1: 0.1763  loss_dice_1: 0.1821  loss_ce_2: 0.1249  loss_mask_2: 0.183  loss_dice_2: 0.1891  loss_ce_3: 0.1249  loss_mask_3: 0.1764  loss_dice_3: 0.1791  loss_ce_4: 0.1249  loss_mask_4: 0.1829  loss_dice_4: 0.1807  loss_ce_5: 0.1249  loss_mask_5: 0.1704  loss_dice_5: 0.1888  loss_ce_6: 0.1254  loss_mask_6: 0.1801  loss_dice_6: 0.1872  loss_ce_7: 0.1257  loss_mask_7: 0.1757  loss_dice_7: 0.1834  loss_ce_8: 0.1262  loss_mask_8: 0.1707  loss_dice_8: 0.1883  time: 0.5588  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:31:29] d2.utils.events INFO:  eta: 3:33:26  iter: 7419  total_loss: 5.451  loss_ce: 0.1459  loss_mask: 0.1944  loss_dice: 0.1747  loss_ce_0: 0.0648  loss_mask_0: 0.2034  loss_dice_0: 0.1776  loss_ce_1: 0.1467  loss_mask_1: 0.196  loss_dice_1: 0.1802  loss_ce_2: 0.1452  loss_mask_2: 0.1876  loss_dice_2: 0.1721  loss_ce_3: 0.1461  loss_mask_3: 0.2013  loss_dice_3: 0.1681  loss_ce_4: 0.1476  loss_mask_4: 0.193  loss_dice_4: 0.1722  loss_ce_5: 0.1479  loss_mask_5: 0.1896  loss_dice_5: 0.1722  loss_ce_6: 0.1468  loss_mask_6: 0.2062  loss_dice_6: 0.1752  loss_ce_7: 0.147  loss_mask_7: 0.1967  loss_dice_7: 0.174  loss_ce_8: 0.1473  loss_mask_8: 0.1882  loss_dice_8: 0.1768  time: 0.5589  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:31:41] d2.utils.events INFO:  eta: 3:33:15  iter: 7439  total_loss: 4.652  loss_ce: 0.1358  loss_mask: 0.1748  loss_dice: 0.1609  loss_ce_0: 0.06634  loss_mask_0: 0.1923  loss_dice_0: 0.1664  loss_ce_1: 0.1387  loss_mask_1: 0.1749  loss_dice_1: 0.1618  loss_ce_2: 0.1382  loss_mask_2: 0.1813  loss_dice_2: 0.1564  loss_ce_3: 0.1382  loss_mask_3: 0.179  loss_dice_3: 0.1596  loss_ce_4: 0.1382  loss_mask_4: 0.1856  loss_dice_4: 0.1604  loss_ce_5: 0.1381  loss_mask_5: 0.1842  loss_dice_5: 0.1583  loss_ce_6: 0.1372  loss_mask_6: 0.1842  loss_dice_6: 0.1641  loss_ce_7: 0.137  loss_mask_7: 0.1854  loss_dice_7: 0.1581  loss_ce_8: 0.1361  loss_mask_8: 0.1832  loss_dice_8: 0.1597  time: 0.5589  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:31:53] d2.utils.events INFO:  eta: 3:33:00  iter: 7459  total_loss: 5.067  loss_ce: 0.1051  loss_mask: 0.1546  loss_dice: 0.1783  loss_ce_0: 0.06002  loss_mask_0: 0.1533  loss_dice_0: 0.1894  loss_ce_1: 0.1077  loss_mask_1: 0.1486  loss_dice_1: 0.1872  loss_ce_2: 0.107  loss_mask_2: 0.1647  loss_dice_2: 0.1822  loss_ce_3: 0.1068  loss_mask_3: 0.1607  loss_dice_3: 0.1792  loss_ce_4: 0.1066  loss_mask_4: 0.1654  loss_dice_4: 0.1827  loss_ce_5: 0.1063  loss_mask_5: 0.1526  loss_dice_5: 0.1811  loss_ce_6: 0.106  loss_mask_6: 0.1642  loss_dice_6: 0.1851  loss_ce_7: 0.106  loss_mask_7: 0.1543  loss_dice_7: 0.1751  loss_ce_8: 0.1048  loss_mask_8: 0.158  loss_dice_8: 0.1783  time: 0.5590  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:32:05] d2.utils.events INFO:  eta: 3:32:43  iter: 7479  total_loss: 5.098  loss_ce: 0.1291  loss_mask: 0.2195  loss_dice: 0.1751  loss_ce_0: 0.06536  loss_mask_0: 0.2281  loss_dice_0: 0.169  loss_ce_1: 0.1295  loss_mask_1: 0.221  loss_dice_1: 0.1749  loss_ce_2: 0.1293  loss_mask_2: 0.2198  loss_dice_2: 0.1742  loss_ce_3: 0.1292  loss_mask_3: 0.2163  loss_dice_3: 0.1711  loss_ce_4: 0.1291  loss_mask_4: 0.2101  loss_dice_4: 0.1734  loss_ce_5: 0.1292  loss_mask_5: 0.2226  loss_dice_5: 0.1767  loss_ce_6: 0.1292  loss_mask_6: 0.2151  loss_dice_6: 0.1761  loss_ce_7: 0.1291  loss_mask_7: 0.2229  loss_dice_7: 0.172  loss_ce_8: 0.1292  loss_mask_8: 0.2265  loss_dice_8: 0.1677  time: 0.5591  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:32:16] d2.utils.events INFO:  eta: 3:32:19  iter: 7499  total_loss: 5.82  loss_ce: 0.1464  loss_mask: 0.2234  loss_dice: 0.2226  loss_ce_0: 0.07123  loss_mask_0: 0.2217  loss_dice_0: 0.2294  loss_ce_1: 0.1445  loss_mask_1: 0.2356  loss_dice_1: 0.2362  loss_ce_2: 0.1452  loss_mask_2: 0.2247  loss_dice_2: 0.2292  loss_ce_3: 0.1451  loss_mask_3: 0.2289  loss_dice_3: 0.2246  loss_ce_4: 0.1446  loss_mask_4: 0.2357  loss_dice_4: 0.2215  loss_ce_5: 0.1449  loss_mask_5: 0.2224  loss_dice_5: 0.2354  loss_ce_6: 0.1456  loss_mask_6: 0.2305  loss_dice_6: 0.2256  loss_ce_7: 0.1454  loss_mask_7: 0.2308  loss_dice_7: 0.2238  loss_ce_8: 0.1462  loss_mask_8: 0.231  loss_dice_8: 0.2371  time: 0.5592  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:32:28] d2.utils.events INFO:  eta: 3:32:00  iter: 7519  total_loss: 4.781  loss_ce: 0.1344  loss_mask: 0.2049  loss_dice: 0.1543  loss_ce_0: 0.06919  loss_mask_0: 0.2139  loss_dice_0: 0.1565  loss_ce_1: 0.1311  loss_mask_1: 0.2027  loss_dice_1: 0.1583  loss_ce_2: 0.1324  loss_mask_2: 0.2136  loss_dice_2: 0.1559  loss_ce_3: 0.1321  loss_mask_3: 0.1985  loss_dice_3: 0.1557  loss_ce_4: 0.1317  loss_mask_4: 0.2078  loss_dice_4: 0.1566  loss_ce_5: 0.1319  loss_mask_5: 0.2168  loss_dice_5: 0.1558  loss_ce_6: 0.1331  loss_mask_6: 0.2064  loss_dice_6: 0.1513  loss_ce_7: 0.1328  loss_mask_7: 0.2048  loss_dice_7: 0.1566  loss_ce_8: 0.1338  loss_mask_8: 0.2021  loss_dice_8: 0.1573  time: 0.5592  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:32:39] d2.utils.events INFO:  eta: 3:31:09  iter: 7539  total_loss: 5.128  loss_ce: 0.1279  loss_mask: 0.2159  loss_dice: 0.1688  loss_ce_0: 0.06754  loss_mask_0: 0.2199  loss_dice_0: 0.1679  loss_ce_1: 0.1284  loss_mask_1: 0.2152  loss_dice_1: 0.1693  loss_ce_2: 0.1278  loss_mask_2: 0.212  loss_dice_2: 0.1679  loss_ce_3: 0.1278  loss_mask_3: 0.2251  loss_dice_3: 0.1665  loss_ce_4: 0.1283  loss_mask_4: 0.214  loss_dice_4: 0.1655  loss_ce_5: 0.1281  loss_mask_5: 0.2114  loss_dice_5: 0.171  loss_ce_6: 0.1278  loss_mask_6: 0.2156  loss_dice_6: 0.1658  loss_ce_7: 0.1278  loss_mask_7: 0.2153  loss_dice_7: 0.1647  loss_ce_8: 0.1276  loss_mask_8: 0.2171  loss_dice_8: 0.1649  time: 0.5593  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:32:51] d2.utils.events INFO:  eta: 3:31:15  iter: 7559  total_loss: 4.989  loss_ce: 0.1273  loss_mask: 0.1926  loss_dice: 0.1937  loss_ce_0: 0.06283  loss_mask_0: 0.1879  loss_dice_0: 0.1933  loss_ce_1: 0.1277  loss_mask_1: 0.1861  loss_dice_1: 0.1976  loss_ce_2: 0.1273  loss_mask_2: 0.1955  loss_dice_2: 0.1947  loss_ce_3: 0.1274  loss_mask_3: 0.1901  loss_dice_3: 0.1935  loss_ce_4: 0.1278  loss_mask_4: 0.1947  loss_dice_4: 0.1995  loss_ce_5: 0.1276  loss_mask_5: 0.1834  loss_dice_5: 0.1931  loss_ce_6: 0.1274  loss_mask_6: 0.1849  loss_dice_6: 0.2048  loss_ce_7: 0.1274  loss_mask_7: 0.1884  loss_dice_7: 0.1948  loss_ce_8: 0.1274  loss_mask_8: 0.1973  loss_dice_8: 0.1928  time: 0.5593  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:33:02] d2.utils.events INFO:  eta: 3:30:56  iter: 7579  total_loss: 4.72  loss_ce: 0.116  loss_mask: 0.1861  loss_dice: 0.1447  loss_ce_0: 0.06103  loss_mask_0: 0.1921  loss_dice_0: 0.1481  loss_ce_1: 0.113  loss_mask_1: 0.1905  loss_dice_1: 0.15  loss_ce_2: 0.1134  loss_mask_2: 0.1926  loss_dice_2: 0.1487  loss_ce_3: 0.1132  loss_mask_3: 0.1872  loss_dice_3: 0.149  loss_ce_4: 0.1137  loss_mask_4: 0.1998  loss_dice_4: 0.146  loss_ce_5: 0.1136  loss_mask_5: 0.1899  loss_dice_5: 0.1512  loss_ce_6: 0.1146  loss_mask_6: 0.1894  loss_dice_6: 0.1453  loss_ce_7: 0.1144  loss_mask_7: 0.185  loss_dice_7: 0.1477  loss_ce_8: 0.1156  loss_mask_8: 0.1888  loss_dice_8: 0.1495  time: 0.5593  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:33:14] d2.utils.events INFO:  eta: 3:30:34  iter: 7599  total_loss: 4.936  loss_ce: 0.1011  loss_mask: 0.1671  loss_dice: 0.2027  loss_ce_0: 0.05809  loss_mask_0: 0.159  loss_dice_0: 0.2012  loss_ce_1: 0.09776  loss_mask_1: 0.1612  loss_dice_1: 0.2092  loss_ce_2: 0.09832  loss_mask_2: 0.1654  loss_dice_2: 0.207  loss_ce_3: 0.09789  loss_mask_3: 0.166  loss_dice_3: 0.1986  loss_ce_4: 0.09797  loss_mask_4: 0.1618  loss_dice_4: 0.208  loss_ce_5: 0.09756  loss_mask_5: 0.1594  loss_dice_5: 0.2069  loss_ce_6: 0.09898  loss_mask_6: 0.1696  loss_dice_6: 0.2078  loss_ce_7: 0.09928  loss_mask_7: 0.1667  loss_dice_7: 0.2027  loss_ce_8: 0.1001  loss_mask_8: 0.1669  loss_dice_8: 0.2017  time: 0.5594  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:33:26] d2.utils.events INFO:  eta: 3:30:14  iter: 7619  total_loss: 4.56  loss_ce: 0.09584  loss_mask: 0.1591  loss_dice: 0.139  loss_ce_0: 0.0562  loss_mask_0: 0.1675  loss_dice_0: 0.1403  loss_ce_1: 0.09498  loss_mask_1: 0.1645  loss_dice_1: 0.1408  loss_ce_2: 0.09584  loss_mask_2: 0.1534  loss_dice_2: 0.1444  loss_ce_3: 0.09547  loss_mask_3: 0.1706  loss_dice_3: 0.147  loss_ce_4: 0.09474  loss_mask_4: 0.1608  loss_dice_4: 0.1418  loss_ce_5: 0.09478  loss_mask_5: 0.1603  loss_dice_5: 0.1389  loss_ce_6: 0.09533  loss_mask_6: 0.1544  loss_dice_6: 0.1468  loss_ce_7: 0.09529  loss_mask_7: 0.1639  loss_dice_7: 0.1432  loss_ce_8: 0.0953  loss_mask_8: 0.1594  loss_dice_8: 0.1485  time: 0.5595  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:33:38] d2.utils.events INFO:  eta: 3:29:50  iter: 7639  total_loss: 4.841  loss_ce: 0.1338  loss_mask: 0.201  loss_dice: 0.1837  loss_ce_0: 0.06563  loss_mask_0: 0.2133  loss_dice_0: 0.1826  loss_ce_1: 0.134  loss_mask_1: 0.2086  loss_dice_1: 0.183  loss_ce_2: 0.1341  loss_mask_2: 0.2081  loss_dice_2: 0.1767  loss_ce_3: 0.1341  loss_mask_3: 0.1899  loss_dice_3: 0.1794  loss_ce_4: 0.1341  loss_mask_4: 0.2044  loss_dice_4: 0.1712  loss_ce_5: 0.1342  loss_mask_5: 0.2055  loss_dice_5: 0.1645  loss_ce_6: 0.1342  loss_mask_6: 0.2006  loss_dice_6: 0.1743  loss_ce_7: 0.1342  loss_mask_7: 0.2174  loss_dice_7: 0.1803  loss_ce_8: 0.134  loss_mask_8: 0.2143  loss_dice_8: 0.1834  time: 0.5596  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:33:49] d2.utils.events INFO:  eta: 3:29:36  iter: 7659  total_loss: 4.72  loss_ce: 0.1337  loss_mask: 0.1931  loss_dice: 0.1565  loss_ce_0: 0.06583  loss_mask_0: 0.2017  loss_dice_0: 0.1583  loss_ce_1: 0.1339  loss_mask_1: 0.2007  loss_dice_1: 0.1675  loss_ce_2: 0.134  loss_mask_2: 0.1977  loss_dice_2: 0.1558  loss_ce_3: 0.1339  loss_mask_3: 0.1981  loss_dice_3: 0.1581  loss_ce_4: 0.1341  loss_mask_4: 0.2003  loss_dice_4: 0.1538  loss_ce_5: 0.1342  loss_mask_5: 0.2027  loss_dice_5: 0.1615  loss_ce_6: 0.134  loss_mask_6: 0.1959  loss_dice_6: 0.1652  loss_ce_7: 0.134  loss_mask_7: 0.1949  loss_dice_7: 0.1637  loss_ce_8: 0.1339  loss_mask_8: 0.2065  loss_dice_8: 0.163  time: 0.5596  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:34:01] d2.utils.events INFO:  eta: 3:29:12  iter: 7679  total_loss: 5.1  loss_ce: 0.1636  loss_mask: 0.2165  loss_dice: 0.1551  loss_ce_0: 0.07886  loss_mask_0: 0.21  loss_dice_0: 0.1531  loss_ce_1: 0.158  loss_mask_1: 0.2103  loss_dice_1: 0.1604  loss_ce_2: 0.159  loss_mask_2: 0.2114  loss_dice_2: 0.1578  loss_ce_3: 0.1593  loss_mask_3: 0.2017  loss_dice_3: 0.1557  loss_ce_4: 0.1598  loss_mask_4: 0.1975  loss_dice_4: 0.1588  loss_ce_5: 0.1598  loss_mask_5: 0.2142  loss_dice_5: 0.1554  loss_ce_6: 0.1615  loss_mask_6: 0.2147  loss_dice_6: 0.1583  loss_ce_7: 0.1615  loss_mask_7: 0.2097  loss_dice_7: 0.1544  loss_ce_8: 0.1632  loss_mask_8: 0.2029  loss_dice_8: 0.1599  time: 0.5597  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:34:13] d2.utils.events INFO:  eta: 3:28:51  iter: 7699  total_loss: 4.604  loss_ce: 0.1315  loss_mask: 0.1902  loss_dice: 0.1647  loss_ce_0: 0.06527  loss_mask_0: 0.1991  loss_dice_0: 0.1681  loss_ce_1: 0.1359  loss_mask_1: 0.1925  loss_dice_1: 0.1732  loss_ce_2: 0.1347  loss_mask_2: 0.2022  loss_dice_2: 0.171  loss_ce_3: 0.1355  loss_mask_3: 0.1981  loss_dice_3: 0.1643  loss_ce_4: 0.1348  loss_mask_4: 0.201  loss_dice_4: 0.1623  loss_ce_5: 0.1347  loss_mask_5: 0.1903  loss_dice_5: 0.1701  loss_ce_6: 0.1325  loss_mask_6: 0.1938  loss_dice_6: 0.165  loss_ce_7: 0.1321  loss_mask_7: 0.1999  loss_dice_7: 0.1707  loss_ce_8: 0.1323  loss_mask_8: 0.1943  loss_dice_8: 0.1725  time: 0.5598  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:34:25] d2.utils.events INFO:  eta: 3:28:13  iter: 7719  total_loss: 4.752  loss_ce: 0.1251  loss_mask: 0.2208  loss_dice: 0.1484  loss_ce_0: 0.06881  loss_mask_0: 0.2225  loss_dice_0: 0.1434  loss_ce_1: 0.123  loss_mask_1: 0.2243  loss_dice_1: 0.1473  loss_ce_2: 0.1246  loss_mask_2: 0.2183  loss_dice_2: 0.1495  loss_ce_3: 0.1243  loss_mask_3: 0.2161  loss_dice_3: 0.1395  loss_ce_4: 0.1223  loss_mask_4: 0.2246  loss_dice_4: 0.1506  loss_ce_5: 0.1225  loss_mask_5: 0.2272  loss_dice_5: 0.1499  loss_ce_6: 0.1243  loss_mask_6: 0.2185  loss_dice_6: 0.1398  loss_ce_7: 0.1238  loss_mask_7: 0.2241  loss_dice_7: 0.1447  loss_ce_8: 0.1243  loss_mask_8: 0.2285  loss_dice_8: 0.1496  time: 0.5598  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:34:36] d2.utils.events INFO:  eta: 3:27:47  iter: 7739  total_loss: 4.911  loss_ce: 0.1168  loss_mask: 0.1958  loss_dice: 0.1546  loss_ce_0: 0.06572  loss_mask_0: 0.2076  loss_dice_0: 0.1506  loss_ce_1: 0.1155  loss_mask_1: 0.2094  loss_dice_1: 0.1584  loss_ce_2: 0.1167  loss_mask_2: 0.2041  loss_dice_2: 0.15  loss_ce_3: 0.1163  loss_mask_3: 0.206  loss_dice_3: 0.1479  loss_ce_4: 0.1149  loss_mask_4: 0.2063  loss_dice_4: 0.1535  loss_ce_5: 0.115  loss_mask_5: 0.2009  loss_dice_5: 0.151  loss_ce_6: 0.1162  loss_mask_6: 0.2121  loss_dice_6: 0.1544  loss_ce_7: 0.1158  loss_mask_7: 0.2102  loss_dice_7: 0.1498  loss_ce_8: 0.116  loss_mask_8: 0.2057  loss_dice_8: 0.1529  time: 0.5599  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:34:48] d2.utils.events INFO:  eta: 3:27:21  iter: 7759  total_loss: 4.65  loss_ce: 0.1282  loss_mask: 0.178  loss_dice: 0.1505  loss_ce_0: 0.06477  loss_mask_0: 0.1855  loss_dice_0: 0.1571  loss_ce_1: 0.1281  loss_mask_1: 0.1887  loss_dice_1: 0.1562  loss_ce_2: 0.1282  loss_mask_2: 0.1942  loss_dice_2: 0.1507  loss_ce_3: 0.1282  loss_mask_3: 0.1969  loss_dice_3: 0.1491  loss_ce_4: 0.1283  loss_mask_4: 0.1818  loss_dice_4: 0.1531  loss_ce_5: 0.1282  loss_mask_5: 0.191  loss_dice_5: 0.1537  loss_ce_6: 0.1283  loss_mask_6: 0.1859  loss_dice_6: 0.1549  loss_ce_7: 0.1283  loss_mask_7: 0.1834  loss_dice_7: 0.1536  loss_ce_8: 0.1282  loss_mask_8: 0.1829  loss_dice_8: 0.1581  time: 0.5600  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:35:00] d2.utils.events INFO:  eta: 3:26:42  iter: 7779  total_loss: 5.135  loss_ce: 0.1103  loss_mask: 0.2399  loss_dice: 0.1453  loss_ce_0: 0.06272  loss_mask_0: 0.2497  loss_dice_0: 0.1544  loss_ce_1: 0.1104  loss_mask_1: 0.2466  loss_dice_1: 0.1583  loss_ce_2: 0.1105  loss_mask_2: 0.2473  loss_dice_2: 0.155  loss_ce_3: 0.1105  loss_mask_3: 0.2443  loss_dice_3: 0.1546  loss_ce_4: 0.1101  loss_mask_4: 0.2427  loss_dice_4: 0.1536  loss_ce_5: 0.1102  loss_mask_5: 0.2492  loss_dice_5: 0.1552  loss_ce_6: 0.1103  loss_mask_6: 0.2496  loss_dice_6: 0.1611  loss_ce_7: 0.1102  loss_mask_7: 0.2505  loss_dice_7: 0.1571  loss_ce_8: 0.11  loss_mask_8: 0.2377  loss_dice_8: 0.156  time: 0.5600  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:35:11] d2.utils.events INFO:  eta: 3:26:28  iter: 7799  total_loss: 5.365  loss_ce: 0.1019  loss_mask: 0.1902  loss_dice: 0.1955  loss_ce_0: 0.05975  loss_mask_0: 0.1998  loss_dice_0: 0.1902  loss_ce_1: 0.1  loss_mask_1: 0.1908  loss_dice_1: 0.1976  loss_ce_2: 0.1006  loss_mask_2: 0.1937  loss_dice_2: 0.1859  loss_ce_3: 0.1002  loss_mask_3: 0.1996  loss_dice_3: 0.1915  loss_ce_4: 0.1003  loss_mask_4: 0.181  loss_dice_4: 0.1893  loss_ce_5: 0.1004  loss_mask_5: 0.1908  loss_dice_5: 0.1923  loss_ce_6: 0.1006  loss_mask_6: 0.1862  loss_dice_6: 0.1946  loss_ce_7: 0.101  loss_mask_7: 0.1868  loss_dice_7: 0.1994  loss_ce_8: 0.1013  loss_mask_8: 0.2005  loss_dice_8: 0.1945  time: 0.5600  data_time: 0.0020  lr: 0.0001  max_mem: 2811M
[07/11 12:35:23] d2.utils.events INFO:  eta: 3:26:14  iter: 7819  total_loss: 4.792  loss_ce: 0.1615  loss_mask: 0.1741  loss_dice: 0.1593  loss_ce_0: 0.07308  loss_mask_0: 0.1675  loss_dice_0: 0.1724  loss_ce_1: 0.1608  loss_mask_1: 0.1582  loss_dice_1: 0.1642  loss_ce_2: 0.1606  loss_mask_2: 0.1623  loss_dice_2: 0.1736  loss_ce_3: 0.1607  loss_mask_3: 0.1725  loss_dice_3: 0.1614  loss_ce_4: 0.1612  loss_mask_4: 0.1575  loss_dice_4: 0.1639  loss_ce_5: 0.1611  loss_mask_5: 0.1689  loss_dice_5: 0.163  loss_ce_6: 0.1613  loss_mask_6: 0.1604  loss_dice_6: 0.176  loss_ce_7: 0.1611  loss_mask_7: 0.163  loss_dice_7: 0.1639  loss_ce_8: 0.1618  loss_mask_8: 0.1637  loss_dice_8: 0.1662  time: 0.5601  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:35:35] d2.utils.events INFO:  eta: 3:25:53  iter: 7839  total_loss: 4.453  loss_ce: 0.107  loss_mask: 0.1763  loss_dice: 0.1309  loss_ce_0: 0.05831  loss_mask_0: 0.1831  loss_dice_0: 0.1364  loss_ce_1: 0.1096  loss_mask_1: 0.1753  loss_dice_1: 0.1356  loss_ce_2: 0.109  loss_mask_2: 0.1811  loss_dice_2: 0.1339  loss_ce_3: 0.1096  loss_mask_3: 0.1771  loss_dice_3: 0.1342  loss_ce_4: 0.1089  loss_mask_4: 0.1817  loss_dice_4: 0.1342  loss_ce_5: 0.1091  loss_mask_5: 0.1867  loss_dice_5: 0.1386  loss_ce_6: 0.1087  loss_mask_6: 0.1894  loss_dice_6: 0.1345  loss_ce_7: 0.1085  loss_mask_7: 0.19  loss_dice_7: 0.1322  loss_ce_8: 0.1074  loss_mask_8: 0.1847  loss_dice_8: 0.1348  time: 0.5602  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:35:46] d2.utils.events INFO:  eta: 3:25:30  iter: 7859  total_loss: 4.843  loss_ce: 0.1623  loss_mask: 0.1962  loss_dice: 0.1267  loss_ce_0: 0.07492  loss_mask_0: 0.1879  loss_dice_0: 0.1315  loss_ce_1: 0.1646  loss_mask_1: 0.1963  loss_dice_1: 0.1312  loss_ce_2: 0.1644  loss_mask_2: 0.1917  loss_dice_2: 0.1325  loss_ce_3: 0.1641  loss_mask_3: 0.198  loss_dice_3: 0.1382  loss_ce_4: 0.1639  loss_mask_4: 0.2057  loss_dice_4: 0.1293  loss_ce_5: 0.1638  loss_mask_5: 0.203  loss_dice_5: 0.129  loss_ce_6: 0.1635  loss_mask_6: 0.1966  loss_dice_6: 0.1309  loss_ce_7: 0.1631  loss_mask_7: 0.1971  loss_dice_7: 0.1299  loss_ce_8: 0.1628  loss_mask_8: 0.1905  loss_dice_8: 0.1337  time: 0.5603  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:35:57] d2.utils.events INFO:  eta: 3:24:53  iter: 7879  total_loss: 4.986  loss_ce: 0.1002  loss_mask: 0.1787  loss_dice: 0.1818  loss_ce_0: 0.05576  loss_mask_0: 0.1814  loss_dice_0: 0.1954  loss_ce_1: 0.1003  loss_mask_1: 0.1909  loss_dice_1: 0.1947  loss_ce_2: 0.1003  loss_mask_2: 0.1932  loss_dice_2: 0.1837  loss_ce_3: 0.1005  loss_mask_3: 0.1882  loss_dice_3: 0.1967  loss_ce_4: 0.1002  loss_mask_4: 0.18  loss_dice_4: 0.1909  loss_ce_5: 0.1002  loss_mask_5: 0.177  loss_dice_5: 0.1993  loss_ce_6: 0.1003  loss_mask_6: 0.1838  loss_dice_6: 0.1851  loss_ce_7: 0.1003  loss_mask_7: 0.19  loss_dice_7: 0.188  loss_ce_8: 0.1001  loss_mask_8: 0.1874  loss_dice_8: 0.2045  time: 0.5602  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:36:08] d2.utils.events INFO:  eta: 3:24:29  iter: 7899  total_loss: 5.064  loss_ce: 0.09996  loss_mask: 0.1924  loss_dice: 0.172  loss_ce_0: 0.05507  loss_mask_0: 0.1944  loss_dice_0: 0.1815  loss_ce_1: 0.1008  loss_mask_1: 0.1947  loss_dice_1: 0.177  loss_ce_2: 0.1008  loss_mask_2: 0.1975  loss_dice_2: 0.1783  loss_ce_3: 0.101  loss_mask_3: 0.1938  loss_dice_3: 0.1804  loss_ce_4: 0.1006  loss_mask_4: 0.1972  loss_dice_4: 0.171  loss_ce_5: 0.1006  loss_mask_5: 0.1961  loss_dice_5: 0.1712  loss_ce_6: 0.1004  loss_mask_6: 0.1876  loss_dice_6: 0.1809  loss_ce_7: 0.1005  loss_mask_7: 0.2005  loss_dice_7: 0.1792  loss_ce_8: 0.1  loss_mask_8: 0.1915  loss_dice_8: 0.1718  time: 0.5601  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:36:18] d2.utils.events INFO:  eta: 3:23:11  iter: 7919  total_loss: 4.749  loss_ce: 0.1001  loss_mask: 0.1601  loss_dice: 0.1712  loss_ce_0: 0.05473  loss_mask_0: 0.1579  loss_dice_0: 0.1713  loss_ce_1: 0.1007  loss_mask_1: 0.1588  loss_dice_1: 0.1723  loss_ce_2: 0.1004  loss_mask_2: 0.1632  loss_dice_2: 0.1734  loss_ce_3: 0.1007  loss_mask_3: 0.1601  loss_dice_3: 0.1648  loss_ce_4: 0.1006  loss_mask_4: 0.1638  loss_dice_4: 0.1728  loss_ce_5: 0.1006  loss_mask_5: 0.1599  loss_dice_5: 0.1758  loss_ce_6: 0.1004  loss_mask_6: 0.159  loss_dice_6: 0.1742  loss_ce_7: 0.1004  loss_mask_7: 0.1605  loss_dice_7: 0.1794  loss_ce_8: 0.1001  loss_mask_8: 0.1563  loss_dice_8: 0.1712  time: 0.5600  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:36:28] d2.utils.events INFO:  eta: 3:22:37  iter: 7939  total_loss: 4.83  loss_ce: 0.1289  loss_mask: 0.2314  loss_dice: 0.1336  loss_ce_0: 0.06535  loss_mask_0: 0.232  loss_dice_0: 0.136  loss_ce_1: 0.1288  loss_mask_1: 0.2202  loss_dice_1: 0.1337  loss_ce_2: 0.1288  loss_mask_2: 0.222  loss_dice_2: 0.1345  loss_ce_3: 0.1288  loss_mask_3: 0.2252  loss_dice_3: 0.1332  loss_ce_4: 0.1288  loss_mask_4: 0.2225  loss_dice_4: 0.1361  loss_ce_5: 0.1289  loss_mask_5: 0.2243  loss_dice_5: 0.1332  loss_ce_6: 0.1288  loss_mask_6: 0.2373  loss_dice_6: 0.1371  loss_ce_7: 0.1288  loss_mask_7: 0.2163  loss_dice_7: 0.1311  loss_ce_8: 0.1289  loss_mask_8: 0.234  loss_dice_8: 0.1357  time: 0.5599  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:36:39] d2.utils.events INFO:  eta: 3:21:39  iter: 7959  total_loss: 4.781  loss_ce: 0.1373  loss_mask: 0.1021  loss_dice: 0.1731  loss_ce_0: 0.07172  loss_mask_0: 0.1043  loss_dice_0: 0.1739  loss_ce_1: 0.1335  loss_mask_1: 0.104  loss_dice_1: 0.1744  loss_ce_2: 0.1342  loss_mask_2: 0.09882  loss_dice_2: 0.1698  loss_ce_3: 0.1337  loss_mask_3: 0.1044  loss_dice_3: 0.1744  loss_ce_4: 0.1342  loss_mask_4: 0.1003  loss_dice_4: 0.1726  loss_ce_5: 0.1339  loss_mask_5: 0.09912  loss_dice_5: 0.1714  loss_ce_6: 0.1353  loss_mask_6: 0.1014  loss_dice_6: 0.1628  loss_ce_7: 0.1357  loss_mask_7: 0.1019  loss_dice_7: 0.1709  loss_ce_8: 0.1364  loss_mask_8: 0.09763  loss_dice_8: 0.1627  time: 0.5598  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:36:49] d2.utils.events INFO:  eta: 3:21:18  iter: 7979  total_loss: 4.589  loss_ce: 0.1183  loss_mask: 0.1581  loss_dice: 0.1667  loss_ce_0: 0.05857  loss_mask_0: 0.15  loss_dice_0: 0.1675  loss_ce_1: 0.1203  loss_mask_1: 0.1613  loss_dice_1: 0.1648  loss_ce_2: 0.1194  loss_mask_2: 0.1578  loss_dice_2: 0.1631  loss_ce_3: 0.1199  loss_mask_3: 0.1548  loss_dice_3: 0.162  loss_ce_4: 0.12  loss_mask_4: 0.153  loss_dice_4: 0.1671  loss_ce_5: 0.12  loss_mask_5: 0.1586  loss_dice_5: 0.1654  loss_ce_6: 0.1193  loss_mask_6: 0.1518  loss_dice_6: 0.1628  loss_ce_7: 0.1194  loss_mask_7: 0.1586  loss_dice_7: 0.1613  loss_ce_8: 0.119  loss_mask_8: 0.1539  loss_dice_8: 0.1626  time: 0.5597  data_time: 0.0020  lr: 0.0001  max_mem: 2811M
[07/11 12:36:59] d2.utils.events INFO:  eta: 3:20:49  iter: 7999  total_loss: 4.744  loss_ce: 0.1069  loss_mask: 0.1911  loss_dice: 0.1596  loss_ce_0: 0.05704  loss_mask_0: 0.1922  loss_dice_0: 0.1568  loss_ce_1: 0.1072  loss_mask_1: 0.1925  loss_dice_1: 0.1649  loss_ce_2: 0.1074  loss_mask_2: 0.1923  loss_dice_2: 0.161  loss_ce_3: 0.1073  loss_mask_3: 0.2024  loss_dice_3: 0.1653  loss_ce_4: 0.1069  loss_mask_4: 0.1929  loss_dice_4: 0.1631  loss_ce_5: 0.1069  loss_mask_5: 0.1988  loss_dice_5: 0.161  loss_ce_6: 0.1072  loss_mask_6: 0.1915  loss_dice_6: 0.1683  loss_ce_7: 0.1069  loss_mask_7: 0.1946  loss_dice_7: 0.1653  loss_ce_8: 0.1069  loss_mask_8: 0.196  loss_dice_8: 0.1593  time: 0.5596  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:37:10] d2.utils.events INFO:  eta: 3:20:19  iter: 8019  total_loss: 4.869  loss_ce: 0.1295  loss_mask: 0.212  loss_dice: 0.1342  loss_ce_0: 0.06542  loss_mask_0: 0.2162  loss_dice_0: 0.1367  loss_ce_1: 0.1296  loss_mask_1: 0.2204  loss_dice_1: 0.1424  loss_ce_2: 0.1296  loss_mask_2: 0.2236  loss_dice_2: 0.144  loss_ce_3: 0.1297  loss_mask_3: 0.209  loss_dice_3: 0.1441  loss_ce_4: 0.1297  loss_mask_4: 0.2111  loss_dice_4: 0.1398  loss_ce_5: 0.1296  loss_mask_5: 0.2247  loss_dice_5: 0.1408  loss_ce_6: 0.1296  loss_mask_6: 0.2002  loss_dice_6: 0.1344  loss_ce_7: 0.1297  loss_mask_7: 0.2108  loss_dice_7: 0.1338  loss_ce_8: 0.1296  loss_mask_8: 0.2205  loss_dice_8: 0.1391  time: 0.5595  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:37:21] d2.utils.events INFO:  eta: 3:19:46  iter: 8039  total_loss: 5.204  loss_ce: 0.1005  loss_mask: 0.2088  loss_dice: 0.1665  loss_ce_0: 0.05542  loss_mask_0: 0.2213  loss_dice_0: 0.1732  loss_ce_1: 0.1002  loss_mask_1: 0.2216  loss_dice_1: 0.1745  loss_ce_2: 0.1004  loss_mask_2: 0.2147  loss_dice_2: 0.1661  loss_ce_3: 0.1003  loss_mask_3: 0.209  loss_dice_3: 0.1684  loss_ce_4: 0.1  loss_mask_4: 0.2071  loss_dice_4: 0.162  loss_ce_5: 0.09995  loss_mask_5: 0.2106  loss_dice_5: 0.1721  loss_ce_6: 0.1003  loss_mask_6: 0.2147  loss_dice_6: 0.1677  loss_ce_7: 0.1003  loss_mask_7: 0.215  loss_dice_7: 0.1673  loss_ce_8: 0.1003  loss_mask_8: 0.211  loss_dice_8: 0.1655  time: 0.5594  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:37:31] d2.utils.events INFO:  eta: 3:19:13  iter: 8059  total_loss: 5.008  loss_ce: 0.0965  loss_mask: 0.2117  loss_dice: 0.1673  loss_ce_0: 0.05409  loss_mask_0: 0.2044  loss_dice_0: 0.1673  loss_ce_1: 0.09666  loss_mask_1: 0.2106  loss_dice_1: 0.1662  loss_ce_2: 0.09685  loss_mask_2: 0.2021  loss_dice_2: 0.162  loss_ce_3: 0.09672  loss_mask_3: 0.2178  loss_dice_3: 0.1632  loss_ce_4: 0.09646  loss_mask_4: 0.2147  loss_dice_4: 0.1635  loss_ce_5: 0.09684  loss_mask_5: 0.2229  loss_dice_5: 0.1624  loss_ce_6: 0.09647  loss_mask_6: 0.2191  loss_dice_6: 0.1645  loss_ce_7: 0.09654  loss_mask_7: 0.2106  loss_dice_7: 0.1565  loss_ce_8: 0.09663  loss_mask_8: 0.2237  loss_dice_8: 0.162  time: 0.5593  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:37:41] d2.utils.events INFO:  eta: 3:18:20  iter: 8079  total_loss: 5.322  loss_ce: 0.1447  loss_mask: 0.1684  loss_dice: 0.1537  loss_ce_0: 0.07316  loss_mask_0: 0.1697  loss_dice_0: 0.1628  loss_ce_1: 0.1409  loss_mask_1: 0.1702  loss_dice_1: 0.162  loss_ce_2: 0.1417  loss_mask_2: 0.1704  loss_dice_2: 0.1576  loss_ce_3: 0.1414  loss_mask_3: 0.1688  loss_dice_3: 0.1567  loss_ce_4: 0.1417  loss_mask_4: 0.1655  loss_dice_4: 0.1535  loss_ce_5: 0.1416  loss_mask_5: 0.169  loss_dice_5: 0.1578  loss_ce_6: 0.1427  loss_mask_6: 0.1667  loss_dice_6: 0.1567  loss_ce_7: 0.1429  loss_mask_7: 0.1679  loss_dice_7: 0.153  loss_ce_8: 0.1441  loss_mask_8: 0.1744  loss_dice_8: 0.1566  time: 0.5592  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:37:51] d2.utils.events INFO:  eta: 3:17:09  iter: 8099  total_loss: 4.904  loss_ce: 0.1141  loss_mask: 0.1861  loss_dice: 0.1539  loss_ce_0: 0.05769  loss_mask_0: 0.187  loss_dice_0: 0.1467  loss_ce_1: 0.1176  loss_mask_1: 0.182  loss_dice_1: 0.15  loss_ce_2: 0.1165  loss_mask_2: 0.1847  loss_dice_2: 0.1547  loss_ce_3: 0.1168  loss_mask_3: 0.1867  loss_dice_3: 0.15  loss_ce_4: 0.1168  loss_mask_4: 0.1874  loss_dice_4: 0.1486  loss_ce_5: 0.1168  loss_mask_5: 0.1858  loss_dice_5: 0.1552  loss_ce_6: 0.1159  loss_mask_6: 0.1883  loss_dice_6: 0.1478  loss_ce_7: 0.1157  loss_mask_7: 0.1877  loss_dice_7: 0.1591  loss_ce_8: 0.1148  loss_mask_8: 0.1799  loss_dice_8: 0.1569  time: 0.5591  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:38:02] d2.utils.events INFO:  eta: 3:16:20  iter: 8119  total_loss: 5.257  loss_ce: 0.1017  loss_mask: 0.1952  loss_dice: 0.1465  loss_ce_0: 0.05519  loss_mask_0: 0.1848  loss_dice_0: 0.1453  loss_ce_1: 0.09959  loss_mask_1: 0.1843  loss_dice_1: 0.1531  loss_ce_2: 0.09973  loss_mask_2: 0.1776  loss_dice_2: 0.1426  loss_ce_3: 0.09976  loss_mask_3: 0.1933  loss_dice_3: 0.1479  loss_ce_4: 0.09999  loss_mask_4: 0.1881  loss_dice_4: 0.1462  loss_ce_5: 0.09988  loss_mask_5: 0.1835  loss_dice_5: 0.1449  loss_ce_6: 0.1005  loss_mask_6: 0.1879  loss_dice_6: 0.1464  loss_ce_7: 0.1006  loss_mask_7: 0.1862  loss_dice_7: 0.1466  loss_ce_8: 0.1015  loss_mask_8: 0.1864  loss_dice_8: 0.146  time: 0.5590  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:38:12] d2.utils.events INFO:  eta: 3:15:19  iter: 8139  total_loss: 4.574  loss_ce: 0.0949  loss_mask: 0.2097  loss_dice: 0.1221  loss_ce_0: 0.05377  loss_mask_0: 0.2113  loss_dice_0: 0.1204  loss_ce_1: 0.09314  loss_mask_1: 0.2158  loss_dice_1: 0.1232  loss_ce_2: 0.09362  loss_mask_2: 0.2174  loss_dice_2: 0.1201  loss_ce_3: 0.09342  loss_mask_3: 0.2036  loss_dice_3: 0.121  loss_ce_4: 0.09339  loss_mask_4: 0.2202  loss_dice_4: 0.1209  loss_ce_5: 0.09336  loss_mask_5: 0.2185  loss_dice_5: 0.1209  loss_ce_6: 0.09386  loss_mask_6: 0.2114  loss_dice_6: 0.1188  loss_ce_7: 0.09397  loss_mask_7: 0.2057  loss_dice_7: 0.1196  loss_ce_8: 0.09447  loss_mask_8: 0.2102  loss_dice_8: 0.1222  time: 0.5589  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:38:23] d2.utils.events INFO:  eta: 3:14:14  iter: 8159  total_loss: 5.161  loss_ce: 0.08412  loss_mask: 0.2144  loss_dice: 0.1663  loss_ce_0: 0.05027  loss_mask_0: 0.2021  loss_dice_0: 0.1661  loss_ce_1: 0.08417  loss_mask_1: 0.2005  loss_dice_1: 0.1623  loss_ce_2: 0.08466  loss_mask_2: 0.2114  loss_dice_2: 0.1636  loss_ce_3: 0.08445  loss_mask_3: 0.2143  loss_dice_3: 0.1668  loss_ce_4: 0.08389  loss_mask_4: 0.2202  loss_dice_4: 0.1699  loss_ce_5: 0.08389  loss_mask_5: 0.2114  loss_dice_5: 0.1664  loss_ce_6: 0.08412  loss_mask_6: 0.2024  loss_dice_6: 0.1604  loss_ce_7: 0.08412  loss_mask_7: 0.2138  loss_dice_7: 0.1673  loss_ce_8: 0.08366  loss_mask_8: 0.2088  loss_dice_8: 0.1652  time: 0.5588  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:38:33] d2.utils.events INFO:  eta: 3:13:27  iter: 8179  total_loss: 4.265  loss_ce: 0.08203  loss_mask: 0.1886  loss_dice: 0.1401  loss_ce_0: 0.04939  loss_mask_0: 0.1926  loss_dice_0: 0.1467  loss_ce_1: 0.08204  loss_mask_1: 0.196  loss_dice_1: 0.1407  loss_ce_2: 0.08231  loss_mask_2: 0.1946  loss_dice_2: 0.1435  loss_ce_3: 0.08219  loss_mask_3: 0.1871  loss_dice_3: 0.1391  loss_ce_4: 0.08187  loss_mask_4: 0.1869  loss_dice_4: 0.1396  loss_ce_5: 0.08181  loss_mask_5: 0.1859  loss_dice_5: 0.1468  loss_ce_6: 0.08207  loss_mask_6: 0.1895  loss_dice_6: 0.1433  loss_ce_7: 0.08207  loss_mask_7: 0.1877  loss_dice_7: 0.1407  loss_ce_8: 0.08171  loss_mask_8: 0.1926  loss_dice_8: 0.1444  time: 0.5587  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:38:44] d2.utils.events INFO:  eta: 3:12:37  iter: 8199  total_loss: 5.164  loss_ce: 0.06802  loss_mask: 0.2184  loss_dice: 0.1697  loss_ce_0: 0.04539  loss_mask_0: 0.2162  loss_dice_0: 0.1668  loss_ce_1: 0.06637  loss_mask_1: 0.216  loss_dice_1: 0.1656  loss_ce_2: 0.06682  loss_mask_2: 0.2231  loss_dice_2: 0.1719  loss_ce_3: 0.06662  loss_mask_3: 0.2184  loss_dice_3: 0.1711  loss_ce_4: 0.06694  loss_mask_4: 0.222  loss_dice_4: 0.1713  loss_ce_5: 0.06665  loss_mask_5: 0.2154  loss_dice_5: 0.1671  loss_ce_6: 0.06711  loss_mask_6: 0.2048  loss_dice_6: 0.1674  loss_ce_7: 0.06741  loss_mask_7: 0.2185  loss_dice_7: 0.1732  loss_ce_8: 0.0675  loss_mask_8: 0.2158  loss_dice_8: 0.1672  time: 0.5586  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:38:54] d2.utils.events INFO:  eta: 3:11:35  iter: 8219  total_loss: 4.992  loss_ce: 0.1415  loss_mask: 0.2055  loss_dice: 0.1507  loss_ce_0: 0.06865  loss_mask_0: 0.2013  loss_dice_0: 0.1498  loss_ce_1: 0.1415  loss_mask_1: 0.2168  loss_dice_1: 0.1491  loss_ce_2: 0.1414  loss_mask_2: 0.2089  loss_dice_2: 0.1563  loss_ce_3: 0.1416  loss_mask_3: 0.2046  loss_dice_3: 0.1527  loss_ce_4: 0.1416  loss_mask_4: 0.2008  loss_dice_4: 0.1504  loss_ce_5: 0.1416  loss_mask_5: 0.2109  loss_dice_5: 0.1496  loss_ce_6: 0.1415  loss_mask_6: 0.2081  loss_dice_6: 0.1506  loss_ce_7: 0.1414  loss_mask_7: 0.2076  loss_dice_7: 0.1498  loss_ce_8: 0.1417  loss_mask_8: 0.2018  loss_dice_8: 0.1501  time: 0.5585  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:39:05] d2.utils.events INFO:  eta: 3:11:10  iter: 8239  total_loss: 4.779  loss_ce: 0.07502  loss_mask: 0.1378  loss_dice: 0.1663  loss_ce_0: 0.04481  loss_mask_0: 0.1396  loss_dice_0: 0.1658  loss_ce_1: 0.07613  loss_mask_1: 0.1418  loss_dice_1: 0.1667  loss_ce_2: 0.07575  loss_mask_2: 0.1449  loss_dice_2: 0.1744  loss_ce_3: 0.0758  loss_mask_3: 0.1581  loss_dice_3: 0.1642  loss_ce_4: 0.07568  loss_mask_4: 0.1503  loss_dice_4: 0.1666  loss_ce_5: 0.07571  loss_mask_5: 0.148  loss_dice_5: 0.1688  loss_ce_6: 0.07553  loss_mask_6: 0.1466  loss_dice_6: 0.1598  loss_ce_7: 0.07553  loss_mask_7: 0.1546  loss_dice_7: 0.1643  loss_ce_8: 0.07517  loss_mask_8: 0.1386  loss_dice_8: 0.1674  time: 0.5585  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:39:16] d2.utils.events INFO:  eta: 3:10:42  iter: 8259  total_loss: 4.961  loss_ce: 0.07653  loss_mask: 0.2072  loss_dice: 0.1548  loss_ce_0: 0.04509  loss_mask_0: 0.2072  loss_dice_0: 0.1507  loss_ce_1: 0.07743  loss_mask_1: 0.2086  loss_dice_1: 0.1501  loss_ce_2: 0.07717  loss_mask_2: 0.2151  loss_dice_2: 0.1528  loss_ce_3: 0.07726  loss_mask_3: 0.2138  loss_dice_3: 0.1544  loss_ce_4: 0.07714  loss_mask_4: 0.2124  loss_dice_4: 0.1479  loss_ce_5: 0.07724  loss_mask_5: 0.2059  loss_dice_5: 0.1516  loss_ce_6: 0.07696  loss_mask_6: 0.214  loss_dice_6: 0.1529  loss_ce_7: 0.07702  loss_mask_7: 0.2067  loss_dice_7: 0.1573  loss_ce_8: 0.07669  loss_mask_8: 0.2149  loss_dice_8: 0.1574  time: 0.5584  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:39:27] d2.utils.events INFO:  eta: 3:10:21  iter: 8279  total_loss: 4.921  loss_ce: 0.07385  loss_mask: 0.2033  loss_dice: 0.1755  loss_ce_0: 0.04431  loss_mask_0: 0.2079  loss_dice_0: 0.1711  loss_ce_1: 0.07382  loss_mask_1: 0.2119  loss_dice_1: 0.1744  loss_ce_2: 0.07363  loss_mask_2: 0.2028  loss_dice_2: 0.1709  loss_ce_3: 0.07367  loss_mask_3: 0.2046  loss_dice_3: 0.1723  loss_ce_4: 0.07391  loss_mask_4: 0.1981  loss_dice_4: 0.1656  loss_ce_5: 0.07388  loss_mask_5: 0.1971  loss_dice_5: 0.1685  loss_ce_6: 0.07376  loss_mask_6: 0.1991  loss_dice_6: 0.1653  loss_ce_7: 0.07385  loss_mask_7: 0.1998  loss_dice_7: 0.175  loss_ce_8: 0.07385  loss_mask_8: 0.1993  loss_dice_8: 0.1711  time: 0.5584  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:39:38] d2.utils.events INFO:  eta: 3:09:55  iter: 8299  total_loss: 5.463  loss_ce: 0.07813  loss_mask: 0.2329  loss_dice: 0.1894  loss_ce_0: 0.04515  loss_mask_0: 0.238  loss_dice_0: 0.1865  loss_ce_1: 0.07886  loss_mask_1: 0.2312  loss_dice_1: 0.1917  loss_ce_2: 0.07868  loss_mask_2: 0.2427  loss_dice_2: 0.1933  loss_ce_3: 0.07872  loss_mask_3: 0.2348  loss_dice_3: 0.1951  loss_ce_4: 0.07869  loss_mask_4: 0.2282  loss_dice_4: 0.1825  loss_ce_5: 0.07876  loss_mask_5: 0.2335  loss_dice_5: 0.1783  loss_ce_6: 0.07848  loss_mask_6: 0.2374  loss_dice_6: 0.1845  loss_ce_7: 0.07838  loss_mask_7: 0.2348  loss_dice_7: 0.1954  loss_ce_8: 0.0782  loss_mask_8: 0.2292  loss_dice_8: 0.1894  time: 0.5584  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:39:48] d2.utils.events INFO:  eta: 3:09:23  iter: 8319  total_loss: 4.908  loss_ce: 0.1341  loss_mask: 0.2285  loss_dice: 0.1367  loss_ce_0: 0.06751  loss_mask_0: 0.2275  loss_dice_0: 0.1394  loss_ce_1: 0.1336  loss_mask_1: 0.2269  loss_dice_1: 0.1359  loss_ce_2: 0.1337  loss_mask_2: 0.2296  loss_dice_2: 0.1374  loss_ce_3: 0.1338  loss_mask_3: 0.2263  loss_dice_3: 0.1344  loss_ce_4: 0.1338  loss_mask_4: 0.229  loss_dice_4: 0.1381  loss_ce_5: 0.1338  loss_mask_5: 0.226  loss_dice_5: 0.1349  loss_ce_6: 0.1339  loss_mask_6: 0.2319  loss_dice_6: 0.1355  loss_ce_7: 0.134  loss_mask_7: 0.2222  loss_dice_7: 0.1309  loss_ce_8: 0.1341  loss_mask_8: 0.2277  loss_dice_8: 0.1364  time: 0.5583  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:39:59] d2.utils.events INFO:  eta: 3:08:48  iter: 8339  total_loss: 5.082  loss_ce: 0.09046  loss_mask: 0.1523  loss_dice: 0.1881  loss_ce_0: 0.04819  loss_mask_0: 0.1637  loss_dice_0: 0.1904  loss_ce_1: 0.09258  loss_mask_1: 0.1563  loss_dice_1: 0.1779  loss_ce_2: 0.09189  loss_mask_2: 0.1581  loss_dice_2: 0.1876  loss_ce_3: 0.09203  loss_mask_3: 0.1515  loss_dice_3: 0.1799  loss_ce_4: 0.09196  loss_mask_4: 0.1658  loss_dice_4: 0.1799  loss_ce_5: 0.09235  loss_mask_5: 0.1552  loss_dice_5: 0.169  loss_ce_6: 0.0914  loss_mask_6: 0.16  loss_dice_6: 0.1894  loss_ce_7: 0.09133  loss_mask_7: 0.1602  loss_dice_7: 0.1876  loss_ce_8: 0.09095  loss_mask_8: 0.1524  loss_dice_8: 0.1896  time: 0.5582  data_time: 0.0020  lr: 0.0001  max_mem: 2811M
[07/11 12:40:10] d2.utils.events INFO:  eta: 3:08:09  iter: 8359  total_loss: 4.658  loss_ce: 0.08618  loss_mask: 0.2011  loss_dice: 0.1594  loss_ce_0: 0.04748  loss_mask_0: 0.204  loss_dice_0: 0.1525  loss_ce_1: 0.08606  loss_mask_1: 0.1945  loss_dice_1: 0.1558  loss_ce_2: 0.08578  loss_mask_2: 0.2028  loss_dice_2: 0.152  loss_ce_3: 0.08584  loss_mask_3: 0.1966  loss_dice_3: 0.1542  loss_ce_4: 0.08625  loss_mask_4: 0.1967  loss_dice_4: 0.1549  loss_ce_5: 0.08628  loss_mask_5: 0.1989  loss_dice_5: 0.1562  loss_ce_6: 0.08618  loss_mask_6: 0.2089  loss_dice_6: 0.1582  loss_ce_7: 0.08615  loss_mask_7: 0.1991  loss_dice_7: 0.1561  loss_ce_8: 0.08632  loss_mask_8: 0.1999  loss_dice_8: 0.1606  time: 0.5582  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:40:21] d2.utils.events INFO:  eta: 3:07:05  iter: 8379  total_loss: 4.72  loss_ce: 0.07335  loss_mask: 0.1695  loss_dice: 0.1497  loss_ce_0: 0.04488  loss_mask_0: 0.1698  loss_dice_0: 0.1504  loss_ce_1: 0.07093  loss_mask_1: 0.1675  loss_dice_1: 0.1544  loss_ce_2: 0.07138  loss_mask_2: 0.1691  loss_dice_2: 0.1513  loss_ce_3: 0.07133  loss_mask_3: 0.1704  loss_dice_3: 0.1512  loss_ce_4: 0.07179  loss_mask_4: 0.1607  loss_dice_4: 0.1519  loss_ce_5: 0.07159  loss_mask_5: 0.1703  loss_dice_5: 0.1521  loss_ce_6: 0.07221  loss_mask_6: 0.1695  loss_dice_6: 0.148  loss_ce_7: 0.07229  loss_mask_7: 0.173  loss_dice_7: 0.1519  loss_ce_8: 0.07297  loss_mask_8: 0.1662  loss_dice_8: 0.1519  time: 0.5581  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:40:31] d2.utils.events INFO:  eta: 3:05:52  iter: 8399  total_loss: 5.039  loss_ce: 0.07659  loss_mask: 0.2015  loss_dice: 0.1445  loss_ce_0: 0.04506  loss_mask_0: 0.2017  loss_dice_0: 0.1483  loss_ce_1: 0.07764  loss_mask_1: 0.2046  loss_dice_1: 0.1485  loss_ce_2: 0.07778  loss_mask_2: 0.1925  loss_dice_2: 0.1515  loss_ce_3: 0.07764  loss_mask_3: 0.2008  loss_dice_3: 0.1433  loss_ce_4: 0.07723  loss_mask_4: 0.1959  loss_dice_4: 0.1492  loss_ce_5: 0.07721  loss_mask_5: 0.2069  loss_dice_5: 0.1469  loss_ce_6: 0.07712  loss_mask_6: 0.2005  loss_dice_6: 0.1461  loss_ce_7: 0.07693  loss_mask_7: 0.2027  loss_dice_7: 0.1421  loss_ce_8: 0.07653  loss_mask_8: 0.21  loss_dice_8: 0.1388  time: 0.5581  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:40:42] d2.utils.events INFO:  eta: 3:04:48  iter: 8419  total_loss: 4.429  loss_ce: 0.172  loss_mask: 0.1702  loss_dice: 0.1288  loss_ce_0: 0.08532  loss_mask_0: 0.1699  loss_dice_0: 0.1241  loss_ce_1: 0.1666  loss_mask_1: 0.182  loss_dice_1: 0.1239  loss_ce_2: 0.1675  loss_mask_2: 0.174  loss_dice_2: 0.1226  loss_ce_3: 0.1676  loss_mask_3: 0.1691  loss_dice_3: 0.1263  loss_ce_4: 0.1685  loss_mask_4: 0.1681  loss_dice_4: 0.1194  loss_ce_5: 0.1678  loss_mask_5: 0.1648  loss_dice_5: 0.1249  loss_ce_6: 0.1695  loss_mask_6: 0.1786  loss_dice_6: 0.1213  loss_ce_7: 0.1699  loss_mask_7: 0.1685  loss_dice_7: 0.1313  loss_ce_8: 0.1712  loss_mask_8: 0.1659  loss_dice_8: 0.1231  time: 0.5580  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:40:53] d2.utils.events INFO:  eta: 3:04:19  iter: 8439  total_loss: 4.44  loss_ce: 0.1284  loss_mask: 0.176  loss_dice: 0.1144  loss_ce_0: 0.0656  loss_mask_0: 0.1719  loss_dice_0: 0.1184  loss_ce_1: 0.1284  loss_mask_1: 0.1846  loss_dice_1: 0.1201  loss_ce_2: 0.1283  loss_mask_2: 0.1843  loss_dice_2: 0.1173  loss_ce_3: 0.1283  loss_mask_3: 0.1816  loss_dice_3: 0.1231  loss_ce_4: 0.1283  loss_mask_4: 0.1783  loss_dice_4: 0.1181  loss_ce_5: 0.1282  loss_mask_5: 0.1863  loss_dice_5: 0.1172  loss_ce_6: 0.1283  loss_mask_6: 0.1838  loss_dice_6: 0.1202  loss_ce_7: 0.1283  loss_mask_7: 0.1838  loss_dice_7: 0.1187  loss_ce_8: 0.1284  loss_mask_8: 0.1834  loss_dice_8: 0.1176  time: 0.5580  data_time: 0.0020  lr: 0.0001  max_mem: 2811M
[07/11 12:41:03] d2.utils.events INFO:  eta: 3:03:53  iter: 8459  total_loss: 4.696  loss_ce: 0.1186  loss_mask: 0.2074  loss_dice: 0.1506  loss_ce_0: 0.0553  loss_mask_0: 0.2077  loss_dice_0: 0.1451  loss_ce_1: 0.122  loss_mask_1: 0.213  loss_dice_1: 0.1553  loss_ce_2: 0.1207  loss_mask_2: 0.2097  loss_dice_2: 0.1432  loss_ce_3: 0.1209  loss_mask_3: 0.2172  loss_dice_3: 0.1474  loss_ce_4: 0.1215  loss_mask_4: 0.2049  loss_dice_4: 0.15  loss_ce_5: 0.1225  loss_mask_5: 0.2185  loss_dice_5: 0.1457  loss_ce_6: 0.1202  loss_mask_6: 0.2011  loss_dice_6: 0.1541  loss_ce_7: 0.12  loss_mask_7: 0.2148  loss_dice_7: 0.1523  loss_ce_8: 0.1196  loss_mask_8: 0.2053  loss_dice_8: 0.1434  time: 0.5579  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:41:14] d2.utils.events INFO:  eta: 3:03:29  iter: 8479  total_loss: 4.73  loss_ce: 0.1006  loss_mask: 0.1921  loss_dice: 0.1777  loss_ce_0: 0.05269  loss_mask_0: 0.1961  loss_dice_0: 0.1836  loss_ce_1: 0.09377  loss_mask_1: 0.1991  loss_dice_1: 0.1861  loss_ce_2: 0.09458  loss_mask_2: 0.179  loss_dice_2: 0.1725  loss_ce_3: 0.09469  loss_mask_3: 0.1863  loss_dice_3: 0.1714  loss_ce_4: 0.09577  loss_mask_4: 0.1871  loss_dice_4: 0.1793  loss_ce_5: 0.09589  loss_mask_5: 0.1901  loss_dice_5: 0.174  loss_ce_6: 0.09728  loss_mask_6: 0.1914  loss_dice_6: 0.1769  loss_ce_7: 0.09754  loss_mask_7: 0.1921  loss_dice_7: 0.1732  loss_ce_8: 0.1001  loss_mask_8: 0.1872  loss_dice_8: 0.1737  time: 0.5578  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:41:25] d2.utils.events INFO:  eta: 3:03:14  iter: 8499  total_loss: 5.19  loss_ce: 0.08717  loss_mask: 0.2083  loss_dice: 0.1584  loss_ce_0: 0.0504  loss_mask_0: 0.2081  loss_dice_0: 0.161  loss_ce_1: 0.08424  loss_mask_1: 0.207  loss_dice_1: 0.1595  loss_ce_2: 0.08517  loss_mask_2: 0.211  loss_dice_2: 0.1693  loss_ce_3: 0.08482  loss_mask_3: 0.2125  loss_dice_3: 0.169  loss_ce_4: 0.08419  loss_mask_4: 0.2125  loss_dice_4: 0.1635  loss_ce_5: 0.08384  loss_mask_5: 0.2129  loss_dice_5: 0.1584  loss_ce_6: 0.08529  loss_mask_6: 0.2079  loss_dice_6: 0.1596  loss_ce_7: 0.08512  loss_mask_7: 0.2148  loss_dice_7: 0.1619  loss_ce_8: 0.08637  loss_mask_8: 0.2141  loss_dice_8: 0.1642  time: 0.5578  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:41:35] d2.utils.events INFO:  eta: 3:02:55  iter: 8519  total_loss: 4.868  loss_ce: 0.08328  loss_mask: 0.1908  loss_dice: 0.1555  loss_ce_0: 0.04952  loss_mask_0: 0.1964  loss_dice_0: 0.159  loss_ce_1: 0.08167  loss_mask_1: 0.2058  loss_dice_1: 0.1527  loss_ce_2: 0.08262  loss_mask_2: 0.1969  loss_dice_2: 0.1511  loss_ce_3: 0.08233  loss_mask_3: 0.2034  loss_dice_3: 0.1562  loss_ce_4: 0.08159  loss_mask_4: 0.2035  loss_dice_4: 0.1563  loss_ce_5: 0.08127  loss_mask_5: 0.1998  loss_dice_5: 0.1493  loss_ce_6: 0.08224  loss_mask_6: 0.2061  loss_dice_6: 0.1635  loss_ce_7: 0.08214  loss_mask_7: 0.2006  loss_dice_7: 0.1521  loss_ce_8: 0.08246  loss_mask_8: 0.201  loss_dice_8: 0.1543  time: 0.5577  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:41:45] d2.utils.events INFO:  eta: 3:02:04  iter: 8539  total_loss: 4.59  loss_ce: 0.07789  loss_mask: 0.1656  loss_dice: 0.1798  loss_ce_0: 0.04776  loss_mask_0: 0.1698  loss_dice_0: 0.177  loss_ce_1: 0.07647  loss_mask_1: 0.1688  loss_dice_1: 0.1745  loss_ce_2: 0.07699  loss_mask_2: 0.1657  loss_dice_2: 0.1755  loss_ce_3: 0.07687  loss_mask_3: 0.1675  loss_dice_3: 0.1846  loss_ce_4: 0.07656  loss_mask_4: 0.1768  loss_dice_4: 0.1741  loss_ce_5: 0.07635  loss_mask_5: 0.1702  loss_dice_5: 0.1843  loss_ce_6: 0.07691  loss_mask_6: 0.171  loss_dice_6: 0.1884  loss_ce_7: 0.07684  loss_mask_7: 0.1733  loss_dice_7: 0.1807  loss_ce_8: 0.07703  loss_mask_8: 0.181  loss_dice_8: 0.1836  time: 0.5576  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:41:56] d2.utils.events INFO:  eta: 3:00:46  iter: 8559  total_loss: 4.645  loss_ce: 0.07798  loss_mask: 0.1817  loss_dice: 0.1358  loss_ce_0: 0.04718  loss_mask_0: 0.1854  loss_dice_0: 0.1302  loss_ce_1: 0.07846  loss_mask_1: 0.1893  loss_dice_1: 0.1317  loss_ce_2: 0.07848  loss_mask_2: 0.1945  loss_dice_2: 0.1306  loss_ce_3: 0.07851  loss_mask_3: 0.1957  loss_dice_3: 0.1363  loss_ce_4: 0.0782  loss_mask_4: 0.1951  loss_dice_4: 0.1332  loss_ce_5: 0.07804  loss_mask_5: 0.1832  loss_dice_5: 0.1303  loss_ce_6: 0.07817  loss_mask_6: 0.1899  loss_dice_6: 0.1341  loss_ce_7: 0.0782  loss_mask_7: 0.1782  loss_dice_7: 0.1326  loss_ce_8: 0.0778  loss_mask_8: 0.1891  loss_dice_8: 0.1295  time: 0.5575  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:42:06] d2.utils.events INFO:  eta: 3:00:09  iter: 8579  total_loss: 4.803  loss_ce: 0.1342  loss_mask: 0.2107  loss_dice: 0.1378  loss_ce_0: 0.06696  loss_mask_0: 0.2101  loss_dice_0: 0.1341  loss_ce_1: 0.1334  loss_mask_1: 0.2112  loss_dice_1: 0.1308  loss_ce_2: 0.1335  loss_mask_2: 0.199  loss_dice_2: 0.1379  loss_ce_3: 0.1335  loss_mask_3: 0.1981  loss_dice_3: 0.1354  loss_ce_4: 0.1337  loss_mask_4: 0.2069  loss_dice_4: 0.1367  loss_ce_5: 0.1335  loss_mask_5: 0.2101  loss_dice_5: 0.136  loss_ce_6: 0.1337  loss_mask_6: 0.1941  loss_dice_6: 0.1341  loss_ce_7: 0.1338  loss_mask_7: 0.2033  loss_dice_7: 0.1357  loss_ce_8: 0.134  loss_mask_8: 0.1954  loss_dice_8: 0.1296  time: 0.5574  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:42:17] d2.utils.events INFO:  eta: 2:59:00  iter: 8599  total_loss: 5.07  loss_ce: 0.09137  loss_mask: 0.2152  loss_dice: 0.1575  loss_ce_0: 0.04977  loss_mask_0: 0.2135  loss_dice_0: 0.1548  loss_ce_1: 0.09582  loss_mask_1: 0.2209  loss_dice_1: 0.1584  loss_ce_2: 0.09488  loss_mask_2: 0.2181  loss_dice_2: 0.1531  loss_ce_3: 0.09497  loss_mask_3: 0.2094  loss_dice_3: 0.1476  loss_ce_4: 0.09425  loss_mask_4: 0.2116  loss_dice_4: 0.1487  loss_ce_5: 0.09493  loss_mask_5: 0.216  loss_dice_5: 0.1524  loss_ce_6: 0.09364  loss_mask_6: 0.2205  loss_dice_6: 0.1594  loss_ce_7: 0.09364  loss_mask_7: 0.2118  loss_dice_7: 0.1584  loss_ce_8: 0.09243  loss_mask_8: 0.2158  loss_dice_8: 0.1568  time: 0.5573  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:42:27] d2.utils.events INFO:  eta: 2:58:32  iter: 8619  total_loss: 4.867  loss_ce: 0.09335  loss_mask: 0.2205  loss_dice: 0.1363  loss_ce_0: 0.05022  loss_mask_0: 0.2105  loss_dice_0: 0.1384  loss_ce_1: 0.09526  loss_mask_1: 0.2144  loss_dice_1: 0.1412  loss_ce_2: 0.0945  loss_mask_2: 0.2216  loss_dice_2: 0.14  loss_ce_3: 0.09475  loss_mask_3: 0.2246  loss_dice_3: 0.1387  loss_ce_4: 0.09482  loss_mask_4: 0.2222  loss_dice_4: 0.1395  loss_ce_5: 0.0953  loss_mask_5: 0.2241  loss_dice_5: 0.1392  loss_ce_6: 0.0944  loss_mask_6: 0.2193  loss_dice_6: 0.1411  loss_ce_7: 0.09454  loss_mask_7: 0.2186  loss_dice_7: 0.1383  loss_ce_8: 0.09407  loss_mask_8: 0.2226  loss_dice_8: 0.1399  time: 0.5573  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:42:38] d2.utils.events INFO:  eta: 2:58:02  iter: 8639  total_loss: 5.333  loss_ce: 0.09393  loss_mask: 0.1798  loss_dice: 0.1863  loss_ce_0: 0.05057  loss_mask_0: 0.1859  loss_dice_0: 0.1843  loss_ce_1: 0.09437  loss_mask_1: 0.1857  loss_dice_1: 0.1822  loss_ce_2: 0.09392  loss_mask_2: 0.1833  loss_dice_2: 0.1815  loss_ce_3: 0.09396  loss_mask_3: 0.1849  loss_dice_3: 0.1895  loss_ce_4: 0.09446  loss_mask_4: 0.1809  loss_dice_4: 0.1892  loss_ce_5: 0.09457  loss_mask_5: 0.1844  loss_dice_5: 0.1758  loss_ce_6: 0.09414  loss_mask_6: 0.1825  loss_dice_6: 0.1814  loss_ce_7: 0.09429  loss_mask_7: 0.1752  loss_dice_7: 0.1866  loss_ce_8: 0.09432  loss_mask_8: 0.1833  loss_dice_8: 0.1874  time: 0.5572  data_time: 0.0019  lr: 0.0001  max_mem: 2811M
[07/11 12:42:49] d2.utils.events INFO:  eta: 2:57:40  iter: 8659  total_loss: 5.11  loss_ce: 0.09126  loss_mask: 0.2209  loss_dice: 0.1742  loss_ce_0: 0.05019  loss_mask_0: 0.2196  loss_dice_0: 0.1697  loss_ce_1: 0.09054  loss_mask_1: 0.2061  loss_dice_1: 0.1581  loss_ce_2: 0.09054  loss_mask_2: 0.2104  loss_dice_2: 0.1668  loss_ce_3: 0.0906  loss_mask_3: 0.2135  loss_dice_3: 0.1775  loss_ce_4: 0.09094  loss_mask_4: 0.2057  loss_dice_4: 0.1716  loss_ce_5: 0.09095  loss_mask_5: 0.2208  loss_dice_5: 0.173  loss_ce_6: 0.09081  loss_mask_6: 0.2245  loss_dice_6: 0.1697  loss_ce_7: 0.09102  loss_mask_7: 0.2097  loss_dice_7: 0.1661  loss_ce_8: 0.09127  loss_mask_8: 0.2105  loss_dice_8: 0.1775  time: 0.5572  data_time: 0.0018  lr: 0.0001  max_mem: 2811M
[07/11 12:43:00] d2.utils.events INFO:  eta: 2:56:51  iter: 8679  total_loss: 5.236  loss_ce: 0.08544  loss_mask: 0.203  loss_dice: 0.1579  loss_ce_0: 0.04889  loss_mask_0: 0.2092  loss_dice_0: 0.1571  loss_ce_1: 0.08366  loss_mask_1: 0.2033  loss_dice_1: 0.1624  loss_ce_2: 0.08392  loss_mask_2: 0.2147  loss_dice_2: 0.1571  loss_ce_3: 0.08382  loss_mask_3: 0.209  loss_dice_3: 0.1538  loss_ce_4: 0.08421  loss_mask_4: 0.2135  loss_dice_4: 0.1551  loss_ce_5: 0.08412  loss_mask_5: 0.2104  loss_dice_5: 0.1635  loss_ce_6: 0.08445  loss_mask_6: 0.2156  loss_dice_6: 0.1603  loss_ce_7: 0.08452  loss_mask_7: 0.2076  loss_dice_7: 0.1533  loss_ce_8: 0.08508  loss_mask_8: 0.2072  loss_dice_8: 0.1571  time: 0.5572  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:43:10] d2.utils.events INFO:  eta: 2:56:12  iter: 8699  total_loss: 5.181  loss_ce: 0.1356  loss_mask: 0.2148  loss_dice: 0.1604  loss_ce_0: 0.06703  loss_mask_0: 0.2155  loss_dice_0: 0.1606  loss_ce_1: 0.1355  loss_mask_1: 0.209  loss_dice_1: 0.1574  loss_ce_2: 0.1355  loss_mask_2: 0.2216  loss_dice_2: 0.1591  loss_ce_3: 0.1356  loss_mask_3: 0.2113  loss_dice_3: 0.1575  loss_ce_4: 0.1356  loss_mask_4: 0.2113  loss_dice_4: 0.1615  loss_ce_5: 0.1357  loss_mask_5: 0.2095  loss_dice_5: 0.1526  loss_ce_6: 0.1357  loss_mask_6: 0.2147  loss_dice_6: 0.1565  loss_ce_7: 0.1357  loss_mask_7: 0.2094  loss_dice_7: 0.1548  loss_ce_8: 0.1357  loss_mask_8: 0.2069  loss_dice_8: 0.1584  time: 0.5571  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:43:21] d2.utils.events INFO:  eta: 2:55:46  iter: 8719  total_loss: 4.997  loss_ce: 0.1315  loss_mask: 0.1533  loss_dice: 0.1432  loss_ce_0: 0.06619  loss_mask_0: 0.1545  loss_dice_0: 0.1453  loss_ce_1: 0.1308  loss_mask_1: 0.1549  loss_dice_1: 0.1442  loss_ce_2: 0.1308  loss_mask_2: 0.1501  loss_dice_2: 0.14  loss_ce_3: 0.1308  loss_mask_3: 0.1579  loss_dice_3: 0.1468  loss_ce_4: 0.131  loss_mask_4: 0.1554  loss_dice_4: 0.14  loss_ce_5: 0.1308  loss_mask_5: 0.1586  loss_dice_5: 0.1409  loss_ce_6: 0.1311  loss_mask_6: 0.1553  loss_dice_6: 0.1406  loss_ce_7: 0.1311  loss_mask_7: 0.1507  loss_dice_7: 0.1399  loss_ce_8: 0.1313  loss_mask_8: 0.1571  loss_dice_8: 0.1403  time: 0.5570  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:43:31] d2.utils.events INFO:  eta: 2:55:27  iter: 8739  total_loss: 4.786  loss_ce: 0.09857  loss_mask: 0.2207  loss_dice: 0.1503  loss_ce_0: 0.05166  loss_mask_0: 0.2206  loss_dice_0: 0.1494  loss_ce_1: 0.1028  loss_mask_1: 0.2174  loss_dice_1: 0.1523  loss_ce_2: 0.1017  loss_mask_2: 0.2285  loss_dice_2: 0.1488  loss_ce_3: 0.1017  loss_mask_3: 0.2187  loss_dice_3: 0.1505  loss_ce_4: 0.1013  loss_mask_4: 0.2243  loss_dice_4: 0.1532  loss_ce_5: 0.102  loss_mask_5: 0.2204  loss_dice_5: 0.15  loss_ce_6: 0.1006  loss_mask_6: 0.2224  loss_dice_6: 0.1466  loss_ce_7: 0.1004  loss_mask_7: 0.2267  loss_dice_7: 0.146  loss_ce_8: 0.09939  loss_mask_8: 0.2203  loss_dice_8: 0.1493  time: 0.5569  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:43:42] d2.utils.events INFO:  eta: 2:54:48  iter: 8759  total_loss: 4.969  loss_ce: 0.1666  loss_mask: 0.203  loss_dice: 0.1495  loss_ce_0: 0.08093  loss_mask_0: 0.1911  loss_dice_0: 0.1439  loss_ce_1: 0.1671  loss_mask_1: 0.196  loss_dice_1: 0.1485  loss_ce_2: 0.1676  loss_mask_2: 0.187  loss_dice_2: 0.1436  loss_ce_3: 0.1671  loss_mask_3: 0.1984  loss_dice_3: 0.1422  loss_ce_4: 0.1668  loss_mask_4: 0.1934  loss_dice_4: 0.1446  loss_ce_5: 0.1662  loss_mask_5: 0.1939  loss_dice_5: 0.1408  loss_ce_6: 0.1667  loss_mask_6: 0.1969  loss_dice_6: 0.1417  loss_ce_7: 0.1666  loss_mask_7: 0.19  loss_dice_7: 0.1407  loss_ce_8: 0.1663  loss_mask_8: 0.1948  loss_dice_8: 0.1498  time: 0.5568  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:43:53] d2.utils.events INFO:  eta: 2:54:38  iter: 8779  total_loss: 5.007  loss_ce: 0.1437  loss_mask: 0.17  loss_dice: 0.1421  loss_ce_0: 0.07449  loss_mask_0: 0.1643  loss_dice_0: 0.1539  loss_ce_1: 0.1368  loss_mask_1: 0.1582  loss_dice_1: 0.1354  loss_ce_2: 0.1383  loss_mask_2: 0.1644  loss_dice_2: 0.1435  loss_ce_3: 0.1383  loss_mask_3: 0.1669  loss_dice_3: 0.1459  loss_ce_4: 0.1387  loss_mask_4: 0.1639  loss_dice_4: 0.1504  loss_ce_5: 0.1385  loss_mask_5: 0.1732  loss_dice_5: 0.1499  loss_ce_6: 0.1404  loss_mask_6: 0.1583  loss_dice_6: 0.1531  loss_ce_7: 0.1409  loss_mask_7: 0.1591  loss_dice_7: 0.1393  loss_ce_8: 0.1427  loss_mask_8: 0.1672  loss_dice_8: 0.1477  time: 0.5568  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:44:04] d2.utils.events INFO:  eta: 2:54:28  iter: 8799  total_loss: 5.378  loss_ce: 0.1275  loss_mask: 0.1669  loss_dice: 0.2057  loss_ce_0: 0.06465  loss_mask_0: 0.1568  loss_dice_0: 0.2039  loss_ce_1: 0.1279  loss_mask_1: 0.1754  loss_dice_1: 0.1999  loss_ce_2: 0.128  loss_mask_2: 0.1756  loss_dice_2: 0.2102  loss_ce_3: 0.1279  loss_mask_3: 0.1668  loss_dice_3: 0.191  loss_ce_4: 0.1276  loss_mask_4: 0.1663  loss_dice_4: 0.1951  loss_ce_5: 0.128  loss_mask_5: 0.1639  loss_dice_5: 0.1883  loss_ce_6: 0.1274  loss_mask_6: 0.1682  loss_dice_6: 0.2007  loss_ce_7: 0.1274  loss_mask_7: 0.1704  loss_dice_7: 0.2034  loss_ce_8: 0.1274  loss_mask_8: 0.1668  loss_dice_8: 0.1908  time: 0.5569  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:44:16] d2.utils.events INFO:  eta: 2:54:40  iter: 8819  total_loss: 5.39  loss_ce: 0.1244  loss_mask: 0.1787  loss_dice: 0.2002  loss_ce_0: 0.05897  loss_mask_0: 0.1838  loss_dice_0: 0.2079  loss_ce_1: 0.126  loss_mask_1: 0.2033  loss_dice_1: 0.2252  loss_ce_2: 0.1248  loss_mask_2: 0.1853  loss_dice_2: 0.2074  loss_ce_3: 0.125  loss_mask_3: 0.1883  loss_dice_3: 0.2001  loss_ce_4: 0.1262  loss_mask_4: 0.1894  loss_dice_4: 0.2108  loss_ce_5: 0.1263  loss_mask_5: 0.1915  loss_dice_5: 0.2165  loss_ce_6: 0.1253  loss_mask_6: 0.2005  loss_dice_6: 0.2185  loss_ce_7: 0.1253  loss_mask_7: 0.1847  loss_dice_7: 0.2124  loss_ce_8: 0.1252  loss_mask_8: 0.1852  loss_dice_8: 0.2124  time: 0.5569  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:44:28] d2.utils.events INFO:  eta: 2:54:51  iter: 8839  total_loss: 4.852  loss_ce: 0.1105  loss_mask: 0.2208  loss_dice: 0.1382  loss_ce_0: 0.05698  loss_mask_0: 0.2238  loss_dice_0: 0.1388  loss_ce_1: 0.1064  loss_mask_1: 0.2209  loss_dice_1: 0.1408  loss_ce_2: 0.1073  loss_mask_2: 0.214  loss_dice_2: 0.1372  loss_ce_3: 0.1075  loss_mask_3: 0.2253  loss_dice_3: 0.1389  loss_ce_4: 0.1079  loss_mask_4: 0.2226  loss_dice_4: 0.1438  loss_ce_5: 0.1082  loss_mask_5: 0.2261  loss_dice_5: 0.1381  loss_ce_6: 0.1092  loss_mask_6: 0.2266  loss_dice_6: 0.1371  loss_ce_7: 0.1089  loss_mask_7: 0.2158  loss_dice_7: 0.1399  loss_ce_8: 0.1104  loss_mask_8: 0.2228  loss_dice_8: 0.136  time: 0.5570  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:44:39] d2.utils.events INFO:  eta: 2:55:15  iter: 8859  total_loss: 4.921  loss_ce: 0.09494  loss_mask: 0.2178  loss_dice: 0.1518  loss_ce_0: 0.05363  loss_mask_0: 0.2131  loss_dice_0: 0.1523  loss_ce_1: 0.0909  loss_mask_1: 0.2111  loss_dice_1: 0.1567  loss_ce_2: 0.09216  loss_mask_2: 0.2135  loss_dice_2: 0.1559  loss_ce_3: 0.09179  loss_mask_3: 0.2096  loss_dice_3: 0.1566  loss_ce_4: 0.09161  loss_mask_4: 0.2087  loss_dice_4: 0.1491  loss_ce_5: 0.09123  loss_mask_5: 0.2118  loss_dice_5: 0.1493  loss_ce_6: 0.09289  loss_mask_6: 0.2125  loss_dice_6: 0.1563  loss_ce_7: 0.09289  loss_mask_7: 0.2125  loss_dice_7: 0.1549  loss_ce_8: 0.09393  loss_mask_8: 0.208  loss_dice_8: 0.1527  time: 0.5571  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:44:51] d2.utils.events INFO:  eta: 2:56:23  iter: 8879  total_loss: 5.035  loss_ce: 0.0832  loss_mask: 0.2084  loss_dice: 0.1693  loss_ce_0: 0.05051  loss_mask_0: 0.2051  loss_dice_0: 0.1688  loss_ce_1: 0.07918  loss_mask_1: 0.2083  loss_dice_1: 0.1639  loss_ce_2: 0.08064  loss_mask_2: 0.2  loss_dice_2: 0.1724  loss_ce_3: 0.08002  loss_mask_3: 0.2065  loss_dice_3: 0.1717  loss_ce_4: 0.07983  loss_mask_4: 0.2107  loss_dice_4: 0.1731  loss_ce_5: 0.07942  loss_mask_5: 0.2139  loss_dice_5: 0.1691  loss_ce_6: 0.0811  loss_mask_6: 0.2007  loss_dice_6: 0.1662  loss_ce_7: 0.08098  loss_mask_7: 0.2036  loss_dice_7: 0.1708  loss_ce_8: 0.08178  loss_mask_8: 0.2095  loss_dice_8: 0.1691  time: 0.5571  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:45:03] d2.utils.events INFO:  eta: 2:57:25  iter: 8899  total_loss: 5.078  loss_ce: 0.1861  loss_mask: 0.1911  loss_dice: 0.1768  loss_ce_0: 0.08324  loss_mask_0: 0.1817  loss_dice_0: 0.1818  loss_ce_1: 0.1826  loss_mask_1: 0.187  loss_dice_1: 0.1849  loss_ce_2: 0.1822  loss_mask_2: 0.1829  loss_dice_2: 0.1831  loss_ce_3: 0.1825  loss_mask_3: 0.1927  loss_dice_3: 0.1781  loss_ce_4: 0.1842  loss_mask_4: 0.183  loss_dice_4: 0.1778  loss_ce_5: 0.1841  loss_mask_5: 0.1829  loss_dice_5: 0.1791  loss_ce_6: 0.1842  loss_mask_6: 0.1838  loss_dice_6: 0.1804  loss_ce_7: 0.1845  loss_mask_7: 0.1864  loss_dice_7: 0.1792  loss_ce_8: 0.1862  loss_mask_8: 0.1941  loss_dice_8: 0.1751  time: 0.5572  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:45:14] d2.utils.events INFO:  eta: 2:58:23  iter: 8919  total_loss: 5.162  loss_ce: 0.1579  loss_mask: 0.1882  loss_dice: 0.1438  loss_ce_0: 0.0771  loss_mask_0: 0.1936  loss_dice_0: 0.1418  loss_ce_1: 0.1461  loss_mask_1: 0.1921  loss_dice_1: 0.1458  loss_ce_2: 0.1487  loss_mask_2: 0.178  loss_dice_2: 0.1521  loss_ce_3: 0.1478  loss_mask_3: 0.1886  loss_dice_3: 0.1457  loss_ce_4: 0.1496  loss_mask_4: 0.1866  loss_dice_4: 0.1472  loss_ce_5: 0.1484  loss_mask_5: 0.1831  loss_dice_5: 0.1505  loss_ce_6: 0.1513  loss_mask_6: 0.1842  loss_dice_6: 0.1473  loss_ce_7: 0.1517  loss_mask_7: 0.1926  loss_dice_7: 0.1521  loss_ce_8: 0.1552  loss_mask_8: 0.1861  loss_dice_8: 0.1459  time: 0.5572  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:45:26] d2.utils.events INFO:  eta: 2:59:26  iter: 8939  total_loss: 5.012  loss_ce: 0.1099  loss_mask: 0.2067  loss_dice: 0.1336  loss_ce_0: 0.05561  loss_mask_0: 0.2017  loss_dice_0: 0.1374  loss_ce_1: 0.1173  loss_mask_1: 0.208  loss_dice_1: 0.1378  loss_ce_2: 0.1146  loss_mask_2: 0.2075  loss_dice_2: 0.1392  loss_ce_3: 0.1156  loss_mask_3: 0.2073  loss_dice_3: 0.1405  loss_ce_4: 0.1143  loss_mask_4: 0.2006  loss_dice_4: 0.1349  loss_ce_5: 0.1156  loss_mask_5: 0.216  loss_dice_5: 0.1409  loss_ce_6: 0.1127  loss_mask_6: 0.2023  loss_dice_6: 0.1328  loss_ce_7: 0.1125  loss_mask_7: 0.2008  loss_dice_7: 0.1413  loss_ce_8: 0.1115  loss_mask_8: 0.2098  loss_dice_8: 0.1337  time: 0.5573  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:45:37] d2.utils.events INFO:  eta: 3:01:27  iter: 8959  total_loss: 5.475  loss_ce: 0.128  loss_mask: 0.1621  loss_dice: 0.2043  loss_ce_0: 0.06495  loss_mask_0: 0.1542  loss_dice_0: 0.1964  loss_ce_1: 0.1283  loss_mask_1: 0.1612  loss_dice_1: 0.2005  loss_ce_2: 0.1282  loss_mask_2: 0.1508  loss_dice_2: 0.2016  loss_ce_3: 0.1282  loss_mask_3: 0.1547  loss_dice_3: 0.1903  loss_ce_4: 0.1282  loss_mask_4: 0.1583  loss_dice_4: 0.1941  loss_ce_5: 0.1282  loss_mask_5: 0.1623  loss_dice_5: 0.1966  loss_ce_6: 0.1281  loss_mask_6: 0.1527  loss_dice_6: 0.1885  loss_ce_7: 0.1281  loss_mask_7: 0.1605  loss_dice_7: 0.1983  loss_ce_8: 0.128  loss_mask_8: 0.1577  loss_dice_8: 0.1842  time: 0.5573  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:45:49] d2.utils.events INFO:  eta: 3:03:15  iter: 8979  total_loss: 5.202  loss_ce: 0.119  loss_mask: 0.1551  loss_dice: 0.1534  loss_ce_0: 0.05873  loss_mask_0: 0.1477  loss_dice_0: 0.1531  loss_ce_1: 0.1214  loss_mask_1: 0.1474  loss_dice_1: 0.1553  loss_ce_2: 0.1211  loss_mask_2: 0.153  loss_dice_2: 0.1569  loss_ce_3: 0.1213  loss_mask_3: 0.1451  loss_dice_3: 0.157  loss_ce_4: 0.1212  loss_mask_4: 0.1596  loss_dice_4: 0.1607  loss_ce_5: 0.1217  loss_mask_5: 0.1499  loss_dice_5: 0.1502  loss_ce_6: 0.1203  loss_mask_6: 0.153  loss_dice_6: 0.1603  loss_ce_7: 0.12  loss_mask_7: 0.1478  loss_dice_7: 0.1516  loss_ce_8: 0.1196  loss_mask_8: 0.1496  loss_dice_8: 0.1525  time: 0.5574  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:46:01] d2.utils.events INFO:  eta: 3:04:07  iter: 8999  total_loss: 4.425  loss_ce: 0.1293  loss_mask: 0.132  loss_dice: 0.165  loss_ce_0: 0.06822  loss_mask_0: 0.1231  loss_dice_0: 0.1625  loss_ce_1: 0.1276  loss_mask_1: 0.1279  loss_dice_1: 0.1624  loss_ce_2: 0.1286  loss_mask_2: 0.1262  loss_dice_2: 0.1565  loss_ce_3: 0.1285  loss_mask_3: 0.1351  loss_dice_3: 0.1676  loss_ce_4: 0.1283  loss_mask_4: 0.1291  loss_dice_4: 0.1591  loss_ce_5: 0.1278  loss_mask_5: 0.1246  loss_dice_5: 0.1592  loss_ce_6: 0.1291  loss_mask_6: 0.1297  loss_dice_6: 0.1638  loss_ce_7: 0.1291  loss_mask_7: 0.1302  loss_dice_7: 0.1634  loss_ce_8: 0.1294  loss_mask_8: 0.133  loss_dice_8: 0.1713  time: 0.5574  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:46:12] d2.utils.events INFO:  eta: 3:05:28  iter: 9019  total_loss: 4.616  loss_ce: 0.121  loss_mask: 0.209  loss_dice: 0.1321  loss_ce_0: 0.06443  loss_mask_0: 0.2065  loss_dice_0: 0.137  loss_ce_1: 0.1149  loss_mask_1: 0.2062  loss_dice_1: 0.133  loss_ce_2: 0.1164  loss_mask_2: 0.1984  loss_dice_2: 0.1327  loss_ce_3: 0.1164  loss_mask_3: 0.2057  loss_dice_3: 0.137  loss_ce_4: 0.1161  loss_mask_4: 0.2049  loss_dice_4: 0.1352  loss_ce_5: 0.1154  loss_mask_5: 0.2041  loss_dice_5: 0.1332  loss_ce_6: 0.118  loss_mask_6: 0.207  loss_dice_6: 0.1331  loss_ce_7: 0.1181  loss_mask_7: 0.1968  loss_dice_7: 0.1394  loss_ce_8: 0.12  loss_mask_8: 0.2032  loss_dice_8: 0.1345  time: 0.5575  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:46:24] d2.utils.events INFO:  eta: 3:06:07  iter: 9039  total_loss: 5.258  loss_ce: 0.1559  loss_mask: 0.1777  loss_dice: 0.1717  loss_ce_0: 0.06778  loss_mask_0: 0.1681  loss_dice_0: 0.1627  loss_ce_1: 0.1577  loss_mask_1: 0.1672  loss_dice_1: 0.1559  loss_ce_2: 0.1562  loss_mask_2: 0.1736  loss_dice_2: 0.1665  loss_ce_3: 0.1564  loss_mask_3: 0.177  loss_dice_3: 0.1631  loss_ce_4: 0.1583  loss_mask_4: 0.1731  loss_dice_4: 0.1663  loss_ce_5: 0.1588  loss_mask_5: 0.1702  loss_dice_5: 0.1671  loss_ce_6: 0.1572  loss_mask_6: 0.1674  loss_dice_6: 0.1573  loss_ce_7: 0.1574  loss_mask_7: 0.168  loss_dice_7: 0.1624  loss_ce_8: 0.1567  loss_mask_8: 0.1721  loss_dice_8: 0.163  time: 0.5575  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:46:36] d2.utils.events INFO:  eta: 3:07:07  iter: 9059  total_loss: 5.295  loss_ce: 0.1346  loss_mask: 0.1967  loss_dice: 0.1418  loss_ce_0: 0.06591  loss_mask_0: 0.1998  loss_dice_0: 0.1373  loss_ce_1: 0.1382  loss_mask_1: 0.2092  loss_dice_1: 0.1414  loss_ce_2: 0.137  loss_mask_2: 0.2086  loss_dice_2: 0.1446  loss_ce_3: 0.1364  loss_mask_3: 0.2187  loss_dice_3: 0.1411  loss_ce_4: 0.1365  loss_mask_4: 0.2051  loss_dice_4: 0.1407  loss_ce_5: 0.1371  loss_mask_5: 0.2013  loss_dice_5: 0.1424  loss_ce_6: 0.1337  loss_mask_6: 0.2157  loss_dice_6: 0.1438  loss_ce_7: 0.1331  loss_mask_7: 0.1984  loss_dice_7: 0.1446  loss_ce_8: 0.1346  loss_mask_8: 0.2074  loss_dice_8: 0.1416  time: 0.5576  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:46:47] d2.utils.events INFO:  eta: 3:07:32  iter: 9079  total_loss: 4.731  loss_ce: 0.1281  loss_mask: 0.2008  loss_dice: 0.1644  loss_ce_0: 0.06539  loss_mask_0: 0.1935  loss_dice_0: 0.1701  loss_ce_1: 0.129  loss_mask_1: 0.1935  loss_dice_1: 0.1652  loss_ce_2: 0.1292  loss_mask_2: 0.2043  loss_dice_2: 0.1601  loss_ce_3: 0.1292  loss_mask_3: 0.1872  loss_dice_3: 0.1634  loss_ce_4: 0.1283  loss_mask_4: 0.2029  loss_dice_4: 0.1645  loss_ce_5: 0.128  loss_mask_5: 0.2035  loss_dice_5: 0.158  loss_ce_6: 0.1287  loss_mask_6: 0.1993  loss_dice_6: 0.1582  loss_ce_7: 0.1285  loss_mask_7: 0.2049  loss_dice_7: 0.1606  loss_ce_8: 0.1283  loss_mask_8: 0.1969  loss_dice_8: 0.1618  time: 0.5576  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:46:59] d2.utils.events INFO:  eta: 3:08:33  iter: 9099  total_loss: 4.484  loss_ce: 0.1287  loss_mask: 0.1584  loss_dice: 0.1361  loss_ce_0: 0.06487  loss_mask_0: 0.1597  loss_dice_0: 0.1425  loss_ce_1: 0.1293  loss_mask_1: 0.1624  loss_dice_1: 0.1413  loss_ce_2: 0.1292  loss_mask_2: 0.1656  loss_dice_2: 0.1397  loss_ce_3: 0.1292  loss_mask_3: 0.1682  loss_dice_3: 0.147  loss_ce_4: 0.1293  loss_mask_4: 0.1591  loss_dice_4: 0.14  loss_ce_5: 0.129  loss_mask_5: 0.164  loss_dice_5: 0.1437  loss_ce_6: 0.1289  loss_mask_6: 0.1629  loss_dice_6: 0.1447  loss_ce_7: 0.129  loss_mask_7: 0.1619  loss_dice_7: 0.1421  loss_ce_8: 0.1288  loss_mask_8: 0.1628  loss_dice_8: 0.1397  time: 0.5577  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:47:10] d2.utils.events INFO:  eta: 3:09:18  iter: 9119  total_loss: 5.234  loss_ce: 0.1225  loss_mask: 0.2135  loss_dice: 0.1689  loss_ce_0: 0.06378  loss_mask_0: 0.2121  loss_dice_0: 0.1746  loss_ce_1: 0.1237  loss_mask_1: 0.2078  loss_dice_1: 0.1741  loss_ce_2: 0.1239  loss_mask_2: 0.2143  loss_dice_2: 0.1696  loss_ce_3: 0.1242  loss_mask_3: 0.2168  loss_dice_3: 0.173  loss_ce_4: 0.1231  loss_mask_4: 0.211  loss_dice_4: 0.1726  loss_ce_5: 0.1231  loss_mask_5: 0.2139  loss_dice_5: 0.1787  loss_ce_6: 0.1234  loss_mask_6: 0.2149  loss_dice_6: 0.1789  loss_ce_7: 0.1232  loss_mask_7: 0.2128  loss_dice_7: 0.1776  loss_ce_8: 0.1226  loss_mask_8: 0.2094  loss_dice_8: 0.1828  time: 0.5577  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:47:22] d2.utils.events INFO:  eta: 3:10:00  iter: 9139  total_loss: 5.611  loss_ce: 0.1149  loss_mask: 0.2175  loss_dice: 0.1697  loss_ce_0: 0.06152  loss_mask_0: 0.2144  loss_dice_0: 0.1655  loss_ce_1: 0.112  loss_mask_1: 0.2254  loss_dice_1: 0.1712  loss_ce_2: 0.113  loss_mask_2: 0.2179  loss_dice_2: 0.1723  loss_ce_3: 0.1131  loss_mask_3: 0.2187  loss_dice_3: 0.1654  loss_ce_4: 0.1125  loss_mask_4: 0.2231  loss_dice_4: 0.1672  loss_ce_5: 0.1129  loss_mask_5: 0.2214  loss_dice_5: 0.1671  loss_ce_6: 0.114  loss_mask_6: 0.216  loss_dice_6: 0.1739  loss_ce_7: 0.1137  loss_mask_7: 0.2212  loss_dice_7: 0.1635  loss_ce_8: 0.1146  loss_mask_8: 0.2222  loss_dice_8: 0.1659  time: 0.5578  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:47:34] d2.utils.events INFO:  eta: 3:10:46  iter: 9159  total_loss: 5  loss_ce: 0.104  loss_mask: 0.2382  loss_dice: 0.138  loss_ce_0: 0.05836  loss_mask_0: 0.2428  loss_dice_0: 0.1404  loss_ce_1: 0.1007  loss_mask_1: 0.2478  loss_dice_1: 0.1352  loss_ce_2: 0.1016  loss_mask_2: 0.2471  loss_dice_2: 0.1364  loss_ce_3: 0.1014  loss_mask_3: 0.2371  loss_dice_3: 0.137  loss_ce_4: 0.101  loss_mask_4: 0.2293  loss_dice_4: 0.134  loss_ce_5: 0.101  loss_mask_5: 0.246  loss_dice_5: 0.1352  loss_ce_6: 0.1023  loss_mask_6: 0.2444  loss_dice_6: 0.1359  loss_ce_7: 0.1026  loss_mask_7: 0.2328  loss_dice_7: 0.1359  loss_ce_8: 0.1033  loss_mask_8: 0.2424  loss_dice_8: 0.1347  time: 0.5578  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:47:45] d2.utils.events INFO:  eta: 3:11:22  iter: 9179  total_loss: 4.893  loss_ce: 0.1747  loss_mask: 0.2046  loss_dice: 0.1454  loss_ce_0: 0.07641  loss_mask_0: 0.211  loss_dice_0: 0.1409  loss_ce_1: 0.1793  loss_mask_1: 0.2122  loss_dice_1: 0.1414  loss_ce_2: 0.1775  loss_mask_2: 0.2042  loss_dice_2: 0.1404  loss_ce_3: 0.1783  loss_mask_3: 0.2003  loss_dice_3: 0.1406  loss_ce_4: 0.1796  loss_mask_4: 0.2025  loss_dice_4: 0.1403  loss_ce_5: 0.1797  loss_mask_5: 0.207  loss_dice_5: 0.1387  loss_ce_6: 0.1775  loss_mask_6: 0.2098  loss_dice_6: 0.1439  loss_ce_7: 0.1768  loss_mask_7: 0.2004  loss_dice_7: 0.1413  loss_ce_8: 0.1758  loss_mask_8: 0.2021  loss_dice_8: 0.1426  time: 0.5579  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:47:57] d2.utils.events INFO:  eta: 3:11:38  iter: 9199  total_loss: 4.471  loss_ce: 0.09526  loss_mask: 0.2098  loss_dice: 0.1293  loss_ce_0: 0.05454  loss_mask_0: 0.2065  loss_dice_0: 0.1332  loss_ce_1: 0.09645  loss_mask_1: 0.2086  loss_dice_1: 0.1304  loss_ce_2: 0.09668  loss_mask_2: 0.2012  loss_dice_2: 0.1319  loss_ce_3: 0.09669  loss_mask_3: 0.2081  loss_dice_3: 0.1325  loss_ce_4: 0.0957  loss_mask_4: 0.2113  loss_dice_4: 0.1357  loss_ce_5: 0.09566  loss_mask_5: 0.2078  loss_dice_5: 0.1308  loss_ce_6: 0.0957  loss_mask_6: 0.2119  loss_dice_6: 0.1366  loss_ce_7: 0.09552  loss_mask_7: 0.2018  loss_dice_7: 0.1354  loss_ce_8: 0.09512  loss_mask_8: 0.2014  loss_dice_8: 0.1291  time: 0.5579  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:48:09] d2.utils.events INFO:  eta: 3:12:07  iter: 9219  total_loss: 4.678  loss_ce: 0.09039  loss_mask: 0.2179  loss_dice: 0.1652  loss_ce_0: 0.05219  loss_mask_0: 0.2126  loss_dice_0: 0.1607  loss_ce_1: 0.08987  loss_mask_1: 0.2239  loss_dice_1: 0.1668  loss_ce_2: 0.09006  loss_mask_2: 0.2156  loss_dice_2: 0.1614  loss_ce_3: 0.09011  loss_mask_3: 0.2067  loss_dice_3: 0.1539  loss_ce_4: 0.08993  loss_mask_4: 0.2165  loss_dice_4: 0.1542  loss_ce_5: 0.0898  loss_mask_5: 0.2193  loss_dice_5: 0.159  loss_ce_6: 0.09015  loss_mask_6: 0.2175  loss_dice_6: 0.169  loss_ce_7: 0.09022  loss_mask_7: 0.2172  loss_dice_7: 0.1598  loss_ce_8: 0.09011  loss_mask_8: 0.2155  loss_dice_8: 0.1502  time: 0.5580  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:48:21] d2.utils.events INFO:  eta: 3:12:57  iter: 9239  total_loss: 4.715  loss_ce: 0.08815  loss_mask: 0.1955  loss_dice: 0.1494  loss_ce_0: 0.05129  loss_mask_0: 0.1924  loss_dice_0: 0.1554  loss_ce_1: 0.08753  loss_mask_1: 0.1912  loss_dice_1: 0.1476  loss_ce_2: 0.08772  loss_mask_2: 0.1908  loss_dice_2: 0.1562  loss_ce_3: 0.08774  loss_mask_3: 0.1937  loss_dice_3: 0.1561  loss_ce_4: 0.0877  loss_mask_4: 0.2004  loss_dice_4: 0.1475  loss_ce_5: 0.08757  loss_mask_5: 0.195  loss_dice_5: 0.1521  loss_ce_6: 0.08791  loss_mask_6: 0.1885  loss_dice_6: 0.1451  loss_ce_7: 0.08794  loss_mask_7: 0.1897  loss_dice_7: 0.1518  loss_ce_8: 0.08784  loss_mask_8: 0.1996  loss_dice_8: 0.1601  time: 0.5581  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:48:34] d2.utils.events INFO:  eta: 3:13:38  iter: 9259  total_loss: 4.794  loss_ce: 0.1672  loss_mask: 0.1952  loss_dice: 0.1573  loss_ce_0: 0.07918  loss_mask_0: 0.1884  loss_dice_0: 0.1577  loss_ce_1: 0.1638  loss_mask_1: 0.1931  loss_dice_1: 0.1633  loss_ce_2: 0.1643  loss_mask_2: 0.1857  loss_dice_2: 0.1618  loss_ce_3: 0.1642  loss_mask_3: 0.2052  loss_dice_3: 0.1651  loss_ce_4: 0.1646  loss_mask_4: 0.193  loss_dice_4: 0.1676  loss_ce_5: 0.1643  loss_mask_5: 0.198  loss_dice_5: 0.1635  loss_ce_6: 0.1655  loss_mask_6: 0.1859  loss_dice_6: 0.1564  loss_ce_7: 0.1658  loss_mask_7: 0.2033  loss_dice_7: 0.1609  loss_ce_8: 0.1669  loss_mask_8: 0.2007  loss_dice_8: 0.1658  time: 0.5583  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:48:47] d2.utils.events INFO:  eta: 3:13:57  iter: 9279  total_loss: 4.465  loss_ce: 0.1501  loss_mask: 0.1755  loss_dice: 0.1257  loss_ce_0: 0.07513  loss_mask_0: 0.1697  loss_dice_0: 0.1274  loss_ce_1: 0.1444  loss_mask_1: 0.1695  loss_dice_1: 0.1253  loss_ce_2: 0.1459  loss_mask_2: 0.1639  loss_dice_2: 0.1258  loss_ce_3: 0.1454  loss_mask_3: 0.1708  loss_dice_3: 0.1213  loss_ce_4: 0.1452  loss_mask_4: 0.1739  loss_dice_4: 0.1249  loss_ce_5: 0.1446  loss_mask_5: 0.1746  loss_dice_5: 0.125  loss_ce_6: 0.1467  loss_mask_6: 0.1721  loss_dice_6: 0.1242  loss_ce_7: 0.1473  loss_mask_7: 0.1808  loss_dice_7: 0.125  loss_ce_8: 0.149  loss_mask_8: 0.1724  loss_dice_8: 0.1305  time: 0.5585  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:48:59] d2.utils.events INFO:  eta: 3:14:43  iter: 9299  total_loss: 4.087  loss_ce: 0.1114  loss_mask: 0.1698  loss_dice: 0.1475  loss_ce_0: 0.05613  loss_mask_0: 0.1676  loss_dice_0: 0.1492  loss_ce_1: 0.1159  loss_mask_1: 0.1599  loss_dice_1: 0.144  loss_ce_2: 0.1145  loss_mask_2: 0.1625  loss_dice_2: 0.1448  loss_ce_3: 0.1149  loss_mask_3: 0.1617  loss_dice_3: 0.1427  loss_ce_4: 0.1154  loss_mask_4: 0.1697  loss_dice_4: 0.1428  loss_ce_5: 0.1159  loss_mask_5: 0.1637  loss_dice_5: 0.1453  loss_ce_6: 0.1142  loss_mask_6: 0.1672  loss_dice_6: 0.1407  loss_ce_7: 0.1135  loss_mask_7: 0.1642  loss_dice_7: 0.1415  loss_ce_8: 0.1125  loss_mask_8: 0.1741  loss_dice_8: 0.1458  time: 0.5586  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:49:12] d2.utils.events INFO:  eta: 3:15:15  iter: 9319  total_loss: 4.843  loss_ce: 0.1094  loss_mask: 0.1846  loss_dice: 0.1561  loss_ce_0: 0.0559  loss_mask_0: 0.1801  loss_dice_0: 0.1603  loss_ce_1: 0.1088  loss_mask_1: 0.1894  loss_dice_1: 0.1546  loss_ce_2: 0.1083  loss_mask_2: 0.1834  loss_dice_2: 0.1553  loss_ce_3: 0.1086  loss_mask_3: 0.1791  loss_dice_3: 0.1505  loss_ce_4: 0.1096  loss_mask_4: 0.1849  loss_dice_4: 0.1553  loss_ce_5: 0.1096  loss_mask_5: 0.1829  loss_dice_5: 0.1528  loss_ce_6: 0.1093  loss_mask_6: 0.1853  loss_dice_6: 0.1556  loss_ce_7: 0.1092  loss_mask_7: 0.1853  loss_dice_7: 0.1554  loss_ce_8: 0.1097  loss_mask_8: 0.193  loss_dice_8: 0.1634  time: 0.5588  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:49:25] d2.utils.events INFO:  eta: 3:16:24  iter: 9339  total_loss: 4.843  loss_ce: 0.1066  loss_mask: 0.1474  loss_dice: 0.1567  loss_ce_0: 0.05589  loss_mask_0: 0.1492  loss_dice_0: 0.1547  loss_ce_1: 0.1073  loss_mask_1: 0.1471  loss_dice_1: 0.1525  loss_ce_2: 0.1071  loss_mask_2: 0.1386  loss_dice_2: 0.1519  loss_ce_3: 0.107  loss_mask_3: 0.1374  loss_dice_3: 0.1513  loss_ce_4: 0.1069  loss_mask_4: 0.1373  loss_dice_4: 0.154  loss_ce_5: 0.1068  loss_mask_5: 0.1339  loss_dice_5: 0.153  loss_ce_6: 0.1067  loss_mask_6: 0.14  loss_dice_6: 0.1602  loss_ce_7: 0.1065  loss_mask_7: 0.1384  loss_dice_7: 0.1561  loss_ce_8: 0.1065  loss_mask_8: 0.1403  loss_dice_8: 0.1605  time: 0.5590  data_time: 0.0016  lr: 0.0001  max_mem: 2811M
[07/11 12:49:38] d2.utils.events INFO:  eta: 3:17:04  iter: 9359  total_loss: 4.804  loss_ce: 0.1289  loss_mask: 0.1994  loss_dice: 0.1641  loss_ce_0: 0.06488  loss_mask_0: 0.191  loss_dice_0: 0.1687  loss_ce_1: 0.129  loss_mask_1: 0.213  loss_dice_1: 0.1646  loss_ce_2: 0.129  loss_mask_2: 0.2035  loss_dice_2: 0.1647  loss_ce_3: 0.129  loss_mask_3: 0.2123  loss_dice_3: 0.1654  loss_ce_4: 0.129  loss_mask_4: 0.1984  loss_dice_4: 0.1587  loss_ce_5: 0.129  loss_mask_5: 0.2069  loss_dice_5: 0.1648  loss_ce_6: 0.1289  loss_mask_6: 0.202  loss_dice_6: 0.1672  loss_ce_7: 0.1289  loss_mask_7: 0.2189  loss_dice_7: 0.1621  loss_ce_8: 0.1289  loss_mask_8: 0.2091  loss_dice_8: 0.1672  time: 0.5591  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:49:50] d2.utils.events INFO:  eta: 3:17:34  iter: 9379  total_loss: 4.854  loss_ce: 0.1109  loss_mask: 0.188  loss_dice: 0.156  loss_ce_0: 0.05718  loss_mask_0: 0.1874  loss_dice_0: 0.156  loss_ce_1: 0.1122  loss_mask_1: 0.1933  loss_dice_1: 0.1551  loss_ce_2: 0.112  loss_mask_2: 0.1996  loss_dice_2: 0.155  loss_ce_3: 0.112  loss_mask_3: 0.1937  loss_dice_3: 0.1562  loss_ce_4: 0.1118  loss_mask_4: 0.1846  loss_dice_4: 0.1546  loss_ce_5: 0.1122  loss_mask_5: 0.1871  loss_dice_5: 0.1594  loss_ce_6: 0.1116  loss_mask_6: 0.1875  loss_dice_6: 0.1581  loss_ce_7: 0.1111  loss_mask_7: 0.1988  loss_dice_7: 0.1499  loss_ce_8: 0.1111  loss_mask_8: 0.1871  loss_dice_8: 0.1539  time: 0.5592  data_time: 0.0017  lr: 0.0001  max_mem: 2811M
[07/11 12:50:02] d2.utils.events INFO:  eta: 3:18:12  iter: 9399  total_loss: 4.572  loss_ce: 0.1358  loss_mask: 0.2002  loss_dice: 0.1268  loss_ce_0: 0.06918  loss_mask_0: 0.2032  loss_dice_0: 0.126  loss_ce_1: 0.1342  loss_mask_1: 0.1974  loss_dice_1: 0.1274  loss_ce_2: 0.1346  loss_mask_2: 0.1996  loss_dice_2: 0.1267  loss_ce_3: 0.1359  loss_mask_3: 0.1991  loss_dice_3: 0.1315  loss_ce_4: 0.1347  loss_mask_4: 0.2101  loss_dice_4: 0.1273  loss_ce_5: 0.135  loss_mask_5: 0.1941  loss_dice_5: 0.1221  loss_ce_6: 0.1362  loss_mask_6: 0.2001  loss_dice_6: 0.1264  loss_ce_7: 0.1351  loss_mask_7: 0.2075  loss_dice_7: 0.13  loss_ce_8: 0.1356  loss_mask_8: 0.2082  loss_dice_8: 0.1273  time: 0.5594  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:50:15] d2.utils.events INFO:  eta: 3:19:14  iter: 9419  total_loss: 5.099  loss_ce: 0.1436  loss_mask: 0.1759  loss_dice: 0.1488  loss_ce_0: 0.06511  loss_mask_0: 0.1735  loss_dice_0: 0.1528  loss_ce_1: 0.1538  loss_mask_1: 0.1703  loss_dice_1: 0.149  loss_ce_2: 0.1498  loss_mask_2: 0.1691  loss_dice_2: 0.1492  loss_ce_3: 0.153  loss_mask_3: 0.1767  loss_dice_3: 0.1488  loss_ce_4: 0.1496  loss_mask_4: 0.1813  loss_dice_4: 0.1553  loss_ce_5: 0.1484  loss_mask_5: 0.1662  loss_dice_5: 0.1558  loss_ce_6: 0.1455  loss_mask_6: 0.1716  loss_dice_6: 0.1562  loss_ce_7: 0.15  loss_mask_7: 0.1755  loss_dice_7: 0.1477  loss_ce_8: 0.1462  loss_mask_8: 0.1941  loss_dice_8: 0.1544  time: 0.5596  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:50:28] d2.utils.events INFO:  eta: 3:19:58  iter: 9439  total_loss: 4.836  loss_ce: 0.1197  loss_mask: 0.2242  loss_dice: 0.1617  loss_ce_0: 0.06357  loss_mask_0: 0.2183  loss_dice_0: 0.1558  loss_ce_1: 0.1136  loss_mask_1: 0.2167  loss_dice_1: 0.1528  loss_ce_2: 0.1173  loss_mask_2: 0.2163  loss_dice_2: 0.1621  loss_ce_3: 0.1172  loss_mask_3: 0.2326  loss_dice_3: 0.158  loss_ce_4: 0.118  loss_mask_4: 0.23  loss_dice_4: 0.1579  loss_ce_5: 0.1196  loss_mask_5: 0.2245  loss_dice_5: 0.1674  loss_ce_6: 0.1203  loss_mask_6: 0.2212  loss_dice_6: 0.1586  loss_ce_7: 0.1163  loss_mask_7: 0.2175  loss_dice_7: 0.1578  loss_ce_8: 0.1188  loss_mask_8: 0.2191  loss_dice_8: 0.1567  time: 0.5597  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:50:41] d2.utils.events INFO:  eta: 3:20:28  iter: 9459  total_loss: 4.733  loss_ce: 0.1283  loss_mask: 0.2081  loss_dice: 0.1232  loss_ce_0: 0.06446  loss_mask_0: 0.1954  loss_dice_0: 0.1238  loss_ce_1: 0.1293  loss_mask_1: 0.2013  loss_dice_1: 0.1251  loss_ce_2: 0.1285  loss_mask_2: 0.1938  loss_dice_2: 0.12  loss_ce_3: 0.1285  loss_mask_3: 0.1968  loss_dice_3: 0.12  loss_ce_4: 0.1285  loss_mask_4: 0.1971  loss_dice_4: 0.1208  loss_ce_5: 0.1286  loss_mask_5: 0.1998  loss_dice_5: 0.1228  loss_ce_6: 0.1286  loss_mask_6: 0.1931  loss_dice_6: 0.1196  loss_ce_7: 0.1286  loss_mask_7: 0.1935  loss_dice_7: 0.1195  loss_ce_8: 0.1284  loss_mask_8: 0.1956  loss_dice_8: 0.1222  time: 0.5599  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:50:53] d2.utils.events INFO:  eta: 3:21:15  iter: 9479  total_loss: 4.829  loss_ce: 0.1493  loss_mask: 0.1897  loss_dice: 0.1489  loss_ce_0: 0.06949  loss_mask_0: 0.1898  loss_dice_0: 0.1515  loss_ce_1: 0.1515  loss_mask_1: 0.1966  loss_dice_1: 0.1496  loss_ce_2: 0.15  loss_mask_2: 0.2084  loss_dice_2: 0.1513  loss_ce_3: 0.1509  loss_mask_3: 0.1969  loss_dice_3: 0.1464  loss_ce_4: 0.1518  loss_mask_4: 0.202  loss_dice_4: 0.1476  loss_ce_5: 0.1514  loss_mask_5: 0.2033  loss_dice_5: 0.1492  loss_ce_6: 0.15  loss_mask_6: 0.2032  loss_dice_6: 0.1435  loss_ce_7: 0.1519  loss_mask_7: 0.2017  loss_dice_7: 0.1533  loss_ce_8: 0.1495  loss_mask_8: 0.2032  loss_dice_8: 0.1506  time: 0.5600  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:51:06] d2.utils.events INFO:  eta: 3:21:52  iter: 9499  total_loss: 4.877  loss_ce: 0.1343  loss_mask: 0.194  loss_dice: 0.1661  loss_ce_0: 0.0678  loss_mask_0: 0.2032  loss_dice_0: 0.1782  loss_ce_1: 0.1303  loss_mask_1: 0.1917  loss_dice_1: 0.1682  loss_ce_2: 0.1308  loss_mask_2: 0.1966  loss_dice_2: 0.173  loss_ce_3: 0.1303  loss_mask_3: 0.193  loss_dice_3: 0.1708  loss_ce_4: 0.1303  loss_mask_4: 0.2112  loss_dice_4: 0.1765  loss_ce_5: 0.1302  loss_mask_5: 0.1879  loss_dice_5: 0.1603  loss_ce_6: 0.1307  loss_mask_6: 0.1923  loss_dice_6: 0.1655  loss_ce_7: 0.1319  loss_mask_7: 0.1995  loss_dice_7: 0.1698  loss_ce_8: 0.1332  loss_mask_8: 0.1867  loss_dice_8: 0.1625  time: 0.5602  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:51:19] d2.utils.events INFO:  eta: 3:22:35  iter: 9519  total_loss: 5.172  loss_ce: 0.126  loss_mask: 0.1623  loss_dice: 0.1929  loss_ce_0: 0.06511  loss_mask_0: 0.1584  loss_dice_0: 0.1885  loss_ce_1: 0.1202  loss_mask_1: 0.1625  loss_dice_1: 0.1835  loss_ce_2: 0.1213  loss_mask_2: 0.1595  loss_dice_2: 0.1887  loss_ce_3: 0.1188  loss_mask_3: 0.1633  loss_dice_3: 0.1832  loss_ce_4: 0.1196  loss_mask_4: 0.1704  loss_dice_4: 0.1864  loss_ce_5: 0.1188  loss_mask_5: 0.1624  loss_dice_5: 0.19  loss_ce_6: 0.1211  loss_mask_6: 0.1661  loss_dice_6: 0.1856  loss_ce_7: 0.1222  loss_mask_7: 0.163  loss_dice_7: 0.1916  loss_ce_8: 0.1239  loss_mask_8: 0.1579  loss_dice_8: 0.1795  time: 0.5603  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:51:31] d2.utils.events INFO:  eta: 3:23:07  iter: 9539  total_loss: 4.975  loss_ce: 0.1052  loss_mask: 0.2157  loss_dice: 0.1573  loss_ce_0: 0.06103  loss_mask_0: 0.2169  loss_dice_0: 0.1607  loss_ce_1: 0.09825  loss_mask_1: 0.2093  loss_dice_1: 0.1557  loss_ce_2: 0.1001  loss_mask_2: 0.212  loss_dice_2: 0.1637  loss_ce_3: 0.09568  loss_mask_3: 0.2141  loss_dice_3: 0.1531  loss_ce_4: 0.09558  loss_mask_4: 0.2156  loss_dice_4: 0.1616  loss_ce_5: 0.09519  loss_mask_5: 0.2076  loss_dice_5: 0.1603  loss_ce_6: 0.09752  loss_mask_6: 0.221  loss_dice_6: 0.1612  loss_ce_7: 0.09921  loss_mask_7: 0.2174  loss_dice_7: 0.1538  loss_ce_8: 0.1011  loss_mask_8: 0.2098  loss_dice_8: 0.1587  time: 0.5605  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:51:43] d2.utils.events INFO:  eta: 3:23:13  iter: 9559  total_loss: 4.898  loss_ce: 0.09519  loss_mask: 0.2103  loss_dice: 0.172  loss_ce_0: 0.05744  loss_mask_0: 0.2135  loss_dice_0: 0.1724  loss_ce_1: 0.09345  loss_mask_1: 0.2113  loss_dice_1: 0.1673  loss_ce_2: 0.09446  loss_mask_2: 0.2058  loss_dice_2: 0.1688  loss_ce_3: 0.09148  loss_mask_3: 0.2103  loss_dice_3: 0.1699  loss_ce_4: 0.09117  loss_mask_4: 0.2071  loss_dice_4: 0.1676  loss_ce_5: 0.09122  loss_mask_5: 0.205  loss_dice_5: 0.1692  loss_ce_6: 0.0928  loss_mask_6: 0.2094  loss_dice_6: 0.1699  loss_ce_7: 0.09295  loss_mask_7: 0.2062  loss_dice_7: 0.1678  loss_ce_8: 0.09275  loss_mask_8: 0.2017  loss_dice_8: 0.166  time: 0.5606  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:51:55] d2.utils.events INFO:  eta: 3:23:18  iter: 9579  total_loss: 4.875  loss_ce: 0.1915  loss_mask: 0.2014  loss_dice: 0.1602  loss_ce_0: 0.07667  loss_mask_0: 0.1962  loss_dice_0: 0.1578  loss_ce_1: 0.1969  loss_mask_1: 0.2077  loss_dice_1: 0.1654  loss_ce_2: 0.1967  loss_mask_2: 0.1947  loss_dice_2: 0.1549  loss_ce_3: 0.2038  loss_mask_3: 0.1939  loss_dice_3: 0.1545  loss_ce_4: 0.2015  loss_mask_4: 0.2088  loss_dice_4: 0.1599  loss_ce_5: 0.1961  loss_mask_5: 0.1943  loss_dice_5: 0.1549  loss_ce_6: 0.1885  loss_mask_6: 0.2009  loss_dice_6: 0.1534  loss_ce_7: 0.1917  loss_mask_7: 0.1934  loss_dice_7: 0.1621  loss_ce_8: 0.1897  loss_mask_8: 0.1998  loss_dice_8: 0.1566  time: 0.5607  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:52:08] d2.utils.events INFO:  eta: 3:23:24  iter: 9599  total_loss: 4.921  loss_ce: 0.0935  loss_mask: 0.2105  loss_dice: 0.1521  loss_ce_0: 0.05202  loss_mask_0: 0.2155  loss_dice_0: 0.1453  loss_ce_1: 0.09191  loss_mask_1: 0.2227  loss_dice_1: 0.1527  loss_ce_2: 0.09346  loss_mask_2: 0.2233  loss_dice_2: 0.1535  loss_ce_3: 0.09619  loss_mask_3: 0.2236  loss_dice_3: 0.1492  loss_ce_4: 0.09584  loss_mask_4: 0.2149  loss_dice_4: 0.1508  loss_ce_5: 0.0988  loss_mask_5: 0.2175  loss_dice_5: 0.1498  loss_ce_6: 0.1004  loss_mask_6: 0.2178  loss_dice_6: 0.1522  loss_ce_7: 0.09733  loss_mask_7: 0.2218  loss_dice_7: 0.1479  loss_ce_8: 0.09803  loss_mask_8: 0.2141  loss_dice_8: 0.1555  time: 0.5608  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:52:20] d2.utils.events INFO:  eta: 3:23:46  iter: 9619  total_loss: 5.402  loss_ce: 0.105  loss_mask: 0.1731  loss_dice: 0.2129  loss_ce_0: 0.0538  loss_mask_0: 0.1802  loss_dice_0: 0.2138  loss_ce_1: 0.1048  loss_mask_1: 0.1788  loss_dice_1: 0.2096  loss_ce_2: 0.1066  loss_mask_2: 0.1794  loss_dice_2: 0.2132  loss_ce_3: 0.1102  loss_mask_3: 0.185  loss_dice_3: 0.214  loss_ce_4: 0.1084  loss_mask_4: 0.1837  loss_dice_4: 0.2203  loss_ce_5: 0.112  loss_mask_5: 0.1814  loss_dice_5: 0.217  loss_ce_6: 0.1119  loss_mask_6: 0.1782  loss_dice_6: 0.2202  loss_ce_7: 0.1102  loss_mask_7: 0.1763  loss_dice_7: 0.2151  loss_ce_8: 0.1132  loss_mask_8: 0.183  loss_dice_8: 0.212  time: 0.5609  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:52:32] d2.utils.events INFO:  eta: 3:23:52  iter: 9639  total_loss: 4.849  loss_ce: 0.1094  loss_mask: 0.2142  loss_dice: 0.1471  loss_ce_0: 0.05506  loss_mask_0: 0.2069  loss_dice_0: 0.1502  loss_ce_1: 0.1111  loss_mask_1: 0.2153  loss_dice_1: 0.151  loss_ce_2: 0.1123  loss_mask_2: 0.198  loss_dice_2: 0.1479  loss_ce_3: 0.1151  loss_mask_3: 0.2047  loss_dice_3: 0.1468  loss_ce_4: 0.1133  loss_mask_4: 0.2035  loss_dice_4: 0.1473  loss_ce_5: 0.115  loss_mask_5: 0.2018  loss_dice_5: 0.1491  loss_ce_6: 0.1142  loss_mask_6: 0.2036  loss_dice_6: 0.1474  loss_ce_7: 0.1128  loss_mask_7: 0.1979  loss_dice_7: 0.1446  loss_ce_8: 0.1137  loss_mask_8: 0.1978  loss_dice_8: 0.1431  time: 0.5610  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:52:45] d2.utils.events INFO:  eta: 3:23:52  iter: 9659  total_loss: 5.102  loss_ce: 0.117  loss_mask: 0.1971  loss_dice: 0.1541  loss_ce_0: 0.0565  loss_mask_0: 0.1838  loss_dice_0: 0.1538  loss_ce_1: 0.1173  loss_mask_1: 0.2015  loss_dice_1: 0.1501  loss_ce_2: 0.1181  loss_mask_2: 0.2  loss_dice_2: 0.152  loss_ce_3: 0.1214  loss_mask_3: 0.1985  loss_dice_3: 0.1505  loss_ce_4: 0.1194  loss_mask_4: 0.1936  loss_dice_4: 0.1543  loss_ce_5: 0.1218  loss_mask_5: 0.2043  loss_dice_5: 0.1547  loss_ce_6: 0.121  loss_mask_6: 0.1952  loss_dice_6: 0.1561  loss_ce_7: 0.1205  loss_mask_7: 0.2027  loss_dice_7: 0.1502  loss_ce_8: 0.1218  loss_mask_8: 0.2116  loss_dice_8: 0.1511  time: 0.5611  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:52:57] d2.utils.events INFO:  eta: 3:24:11  iter: 9679  total_loss: 5.2  loss_ce: 0.1535  loss_mask: 0.1994  loss_dice: 0.1824  loss_ce_0: 0.07468  loss_mask_0: 0.2019  loss_dice_0: 0.1826  loss_ce_1: 0.1517  loss_mask_1: 0.2072  loss_dice_1: 0.1755  loss_ce_2: 0.1519  loss_mask_2: 0.2116  loss_dice_2: 0.1791  loss_ce_3: 0.1518  loss_mask_3: 0.2135  loss_dice_3: 0.1746  loss_ce_4: 0.1515  loss_mask_4: 0.2101  loss_dice_4: 0.18  loss_ce_5: 0.1539  loss_mask_5: 0.2099  loss_dice_5: 0.1735  loss_ce_6: 0.1547  loss_mask_6: 0.2056  loss_dice_6: 0.1768  loss_ce_7: 0.1557  loss_mask_7: 0.209  loss_dice_7: 0.1788  loss_ce_8: 0.1574  loss_mask_8: 0.1966  loss_dice_8: 0.1825  time: 0.5612  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:53:09] d2.utils.events INFO:  eta: 3:24:33  iter: 9699  total_loss: 4.567  loss_ce: 0.1172  loss_mask: 0.149  loss_dice: 0.1591  loss_ce_0: 0.05835  loss_mask_0: 0.1663  loss_dice_0: 0.165  loss_ce_1: 0.1185  loss_mask_1: 0.1624  loss_dice_1: 0.1585  loss_ce_2: 0.1177  loss_mask_2: 0.1602  loss_dice_2: 0.1609  loss_ce_3: 0.1197  loss_mask_3: 0.1597  loss_dice_3: 0.1587  loss_ce_4: 0.1198  loss_mask_4: 0.1624  loss_dice_4: 0.16  loss_ce_5: 0.1195  loss_mask_5: 0.1608  loss_dice_5: 0.1577  loss_ce_6: 0.119  loss_mask_6: 0.1676  loss_dice_6: 0.1522  loss_ce_7: 0.1179  loss_mask_7: 0.168  loss_dice_7: 0.1574  loss_ce_8: 0.1165  loss_mask_8: 0.1594  loss_dice_8: 0.1594  time: 0.5613  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:53:22] d2.utils.events INFO:  eta: 3:24:38  iter: 9719  total_loss: 4.684  loss_ce: 0.1144  loss_mask: 0.2103  loss_dice: 0.1515  loss_ce_0: 0.05792  loss_mask_0: 0.2085  loss_dice_0: 0.1451  loss_ce_1: 0.1145  loss_mask_1: 0.2169  loss_dice_1: 0.1594  loss_ce_2: 0.1138  loss_mask_2: 0.2133  loss_dice_2: 0.1654  loss_ce_3: 0.115  loss_mask_3: 0.2118  loss_dice_3: 0.1515  loss_ce_4: 0.1154  loss_mask_4: 0.2139  loss_dice_4: 0.1534  loss_ce_5: 0.115  loss_mask_5: 0.2191  loss_dice_5: 0.1604  loss_ce_6: 0.1149  loss_mask_6: 0.2126  loss_dice_6: 0.1532  loss_ce_7: 0.1142  loss_mask_7: 0.2136  loss_dice_7: 0.1526  loss_ce_8: 0.1135  loss_mask_8: 0.2103  loss_dice_8: 0.1497  time: 0.5615  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:53:34] d2.utils.events INFO:  eta: 3:24:55  iter: 9739  total_loss: 5.207  loss_ce: 0.1297  loss_mask: 0.1821  loss_dice: 0.2042  loss_ce_0: 0.06512  loss_mask_0: 0.1852  loss_dice_0: 0.2017  loss_ce_1: 0.13  loss_mask_1: 0.1821  loss_dice_1: 0.2064  loss_ce_2: 0.13  loss_mask_2: 0.1954  loss_dice_2: 0.2115  loss_ce_3: 0.1301  loss_mask_3: 0.1905  loss_dice_3: 0.1996  loss_ce_4: 0.1299  loss_mask_4: 0.194  loss_dice_4: 0.2075  loss_ce_5: 0.1301  loss_mask_5: 0.1852  loss_dice_5: 0.2056  loss_ce_6: 0.13  loss_mask_6: 0.1931  loss_dice_6: 0.2077  loss_ce_7: 0.1299  loss_mask_7: 0.1851  loss_dice_7: 0.2114  loss_ce_8: 0.1299  loss_mask_8: 0.1913  loss_dice_8: 0.2065  time: 0.5616  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:53:47] d2.utils.events INFO:  eta: 3:25:15  iter: 9759  total_loss: 4.738  loss_ce: 0.1013  loss_mask: 0.1913  loss_dice: 0.1686  loss_ce_0: 0.05539  loss_mask_0: 0.1962  loss_dice_0: 0.1672  loss_ce_1: 0.1014  loss_mask_1: 0.1914  loss_dice_1: 0.1762  loss_ce_2: 0.1013  loss_mask_2: 0.1866  loss_dice_2: 0.1705  loss_ce_3: 0.1007  loss_mask_3: 0.1996  loss_dice_3: 0.1697  loss_ce_4: 0.1011  loss_mask_4: 0.1951  loss_dice_4: 0.1676  loss_ce_5: 0.1007  loss_mask_5: 0.1883  loss_dice_5: 0.1683  loss_ce_6: 0.101  loss_mask_6: 0.1948  loss_dice_6: 0.172  loss_ce_7: 0.1008  loss_mask_7: 0.1959  loss_dice_7: 0.1779  loss_ce_8: 0.1006  loss_mask_8: 0.1906  loss_dice_8: 0.1573  time: 0.5617  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:54:00] d2.utils.events INFO:  eta: 3:25:40  iter: 9779  total_loss: 4.544  loss_ce: 0.09751  loss_mask: 0.2002  loss_dice: 0.1424  loss_ce_0: 0.05424  loss_mask_0: 0.2055  loss_dice_0: 0.1461  loss_ce_1: 0.09683  loss_mask_1: 0.2004  loss_dice_1: 0.1451  loss_ce_2: 0.09678  loss_mask_2: 0.2033  loss_dice_2: 0.1565  loss_ce_3: 0.09614  loss_mask_3: 0.204  loss_dice_3: 0.1458  loss_ce_4: 0.09654  loss_mask_4: 0.1953  loss_dice_4: 0.1495  loss_ce_5: 0.0961  loss_mask_5: 0.2019  loss_dice_5: 0.142  loss_ce_6: 0.09658  loss_mask_6: 0.2017  loss_dice_6: 0.1475  loss_ce_7: 0.09652  loss_mask_7: 0.2086  loss_dice_7: 0.1481  loss_ce_8: 0.09673  loss_mask_8: 0.2055  loss_dice_8: 0.1448  time: 0.5619  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:54:12] d2.utils.events INFO:  eta: 3:25:36  iter: 9799  total_loss: 4.87  loss_ce: 0.1676  loss_mask: 0.2044  loss_dice: 0.1438  loss_ce_0: 0.07802  loss_mask_0: 0.2007  loss_dice_0: 0.1406  loss_ce_1: 0.1659  loss_mask_1: 0.2146  loss_dice_1: 0.1402  loss_ce_2: 0.1657  loss_mask_2: 0.2101  loss_dice_2: 0.1413  loss_ce_3: 0.166  loss_mask_3: 0.2056  loss_dice_3: 0.1469  loss_ce_4: 0.1664  loss_mask_4: 0.2122  loss_dice_4: 0.1421  loss_ce_5: 0.1665  loss_mask_5: 0.2149  loss_dice_5: 0.1424  loss_ce_6: 0.1669  loss_mask_6: 0.2017  loss_dice_6: 0.1432  loss_ce_7: 0.167  loss_mask_7: 0.2093  loss_dice_7: 0.1412  loss_ce_8: 0.1682  loss_mask_8: 0.2065  loss_dice_8: 0.1449  time: 0.5620  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:54:24] d2.utils.events INFO:  eta: 3:25:28  iter: 9819  total_loss: 4.843  loss_ce: 0.1028  loss_mask: 0.1859  loss_dice: 0.1621  loss_ce_0: 0.05473  loss_mask_0: 0.1932  loss_dice_0: 0.1496  loss_ce_1: 0.1055  loss_mask_1: 0.1871  loss_dice_1: 0.1596  loss_ce_2: 0.1053  loss_mask_2: 0.1909  loss_dice_2: 0.1486  loss_ce_3: 0.106  loss_mask_3: 0.1881  loss_dice_3: 0.1476  loss_ce_4: 0.1054  loss_mask_4: 0.1951  loss_dice_4: 0.1496  loss_ce_5: 0.106  loss_mask_5: 0.1808  loss_dice_5: 0.1557  loss_ce_6: 0.1049  loss_mask_6: 0.1907  loss_dice_6: 0.1616  loss_ce_7: 0.1046  loss_mask_7: 0.1841  loss_dice_7: 0.153  loss_ce_8: 0.1032  loss_mask_8: 0.182  loss_dice_8: 0.1508  time: 0.5621  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:54:36] d2.utils.events INFO:  eta: 3:25:24  iter: 9839  total_loss: 5.076  loss_ce: 0.09959  loss_mask: 0.238  loss_dice: 0.1604  loss_ce_0: 0.05379  loss_mask_0: 0.2455  loss_dice_0: 0.1591  loss_ce_1: 0.09963  loss_mask_1: 0.2544  loss_dice_1: 0.1712  loss_ce_2: 0.0996  loss_mask_2: 0.2487  loss_dice_2: 0.159  loss_ce_3: 0.09984  loss_mask_3: 0.2387  loss_dice_3: 0.1574  loss_ce_4: 0.09999  loss_mask_4: 0.2469  loss_dice_4: 0.1525  loss_ce_5: 0.1003  loss_mask_5: 0.2352  loss_dice_5: 0.1543  loss_ce_6: 0.09992  loss_mask_6: 0.2487  loss_dice_6: 0.1637  loss_ce_7: 0.09989  loss_mask_7: 0.2544  loss_dice_7: 0.1524  loss_ce_8: 0.0997  loss_mask_8: 0.2515  loss_dice_8: 0.1603  time: 0.5622  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:54:48] d2.utils.events INFO:  eta: 3:25:15  iter: 9859  total_loss: 4.536  loss_ce: 0.1324  loss_mask: 0.1706  loss_dice: 0.1408  loss_ce_0: 0.06595  loss_mask_0: 0.1783  loss_dice_0: 0.1466  loss_ce_1: 0.1336  loss_mask_1: 0.1665  loss_dice_1: 0.1466  loss_ce_2: 0.1333  loss_mask_2: 0.1837  loss_dice_2: 0.1447  loss_ce_3: 0.1335  loss_mask_3: 0.17  loss_dice_3: 0.1417  loss_ce_4: 0.1333  loss_mask_4: 0.1797  loss_dice_4: 0.1387  loss_ce_5: 0.1333  loss_mask_5: 0.1655  loss_dice_5: 0.1392  loss_ce_6: 0.1331  loss_mask_6: 0.1695  loss_dice_6: 0.1426  loss_ce_7: 0.1331  loss_mask_7: 0.1728  loss_dice_7: 0.1439  loss_ce_8: 0.1326  loss_mask_8: 0.1666  loss_dice_8: 0.1469  time: 0.5622  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:55:00] d2.utils.events INFO:  eta: 3:25:09  iter: 9879  total_loss: 4.735  loss_ce: 0.129  loss_mask: 0.1871  loss_dice: 0.149  loss_ce_0: 0.0652  loss_mask_0: 0.1831  loss_dice_0: 0.1472  loss_ce_1: 0.1287  loss_mask_1: 0.1846  loss_dice_1: 0.149  loss_ce_2: 0.1285  loss_mask_2: 0.1922  loss_dice_2: 0.1502  loss_ce_3: 0.1285  loss_mask_3: 0.183  loss_dice_3: 0.144  loss_ce_4: 0.1285  loss_mask_4: 0.1962  loss_dice_4: 0.1475  loss_ce_5: 0.1284  loss_mask_5: 0.1922  loss_dice_5: 0.1516  loss_ce_6: 0.1287  loss_mask_6: 0.1923  loss_dice_6: 0.1478  loss_ce_7: 0.1287  loss_mask_7: 0.1924  loss_dice_7: 0.1513  loss_ce_8: 0.1289  loss_mask_8: 0.1775  loss_dice_8: 0.1503  time: 0.5623  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:55:12] d2.utils.events INFO:  eta: 3:25:00  iter: 9899  total_loss: 4.93  loss_ce: 0.1104  loss_mask: 0.1987  loss_dice: 0.1632  loss_ce_0: 0.05637  loss_mask_0: 0.2035  loss_dice_0: 0.165  loss_ce_1: 0.1137  loss_mask_1: 0.1994  loss_dice_1: 0.166  loss_ce_2: 0.1131  loss_mask_2: 0.2055  loss_dice_2: 0.1689  loss_ce_3: 0.114  loss_mask_3: 0.2019  loss_dice_3: 0.177  loss_ce_4: 0.1137  loss_mask_4: 0.2116  loss_dice_4: 0.1726  loss_ce_5: 0.1142  loss_mask_5: 0.2152  loss_dice_5: 0.1754  loss_ce_6: 0.1127  loss_mask_6: 0.2006  loss_dice_6: 0.1659  loss_ce_7: 0.1125  loss_mask_7: 0.2075  loss_dice_7: 0.1704  loss_ce_8: 0.1114  loss_mask_8: 0.2035  loss_dice_8: 0.1654  time: 0.5623  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:55:24] d2.utils.events INFO:  eta: 3:24:53  iter: 9919  total_loss: 5.082  loss_ce: 0.1112  loss_mask: 0.1953  loss_dice: 0.195  loss_ce_0: 0.0566  loss_mask_0: 0.1981  loss_dice_0: 0.2046  loss_ce_1: 0.1131  loss_mask_1: 0.1933  loss_dice_1: 0.2063  loss_ce_2: 0.1124  loss_mask_2: 0.2003  loss_dice_2: 0.193  loss_ce_3: 0.1134  loss_mask_3: 0.1947  loss_dice_3: 0.1969  loss_ce_4: 0.1134  loss_mask_4: 0.1916  loss_dice_4: 0.205  loss_ce_5: 0.1139  loss_mask_5: 0.1961  loss_dice_5: 0.1932  loss_ce_6: 0.1127  loss_mask_6: 0.1982  loss_dice_6: 0.1873  loss_ce_7: 0.1126  loss_mask_7: 0.1823  loss_dice_7: 0.1843  loss_ce_8: 0.1121  loss_mask_8: 0.1901  loss_dice_8: 0.1912  time: 0.5624  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:55:35] d2.utils.events INFO:  eta: 3:24:44  iter: 9939  total_loss: 4.931  loss_ce: 0.1283  loss_mask: 0.2086  loss_dice: 0.1753  loss_ce_0: 0.06485  loss_mask_0: 0.1936  loss_dice_0: 0.1582  loss_ce_1: 0.1281  loss_mask_1: 0.2051  loss_dice_1: 0.1724  loss_ce_2: 0.1282  loss_mask_2: 0.2135  loss_dice_2: 0.1708  loss_ce_3: 0.1282  loss_mask_3: 0.2134  loss_dice_3: 0.1694  loss_ce_4: 0.1281  loss_mask_4: 0.2087  loss_dice_4: 0.1702  loss_ce_5: 0.1282  loss_mask_5: 0.2099  loss_dice_5: 0.1697  loss_ce_6: 0.1282  loss_mask_6: 0.2159  loss_dice_6: 0.1667  loss_ce_7: 0.1282  loss_mask_7: 0.193  loss_dice_7: 0.1683  loss_ce_8: 0.1282  loss_mask_8: 0.2048  loss_dice_8: 0.1695  time: 0.5624  data_time: 0.0014  lr: 0.0001  max_mem: 2811M
[07/11 12:55:47] d2.utils.events INFO:  eta: 3:24:35  iter: 9959  total_loss: 4.776  loss_ce: 0.1099  loss_mask: 0.2005  loss_dice: 0.1513  loss_ce_0: 0.05682  loss_mask_0: 0.2039  loss_dice_0: 0.1435  loss_ce_1: 0.1099  loss_mask_1: 0.2006  loss_dice_1: 0.1461  loss_ce_2: 0.1098  loss_mask_2: 0.1869  loss_dice_2: 0.1477  loss_ce_3: 0.11  loss_mask_3: 0.1878  loss_dice_3: 0.1461  loss_ce_4: 0.1102  loss_mask_4: 0.1965  loss_dice_4: 0.1523  loss_ce_5: 0.1102  loss_mask_5: 0.1976  loss_dice_5: 0.1489  loss_ce_6: 0.1099  loss_mask_6: 0.2009  loss_dice_6: 0.1481  loss_ce_7: 0.1099  loss_mask_7: 0.1912  loss_dice_7: 0.1503  loss_ce_8: 0.11  loss_mask_8: 0.1972  loss_dice_8: 0.1508  time: 0.5625  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:55:59] d2.utils.events INFO:  eta: 3:24:23  iter: 9979  total_loss: 5.042  loss_ce: 0.1051  loss_mask: 0.1739  loss_dice: 0.1586  loss_ce_0: 0.05594  loss_mask_0: 0.1865  loss_dice_0: 0.1579  loss_ce_1: 0.1046  loss_mask_1: 0.1723  loss_dice_1: 0.1562  loss_ce_2: 0.1044  loss_mask_2: 0.1786  loss_dice_2: 0.1658  loss_ce_3: 0.1043  loss_mask_3: 0.1756  loss_dice_3: 0.1591  loss_ce_4: 0.1045  loss_mask_4: 0.1825  loss_dice_4: 0.1611  loss_ce_5: 0.1043  loss_mask_5: 0.1882  loss_dice_5: 0.1605  loss_ce_6: 0.1045  loss_mask_6: 0.181  loss_dice_6: 0.1617  loss_ce_7: 0.1044  loss_mask_7: 0.1788  loss_dice_7: 0.1637  loss_ce_8: 0.1049  loss_mask_8: 0.1921  loss_dice_8: 0.1646  time: 0.5625  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 12:56:09] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256to256_instance/model_0009999.pth
[07/11 12:56:10] d2.data.datasets.coco INFO: Loaded 754 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_val2017.json
[07/11 12:56:10] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/11 12:56:10] d2.data.common INFO: Serializing 754 elements to byte tensors and concatenating them all ...
[07/11 12:56:10] d2.data.common INFO: Serialized dataset takes 2.49 MiB
[07/11 12:56:10] d2.evaluation.evaluator INFO: Start inference on 754 batches
[07/11 12:56:15] d2.evaluation.evaluator INFO: Inference done 11/754. Dataloading: 0.0004 s/iter. Inference: 0.1407 s/iter. Eval: 0.2717 s/iter. Total: 0.4129 s/iter. ETA=0:05:06
[07/11 12:56:20] d2.evaluation.evaluator INFO: Inference done 24/754. Dataloading: 0.0007 s/iter. Inference: 0.1390 s/iter. Eval: 0.2691 s/iter. Total: 0.4088 s/iter. ETA=0:04:58
[07/11 12:56:25] d2.evaluation.evaluator INFO: Inference done 37/754. Dataloading: 0.0007 s/iter. Inference: 0.1381 s/iter. Eval: 0.2649 s/iter. Total: 0.4038 s/iter. ETA=0:04:49
[07/11 12:56:30] d2.evaluation.evaluator INFO: Inference done 50/754. Dataloading: 0.0007 s/iter. Inference: 0.1387 s/iter. Eval: 0.2639 s/iter. Total: 0.4034 s/iter. ETA=0:04:43
[07/11 12:56:35] d2.evaluation.evaluator INFO: Inference done 63/754. Dataloading: 0.0007 s/iter. Inference: 0.1374 s/iter. Eval: 0.2631 s/iter. Total: 0.4013 s/iter. ETA=0:04:37
[07/11 12:56:40] d2.evaluation.evaluator INFO: Inference done 76/754. Dataloading: 0.0007 s/iter. Inference: 0.1371 s/iter. Eval: 0.2606 s/iter. Total: 0.3985 s/iter. ETA=0:04:30
[07/11 12:56:46] d2.evaluation.evaluator INFO: Inference done 89/754. Dataloading: 0.0007 s/iter. Inference: 0.1372 s/iter. Eval: 0.2611 s/iter. Total: 0.3990 s/iter. ETA=0:04:25
[07/11 12:56:51] d2.evaluation.evaluator INFO: Inference done 101/754. Dataloading: 0.0007 s/iter. Inference: 0.1380 s/iter. Eval: 0.2631 s/iter. Total: 0.4020 s/iter. ETA=0:04:22
[07/11 12:56:56] d2.evaluation.evaluator INFO: Inference done 114/754. Dataloading: 0.0007 s/iter. Inference: 0.1378 s/iter. Eval: 0.2648 s/iter. Total: 0.4033 s/iter. ETA=0:04:18
[07/11 12:57:01] d2.evaluation.evaluator INFO: Inference done 127/754. Dataloading: 0.0007 s/iter. Inference: 0.1374 s/iter. Eval: 0.2665 s/iter. Total: 0.4046 s/iter. ETA=0:04:13
[07/11 12:57:07] d2.evaluation.evaluator INFO: Inference done 140/754. Dataloading: 0.0007 s/iter. Inference: 0.1370 s/iter. Eval: 0.2662 s/iter. Total: 0.4040 s/iter. ETA=0:04:08
[07/11 12:57:12] d2.evaluation.evaluator INFO: Inference done 153/754. Dataloading: 0.0007 s/iter. Inference: 0.1362 s/iter. Eval: 0.2672 s/iter. Total: 0.4042 s/iter. ETA=0:04:02
[07/11 12:57:17] d2.evaluation.evaluator INFO: Inference done 166/754. Dataloading: 0.0007 s/iter. Inference: 0.1358 s/iter. Eval: 0.2682 s/iter. Total: 0.4048 s/iter. ETA=0:03:58
[07/11 12:57:23] d2.evaluation.evaluator INFO: Inference done 179/754. Dataloading: 0.0007 s/iter. Inference: 0.1356 s/iter. Eval: 0.2689 s/iter. Total: 0.4054 s/iter. ETA=0:03:53
[07/11 12:57:28] d2.evaluation.evaluator INFO: Inference done 192/754. Dataloading: 0.0007 s/iter. Inference: 0.1352 s/iter. Eval: 0.2695 s/iter. Total: 0.4055 s/iter. ETA=0:03:47
[07/11 12:57:33] d2.evaluation.evaluator INFO: Inference done 205/754. Dataloading: 0.0007 s/iter. Inference: 0.1353 s/iter. Eval: 0.2702 s/iter. Total: 0.4062 s/iter. ETA=0:03:43
[07/11 12:57:39] d2.evaluation.evaluator INFO: Inference done 218/754. Dataloading: 0.0007 s/iter. Inference: 0.1344 s/iter. Eval: 0.2709 s/iter. Total: 0.4061 s/iter. ETA=0:03:37
[07/11 12:57:44] d2.evaluation.evaluator INFO: Inference done 230/754. Dataloading: 0.0007 s/iter. Inference: 0.1344 s/iter. Eval: 0.2716 s/iter. Total: 0.4069 s/iter. ETA=0:03:33
[07/11 12:57:49] d2.evaluation.evaluator INFO: Inference done 243/754. Dataloading: 0.0007 s/iter. Inference: 0.1346 s/iter. Eval: 0.2710 s/iter. Total: 0.4063 s/iter. ETA=0:03:27
[07/11 12:57:54] d2.evaluation.evaluator INFO: Inference done 256/754. Dataloading: 0.0007 s/iter. Inference: 0.1343 s/iter. Eval: 0.2704 s/iter. Total: 0.4055 s/iter. ETA=0:03:21
[07/11 12:57:59] d2.evaluation.evaluator INFO: Inference done 269/754. Dataloading: 0.0007 s/iter. Inference: 0.1339 s/iter. Eval: 0.2704 s/iter. Total: 0.4050 s/iter. ETA=0:03:16
[07/11 12:58:04] d2.evaluation.evaluator INFO: Inference done 281/754. Dataloading: 0.0007 s/iter. Inference: 0.1341 s/iter. Eval: 0.2706 s/iter. Total: 0.4055 s/iter. ETA=0:03:11
[07/11 12:58:09] d2.evaluation.evaluator INFO: Inference done 293/754. Dataloading: 0.0007 s/iter. Inference: 0.1341 s/iter. Eval: 0.2713 s/iter. Total: 0.4062 s/iter. ETA=0:03:07
[07/11 12:58:14] d2.evaluation.evaluator INFO: Inference done 306/754. Dataloading: 0.0007 s/iter. Inference: 0.1341 s/iter. Eval: 0.2715 s/iter. Total: 0.4064 s/iter. ETA=0:03:02
[07/11 12:58:20] d2.evaluation.evaluator INFO: Inference done 319/754. Dataloading: 0.0007 s/iter. Inference: 0.1343 s/iter. Eval: 0.2714 s/iter. Total: 0.4065 s/iter. ETA=0:02:56
[07/11 12:58:25] d2.evaluation.evaluator INFO: Inference done 332/754. Dataloading: 0.0007 s/iter. Inference: 0.1342 s/iter. Eval: 0.2710 s/iter. Total: 0.4059 s/iter. ETA=0:02:51
[07/11 12:58:30] d2.evaluation.evaluator INFO: Inference done 345/754. Dataloading: 0.0007 s/iter. Inference: 0.1343 s/iter. Eval: 0.2706 s/iter. Total: 0.4058 s/iter. ETA=0:02:45
[07/11 12:58:35] d2.evaluation.evaluator INFO: Inference done 358/754. Dataloading: 0.0007 s/iter. Inference: 0.1343 s/iter. Eval: 0.2706 s/iter. Total: 0.4058 s/iter. ETA=0:02:40
[07/11 12:58:40] d2.evaluation.evaluator INFO: Inference done 370/754. Dataloading: 0.0007 s/iter. Inference: 0.1346 s/iter. Eval: 0.2710 s/iter. Total: 0.4064 s/iter. ETA=0:02:36
[07/11 12:58:45] d2.evaluation.evaluator INFO: Inference done 382/754. Dataloading: 0.0007 s/iter. Inference: 0.1349 s/iter. Eval: 0.2711 s/iter. Total: 0.4068 s/iter. ETA=0:02:31
[07/11 12:58:51] d2.evaluation.evaluator INFO: Inference done 395/754. Dataloading: 0.0007 s/iter. Inference: 0.1352 s/iter. Eval: 0.2712 s/iter. Total: 0.4072 s/iter. ETA=0:02:26
[07/11 12:58:56] d2.evaluation.evaluator INFO: Inference done 408/754. Dataloading: 0.0007 s/iter. Inference: 0.1352 s/iter. Eval: 0.2713 s/iter. Total: 0.4073 s/iter. ETA=0:02:20
[07/11 12:59:01] d2.evaluation.evaluator INFO: Inference done 421/754. Dataloading: 0.0007 s/iter. Inference: 0.1352 s/iter. Eval: 0.2709 s/iter. Total: 0.4069 s/iter. ETA=0:02:15
[07/11 12:59:07] d2.evaluation.evaluator INFO: Inference done 434/754. Dataloading: 0.0007 s/iter. Inference: 0.1352 s/iter. Eval: 0.2706 s/iter. Total: 0.4066 s/iter. ETA=0:02:10
[07/11 12:59:12] d2.evaluation.evaluator INFO: Inference done 447/754. Dataloading: 0.0007 s/iter. Inference: 0.1352 s/iter. Eval: 0.2701 s/iter. Total: 0.4061 s/iter. ETA=0:02:04
[07/11 12:59:17] d2.evaluation.evaluator INFO: Inference done 460/754. Dataloading: 0.0007 s/iter. Inference: 0.1353 s/iter. Eval: 0.2696 s/iter. Total: 0.4056 s/iter. ETA=0:01:59
[07/11 12:59:22] d2.evaluation.evaluator INFO: Inference done 473/754. Dataloading: 0.0007 s/iter. Inference: 0.1355 s/iter. Eval: 0.2694 s/iter. Total: 0.4057 s/iter. ETA=0:01:54
[07/11 12:59:27] d2.evaluation.evaluator INFO: Inference done 486/754. Dataloading: 0.0007 s/iter. Inference: 0.1354 s/iter. Eval: 0.2691 s/iter. Total: 0.4053 s/iter. ETA=0:01:48
[07/11 12:59:32] d2.evaluation.evaluator INFO: Inference done 499/754. Dataloading: 0.0007 s/iter. Inference: 0.1357 s/iter. Eval: 0.2688 s/iter. Total: 0.4054 s/iter. ETA=0:01:43
[07/11 12:59:38] d2.evaluation.evaluator INFO: Inference done 512/754. Dataloading: 0.0007 s/iter. Inference: 0.1358 s/iter. Eval: 0.2685 s/iter. Total: 0.4052 s/iter. ETA=0:01:38
[07/11 12:59:43] d2.evaluation.evaluator INFO: Inference done 525/754. Dataloading: 0.0007 s/iter. Inference: 0.1358 s/iter. Eval: 0.2683 s/iter. Total: 0.4049 s/iter. ETA=0:01:32
[07/11 12:59:48] d2.evaluation.evaluator INFO: Inference done 538/754. Dataloading: 0.0007 s/iter. Inference: 0.1360 s/iter. Eval: 0.2679 s/iter. Total: 0.4047 s/iter. ETA=0:01:27
[07/11 12:59:53] d2.evaluation.evaluator INFO: Inference done 551/754. Dataloading: 0.0007 s/iter. Inference: 0.1363 s/iter. Eval: 0.2676 s/iter. Total: 0.4047 s/iter. ETA=0:01:22
[07/11 12:59:58] d2.evaluation.evaluator INFO: Inference done 564/754. Dataloading: 0.0007 s/iter. Inference: 0.1364 s/iter. Eval: 0.2673 s/iter. Total: 0.4044 s/iter. ETA=0:01:16
[07/11 13:00:04] d2.evaluation.evaluator INFO: Inference done 577/754. Dataloading: 0.0007 s/iter. Inference: 0.1365 s/iter. Eval: 0.2673 s/iter. Total: 0.4046 s/iter. ETA=0:01:11
[07/11 13:00:09] d2.evaluation.evaluator INFO: Inference done 590/754. Dataloading: 0.0007 s/iter. Inference: 0.1368 s/iter. Eval: 0.2670 s/iter. Total: 0.4046 s/iter. ETA=0:01:06
[07/11 13:00:14] d2.evaluation.evaluator INFO: Inference done 603/754. Dataloading: 0.0007 s/iter. Inference: 0.1370 s/iter. Eval: 0.2669 s/iter. Total: 0.4047 s/iter. ETA=0:01:01
[07/11 13:00:19] d2.evaluation.evaluator INFO: Inference done 616/754. Dataloading: 0.0007 s/iter. Inference: 0.1371 s/iter. Eval: 0.2668 s/iter. Total: 0.4047 s/iter. ETA=0:00:55
[07/11 13:00:24] d2.evaluation.evaluator INFO: Inference done 628/754. Dataloading: 0.0007 s/iter. Inference: 0.1375 s/iter. Eval: 0.2666 s/iter. Total: 0.4050 s/iter. ETA=0:00:51
[07/11 13:00:30] d2.evaluation.evaluator INFO: Inference done 641/754. Dataloading: 0.0007 s/iter. Inference: 0.1377 s/iter. Eval: 0.2665 s/iter. Total: 0.4050 s/iter. ETA=0:00:45
[07/11 13:00:35] d2.evaluation.evaluator INFO: Inference done 654/754. Dataloading: 0.0007 s/iter. Inference: 0.1378 s/iter. Eval: 0.2662 s/iter. Total: 0.4049 s/iter. ETA=0:00:40
[07/11 13:00:40] d2.evaluation.evaluator INFO: Inference done 666/754. Dataloading: 0.0007 s/iter. Inference: 0.1380 s/iter. Eval: 0.2663 s/iter. Total: 0.4052 s/iter. ETA=0:00:35
[07/11 13:00:45] d2.evaluation.evaluator INFO: Inference done 679/754. Dataloading: 0.0007 s/iter. Inference: 0.1382 s/iter. Eval: 0.2662 s/iter. Total: 0.4053 s/iter. ETA=0:00:30
[07/11 13:00:51] d2.evaluation.evaluator INFO: Inference done 692/754. Dataloading: 0.0007 s/iter. Inference: 0.1385 s/iter. Eval: 0.2660 s/iter. Total: 0.4053 s/iter. ETA=0:00:25
[07/11 13:00:56] d2.evaluation.evaluator INFO: Inference done 705/754. Dataloading: 0.0007 s/iter. Inference: 0.1387 s/iter. Eval: 0.2659 s/iter. Total: 0.4054 s/iter. ETA=0:00:19
[07/11 13:01:01] d2.evaluation.evaluator INFO: Inference done 718/754. Dataloading: 0.0007 s/iter. Inference: 0.1390 s/iter. Eval: 0.2657 s/iter. Total: 0.4055 s/iter. ETA=0:00:14
[07/11 13:01:07] d2.evaluation.evaluator INFO: Inference done 731/754. Dataloading: 0.0007 s/iter. Inference: 0.1391 s/iter. Eval: 0.2656 s/iter. Total: 0.4055 s/iter. ETA=0:00:09
[07/11 13:01:12] d2.evaluation.evaluator INFO: Inference done 744/754. Dataloading: 0.0007 s/iter. Inference: 0.1392 s/iter. Eval: 0.2654 s/iter. Total: 0.4054 s/iter. ETA=0:00:04
[07/11 13:01:16] d2.evaluation.evaluator INFO: Total inference time: 0:05:03.907337 (0.405751 s / iter per device, on 1 devices)
[07/11 13:01:16] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:44 (0.139339 s / iter per device, on 1 devices)
[07/11 13:01:17] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/11 13:01:17] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/11 13:01:18] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/11 13:01:20] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/11 13:01:20] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 13:01:20] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/11 13:01:28] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 66.148 | 66.148 | 66.148 |  nan  |  nan  | 66.148 |
[07/11 13:01:28] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 13:01:28] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 79.379 | defect     | 52.918 |
[07/11 13:01:28] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_new in csv format:
[07/11 13:01:28] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/11 13:01:28] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 13:01:28] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/11 13:01:28] d2.evaluation.testing INFO: copypaste: Task: segm
[07/11 13:01:28] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 13:01:28] d2.evaluation.testing INFO: copypaste: 66.1484,66.1484,66.1484,nan,nan,66.1484
[07/11 13:01:28] d2.utils.events INFO:  eta: 3:23:57  iter: 9999  total_loss: 5.323  loss_ce: 0.1617  loss_mask: 0.2313  loss_dice: 0.1728  loss_ce_0: 0.07613  loss_mask_0: 0.2249  loss_dice_0: 0.1667  loss_ce_1: 0.1609  loss_mask_1: 0.2226  loss_dice_1: 0.1634  loss_ce_2: 0.1608  loss_mask_2: 0.2186  loss_dice_2: 0.1664  loss_ce_3: 0.1611  loss_mask_3: 0.2222  loss_dice_3: 0.1617  loss_ce_4: 0.1615  loss_mask_4: 0.2325  loss_dice_4: 0.1626  loss_ce_5: 0.1617  loss_mask_5: 0.2242  loss_dice_5: 0.1654  loss_ce_6: 0.1616  loss_mask_6: 0.2194  loss_dice_6: 0.161  loss_ce_7: 0.1617  loss_mask_7: 0.2253  loss_dice_7: 0.1609  loss_ce_8: 0.1621  loss_mask_8: 0.2314  loss_dice_8: 0.175  time: 0.5625  data_time: 0.0015  lr: 0.0001  max_mem: 2811M
[07/11 13:01:28] d2.engine.hooks INFO: Not saving as latest eval score for total_loss is 5.10432, not better than best score 4.45740 @ iteration 4999.
[07/11 13:01:41] d2.utils.events INFO:  eta: 3:24:09  iter: 10019  total_loss: 4.385  loss_ce: 0.09893  loss_mask: 0.1597  loss_dice: 0.1249  loss_ce_0: 0.05423  loss_mask_0: 0.1545  loss_dice_0: 0.1314  loss_ce_1: 0.09979  loss_mask_1: 0.165  loss_dice_1: 0.1302  loss_ce_2: 0.09987  loss_mask_2: 0.1582  loss_dice_2: 0.1248  loss_ce_3: 0.09964  loss_mask_3: 0.1479  loss_dice_3: 0.1283  loss_ce_4: 0.09941  loss_mask_4: 0.1487  loss_dice_4: 0.1279  loss_ce_5: 0.09926  loss_mask_5: 0.1548  loss_dice_5: 0.137  loss_ce_6: 0.09919  loss_mask_6: 0.1478  loss_dice_6: 0.123  loss_ce_7: 0.09912  loss_mask_7: 0.1507  loss_dice_7: 0.1232  loss_ce_8: 0.09871  loss_mask_8: 0.1524  loss_dice_8: 0.1277  time: 0.5626  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:01:54] d2.utils.events INFO:  eta: 3:24:11  iter: 10039  total_loss: 5.159  loss_ce: 0.1299  loss_mask: 0.1874  loss_dice: 0.1627  loss_ce_0: 0.06507  loss_mask_0: 0.1876  loss_dice_0: 0.1583  loss_ce_1: 0.1298  loss_mask_1: 0.1921  loss_dice_1: 0.1544  loss_ce_2: 0.1295  loss_mask_2: 0.1927  loss_dice_2: 0.1582  loss_ce_3: 0.1297  loss_mask_3: 0.1859  loss_dice_3: 0.1468  loss_ce_4: 0.1298  loss_mask_4: 0.185  loss_dice_4: 0.1513  loss_ce_5: 0.1297  loss_mask_5: 0.1806  loss_dice_5: 0.1647  loss_ce_6: 0.1298  loss_mask_6: 0.1889  loss_dice_6: 0.1581  loss_ce_7: 0.1298  loss_mask_7: 0.1867  loss_dice_7: 0.1526  loss_ce_8: 0.1294  loss_mask_8: 0.1923  loss_dice_8: 0.1639  time: 0.5627  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:02:06] d2.utils.events INFO:  eta: 3:24:08  iter: 10059  total_loss: 4.648  loss_ce: 0.09893  loss_mask: 0.1778  loss_dice: 0.1687  loss_ce_0: 0.05419  loss_mask_0: 0.1899  loss_dice_0: 0.1682  loss_ce_1: 0.09946  loss_mask_1: 0.1821  loss_dice_1: 0.1661  loss_ce_2: 0.09968  loss_mask_2: 0.1825  loss_dice_2: 0.1682  loss_ce_3: 0.09949  loss_mask_3: 0.1831  loss_dice_3: 0.1704  loss_ce_4: 0.09926  loss_mask_4: 0.1821  loss_dice_4: 0.1623  loss_ce_5: 0.09919  loss_mask_5: 0.1788  loss_dice_5: 0.1658  loss_ce_6: 0.09912  loss_mask_6: 0.181  loss_dice_6: 0.1671  loss_ce_7: 0.09901  loss_mask_7: 0.1849  loss_dice_7: 0.1619  loss_ce_8: 0.09867  loss_mask_8: 0.1785  loss_dice_8: 0.1773  time: 0.5629  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:02:19] d2.utils.events INFO:  eta: 3:24:15  iter: 10079  total_loss: 4.99  loss_ce: 0.1302  loss_mask: 0.1661  loss_dice: 0.1576  loss_ce_0: 0.06517  loss_mask_0: 0.1741  loss_dice_0: 0.161  loss_ce_1: 0.1301  loss_mask_1: 0.1679  loss_dice_1: 0.1566  loss_ce_2: 0.1302  loss_mask_2: 0.166  loss_dice_2: 0.1639  loss_ce_3: 0.1303  loss_mask_3: 0.1663  loss_dice_3: 0.1577  loss_ce_4: 0.1302  loss_mask_4: 0.1671  loss_dice_4: 0.1588  loss_ce_5: 0.1303  loss_mask_5: 0.1717  loss_dice_5: 0.1637  loss_ce_6: 0.1302  loss_mask_6: 0.1631  loss_dice_6: 0.1598  loss_ce_7: 0.1303  loss_mask_7: 0.1654  loss_dice_7: 0.1564  loss_ce_8: 0.1303  loss_mask_8: 0.1602  loss_dice_8: 0.1606  time: 0.5630  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:02:32] d2.utils.events INFO:  eta: 3:24:34  iter: 10099  total_loss: 4.557  loss_ce: 0.1301  loss_mask: 0.1965  loss_dice: 0.1261  loss_ce_0: 0.06514  loss_mask_0: 0.2011  loss_dice_0: 0.1258  loss_ce_1: 0.1298  loss_mask_1: 0.2014  loss_dice_1: 0.1248  loss_ce_2: 0.1301  loss_mask_2: 0.1989  loss_dice_2: 0.1248  loss_ce_3: 0.1301  loss_mask_3: 0.2032  loss_dice_3: 0.1278  loss_ce_4: 0.1301  loss_mask_4: 0.2044  loss_dice_4: 0.1311  loss_ce_5: 0.1302  loss_mask_5: 0.1961  loss_dice_5: 0.1258  loss_ce_6: 0.1301  loss_mask_6: 0.1988  loss_dice_6: 0.1318  loss_ce_7: 0.1301  loss_mask_7: 0.2084  loss_dice_7: 0.1278  loss_ce_8: 0.1302  loss_mask_8: 0.194  loss_dice_8: 0.1311  time: 0.5631  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:02:44] d2.utils.events INFO:  eta: 3:24:40  iter: 10119  total_loss: 4.722  loss_ce: 0.1615  loss_mask: 0.1817  loss_dice: 0.1278  loss_ce_0: 0.07625  loss_mask_0: 0.1845  loss_dice_0: 0.1288  loss_ce_1: 0.1609  loss_mask_1: 0.1859  loss_dice_1: 0.1302  loss_ce_2: 0.1608  loss_mask_2: 0.1858  loss_dice_2: 0.134  loss_ce_3: 0.1613  loss_mask_3: 0.1883  loss_dice_3: 0.132  loss_ce_4: 0.1614  loss_mask_4: 0.1887  loss_dice_4: 0.1311  loss_ce_5: 0.1613  loss_mask_5: 0.1816  loss_dice_5: 0.1343  loss_ce_6: 0.1614  loss_mask_6: 0.19  loss_dice_6: 0.1289  loss_ce_7: 0.1616  loss_mask_7: 0.1851  loss_dice_7: 0.1282  loss_ce_8: 0.1608  loss_mask_8: 0.184  loss_dice_8: 0.1283  time: 0.5633  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:02:57] d2.utils.events INFO:  eta: 3:24:53  iter: 10139  total_loss: 4.587  loss_ce: 0.1298  loss_mask: 0.2059  loss_dice: 0.1446  loss_ce_0: 0.06509  loss_mask_0: 0.2021  loss_dice_0: 0.1387  loss_ce_1: 0.1298  loss_mask_1: 0.2046  loss_dice_1: 0.1416  loss_ce_2: 0.1298  loss_mask_2: 0.2037  loss_dice_2: 0.1391  loss_ce_3: 0.1298  loss_mask_3: 0.1959  loss_dice_3: 0.142  loss_ce_4: 0.1298  loss_mask_4: 0.197  loss_dice_4: 0.1373  loss_ce_5: 0.1299  loss_mask_5: 0.1933  loss_dice_5: 0.1431  loss_ce_6: 0.1297  loss_mask_6: 0.1972  loss_dice_6: 0.1432  loss_ce_7: 0.1298  loss_mask_7: 0.1916  loss_dice_7: 0.1366  loss_ce_8: 0.1305  loss_mask_8: 0.2081  loss_dice_8: 0.1461  time: 0.5634  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:03:10] d2.utils.events INFO:  eta: 3:24:48  iter: 10159  total_loss: 4.383  loss_ce: 0.09897  loss_mask: 0.204  loss_dice: 0.1441  loss_ce_0: 0.05408  loss_mask_0: 0.1992  loss_dice_0: 0.1382  loss_ce_1: 0.09949  loss_mask_1: 0.2051  loss_dice_1: 0.1444  loss_ce_2: 0.09942  loss_mask_2: 0.2019  loss_dice_2: 0.1425  loss_ce_3: 0.0993  loss_mask_3: 0.1968  loss_dice_3: 0.1442  loss_ce_4: 0.09926  loss_mask_4: 0.1912  loss_dice_4: 0.1389  loss_ce_5: 0.09919  loss_mask_5: 0.1992  loss_dice_5: 0.1395  loss_ce_6: 0.09911  loss_mask_6: 0.2038  loss_dice_6: 0.145  loss_ce_7: 0.09908  loss_mask_7: 0.1975  loss_dice_7: 0.1444  loss_ce_8: 0.09882  loss_mask_8: 0.1919  loss_dice_8: 0.1375  time: 0.5635  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:03:22] d2.utils.events INFO:  eta: 3:24:55  iter: 10179  total_loss: 4.47  loss_ce: 0.09804  loss_mask: 0.1691  loss_dice: 0.1333  loss_ce_0: 0.05383  loss_mask_0: 0.1722  loss_dice_0: 0.1388  loss_ce_1: 0.09813  loss_mask_1: 0.1713  loss_dice_1: 0.1395  loss_ce_2: 0.09813  loss_mask_2: 0.1678  loss_dice_2: 0.1347  loss_ce_3: 0.09803  loss_mask_3: 0.1661  loss_dice_3: 0.1396  loss_ce_4: 0.09803  loss_mask_4: 0.1618  loss_dice_4: 0.1332  loss_ce_5: 0.09799  loss_mask_5: 0.1686  loss_dice_5: 0.1392  loss_ce_6: 0.09804  loss_mask_6: 0.1684  loss_dice_6: 0.135  loss_ce_7: 0.098  loss_mask_7: 0.1793  loss_dice_7: 0.1358  loss_ce_8: 0.09782  loss_mask_8: 0.1722  loss_dice_8: 0.1341  time: 0.5637  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:03:35] d2.utils.events INFO:  eta: 3:24:58  iter: 10199  total_loss: 4.382  loss_ce: 0.1616  loss_mask: 0.1991  loss_dice: 0.1111  loss_ce_0: 0.07635  loss_mask_0: 0.1992  loss_dice_0: 0.1112  loss_ce_1: 0.1614  loss_mask_1: 0.2053  loss_dice_1: 0.1155  loss_ce_2: 0.1613  loss_mask_2: 0.2076  loss_dice_2: 0.1107  loss_ce_3: 0.1614  loss_mask_3: 0.2026  loss_dice_3: 0.1102  loss_ce_4: 0.1615  loss_mask_4: 0.1999  loss_dice_4: 0.1128  loss_ce_5: 0.1615  loss_mask_5: 0.189  loss_dice_5: 0.1125  loss_ce_6: 0.1616  loss_mask_6: 0.1953  loss_dice_6: 0.1153  loss_ce_7: 0.1616  loss_mask_7: 0.2028  loss_dice_7: 0.1145  loss_ce_8: 0.1619  loss_mask_8: 0.1981  loss_dice_8: 0.1146  time: 0.5638  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:03:48] d2.utils.events INFO:  eta: 3:24:49  iter: 10219  total_loss: 4.516  loss_ce: 0.09889  loss_mask: 0.1985  loss_dice: 0.1293  loss_ce_0: 0.05398  loss_mask_0: 0.2008  loss_dice_0: 0.1292  loss_ce_1: 0.09957  loss_mask_1: 0.2092  loss_dice_1: 0.1265  loss_ce_2: 0.09928  loss_mask_2: 0.2058  loss_dice_2: 0.1313  loss_ce_3: 0.09915  loss_mask_3: 0.2062  loss_dice_3: 0.1305  loss_ce_4: 0.09922  loss_mask_4: 0.1987  loss_dice_4: 0.1308  loss_ce_5: 0.09907  loss_mask_5: 0.2052  loss_dice_5: 0.1263  loss_ce_6: 0.09904  loss_mask_6: 0.1966  loss_dice_6: 0.1321  loss_ce_7: 0.09897  loss_mask_7: 0.1958  loss_dice_7: 0.1335  loss_ce_8: 0.09874  loss_mask_8: 0.2134  loss_dice_8: 0.125  time: 0.5639  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:04:00] d2.utils.events INFO:  eta: 3:24:35  iter: 10239  total_loss: 4.532  loss_ce: 0.099  loss_mask: 0.1263  loss_dice: 0.1479  loss_ce_0: 0.05399  loss_mask_0: 0.1274  loss_dice_0: 0.1548  loss_ce_1: 0.09938  loss_mask_1: 0.1278  loss_dice_1: 0.1499  loss_ce_2: 0.09936  loss_mask_2: 0.1263  loss_dice_2: 0.1596  loss_ce_3: 0.09926  loss_mask_3: 0.1315  loss_dice_3: 0.1543  loss_ce_4: 0.09922  loss_mask_4: 0.1275  loss_dice_4: 0.1515  loss_ce_5: 0.09922  loss_mask_5: 0.1189  loss_dice_5: 0.1457  loss_ce_6: 0.09904  loss_mask_6: 0.1272  loss_dice_6: 0.1483  loss_ce_7: 0.09912  loss_mask_7: 0.1247  loss_dice_7: 0.151  loss_ce_8: 0.09885  loss_mask_8: 0.1325  loss_dice_8: 0.158  time: 0.5641  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:04:13] d2.utils.events INFO:  eta: 3:24:20  iter: 10259  total_loss: 4.634  loss_ce: 0.098  loss_mask: 0.1943  loss_dice: 0.1357  loss_ce_0: 0.05374  loss_mask_0: 0.2005  loss_dice_0: 0.1429  loss_ce_1: 0.09793  loss_mask_1: 0.1977  loss_dice_1: 0.1388  loss_ce_2: 0.09794  loss_mask_2: 0.1962  loss_dice_2: 0.1416  loss_ce_3: 0.09788  loss_mask_3: 0.1967  loss_dice_3: 0.1357  loss_ce_4: 0.09796  loss_mask_4: 0.2045  loss_dice_4: 0.1381  loss_ce_5: 0.09788  loss_mask_5: 0.1924  loss_dice_5: 0.1391  loss_ce_6: 0.09789  loss_mask_6: 0.1993  loss_dice_6: 0.1383  loss_ce_7: 0.09793  loss_mask_7: 0.2017  loss_dice_7: 0.1372  loss_ce_8: 0.09774  loss_mask_8: 0.2027  loss_dice_8: 0.138  time: 0.5642  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:04:26] d2.utils.events INFO:  eta: 3:24:07  iter: 10279  total_loss: 4.868  loss_ce: 0.1302  loss_mask: 0.2284  loss_dice: 0.1449  loss_ce_0: 0.06514  loss_mask_0: 0.2212  loss_dice_0: 0.1469  loss_ce_1: 0.1303  loss_mask_1: 0.2145  loss_dice_1: 0.1443  loss_ce_2: 0.1303  loss_mask_2: 0.2217  loss_dice_2: 0.148  loss_ce_3: 0.1303  loss_mask_3: 0.2073  loss_dice_3: 0.1463  loss_ce_4: 0.1303  loss_mask_4: 0.2174  loss_dice_4: 0.1505  loss_ce_5: 0.1302  loss_mask_5: 0.2138  loss_dice_5: 0.1473  loss_ce_6: 0.1304  loss_mask_6: 0.2234  loss_dice_6: 0.1497  loss_ce_7: 0.1302  loss_mask_7: 0.22  loss_dice_7: 0.1457  loss_ce_8: 0.1303  loss_mask_8: 0.2256  loss_dice_8: 0.1424  time: 0.5643  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:04:38] d2.utils.events INFO:  eta: 3:23:54  iter: 10299  total_loss: 4.779  loss_ce: 0.09778  loss_mask: 0.1892  loss_dice: 0.1574  loss_ce_0: 0.05362  loss_mask_0: 0.1942  loss_dice_0: 0.1611  loss_ce_1: 0.09776  loss_mask_1: 0.1915  loss_dice_1: 0.1613  loss_ce_2: 0.09778  loss_mask_2: 0.1976  loss_dice_2: 0.1623  loss_ce_3: 0.0977  loss_mask_3: 0.188  loss_dice_3: 0.1614  loss_ce_4: 0.09774  loss_mask_4: 0.1976  loss_dice_4: 0.1552  loss_ce_5: 0.09773  loss_mask_5: 0.1902  loss_dice_5: 0.1584  loss_ce_6: 0.09774  loss_mask_6: 0.1968  loss_dice_6: 0.1586  loss_ce_7: 0.09774  loss_mask_7: 0.1903  loss_dice_7: 0.1626  loss_ce_8: 0.09759  loss_mask_8: 0.1917  loss_dice_8: 0.1565  time: 0.5645  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:04:51] d2.utils.events INFO:  eta: 3:23:42  iter: 10319  total_loss: 4.616  loss_ce: 0.09704  loss_mask: 0.1797  loss_dice: 0.1443  loss_ce_0: 0.05341  loss_mask_0: 0.1907  loss_dice_0: 0.1452  loss_ce_1: 0.09682  loss_mask_1: 0.1894  loss_dice_1: 0.1495  loss_ce_2: 0.09674  loss_mask_2: 0.1882  loss_dice_2: 0.1403  loss_ce_3: 0.09667  loss_mask_3: 0.1791  loss_dice_3: 0.1372  loss_ce_4: 0.09674  loss_mask_4: 0.1893  loss_dice_4: 0.1388  loss_ce_5: 0.0967  loss_mask_5: 0.1842  loss_dice_5: 0.1413  loss_ce_6: 0.09678  loss_mask_6: 0.1844  loss_dice_6: 0.1428  loss_ce_7: 0.09686  loss_mask_7: 0.1851  loss_dice_7: 0.1431  loss_ce_8: 0.09674  loss_mask_8: 0.1807  loss_dice_8: 0.145  time: 0.5646  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:05:03] d2.utils.events INFO:  eta: 3:23:28  iter: 10339  total_loss: 4.714  loss_ce: 0.09583  loss_mask: 0.1803  loss_dice: 0.1565  loss_ce_0: 0.05306  loss_mask_0: 0.1806  loss_dice_0: 0.1485  loss_ce_1: 0.09563  loss_mask_1: 0.1785  loss_dice_1: 0.1546  loss_ce_2: 0.09539  loss_mask_2: 0.1751  loss_dice_2: 0.1461  loss_ce_3: 0.09528  loss_mask_3: 0.1745  loss_dice_3: 0.1561  loss_ce_4: 0.09539  loss_mask_4: 0.1786  loss_dice_4: 0.152  loss_ce_5: 0.09535  loss_mask_5: 0.1783  loss_dice_5: 0.1573  loss_ce_6: 0.09553  loss_mask_6: 0.1787  loss_dice_6: 0.153  loss_ce_7: 0.09547  loss_mask_7: 0.1821  loss_dice_7: 0.158  loss_ce_8: 0.09547  loss_mask_8: 0.1758  loss_dice_8: 0.1553  time: 0.5647  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:05:15] d2.utils.events INFO:  eta: 3:23:06  iter: 10359  total_loss: 4.931  loss_ce: 0.1306  loss_mask: 0.2319  loss_dice: 0.1388  loss_ce_0: 0.06521  loss_mask_0: 0.2377  loss_dice_0: 0.1412  loss_ce_1: 0.1307  loss_mask_1: 0.2349  loss_dice_1: 0.1404  loss_ce_2: 0.1307  loss_mask_2: 0.244  loss_dice_2: 0.143  loss_ce_3: 0.1307  loss_mask_3: 0.2301  loss_dice_3: 0.1362  loss_ce_4: 0.1307  loss_mask_4: 0.2358  loss_dice_4: 0.1404  loss_ce_5: 0.1307  loss_mask_5: 0.2379  loss_dice_5: 0.1381  loss_ce_6: 0.1307  loss_mask_6: 0.2331  loss_dice_6: 0.1458  loss_ce_7: 0.1307  loss_mask_7: 0.2447  loss_dice_7: 0.14  loss_ce_8: 0.1306  loss_mask_8: 0.2353  loss_dice_8: 0.1364  time: 0.5648  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:05:27] d2.utils.events INFO:  eta: 3:22:46  iter: 10379  total_loss: 4.301  loss_ce: 0.09568  loss_mask: 0.1687  loss_dice: 0.1323  loss_ce_0: 0.05301  loss_mask_0: 0.175  loss_dice_0: 0.1468  loss_ce_1: 0.09527  loss_mask_1: 0.1721  loss_dice_1: 0.1358  loss_ce_2: 0.09528  loss_mask_2: 0.1624  loss_dice_2: 0.1375  loss_ce_3: 0.09514  loss_mask_3: 0.1729  loss_dice_3: 0.1314  loss_ce_4: 0.09521  loss_mask_4: 0.1781  loss_dice_4: 0.1373  loss_ce_5: 0.0952  loss_mask_5: 0.1802  loss_dice_5: 0.1405  loss_ce_6: 0.09535  loss_mask_6: 0.1785  loss_dice_6: 0.1339  loss_ce_7: 0.09539  loss_mask_7: 0.1752  loss_dice_7: 0.1365  loss_ce_8: 0.09536  loss_mask_8: 0.1617  loss_dice_8: 0.134  time: 0.5648  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:05:38] d2.utils.events INFO:  eta: 3:22:17  iter: 10399  total_loss: 4.413  loss_ce: 0.09496  loss_mask: 0.1945  loss_dice: 0.131  loss_ce_0: 0.05281  loss_mask_0: 0.1963  loss_dice_0: 0.1351  loss_ce_1: 0.09456  loss_mask_1: 0.2023  loss_dice_1: 0.1393  loss_ce_2: 0.09454  loss_mask_2: 0.1955  loss_dice_2: 0.1347  loss_ce_3: 0.09441  loss_mask_3: 0.1872  loss_dice_3: 0.1345  loss_ce_4: 0.09448  loss_mask_4: 0.1889  loss_dice_4: 0.1391  loss_ce_5: 0.09445  loss_mask_5: 0.189  loss_dice_5: 0.1394  loss_ce_6: 0.09463  loss_mask_6: 0.192  loss_dice_6: 0.1394  loss_ce_7: 0.0947  loss_mask_7: 0.198  loss_dice_7: 0.1331  loss_ce_8: 0.09467  loss_mask_8: 0.1925  loss_dice_8: 0.137  time: 0.5648  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:05:50] d2.utils.events INFO:  eta: 3:21:46  iter: 10419  total_loss: 4.699  loss_ce: 0.09474  loss_mask: 0.2256  loss_dice: 0.1372  loss_ce_0: 0.05274  loss_mask_0: 0.2263  loss_dice_0: 0.1348  loss_ce_1: 0.09424  loss_mask_1: 0.2144  loss_dice_1: 0.1368  loss_ce_2: 0.09431  loss_mask_2: 0.2189  loss_dice_2: 0.1391  loss_ce_3: 0.0942  loss_mask_3: 0.221  loss_dice_3: 0.1398  loss_ce_4: 0.0943  loss_mask_4: 0.2178  loss_dice_4: 0.1389  loss_ce_5: 0.09423  loss_mask_5: 0.2176  loss_dice_5: 0.1405  loss_ce_6: 0.09438  loss_mask_6: 0.2231  loss_dice_6: 0.1396  loss_ce_7: 0.09445  loss_mask_7: 0.2182  loss_dice_7: 0.1332  loss_ce_8: 0.09445  loss_mask_8: 0.2231  loss_dice_8: 0.138  time: 0.5648  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:06:01] d2.utils.events INFO:  eta: 3:21:13  iter: 10439  total_loss: 4.251  loss_ce: 0.09456  loss_mask: 0.1622  loss_dice: 0.1404  loss_ce_0: 0.05266  loss_mask_0: 0.162  loss_dice_0: 0.1406  loss_ce_1: 0.09418  loss_mask_1: 0.1607  loss_dice_1: 0.1434  loss_ce_2: 0.09418  loss_mask_2: 0.1585  loss_dice_2: 0.1387  loss_ce_3: 0.09409  loss_mask_3: 0.1633  loss_dice_3: 0.1345  loss_ce_4: 0.09432  loss_mask_4: 0.1619  loss_dice_4: 0.1437  loss_ce_5: 0.09405  loss_mask_5: 0.1619  loss_dice_5: 0.14  loss_ce_6: 0.09413  loss_mask_6: 0.165  loss_dice_6: 0.1439  loss_ce_7: 0.09429  loss_mask_7: 0.1657  loss_dice_7: 0.1367  loss_ce_8: 0.0942  loss_mask_8: 0.1606  loss_dice_8: 0.1363  time: 0.5649  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:06:13] d2.utils.events INFO:  eta: 3:20:54  iter: 10459  total_loss: 4.355  loss_ce: 0.09467  loss_mask: 0.1689  loss_dice: 0.1358  loss_ce_0: 0.05266  loss_mask_0: 0.1696  loss_dice_0: 0.1321  loss_ce_1: 0.09449  loss_mask_1: 0.1786  loss_dice_1: 0.1369  loss_ce_2: 0.09443  loss_mask_2: 0.1722  loss_dice_2: 0.136  loss_ce_3: 0.09437  loss_mask_3: 0.1623  loss_dice_3: 0.1322  loss_ce_4: 0.09448  loss_mask_4: 0.175  loss_dice_4: 0.1384  loss_ce_5: 0.09437  loss_mask_5: 0.1696  loss_dice_5: 0.1325  loss_ce_6: 0.09445  loss_mask_6: 0.1784  loss_dice_6: 0.1292  loss_ce_7: 0.09456  loss_mask_7: 0.1791  loss_dice_7: 0.1369  loss_ce_8: 0.09434  loss_mask_8: 0.1707  loss_dice_8: 0.1392  time: 0.5649  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:06:24] d2.utils.events INFO:  eta: 3:20:12  iter: 10479  total_loss: 4.52  loss_ce: 0.09467  loss_mask: 0.1704  loss_dice: 0.1433  loss_ce_0: 0.05267  loss_mask_0: 0.1737  loss_dice_0: 0.1515  loss_ce_1: 0.09461  loss_mask_1: 0.1699  loss_dice_1: 0.1425  loss_ce_2: 0.09448  loss_mask_2: 0.1758  loss_dice_2: 0.148  loss_ce_3: 0.09437  loss_mask_3: 0.1711  loss_dice_3: 0.1502  loss_ce_4: 0.09448  loss_mask_4: 0.1724  loss_dice_4: 0.1492  loss_ce_5: 0.09437  loss_mask_5: 0.1744  loss_dice_5: 0.1507  loss_ce_6: 0.09452  loss_mask_6: 0.173  loss_dice_6: 0.1561  loss_ce_7: 0.09452  loss_mask_7: 0.1727  loss_dice_7: 0.1436  loss_ce_8: 0.09438  loss_mask_8: 0.1742  loss_dice_8: 0.1571  time: 0.5649  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:06:36] d2.utils.events INFO:  eta: 3:19:38  iter: 10499  total_loss: 4.168  loss_ce: 0.09477  loss_mask: 0.193  loss_dice: 0.1273  loss_ce_0: 0.05265  loss_mask_0: 0.1931  loss_dice_0: 0.1248  loss_ce_1: 0.09469  loss_mask_1: 0.1997  loss_dice_1: 0.1301  loss_ce_2: 0.09463  loss_mask_2: 0.1898  loss_dice_2: 0.1242  loss_ce_3: 0.09459  loss_mask_3: 0.1908  loss_dice_3: 0.1262  loss_ce_4: 0.09466  loss_mask_4: 0.1889  loss_dice_4: 0.1241  loss_ce_5: 0.09452  loss_mask_5: 0.1933  loss_dice_5: 0.1262  loss_ce_6: 0.09456  loss_mask_6: 0.19  loss_dice_6: 0.1271  loss_ce_7: 0.09467  loss_mask_7: 0.1893  loss_dice_7: 0.1283  loss_ce_8: 0.09452  loss_mask_8: 0.2003  loss_dice_8: 0.1351  time: 0.5649  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:06:48] d2.utils.events INFO:  eta: 3:19:13  iter: 10519  total_loss: 4.86  loss_ce: 0.1309  loss_mask: 0.2102  loss_dice: 0.1258  loss_ce_0: 0.06527  loss_mask_0: 0.2117  loss_dice_0: 0.1237  loss_ce_1: 0.1311  loss_mask_1: 0.2036  loss_dice_1: 0.1227  loss_ce_2: 0.1312  loss_mask_2: 0.2156  loss_dice_2: 0.1261  loss_ce_3: 0.131  loss_mask_3: 0.2116  loss_dice_3: 0.1267  loss_ce_4: 0.1309  loss_mask_4: 0.2078  loss_dice_4: 0.1234  loss_ce_5: 0.1311  loss_mask_5: 0.2017  loss_dice_5: 0.1189  loss_ce_6: 0.131  loss_mask_6: 0.2119  loss_dice_6: 0.1299  loss_ce_7: 0.131  loss_mask_7: 0.2045  loss_dice_7: 0.1222  loss_ce_8: 0.1317  loss_mask_8: 0.2182  loss_dice_8: 0.1238  time: 0.5649  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:06:59] d2.utils.events INFO:  eta: 3:18:32  iter: 10539  total_loss: 4.44  loss_ce: 0.09427  loss_mask: 0.1576  loss_dice: 0.1435  loss_ce_0: 0.05249  loss_mask_0: 0.1599  loss_dice_0: 0.143  loss_ce_1: 0.09404  loss_mask_1: 0.1586  loss_dice_1: 0.1424  loss_ce_2: 0.09411  loss_mask_2: 0.1502  loss_dice_2: 0.1407  loss_ce_3: 0.09401  loss_mask_3: 0.1641  loss_dice_3: 0.1486  loss_ce_4: 0.09418  loss_mask_4: 0.1572  loss_dice_4: 0.1347  loss_ce_5: 0.09398  loss_mask_5: 0.1593  loss_dice_5: 0.1506  loss_ce_6: 0.09405  loss_mask_6: 0.1631  loss_dice_6: 0.1435  loss_ce_7: 0.09416  loss_mask_7: 0.1586  loss_dice_7: 0.1407  loss_ce_8: 0.09402  loss_mask_8: 0.1574  loss_dice_8: 0.1408  time: 0.5650  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:07:11] d2.utils.events INFO:  eta: 3:18:15  iter: 10559  total_loss: 4.456  loss_ce: 0.09517  loss_mask: 0.1818  loss_dice: 0.1376  loss_ce_0: 0.0527  loss_mask_0: 0.1686  loss_dice_0: 0.1367  loss_ce_1: 0.09532  loss_mask_1: 0.1816  loss_dice_1: 0.1361  loss_ce_2: 0.09548  loss_mask_2: 0.1733  loss_dice_2: 0.1322  loss_ce_3: 0.09517  loss_mask_3: 0.1726  loss_dice_3: 0.1343  loss_ce_4: 0.09531  loss_mask_4: 0.1783  loss_dice_4: 0.1348  loss_ce_5: 0.0953  loss_mask_5: 0.1669  loss_dice_5: 0.1398  loss_ce_6: 0.09517  loss_mask_6: 0.1833  loss_dice_6: 0.1354  loss_ce_7: 0.09525  loss_mask_7: 0.1823  loss_dice_7: 0.1307  loss_ce_8: 0.09566  loss_mask_8: 0.1878  loss_dice_8: 0.1337  time: 0.5650  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:07:22] d2.utils.events INFO:  eta: 3:18:03  iter: 10579  total_loss: 4.935  loss_ce: 0.1307  loss_mask: 0.1817  loss_dice: 0.1392  loss_ce_0: 0.06522  loss_mask_0: 0.1692  loss_dice_0: 0.1345  loss_ce_1: 0.1306  loss_mask_1: 0.1715  loss_dice_1: 0.1374  loss_ce_2: 0.1306  loss_mask_2: 0.1648  loss_dice_2: 0.1333  loss_ce_3: 0.1306  loss_mask_3: 0.1698  loss_dice_3: 0.1333  loss_ce_4: 0.1306  loss_mask_4: 0.1742  loss_dice_4: 0.1292  loss_ce_5: 0.1306  loss_mask_5: 0.1744  loss_dice_5: 0.1323  loss_ce_6: 0.1307  loss_mask_6: 0.1701  loss_dice_6: 0.1362  loss_ce_7: 0.1306  loss_mask_7: 0.1691  loss_dice_7: 0.1383  loss_ce_8: 0.1307  loss_mask_8: 0.1692  loss_dice_8: 0.1354  time: 0.5650  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:07:34] d2.utils.events INFO:  eta: 3:17:50  iter: 10599  total_loss: 4.618  loss_ce: 0.09488  loss_mask: 0.1855  loss_dice: 0.1497  loss_ce_0: 0.05257  loss_mask_0: 0.1789  loss_dice_0: 0.1531  loss_ce_1: 0.09479  loss_mask_1: 0.1863  loss_dice_1: 0.1534  loss_ce_2: 0.09488  loss_mask_2: 0.1819  loss_dice_2: 0.151  loss_ce_3: 0.09477  loss_mask_3: 0.1802  loss_dice_3: 0.1497  loss_ce_4: 0.09486  loss_mask_4: 0.1755  loss_dice_4: 0.1502  loss_ce_5: 0.09477  loss_mask_5: 0.1724  loss_dice_5: 0.1505  loss_ce_6: 0.09484  loss_mask_6: 0.1734  loss_dice_6: 0.1504  loss_ce_7: 0.09488  loss_mask_7: 0.1755  loss_dice_7: 0.1528  loss_ce_8: 0.09467  loss_mask_8: 0.1809  loss_dice_8: 0.1529  time: 0.5650  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:07:45] d2.utils.events INFO:  eta: 3:17:37  iter: 10619  total_loss: 4.611  loss_ce: 0.09384  loss_mask: 0.195  loss_dice: 0.1406  loss_ce_0: 0.05226  loss_mask_0: 0.187  loss_dice_0: 0.1404  loss_ce_1: 0.09364  loss_mask_1: 0.1868  loss_dice_1: 0.1413  loss_ce_2: 0.0936  loss_mask_2: 0.1898  loss_dice_2: 0.1405  loss_ce_3: 0.09351  loss_mask_3: 0.1907  loss_dice_3: 0.1382  loss_ce_4: 0.09358  loss_mask_4: 0.186  loss_dice_4: 0.1456  loss_ce_5: 0.09355  loss_mask_5: 0.1896  loss_dice_5: 0.1397  loss_ce_6: 0.09362  loss_mask_6: 0.1771  loss_dice_6: 0.1392  loss_ce_7: 0.09366  loss_mask_7: 0.1797  loss_dice_7: 0.1401  loss_ce_8: 0.09359  loss_mask_8: 0.1935  loss_dice_8: 0.1398  time: 0.5651  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:07:57] d2.utils.events INFO:  eta: 3:16:56  iter: 10639  total_loss: 4.623  loss_ce: 0.09366  loss_mask: 0.1707  loss_dice: 0.1276  loss_ce_0: 0.05222  loss_mask_0: 0.1738  loss_dice_0: 0.1326  loss_ce_1: 0.09339  loss_mask_1: 0.1771  loss_dice_1: 0.1332  loss_ce_2: 0.09341  loss_mask_2: 0.1753  loss_dice_2: 0.1261  loss_ce_3: 0.09333  loss_mask_3: 0.1794  loss_dice_3: 0.1276  loss_ce_4: 0.0934  loss_mask_4: 0.179  loss_dice_4: 0.1343  loss_ce_5: 0.09337  loss_mask_5: 0.1799  loss_dice_5: 0.1285  loss_ce_6: 0.09337  loss_mask_6: 0.1774  loss_dice_6: 0.1311  loss_ce_7: 0.09352  loss_mask_7: 0.1752  loss_dice_7: 0.1356  loss_ce_8: 0.09341  loss_mask_8: 0.1792  loss_dice_8: 0.1273  time: 0.5651  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:08:08] d2.utils.events INFO:  eta: 3:16:18  iter: 10659  total_loss: 4.536  loss_ce: 0.1684  loss_mask: 0.1848  loss_dice: 0.1324  loss_ce_0: 0.07838  loss_mask_0: 0.1903  loss_dice_0: 0.142  loss_ce_1: 0.1685  loss_mask_1: 0.1849  loss_dice_1: 0.1375  loss_ce_2: 0.1687  loss_mask_2: 0.1859  loss_dice_2: 0.1367  loss_ce_3: 0.1689  loss_mask_3: 0.1807  loss_dice_3: 0.1397  loss_ce_4: 0.1685  loss_mask_4: 0.1857  loss_dice_4: 0.1376  loss_ce_5: 0.1687  loss_mask_5: 0.1797  loss_dice_5: 0.1377  loss_ce_6: 0.1686  loss_mask_6: 0.1809  loss_dice_6: 0.1399  loss_ce_7: 0.1686  loss_mask_7: 0.1884  loss_dice_7: 0.1336  loss_ce_8: 0.1688  loss_mask_8: 0.1875  loss_dice_8: 0.1371  time: 0.5651  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:08:20] d2.utils.events INFO:  eta: 3:15:35  iter: 10679  total_loss: 4.569  loss_ce: 0.09387  loss_mask: 0.1731  loss_dice: 0.1666  loss_ce_0: 0.05225  loss_mask_0: 0.1865  loss_dice_0: 0.1595  loss_ce_1: 0.09393  loss_mask_1: 0.1775  loss_dice_1: 0.1587  loss_ce_2: 0.09378  loss_mask_2: 0.182  loss_dice_2: 0.1595  loss_ce_3: 0.09373  loss_mask_3: 0.1729  loss_dice_3: 0.1571  loss_ce_4: 0.0938  loss_mask_4: 0.1689  loss_dice_4: 0.1634  loss_ce_5: 0.09379  loss_mask_5: 0.168  loss_dice_5: 0.1606  loss_ce_6: 0.09373  loss_mask_6: 0.1786  loss_dice_6: 0.1677  loss_ce_7: 0.09387  loss_mask_7: 0.1768  loss_dice_7: 0.1591  loss_ce_8: 0.09369  loss_mask_8: 0.1815  loss_dice_8: 0.1619  time: 0.5651  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:08:32] d2.utils.events INFO:  eta: 3:14:48  iter: 10699  total_loss: 4.568  loss_ce: 0.09366  loss_mask: 0.1642  loss_dice: 0.1837  loss_ce_0: 0.05216  loss_mask_0: 0.1729  loss_dice_0: 0.181  loss_ce_1: 0.09368  loss_mask_1: 0.1666  loss_dice_1: 0.1738  loss_ce_2: 0.09355  loss_mask_2: 0.1617  loss_dice_2: 0.1746  loss_ce_3: 0.09351  loss_mask_3: 0.168  loss_dice_3: 0.1769  loss_ce_4: 0.09351  loss_mask_4: 0.159  loss_dice_4: 0.1762  loss_ce_5: 0.09347  loss_mask_5: 0.1589  loss_dice_5: 0.1786  loss_ce_6: 0.09351  loss_mask_6: 0.1733  loss_dice_6: 0.1917  loss_ce_7: 0.09359  loss_mask_7: 0.1704  loss_dice_7: 0.1789  loss_ce_8: 0.09344  loss_mask_8: 0.1699  loss_dice_8: 0.1738  time: 0.5652  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:08:43] d2.utils.events INFO:  eta: 3:14:12  iter: 10719  total_loss: 4.682  loss_ce: 0.09216  loss_mask: 0.1821  loss_dice: 0.1726  loss_ce_0: 0.05173  loss_mask_0: 0.1814  loss_dice_0: 0.1721  loss_ce_1: 0.09176  loss_mask_1: 0.176  loss_dice_1: 0.1716  loss_ce_2: 0.09175  loss_mask_2: 0.1771  loss_dice_2: 0.1737  loss_ce_3: 0.09159  loss_mask_3: 0.1798  loss_dice_3: 0.1723  loss_ce_4: 0.09177  loss_mask_4: 0.1838  loss_dice_4: 0.1839  loss_ce_5: 0.09163  loss_mask_5: 0.1738  loss_dice_5: 0.1699  loss_ce_6: 0.09166  loss_mask_6: 0.1834  loss_dice_6: 0.1784  loss_ce_7: 0.09195  loss_mask_7: 0.1829  loss_dice_7: 0.1816  loss_ce_8: 0.09198  loss_mask_8: 0.173  loss_dice_8: 0.1789  time: 0.5652  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:08:55] d2.utils.events INFO:  eta: 3:13:28  iter: 10739  total_loss: 4.693  loss_ce: 0.09044  loss_mask: 0.189  loss_dice: 0.141  loss_ce_0: 0.05128  loss_mask_0: 0.1793  loss_dice_0: 0.1472  loss_ce_1: 0.08971  loss_mask_1: 0.1828  loss_dice_1: 0.1443  loss_ce_2: 0.08966  loss_mask_2: 0.1832  loss_dice_2: 0.1532  loss_ce_3: 0.0895  loss_mask_3: 0.1846  loss_dice_3: 0.1494  loss_ce_4: 0.08971  loss_mask_4: 0.1783  loss_dice_4: 0.1458  loss_ce_5: 0.0896  loss_mask_5: 0.1808  loss_dice_5: 0.1484  loss_ce_6: 0.08981  loss_mask_6: 0.1869  loss_dice_6: 0.1515  loss_ce_7: 0.09013  loss_mask_7: 0.1838  loss_dice_7: 0.1513  loss_ce_8: 0.09006  loss_mask_8: 0.1872  loss_dice_8: 0.1431  time: 0.5652  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:09:06] d2.utils.events INFO:  eta: 3:12:56  iter: 10759  total_loss: 4.783  loss_ce: 0.172  loss_mask: 0.1678  loss_dice: 0.1469  loss_ce_0: 0.07936  loss_mask_0: 0.1662  loss_dice_0: 0.141  loss_ce_1: 0.1732  loss_mask_1: 0.1619  loss_dice_1: 0.1513  loss_ce_2: 0.1726  loss_mask_2: 0.1651  loss_dice_2: 0.1451  loss_ce_3: 0.1728  loss_mask_3: 0.1665  loss_dice_3: 0.1427  loss_ce_4: 0.1723  loss_mask_4: 0.1562  loss_dice_4: 0.1444  loss_ce_5: 0.1728  loss_mask_5: 0.1641  loss_dice_5: 0.1485  loss_ce_6: 0.1727  loss_mask_6: 0.1581  loss_dice_6: 0.1429  loss_ce_7: 0.1724  loss_mask_7: 0.1586  loss_dice_7: 0.1435  loss_ce_8: 0.1724  loss_mask_8: 0.1648  loss_dice_8: 0.142  time: 0.5652  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:09:18] d2.utils.events INFO:  eta: 3:12:14  iter: 10779  total_loss: 4.971  loss_ce: 0.09223  loss_mask: 0.15  loss_dice: 0.1754  loss_ce_0: 0.05169  loss_mask_0: 0.1434  loss_dice_0: 0.1757  loss_ce_1: 0.09218  loss_mask_1: 0.1536  loss_dice_1: 0.1742  loss_ce_2: 0.0922  loss_mask_2: 0.1486  loss_dice_2: 0.1729  loss_ce_3: 0.09212  loss_mask_3: 0.1535  loss_dice_3: 0.1745  loss_ce_4: 0.09216  loss_mask_4: 0.1431  loss_dice_4: 0.1683  loss_ce_5: 0.09212  loss_mask_5: 0.1468  loss_dice_5: 0.1795  loss_ce_6: 0.09209  loss_mask_6: 0.1472  loss_dice_6: 0.1708  loss_ce_7: 0.0922  loss_mask_7: 0.1457  loss_dice_7: 0.1701  loss_ce_8: 0.09202  loss_mask_8: 0.1586  loss_dice_8: 0.1724  time: 0.5652  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:09:29] d2.utils.events INFO:  eta: 3:11:50  iter: 10799  total_loss: 4.798  loss_ce: 0.1314  loss_mask: 0.2074  loss_dice: 0.1513  loss_ce_0: 0.06537  loss_mask_0: 0.2056  loss_dice_0: 0.1528  loss_ce_1: 0.1312  loss_mask_1: 0.2065  loss_dice_1: 0.1502  loss_ce_2: 0.1314  loss_mask_2: 0.2094  loss_dice_2: 0.1529  loss_ce_3: 0.1315  loss_mask_3: 0.2025  loss_dice_3: 0.1571  loss_ce_4: 0.1314  loss_mask_4: 0.2079  loss_dice_4: 0.161  loss_ce_5: 0.1314  loss_mask_5: 0.2117  loss_dice_5: 0.1565  loss_ce_6: 0.1315  loss_mask_6: 0.2038  loss_dice_6: 0.1594  loss_ce_7: 0.1314  loss_mask_7: 0.2046  loss_dice_7: 0.1554  loss_ce_8: 0.1315  loss_mask_8: 0.2043  loss_dice_8: 0.1532  time: 0.5652  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:09:41] d2.utils.events INFO:  eta: 3:11:22  iter: 10819  total_loss: 4.974  loss_ce: 0.0923  loss_mask: 0.2101  loss_dice: 0.1708  loss_ce_0: 0.05166  loss_mask_0: 0.214  loss_dice_0: 0.1692  loss_ce_1: 0.09218  loss_mask_1: 0.2163  loss_dice_1: 0.1631  loss_ce_2: 0.09227  loss_mask_2: 0.2161  loss_dice_2: 0.1773  loss_ce_3: 0.09216  loss_mask_3: 0.2164  loss_dice_3: 0.17  loss_ce_4: 0.09223  loss_mask_4: 0.2097  loss_dice_4: 0.165  loss_ce_5: 0.09216  loss_mask_5: 0.2248  loss_dice_5: 0.1711  loss_ce_6: 0.09216  loss_mask_6: 0.2147  loss_dice_6: 0.1587  loss_ce_7: 0.09223  loss_mask_7: 0.2131  loss_dice_7: 0.1662  loss_ce_8: 0.09209  loss_mask_8: 0.2113  loss_dice_8: 0.1656  time: 0.5653  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:09:52] d2.utils.events INFO:  eta: 3:10:50  iter: 10839  total_loss: 4.664  loss_ce: 0.09124  loss_mask: 0.1959  loss_dice: 0.1443  loss_ce_0: 0.05136  loss_mask_0: 0.195  loss_dice_0: 0.1384  loss_ce_1: 0.09111  loss_mask_1: 0.2002  loss_dice_1: 0.1401  loss_ce_2: 0.09129  loss_mask_2: 0.2001  loss_dice_2: 0.1392  loss_ce_3: 0.09089  loss_mask_3: 0.1972  loss_dice_3: 0.1382  loss_ce_4: 0.09103  loss_mask_4: 0.1986  loss_dice_4: 0.1429  loss_ce_5: 0.09101  loss_mask_5: 0.1934  loss_dice_5: 0.1407  loss_ce_6: 0.09103  loss_mask_6: 0.1998  loss_dice_6: 0.1376  loss_ce_7: 0.09114  loss_mask_7: 0.2016  loss_dice_7: 0.1416  loss_ce_8: 0.09168  loss_mask_8: 0.1958  loss_dice_8: 0.14  time: 0.5653  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:10:04] d2.utils.events INFO:  eta: 3:10:36  iter: 10859  total_loss: 4.965  loss_ce: 0.1316  loss_mask: 0.2054  loss_dice: 0.1463  loss_ce_0: 0.06541  loss_mask_0: 0.2071  loss_dice_0: 0.1446  loss_ce_1: 0.1315  loss_mask_1: 0.2089  loss_dice_1: 0.1449  loss_ce_2: 0.1316  loss_mask_2: 0.2132  loss_dice_2: 0.1453  loss_ce_3: 0.1317  loss_mask_3: 0.2035  loss_dice_3: 0.1435  loss_ce_4: 0.1319  loss_mask_4: 0.1997  loss_dice_4: 0.1439  loss_ce_5: 0.1316  loss_mask_5: 0.2004  loss_dice_5: 0.1474  loss_ce_6: 0.1316  loss_mask_6: 0.21  loss_dice_6: 0.1459  loss_ce_7: 0.1317  loss_mask_7: 0.2103  loss_dice_7: 0.1441  loss_ce_8: 0.1317  loss_mask_8: 0.2049  loss_dice_8: 0.1404  time: 0.5653  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:10:15] d2.utils.events INFO:  eta: 3:10:26  iter: 10879  total_loss: 4.565  loss_ce: 0.09156  loss_mask: 0.2048  loss_dice: 0.1432  loss_ce_0: 0.05137  loss_mask_0: 0.198  loss_dice_0: 0.142  loss_ce_1: 0.09156  loss_mask_1: 0.1942  loss_dice_1: 0.1439  loss_ce_2: 0.09144  loss_mask_2: 0.1937  loss_dice_2: 0.1431  loss_ce_3: 0.09131  loss_mask_3: 0.2058  loss_dice_3: 0.143  loss_ce_4: 0.09149  loss_mask_4: 0.2015  loss_dice_4: 0.1443  loss_ce_5: 0.09138  loss_mask_5: 0.1939  loss_dice_5: 0.1447  loss_ce_6: 0.09142  loss_mask_6: 0.2013  loss_dice_6: 0.14  loss_ce_7: 0.09153  loss_mask_7: 0.1991  loss_dice_7: 0.1363  loss_ce_8: 0.09135  loss_mask_8: 0.1957  loss_dice_8: 0.14  time: 0.5653  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:10:27] d2.utils.events INFO:  eta: 3:10:11  iter: 10899  total_loss: 4.553  loss_ce: 0.1315  loss_mask: 0.1664  loss_dice: 0.1343  loss_ce_0: 0.0654  loss_mask_0: 0.166  loss_dice_0: 0.1401  loss_ce_1: 0.131  loss_mask_1: 0.1645  loss_dice_1: 0.1396  loss_ce_2: 0.1314  loss_mask_2: 0.1736  loss_dice_2: 0.1388  loss_ce_3: 0.1314  loss_mask_3: 0.1685  loss_dice_3: 0.1352  loss_ce_4: 0.1314  loss_mask_4: 0.1734  loss_dice_4: 0.1341  loss_ce_5: 0.1314  loss_mask_5: 0.1685  loss_dice_5: 0.1322  loss_ce_6: 0.1314  loss_mask_6: 0.1772  loss_dice_6: 0.1356  loss_ce_7: 0.1314  loss_mask_7: 0.1631  loss_dice_7: 0.1326  loss_ce_8: 0.1315  loss_mask_8: 0.1697  loss_dice_8: 0.1376  time: 0.5653  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:10:38] d2.utils.events INFO:  eta: 3:09:42  iter: 10919  total_loss: 4.708  loss_ce: 0.09216  loss_mask: 0.2206  loss_dice: 0.1472  loss_ce_0: 0.05151  loss_mask_0: 0.2128  loss_dice_0: 0.1436  loss_ce_1: 0.0923  loss_mask_1: 0.2166  loss_dice_1: 0.1492  loss_ce_2: 0.09228  loss_mask_2: 0.2165  loss_dice_2: 0.147  loss_ce_3: 0.09223  loss_mask_3: 0.216  loss_dice_3: 0.1534  loss_ce_4: 0.09228  loss_mask_4: 0.2223  loss_dice_4: 0.1475  loss_ce_5: 0.09223  loss_mask_5: 0.223  loss_dice_5: 0.1467  loss_ce_6: 0.09216  loss_mask_6: 0.2166  loss_dice_6: 0.1489  loss_ce_7: 0.09219  loss_mask_7: 0.2182  loss_dice_7: 0.1489  loss_ce_8: 0.09195  loss_mask_8: 0.2118  loss_dice_8: 0.1496  time: 0.5653  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:10:50] d2.utils.events INFO:  eta: 3:09:24  iter: 10939  total_loss: 4.868  loss_ce: 0.09163  loss_mask: 0.2218  loss_dice: 0.1569  loss_ce_0: 0.05133  loss_mask_0: 0.2225  loss_dice_0: 0.1587  loss_ce_1: 0.09167  loss_mask_1: 0.2197  loss_dice_1: 0.1624  loss_ce_2: 0.09158  loss_mask_2: 0.2182  loss_dice_2: 0.16  loss_ce_3: 0.09149  loss_mask_3: 0.2316  loss_dice_3: 0.1627  loss_ce_4: 0.09159  loss_mask_4: 0.2305  loss_dice_4: 0.162  loss_ce_5: 0.09155  loss_mask_5: 0.2219  loss_dice_5: 0.1616  loss_ce_6: 0.09149  loss_mask_6: 0.2158  loss_dice_6: 0.1604  loss_ce_7: 0.0916  loss_mask_7: 0.22  loss_dice_7: 0.1572  loss_ce_8: 0.09145  loss_mask_8: 0.2234  loss_dice_8: 0.1596  time: 0.5654  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:11:01] d2.utils.events INFO:  eta: 3:09:04  iter: 10959  total_loss: 4.663  loss_ce: 0.1722  loss_mask: 0.1299  loss_dice: 0.1674  loss_ce_0: 0.0797  loss_mask_0: 0.1303  loss_dice_0: 0.1629  loss_ce_1: 0.1721  loss_mask_1: 0.1361  loss_dice_1: 0.1721  loss_ce_2: 0.1725  loss_mask_2: 0.1307  loss_dice_2: 0.1654  loss_ce_3: 0.1726  loss_mask_3: 0.128  loss_dice_3: 0.1648  loss_ce_4: 0.1724  loss_mask_4: 0.1402  loss_dice_4: 0.1748  loss_ce_5: 0.1725  loss_mask_5: 0.1347  loss_dice_5: 0.1572  loss_ce_6: 0.1725  loss_mask_6: 0.1408  loss_dice_6: 0.1757  loss_ce_7: 0.1723  loss_mask_7: 0.1322  loss_dice_7: 0.1712  loss_ce_8: 0.1726  loss_mask_8: 0.1328  loss_dice_8: 0.1672  time: 0.5654  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:11:13] d2.utils.events INFO:  eta: 3:08:52  iter: 10979  total_loss: 4.716  loss_ce: 0.1686  loss_mask: 0.1787  loss_dice: 0.1359  loss_ce_0: 0.07879  loss_mask_0: 0.1916  loss_dice_0: 0.1364  loss_ce_1: 0.1681  loss_mask_1: 0.1861  loss_dice_1: 0.1384  loss_ce_2: 0.1681  loss_mask_2: 0.1852  loss_dice_2: 0.1342  loss_ce_3: 0.1681  loss_mask_3: 0.1934  loss_dice_3: 0.1327  loss_ce_4: 0.1682  loss_mask_4: 0.1871  loss_dice_4: 0.1312  loss_ce_5: 0.168  loss_mask_5: 0.185  loss_dice_5: 0.1344  loss_ce_6: 0.1683  loss_mask_6: 0.1884  loss_dice_6: 0.1334  loss_ce_7: 0.1684  loss_mask_7: 0.1827  loss_dice_7: 0.1358  loss_ce_8: 0.1688  loss_mask_8: 0.184  loss_dice_8: 0.1351  time: 0.5654  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:11:24] d2.utils.events INFO:  eta: 3:08:50  iter: 10999  total_loss: 4.534  loss_ce: 0.09358  loss_mask: 0.1874  loss_dice: 0.1519  loss_ce_0: 0.0518  loss_mask_0: 0.1876  loss_dice_0: 0.1553  loss_ce_1: 0.09405  loss_mask_1: 0.1948  loss_dice_1: 0.1563  loss_ce_2: 0.09398  loss_mask_2: 0.1927  loss_dice_2: 0.1632  loss_ce_3: 0.09394  loss_mask_3: 0.1899  loss_dice_3: 0.151  loss_ce_4: 0.09401  loss_mask_4: 0.1883  loss_dice_4: 0.1544  loss_ce_5: 0.09408  loss_mask_5: 0.1847  loss_dice_5: 0.1499  loss_ce_6: 0.0938  loss_mask_6: 0.1922  loss_dice_6: 0.1567  loss_ce_7: 0.0938  loss_mask_7: 0.1874  loss_dice_7: 0.1567  loss_ce_8: 0.09351  loss_mask_8: 0.1942  loss_dice_8: 0.1591  time: 0.5654  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:11:36] d2.utils.events INFO:  eta: 3:07:59  iter: 11019  total_loss: 4.881  loss_ce: 0.131  loss_mask: 0.2169  loss_dice: 0.1439  loss_ce_0: 0.0653  loss_mask_0: 0.222  loss_dice_0: 0.1451  loss_ce_1: 0.1306  loss_mask_1: 0.2084  loss_dice_1: 0.1477  loss_ce_2: 0.1309  loss_mask_2: 0.213  loss_dice_2: 0.1414  loss_ce_3: 0.1309  loss_mask_3: 0.2223  loss_dice_3: 0.1405  loss_ce_4: 0.1308  loss_mask_4: 0.226  loss_dice_4: 0.1391  loss_ce_5: 0.1308  loss_mask_5: 0.2197  loss_dice_5: 0.1402  loss_ce_6: 0.1309  loss_mask_6: 0.2075  loss_dice_6: 0.1417  loss_ce_7: 0.1309  loss_mask_7: 0.2203  loss_dice_7: 0.1423  loss_ce_8: 0.131  loss_mask_8: 0.2086  loss_dice_8: 0.1391  time: 0.5655  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:11:47] d2.utils.events INFO:  eta: 3:07:27  iter: 11039  total_loss: 4.67  loss_ce: 0.09473  loss_mask: 0.1546  loss_dice: 0.1576  loss_ce_0: 0.05204  loss_mask_0: 0.1646  loss_dice_0: 0.1533  loss_ce_1: 0.09561  loss_mask_1: 0.158  loss_dice_1: 0.1557  loss_ce_2: 0.09538  loss_mask_2: 0.1735  loss_dice_2: 0.1524  loss_ce_3: 0.09538  loss_mask_3: 0.1756  loss_dice_3: 0.157  loss_ce_4: 0.09545  loss_mask_4: 0.1681  loss_dice_4: 0.1558  loss_ce_5: 0.09552  loss_mask_5: 0.1703  loss_dice_5: 0.1589  loss_ce_6: 0.0952  loss_mask_6: 0.1788  loss_dice_6: 0.1583  loss_ce_7: 0.09517  loss_mask_7: 0.17  loss_dice_7: 0.1561  loss_ce_8: 0.0948  loss_mask_8: 0.1602  loss_dice_8: 0.1595  time: 0.5655  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:11:59] d2.utils.events INFO:  eta: 3:06:57  iter: 11059  total_loss: 4.791  loss_ce: 0.0943  loss_mask: 0.1427  loss_dice: 0.1853  loss_ce_0: 0.05191  loss_mask_0: 0.1413  loss_dice_0: 0.1821  loss_ce_1: 0.09488  loss_mask_1: 0.1386  loss_dice_1: 0.1776  loss_ce_2: 0.09475  loss_mask_2: 0.1423  loss_dice_2: 0.1818  loss_ce_3: 0.09473  loss_mask_3: 0.1459  loss_dice_3: 0.1835  loss_ce_4: 0.09493  loss_mask_4: 0.1373  loss_dice_4: 0.1857  loss_ce_5: 0.09484  loss_mask_5: 0.1424  loss_dice_5: 0.1778  loss_ce_6: 0.09462  loss_mask_6: 0.1359  loss_dice_6: 0.1763  loss_ce_7: 0.09455  loss_mask_7: 0.1408  loss_dice_7: 0.1766  loss_ce_8: 0.09434  loss_mask_8: 0.1295  loss_dice_8: 0.1733  time: 0.5655  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:12:10] d2.utils.events INFO:  eta: 3:06:14  iter: 11079  total_loss: 4.698  loss_ce: 0.09365  loss_mask: 0.1944  loss_dice: 0.1542  loss_ce_0: 0.05175  loss_mask_0: 0.1936  loss_dice_0: 0.1561  loss_ce_1: 0.09391  loss_mask_1: 0.1878  loss_dice_1: 0.1532  loss_ce_2: 0.09383  loss_mask_2: 0.1934  loss_dice_2: 0.1521  loss_ce_3: 0.09383  loss_mask_3: 0.1994  loss_dice_3: 0.1517  loss_ce_4: 0.09397  loss_mask_4: 0.195  loss_dice_4: 0.1567  loss_ce_5: 0.09397  loss_mask_5: 0.193  loss_dice_5: 0.1579  loss_ce_6: 0.09372  loss_mask_6: 0.2022  loss_dice_6: 0.157  loss_ce_7: 0.0938  loss_mask_7: 0.1928  loss_dice_7: 0.1588  loss_ce_8: 0.09358  loss_mask_8: 0.1957  loss_dice_8: 0.1576  time: 0.5655  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:12:22] d2.utils.events INFO:  eta: 3:05:41  iter: 11099  total_loss: 4.685  loss_ce: 0.0923  loss_mask: 0.1885  loss_dice: 0.1363  loss_ce_0: 0.05141  loss_mask_0: 0.1877  loss_dice_0: 0.139  loss_ce_1: 0.09213  loss_mask_1: 0.1924  loss_dice_1: 0.1443  loss_ce_2: 0.09216  loss_mask_2: 0.1934  loss_dice_2: 0.139  loss_ce_3: 0.09216  loss_mask_3: 0.195  loss_dice_3: 0.1379  loss_ce_4: 0.09223  loss_mask_4: 0.195  loss_dice_4: 0.1397  loss_ce_5: 0.09226  loss_mask_5: 0.195  loss_dice_5: 0.1366  loss_ce_6: 0.09216  loss_mask_6: 0.1838  loss_dice_6: 0.1371  loss_ce_7: 0.09226  loss_mask_7: 0.1812  loss_dice_7: 0.1336  loss_ce_8: 0.09219  loss_mask_8: 0.1876  loss_dice_8: 0.136  time: 0.5655  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:12:33] d2.utils.events INFO:  eta: 3:05:03  iter: 11119  total_loss: 4.622  loss_ce: 0.09142  loss_mask: 0.1483  loss_dice: 0.176  loss_ce_0: 0.05112  loss_mask_0: 0.1472  loss_dice_0: 0.1746  loss_ce_1: 0.09128  loss_mask_1: 0.1435  loss_dice_1: 0.1662  loss_ce_2: 0.09117  loss_mask_2: 0.1441  loss_dice_2: 0.165  loss_ce_3: 0.09106  loss_mask_3: 0.1475  loss_dice_3: 0.1684  loss_ce_4: 0.09131  loss_mask_4: 0.1472  loss_dice_4: 0.1731  loss_ce_5: 0.09124  loss_mask_5: 0.1461  loss_dice_5: 0.1763  loss_ce_6: 0.09124  loss_mask_6: 0.1444  loss_dice_6: 0.17  loss_ce_7: 0.09135  loss_mask_7: 0.1397  loss_dice_7: 0.1655  loss_ce_8: 0.09131  loss_mask_8: 0.1475  loss_dice_8: 0.1801  time: 0.5655  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:12:45] d2.utils.events INFO:  eta: 3:04:16  iter: 11139  total_loss: 4.603  loss_ce: 0.09202  loss_mask: 0.1974  loss_dice: 0.1562  loss_ce_0: 0.05131  loss_mask_0: 0.1905  loss_dice_0: 0.1514  loss_ce_1: 0.0923  loss_mask_1: 0.185  loss_dice_1: 0.1536  loss_ce_2: 0.09203  loss_mask_2: 0.1888  loss_dice_2: 0.1559  loss_ce_3: 0.09201  loss_mask_3: 0.1925  loss_dice_3: 0.1538  loss_ce_4: 0.09219  loss_mask_4: 0.1888  loss_dice_4: 0.1548  loss_ce_5: 0.09212  loss_mask_5: 0.1842  loss_dice_5: 0.1579  loss_ce_6: 0.09205  loss_mask_6: 0.1802  loss_dice_6: 0.1575  loss_ce_7: 0.09209  loss_mask_7: 0.1983  loss_dice_7: 0.1535  loss_ce_8: 0.09191  loss_mask_8: 0.1962  loss_dice_8: 0.1514  time: 0.5655  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:12:56] d2.utils.events INFO:  eta: 3:03:31  iter: 11159  total_loss: 4.796  loss_ce: 0.09226  loss_mask: 0.2004  loss_dice: 0.1684  loss_ce_0: 0.05133  loss_mask_0: 0.2021  loss_dice_0: 0.1592  loss_ce_1: 0.09227  loss_mask_1: 0.2106  loss_dice_1: 0.163  loss_ce_2: 0.09223  loss_mask_2: 0.2061  loss_dice_2: 0.159  loss_ce_3: 0.09212  loss_mask_3: 0.204  loss_dice_3: 0.1625  loss_ce_4: 0.09233  loss_mask_4: 0.2005  loss_dice_4: 0.164  loss_ce_5: 0.09229  loss_mask_5: 0.2032  loss_dice_5: 0.164  loss_ce_6: 0.09223  loss_mask_6: 0.2131  loss_dice_6: 0.1643  loss_ce_7: 0.09223  loss_mask_7: 0.2129  loss_dice_7: 0.1674  loss_ce_8: 0.09212  loss_mask_8: 0.2066  loss_dice_8: 0.1607  time: 0.5655  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:13:08] d2.utils.events INFO:  eta: 3:03:15  iter: 11179  total_loss: 4.898  loss_ce: 0.09202  loss_mask: 0.2066  loss_dice: 0.153  loss_ce_0: 0.05128  loss_mask_0: 0.206  loss_dice_0: 0.1483  loss_ce_1: 0.09209  loss_mask_1: 0.2065  loss_dice_1: 0.1496  loss_ce_2: 0.09194  loss_mask_2: 0.1987  loss_dice_2: 0.1472  loss_ce_3: 0.09194  loss_mask_3: 0.2064  loss_dice_3: 0.1505  loss_ce_4: 0.09208  loss_mask_4: 0.1904  loss_dice_4: 0.1496  loss_ce_5: 0.09201  loss_mask_5: 0.2031  loss_dice_5: 0.1467  loss_ce_6: 0.09194  loss_mask_6: 0.2055  loss_dice_6: 0.1485  loss_ce_7: 0.09202  loss_mask_7: 0.2054  loss_dice_7: 0.1536  loss_ce_8: 0.09187  loss_mask_8: 0.201  loss_dice_8: 0.1468  time: 0.5656  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:13:21] d2.utils.events INFO:  eta: 3:03:01  iter: 11199  total_loss: 4.645  loss_ce: 0.1715  loss_mask: 0.1551  loss_dice: 0.157  loss_ce_0: 0.07964  loss_mask_0: 0.1418  loss_dice_0: 0.1563  loss_ce_1: 0.1715  loss_mask_1: 0.1511  loss_dice_1: 0.1619  loss_ce_2: 0.1714  loss_mask_2: 0.1411  loss_dice_2: 0.1597  loss_ce_3: 0.1718  loss_mask_3: 0.1422  loss_dice_3: 0.1604  loss_ce_4: 0.1714  loss_mask_4: 0.1441  loss_dice_4: 0.1515  loss_ce_5: 0.1716  loss_mask_5: 0.1455  loss_dice_5: 0.1649  loss_ce_6: 0.1717  loss_mask_6: 0.1438  loss_dice_6: 0.1544  loss_ce_7: 0.1714  loss_mask_7: 0.1379  loss_dice_7: 0.1571  loss_ce_8: 0.1708  loss_mask_8: 0.1452  loss_dice_8: 0.1556  time: 0.5657  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:13:34] d2.utils.events INFO:  eta: 3:02:46  iter: 11219  total_loss: 4.815  loss_ce: 0.1701  loss_mask: 0.176  loss_dice: 0.158  loss_ce_0: 0.07929  loss_mask_0: 0.169  loss_dice_0: 0.1549  loss_ce_1: 0.1693  loss_mask_1: 0.1663  loss_dice_1: 0.1545  loss_ce_2: 0.1698  loss_mask_2: 0.1676  loss_dice_2: 0.1508  loss_ce_3: 0.1698  loss_mask_3: 0.1666  loss_dice_3: 0.1501  loss_ce_4: 0.1697  loss_mask_4: 0.1802  loss_dice_4: 0.1573  loss_ce_5: 0.1697  loss_mask_5: 0.1717  loss_dice_5: 0.15  loss_ce_6: 0.1698  loss_mask_6: 0.1644  loss_dice_6: 0.151  loss_ce_7: 0.1698  loss_mask_7: 0.1688  loss_dice_7: 0.1496  loss_ce_8: 0.1698  loss_mask_8: 0.1793  loss_dice_8: 0.1611  time: 0.5658  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:13:46] d2.utils.events INFO:  eta: 3:02:31  iter: 11239  total_loss: 4.536  loss_ce: 0.09304  loss_mask: 0.1856  loss_dice: 0.1483  loss_ce_0: 0.0515  loss_mask_0: 0.1949  loss_dice_0: 0.1451  loss_ce_1: 0.09358  loss_mask_1: 0.1897  loss_dice_1: 0.1451  loss_ce_2: 0.09335  loss_mask_2: 0.1919  loss_dice_2: 0.1448  loss_ce_3: 0.09329  loss_mask_3: 0.1912  loss_dice_3: 0.1424  loss_ce_4: 0.0935  loss_mask_4: 0.2018  loss_dice_4: 0.1488  loss_ce_5: 0.0934  loss_mask_5: 0.1978  loss_dice_5: 0.1413  loss_ce_6: 0.09322  loss_mask_6: 0.1888  loss_dice_6: 0.1404  loss_ce_7: 0.09329  loss_mask_7: 0.2008  loss_dice_7: 0.1432  loss_ce_8: 0.09304  loss_mask_8: 0.1884  loss_dice_8: 0.1422  time: 0.5660  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:13:59] d2.utils.events INFO:  eta: 3:02:24  iter: 11259  total_loss: 4.756  loss_ce: 0.09262  loss_mask: 0.1965  loss_dice: 0.1517  loss_ce_0: 0.05136  loss_mask_0: 0.1933  loss_dice_0: 0.1508  loss_ce_1: 0.09266  loss_mask_1: 0.1902  loss_dice_1: 0.1477  loss_ce_2: 0.09272  loss_mask_2: 0.1868  loss_dice_2: 0.146  loss_ce_3: 0.09272  loss_mask_3: 0.1983  loss_dice_3: 0.1486  loss_ce_4: 0.09299  loss_mask_4: 0.191  loss_dice_4: 0.1472  loss_ce_5: 0.0929  loss_mask_5: 0.1877  loss_dice_5: 0.1436  loss_ce_6: 0.09265  loss_mask_6: 0.2038  loss_dice_6: 0.1413  loss_ce_7: 0.09277  loss_mask_7: 0.1954  loss_dice_7: 0.1481  loss_ce_8: 0.09255  loss_mask_8: 0.1948  loss_dice_8: 0.1436  time: 0.5661  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:14:12] d2.utils.events INFO:  eta: 3:02:13  iter: 11279  total_loss: 5.035  loss_ce: 0.171  loss_mask: 0.2064  loss_dice: 0.1517  loss_ce_0: 0.07952  loss_mask_0: 0.21  loss_dice_0: 0.1595  loss_ce_1: 0.1709  loss_mask_1: 0.2069  loss_dice_1: 0.1539  loss_ce_2: 0.1711  loss_mask_2: 0.2016  loss_dice_2: 0.1532  loss_ce_3: 0.1711  loss_mask_3: 0.2009  loss_dice_3: 0.1484  loss_ce_4: 0.1709  loss_mask_4: 0.2051  loss_dice_4: 0.1558  loss_ce_5: 0.171  loss_mask_5: 0.201  loss_dice_5: 0.1575  loss_ce_6: 0.171  loss_mask_6: 0.2093  loss_dice_6: 0.156  loss_ce_7: 0.1709  loss_mask_7: 0.2132  loss_dice_7: 0.1591  loss_ce_8: 0.1712  loss_mask_8: 0.2099  loss_dice_8: 0.1604  time: 0.5662  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:14:24] d2.utils.events INFO:  eta: 3:02:01  iter: 11299  total_loss: 4.512  loss_ce: 0.09304  loss_mask: 0.1809  loss_dice: 0.1366  loss_ce_0: 0.05142  loss_mask_0: 0.1768  loss_dice_0: 0.1405  loss_ce_1: 0.09355  loss_mask_1: 0.1724  loss_dice_1: 0.137  loss_ce_2: 0.0932  loss_mask_2: 0.1802  loss_dice_2: 0.1343  loss_ce_3: 0.09325  loss_mask_3: 0.1855  loss_dice_3: 0.1356  loss_ce_4: 0.09336  loss_mask_4: 0.1777  loss_dice_4: 0.1321  loss_ce_5: 0.09336  loss_mask_5: 0.1804  loss_dice_5: 0.1366  loss_ce_6: 0.09322  loss_mask_6: 0.1832  loss_dice_6: 0.1343  loss_ce_7: 0.09319  loss_mask_7: 0.1704  loss_dice_7: 0.1319  loss_ce_8: 0.09297  loss_mask_8: 0.1778  loss_dice_8: 0.1356  time: 0.5663  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:14:37] d2.utils.events INFO:  eta: 3:01:50  iter: 11319  total_loss: 4.486  loss_ce: 0.1682  loss_mask: 0.1865  loss_dice: 0.1273  loss_ce_0: 0.07893  loss_mask_0: 0.186  loss_dice_0: 0.1289  loss_ce_1: 0.1672  loss_mask_1: 0.1918  loss_dice_1: 0.1267  loss_ce_2: 0.1678  loss_mask_2: 0.1847  loss_dice_2: 0.125  loss_ce_3: 0.1678  loss_mask_3: 0.1796  loss_dice_3: 0.1261  loss_ce_4: 0.1675  loss_mask_4: 0.1931  loss_dice_4: 0.1264  loss_ce_5: 0.1675  loss_mask_5: 0.1822  loss_dice_5: 0.1212  loss_ce_6: 0.1679  loss_mask_6: 0.1877  loss_dice_6: 0.1324  loss_ce_7: 0.1678  loss_mask_7: 0.192  loss_dice_7: 0.1293  loss_ce_8: 0.1683  loss_mask_8: 0.1858  loss_dice_8: 0.1249  time: 0.5664  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:14:49] d2.utils.events INFO:  eta: 3:01:43  iter: 11339  total_loss: 4.429  loss_ce: 0.09415  loss_mask: 0.1716  loss_dice: 0.1396  loss_ce_0: 0.05167  loss_mask_0: 0.1735  loss_dice_0: 0.1449  loss_ce_1: 0.09482  loss_mask_1: 0.1876  loss_dice_1: 0.1486  loss_ce_2: 0.09459  loss_mask_2: 0.1816  loss_dice_2: 0.1404  loss_ce_3: 0.09458  loss_mask_3: 0.1683  loss_dice_3: 0.1437  loss_ce_4: 0.09469  loss_mask_4: 0.1783  loss_dice_4: 0.1406  loss_ce_5: 0.0948  loss_mask_5: 0.1659  loss_dice_5: 0.1495  loss_ce_6: 0.09451  loss_mask_6: 0.1738  loss_dice_6: 0.1418  loss_ce_7: 0.09444  loss_mask_7: 0.1767  loss_dice_7: 0.1452  loss_ce_8: 0.09412  loss_mask_8: 0.1753  loss_dice_8: 0.1489  time: 0.5665  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:15:02] d2.utils.events INFO:  eta: 3:01:43  iter: 11359  total_loss: 4.707  loss_ce: 0.1308  loss_mask: 0.1984  loss_dice: 0.1212  loss_ce_0: 0.0652  loss_mask_0: 0.1951  loss_dice_0: 0.1248  loss_ce_1: 0.1306  loss_mask_1: 0.2007  loss_dice_1: 0.1262  loss_ce_2: 0.1309  loss_mask_2: 0.2032  loss_dice_2: 0.1248  loss_ce_3: 0.1307  loss_mask_3: 0.2034  loss_dice_3: 0.1256  loss_ce_4: 0.1307  loss_mask_4: 0.211  loss_dice_4: 0.1254  loss_ce_5: 0.1307  loss_mask_5: 0.1999  loss_dice_5: 0.122  loss_ce_6: 0.1308  loss_mask_6: 0.2  loss_dice_6: 0.1266  loss_ce_7: 0.1307  loss_mask_7: 0.1975  loss_dice_7: 0.1234  loss_ce_8: 0.1314  loss_mask_8: 0.1947  loss_dice_8: 0.1216  time: 0.5666  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:15:15] d2.utils.events INFO:  eta: 3:01:55  iter: 11379  total_loss: 5.009  loss_ce: 0.09487  loss_mask: 0.2201  loss_dice: 0.1421  loss_ce_0: 0.05182  loss_mask_0: 0.2115  loss_dice_0: 0.1506  loss_ce_1: 0.09555  loss_mask_1: 0.2199  loss_dice_1: 0.1517  loss_ce_2: 0.09533  loss_mask_2: 0.2123  loss_dice_2: 0.1512  loss_ce_3: 0.09538  loss_mask_3: 0.2154  loss_dice_3: 0.1547  loss_ce_4: 0.09545  loss_mask_4: 0.2092  loss_dice_4: 0.1506  loss_ce_5: 0.09552  loss_mask_5: 0.223  loss_dice_5: 0.1424  loss_ce_6: 0.09524  loss_mask_6: 0.2123  loss_dice_6: 0.1501  loss_ce_7: 0.09513  loss_mask_7: 0.2134  loss_dice_7: 0.1458  loss_ce_8: 0.09487  loss_mask_8: 0.2127  loss_dice_8: 0.1539  time: 0.5668  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:15:27] d2.utils.events INFO:  eta: 3:01:54  iter: 11399  total_loss: 4.436  loss_ce: 0.1307  loss_mask: 0.1585  loss_dice: 0.1525  loss_ce_0: 0.06515  loss_mask_0: 0.1678  loss_dice_0: 0.1541  loss_ce_1: 0.1304  loss_mask_1: 0.1608  loss_dice_1: 0.1633  loss_ce_2: 0.1305  loss_mask_2: 0.1683  loss_dice_2: 0.1535  loss_ce_3: 0.1305  loss_mask_3: 0.159  loss_dice_3: 0.1545  loss_ce_4: 0.1304  loss_mask_4: 0.1685  loss_dice_4: 0.1565  loss_ce_5: 0.1305  loss_mask_5: 0.1683  loss_dice_5: 0.1597  loss_ce_6: 0.1306  loss_mask_6: 0.1644  loss_dice_6: 0.1572  loss_ce_7: 0.1305  loss_mask_7: 0.1597  loss_dice_7: 0.1574  loss_ce_8: 0.1307  loss_mask_8: 0.1656  loss_dice_8: 0.1549  time: 0.5669  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 13:15:40] d2.utils.events INFO:  eta: 3:02:06  iter: 11419  total_loss: 4.711  loss_ce: 0.1302  loss_mask: 0.1976  loss_dice: 0.1374  loss_ce_0: 0.06507  loss_mask_0: 0.2063  loss_dice_0: 0.1323  loss_ce_1: 0.1298  loss_mask_1: 0.2054  loss_dice_1: 0.1362  loss_ce_2: 0.13  loss_mask_2: 0.2071  loss_dice_2: 0.1353  loss_ce_3: 0.13  loss_mask_3: 0.2008  loss_dice_3: 0.1353  loss_ce_4: 0.13  loss_mask_4: 0.2075  loss_dice_4: 0.1332  loss_ce_5: 0.13  loss_mask_5: 0.1974  loss_dice_5: 0.1342  loss_ce_6: 0.13  loss_mask_6: 0.2099  loss_dice_6: 0.1324  loss_ce_7: 0.1301  loss_mask_7: 0.2027  loss_dice_7: 0.135  loss_ce_8: 0.1302  loss_mask_8: 0.2169  loss_dice_8: 0.1345  time: 0.5670  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:15:53] d2.utils.events INFO:  eta: 3:02:12  iter: 11439  total_loss: 4.617  loss_ce: 0.1622  loss_mask: 0.2175  loss_dice: 0.1218  loss_ce_0: 0.07752  loss_mask_0: 0.218  loss_dice_0: 0.1287  loss_ce_1: 0.1603  loss_mask_1: 0.216  loss_dice_1: 0.1235  loss_ce_2: 0.161  loss_mask_2: 0.2246  loss_dice_2: 0.1274  loss_ce_3: 0.1609  loss_mask_3: 0.2202  loss_dice_3: 0.1181  loss_ce_4: 0.1606  loss_mask_4: 0.2209  loss_dice_4: 0.1275  loss_ce_5: 0.1604  loss_mask_5: 0.2113  loss_dice_5: 0.1244  loss_ce_6: 0.1612  loss_mask_6: 0.222  loss_dice_6: 0.1254  loss_ce_7: 0.1616  loss_mask_7: 0.2244  loss_dice_7: 0.1261  loss_ce_8: 0.162  loss_mask_8: 0.2208  loss_dice_8: 0.1248  time: 0.5671  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:16:06] d2.utils.events INFO:  eta: 3:02:25  iter: 11459  total_loss: 4.714  loss_ce: 0.09978  loss_mask: 0.1993  loss_dice: 0.1251  loss_ce_0: 0.05305  loss_mask_0: 0.2008  loss_dice_0: 0.1222  loss_ce_1: 0.1016  loss_mask_1: 0.2059  loss_dice_1: 0.1285  loss_ce_2: 0.1011  loss_mask_2: 0.1894  loss_dice_2: 0.1267  loss_ce_3: 0.1011  loss_mask_3: 0.1998  loss_dice_3: 0.1361  loss_ce_4: 0.1013  loss_mask_4: 0.1989  loss_dice_4: 0.1372  loss_ce_5: 0.1017  loss_mask_5: 0.1937  loss_dice_5: 0.1336  loss_ce_6: 0.101  loss_mask_6: 0.1976  loss_dice_6: 0.1331  loss_ce_7: 0.1005  loss_mask_7: 0.2013  loss_dice_7: 0.1312  loss_ce_8: 0.1  loss_mask_8: 0.1993  loss_dice_8: 0.1324  time: 0.5672  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:16:18] d2.utils.events INFO:  eta: 3:02:36  iter: 11479  total_loss: 4.487  loss_ce: 0.1588  loss_mask: 0.1751  loss_dice: 0.1373  loss_ce_0: 0.07658  loss_mask_0: 0.1703  loss_dice_0: 0.1399  loss_ce_1: 0.1564  loss_mask_1: 0.1816  loss_dice_1: 0.14  loss_ce_2: 0.1572  loss_mask_2: 0.1816  loss_dice_2: 0.1339  loss_ce_3: 0.157  loss_mask_3: 0.1835  loss_dice_3: 0.1385  loss_ce_4: 0.1566  loss_mask_4: 0.1802  loss_dice_4: 0.1395  loss_ce_5: 0.1562  loss_mask_5: 0.1723  loss_dice_5: 0.1358  loss_ce_6: 0.1573  loss_mask_6: 0.182  loss_dice_6: 0.1381  loss_ce_7: 0.1578  loss_mask_7: 0.1828  loss_dice_7: 0.132  loss_ce_8: 0.1584  loss_mask_8: 0.1804  loss_dice_8: 0.1359  time: 0.5673  data_time: 0.0023  lr: 1e-05  max_mem: 2811M
[07/11 13:16:31] d2.utils.events INFO:  eta: 3:02:41  iter: 11499  total_loss: 4.119  loss_ce: 0.1277  loss_mask: 0.1553  loss_dice: 0.1378  loss_ce_0: 0.06485  loss_mask_0: 0.1474  loss_dice_0: 0.1327  loss_ce_1: 0.1291  loss_mask_1: 0.1531  loss_dice_1: 0.136  loss_ce_2: 0.129  loss_mask_2: 0.1634  loss_dice_2: 0.1275  loss_ce_3: 0.1285  loss_mask_3: 0.1569  loss_dice_3: 0.1286  loss_ce_4: 0.1292  loss_mask_4: 0.1575  loss_dice_4: 0.1355  loss_ce_5: 0.1291  loss_mask_5: 0.1583  loss_dice_5: 0.1332  loss_ce_6: 0.1273  loss_mask_6: 0.1627  loss_dice_6: 0.1307  loss_ce_7: 0.1292  loss_mask_7: 0.1592  loss_dice_7: 0.1328  loss_ce_8: 0.1281  loss_mask_8: 0.1533  loss_dice_8: 0.128  time: 0.5674  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 13:16:43] d2.utils.events INFO:  eta: 3:02:38  iter: 11519  total_loss: 4.63  loss_ce: 0.1566  loss_mask: 0.1727  loss_dice: 0.1367  loss_ce_0: 0.07613  loss_mask_0: 0.1707  loss_dice_0: 0.1402  loss_ce_1: 0.1549  loss_mask_1: 0.1634  loss_dice_1: 0.1352  loss_ce_2: 0.1553  loss_mask_2: 0.1718  loss_dice_2: 0.1428  loss_ce_3: 0.1552  loss_mask_3: 0.1674  loss_dice_3: 0.1384  loss_ce_4: 0.1549  loss_mask_4: 0.1703  loss_dice_4: 0.1357  loss_ce_5: 0.1545  loss_mask_5: 0.1725  loss_dice_5: 0.1341  loss_ce_6: 0.1551  loss_mask_6: 0.1603  loss_dice_6: 0.1352  loss_ce_7: 0.1559  loss_mask_7: 0.1719  loss_dice_7: 0.1343  loss_ce_8: 0.1561  loss_mask_8: 0.1686  loss_dice_8: 0.1378  time: 0.5675  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:16:56] d2.utils.events INFO:  eta: 3:02:45  iter: 11539  total_loss: 4.723  loss_ce: 0.1042  loss_mask: 0.2044  loss_dice: 0.1106  loss_ce_0: 0.05405  loss_mask_0: 0.2006  loss_dice_0: 0.1145  loss_ce_1: 0.1065  loss_mask_1: 0.2004  loss_dice_1: 0.1151  loss_ce_2: 0.1059  loss_mask_2: 0.2082  loss_dice_2: 0.1096  loss_ce_3: 0.1059  loss_mask_3: 0.1994  loss_dice_3: 0.1114  loss_ce_4: 0.1061  loss_mask_4: 0.1985  loss_dice_4: 0.1119  loss_ce_5: 0.1065  loss_mask_5: 0.2064  loss_dice_5: 0.1135  loss_ce_6: 0.1057  loss_mask_6: 0.2078  loss_dice_6: 0.1131  loss_ce_7: 0.105  loss_mask_7: 0.2133  loss_dice_7: 0.1128  loss_ce_8: 0.1054  loss_mask_8: 0.2053  loss_dice_8: 0.111  time: 0.5677  data_time: 0.0027  lr: 1e-05  max_mem: 2811M
[07/11 13:17:09] d2.utils.events INFO:  eta: 3:02:47  iter: 11559  total_loss: 4.297  loss_ce: 0.1044  loss_mask: 0.1935  loss_dice: 0.1283  loss_ce_0: 0.05413  loss_mask_0: 0.1878  loss_dice_0: 0.1264  loss_ce_1: 0.1065  loss_mask_1: 0.1941  loss_dice_1: 0.1298  loss_ce_2: 0.1058  loss_mask_2: 0.1886  loss_dice_2: 0.1311  loss_ce_3: 0.1059  loss_mask_3: 0.1857  loss_dice_3: 0.1272  loss_ce_4: 0.1061  loss_mask_4: 0.1933  loss_dice_4: 0.1201  loss_ce_5: 0.1065  loss_mask_5: 0.1882  loss_dice_5: 0.1278  loss_ce_6: 0.1058  loss_mask_6: 0.1857  loss_dice_6: 0.126  loss_ce_7: 0.1052  loss_mask_7: 0.1859  loss_dice_7: 0.1256  loss_ce_8: 0.1049  loss_mask_8: 0.1947  loss_dice_8: 0.1251  time: 0.5678  data_time: 0.0023  lr: 1e-05  max_mem: 2811M
[07/11 13:17:21] d2.utils.events INFO:  eta: 3:02:49  iter: 11579  total_loss: 4.602  loss_ce: 0.1036  loss_mask: 0.2249  loss_dice: 0.1332  loss_ce_0: 0.05393  loss_mask_0: 0.2305  loss_dice_0: 0.1334  loss_ce_1: 0.1047  loss_mask_1: 0.2326  loss_dice_1: 0.1366  loss_ce_2: 0.1043  loss_mask_2: 0.2224  loss_dice_2: 0.1363  loss_ce_3: 0.1045  loss_mask_3: 0.2246  loss_dice_3: 0.133  loss_ce_4: 0.1048  loss_mask_4: 0.2235  loss_dice_4: 0.1287  loss_ce_5: 0.1051  loss_mask_5: 0.2157  loss_dice_5: 0.132  loss_ce_6: 0.1045  loss_mask_6: 0.2261  loss_dice_6: 0.1317  loss_ce_7: 0.1041  loss_mask_7: 0.2258  loss_dice_7: 0.1357  loss_ce_8: 0.1039  loss_mask_8: 0.2292  loss_dice_8: 0.1359  time: 0.5679  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 13:17:34] d2.utils.events INFO:  eta: 3:02:46  iter: 11599  total_loss: 4.533  loss_ce: 0.102  loss_mask: 0.1905  loss_dice: 0.1344  loss_ce_0: 0.05357  loss_mask_0: 0.1999  loss_dice_0: 0.1405  loss_ce_1: 0.1022  loss_mask_1: 0.2011  loss_dice_1: 0.1379  loss_ce_2: 0.1021  loss_mask_2: 0.1935  loss_dice_2: 0.1379  loss_ce_3: 0.1021  loss_mask_3: 0.1897  loss_dice_3: 0.1329  loss_ce_4: 0.1025  loss_mask_4: 0.1913  loss_dice_4: 0.1348  loss_ce_5: 0.1028  loss_mask_5: 0.1923  loss_dice_5: 0.1331  loss_ce_6: 0.1024  loss_mask_6: 0.1898  loss_dice_6: 0.1349  loss_ce_7: 0.1021  loss_mask_7: 0.1944  loss_dice_7: 0.1426  loss_ce_8: 0.1021  loss_mask_8: 0.1932  loss_dice_8: 0.1333  time: 0.5680  data_time: 0.0021  lr: 1e-05  max_mem: 2811M
[07/11 13:17:47] d2.utils.events INFO:  eta: 3:03:01  iter: 11619  total_loss: 4.844  loss_ce: 0.1015  loss_mask: 0.2104  loss_dice: 0.1419  loss_ce_0: 0.05349  loss_mask_0: 0.2075  loss_dice_0: 0.1368  loss_ce_1: 0.1019  loss_mask_1: 0.1969  loss_dice_1: 0.1363  loss_ce_2: 0.1015  loss_mask_2: 0.2081  loss_dice_2: 0.1401  loss_ce_3: 0.1015  loss_mask_3: 0.1994  loss_dice_3: 0.1344  loss_ce_4: 0.102  loss_mask_4: 0.2072  loss_dice_4: 0.1404  loss_ce_5: 0.102  loss_mask_5: 0.2065  loss_dice_5: 0.1447  loss_ce_6: 0.1019  loss_mask_6: 0.2086  loss_dice_6: 0.1348  loss_ce_7: 0.1015  loss_mask_7: 0.2038  loss_dice_7: 0.1405  loss_ce_8: 0.1016  loss_mask_8: 0.2066  loss_dice_8: 0.1412  time: 0.5681  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:18:00] d2.utils.events INFO:  eta: 3:03:15  iter: 11639  total_loss: 4.505  loss_ce: 0.1577  loss_mask: 0.2021  loss_dice: 0.136  loss_ce_0: 0.07619  loss_mask_0: 0.2123  loss_dice_0: 0.1421  loss_ce_1: 0.1572  loss_mask_1: 0.198  loss_dice_1: 0.1378  loss_ce_2: 0.1577  loss_mask_2: 0.2003  loss_dice_2: 0.1291  loss_ce_3: 0.1578  loss_mask_3: 0.2069  loss_dice_3: 0.136  loss_ce_4: 0.1572  loss_mask_4: 0.2046  loss_dice_4: 0.1437  loss_ce_5: 0.1571  loss_mask_5: 0.2007  loss_dice_5: 0.1361  loss_ce_6: 0.1573  loss_mask_6: 0.2105  loss_dice_6: 0.138  loss_ce_7: 0.1578  loss_mask_7: 0.2103  loss_dice_7: 0.1374  loss_ce_8: 0.1576  loss_mask_8: 0.2038  loss_dice_8: 0.1318  time: 0.5683  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:18:12] d2.utils.events INFO:  eta: 3:03:26  iter: 11659  total_loss: 4.292  loss_ce: 0.1014  loss_mask: 0.1857  loss_dice: 0.1294  loss_ce_0: 0.05351  loss_mask_0: 0.1951  loss_dice_0: 0.1295  loss_ce_1: 0.1019  loss_mask_1: 0.1936  loss_dice_1: 0.1307  loss_ce_2: 0.1015  loss_mask_2: 0.1938  loss_dice_2: 0.1297  loss_ce_3: 0.1015  loss_mask_3: 0.1875  loss_dice_3: 0.1296  loss_ce_4: 0.1019  loss_mask_4: 0.1936  loss_dice_4: 0.1338  loss_ce_5: 0.102  loss_mask_5: 0.1867  loss_dice_5: 0.1325  loss_ce_6: 0.1018  loss_mask_6: 0.1855  loss_dice_6: 0.1278  loss_ce_7: 0.1015  loss_mask_7: 0.1843  loss_dice_7: 0.1272  loss_ce_8: 0.1015  loss_mask_8: 0.1902  loss_dice_8: 0.1323  time: 0.5684  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 13:18:25] d2.utils.events INFO:  eta: 3:03:35  iter: 11679  total_loss: 4.661  loss_ce: 0.1017  loss_mask: 0.2174  loss_dice: 0.1588  loss_ce_0: 0.05365  loss_mask_0: 0.2232  loss_dice_0: 0.1599  loss_ce_1: 0.1021  loss_mask_1: 0.2209  loss_dice_1: 0.1536  loss_ce_2: 0.1019  loss_mask_2: 0.2175  loss_dice_2: 0.1559  loss_ce_3: 0.1019  loss_mask_3: 0.2191  loss_dice_3: 0.1614  loss_ce_4: 0.1023  loss_mask_4: 0.2187  loss_dice_4: 0.1609  loss_ce_5: 0.1024  loss_mask_5: 0.2122  loss_dice_5: 0.1582  loss_ce_6: 0.1021  loss_mask_6: 0.2155  loss_dice_6: 0.1619  loss_ce_7: 0.1018  loss_mask_7: 0.2215  loss_dice_7: 0.1597  loss_ce_8: 0.1019  loss_mask_8: 0.2223  loss_dice_8: 0.1605  time: 0.5685  data_time: 0.0025  lr: 1e-05  max_mem: 2811M
[07/11 13:18:38] d2.utils.events INFO:  eta: 3:03:48  iter: 11699  total_loss: 4.649  loss_ce: 0.1017  loss_mask: 0.199  loss_dice: 0.1534  loss_ce_0: 0.05362  loss_mask_0: 0.191  loss_dice_0: 0.1523  loss_ce_1: 0.1021  loss_mask_1: 0.1943  loss_dice_1: 0.1489  loss_ce_2: 0.1018  loss_mask_2: 0.1892  loss_dice_2: 0.1454  loss_ce_3: 0.1017  loss_mask_3: 0.1884  loss_dice_3: 0.1538  loss_ce_4: 0.1021  loss_mask_4: 0.2013  loss_dice_4: 0.1549  loss_ce_5: 0.1022  loss_mask_5: 0.188  loss_dice_5: 0.1474  loss_ce_6: 0.102  loss_mask_6: 0.1853  loss_dice_6: 0.1521  loss_ce_7: 0.1017  loss_mask_7: 0.1855  loss_dice_7: 0.1527  loss_ce_8: 0.1018  loss_mask_8: 0.1863  loss_dice_8: 0.1548  time: 0.5686  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:18:50] d2.utils.events INFO:  eta: 3:03:48  iter: 11719  total_loss: 4.762  loss_ce: 0.1563  loss_mask: 0.1975  loss_dice: 0.1344  loss_ce_0: 0.07561  loss_mask_0: 0.2  loss_dice_0: 0.1363  loss_ce_1: 0.1557  loss_mask_1: 0.2019  loss_dice_1: 0.1343  loss_ce_2: 0.1558  loss_mask_2: 0.1994  loss_dice_2: 0.1349  loss_ce_3: 0.1558  loss_mask_3: 0.2036  loss_dice_3: 0.1357  loss_ce_4: 0.1555  loss_mask_4: 0.1927  loss_dice_4: 0.1296  loss_ce_5: 0.1553  loss_mask_5: 0.2032  loss_dice_5: 0.1344  loss_ce_6: 0.1557  loss_mask_6: 0.203  loss_dice_6: 0.1343  loss_ce_7: 0.156  loss_mask_7: 0.2005  loss_dice_7: 0.1312  loss_ce_8: 0.1562  loss_mask_8: 0.2037  loss_dice_8: 0.1378  time: 0.5687  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:19:03] d2.utils.events INFO:  eta: 3:03:58  iter: 11739  total_loss: 4.948  loss_ce: 0.1548  loss_mask: 0.2056  loss_dice: 0.1525  loss_ce_0: 0.07529  loss_mask_0: 0.1941  loss_dice_0: 0.1457  loss_ce_1: 0.1535  loss_mask_1: 0.1911  loss_dice_1: 0.1445  loss_ce_2: 0.1539  loss_mask_2: 0.2021  loss_dice_2: 0.1558  loss_ce_3: 0.1541  loss_mask_3: 0.2026  loss_dice_3: 0.1552  loss_ce_4: 0.1537  loss_mask_4: 0.1946  loss_dice_4: 0.1525  loss_ce_5: 0.1536  loss_mask_5: 0.21  loss_dice_5: 0.1582  loss_ce_6: 0.1541  loss_mask_6: 0.1983  loss_dice_6: 0.1476  loss_ce_7: 0.1545  loss_mask_7: 0.2053  loss_dice_7: 0.1551  loss_ce_8: 0.1546  loss_mask_8: 0.2056  loss_dice_8: 0.1548  time: 0.5688  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:19:16] d2.utils.events INFO:  eta: 3:04:14  iter: 11759  total_loss: 4.347  loss_ce: 0.1048  loss_mask: 0.145  loss_dice: 0.1537  loss_ce_0: 0.05442  loss_mask_0: 0.1287  loss_dice_0: 0.1558  loss_ce_1: 0.1062  loss_mask_1: 0.1366  loss_dice_1: 0.1618  loss_ce_2: 0.1058  loss_mask_2: 0.1336  loss_dice_2: 0.1635  loss_ce_3: 0.1059  loss_mask_3: 0.1335  loss_dice_3: 0.1603  loss_ce_4: 0.1061  loss_mask_4: 0.1409  loss_dice_4: 0.1639  loss_ce_5: 0.1062  loss_mask_5: 0.1283  loss_dice_5: 0.1591  loss_ce_6: 0.1056  loss_mask_6: 0.1462  loss_dice_6: 0.1562  loss_ce_7: 0.1053  loss_mask_7: 0.1297  loss_dice_7: 0.1591  loss_ce_8: 0.1051  loss_mask_8: 0.135  loss_dice_8: 0.1573  time: 0.5689  data_time: 0.0021  lr: 1e-05  max_mem: 2811M
[07/11 13:19:28] d2.utils.events INFO:  eta: 3:04:22  iter: 11779  total_loss: 4.613  loss_ce: 0.1047  loss_mask: 0.1376  loss_dice: 0.1651  loss_ce_0: 0.05442  loss_mask_0: 0.1337  loss_dice_0: 0.1691  loss_ce_1: 0.1061  loss_mask_1: 0.1295  loss_dice_1: 0.1681  loss_ce_2: 0.1055  loss_mask_2: 0.1421  loss_dice_2: 0.1747  loss_ce_3: 0.1056  loss_mask_3: 0.1399  loss_dice_3: 0.174  loss_ce_4: 0.1059  loss_mask_4: 0.14  loss_dice_4: 0.1726  loss_ce_5: 0.1061  loss_mask_5: 0.1315  loss_dice_5: 0.1635  loss_ce_6: 0.1055  loss_mask_6: 0.143  loss_dice_6: 0.1711  loss_ce_7: 0.1051  loss_mask_7: 0.1322  loss_dice_7: 0.1653  loss_ce_8: 0.105  loss_mask_8: 0.1384  loss_dice_8: 0.1682  time: 0.5690  data_time: 0.0023  lr: 1e-05  max_mem: 2811M
[07/11 13:19:41] d2.utils.events INFO:  eta: 3:04:49  iter: 11799  total_loss: 4.497  loss_ce: 0.1517  loss_mask: 0.1723  loss_dice: 0.1397  loss_ce_0: 0.07437  loss_mask_0: 0.1661  loss_dice_0: 0.1361  loss_ce_1: 0.1498  loss_mask_1: 0.1733  loss_dice_1: 0.1369  loss_ce_2: 0.1505  loss_mask_2: 0.1602  loss_dice_2: 0.1329  loss_ce_3: 0.1504  loss_mask_3: 0.1719  loss_dice_3: 0.1299  loss_ce_4: 0.15  loss_mask_4: 0.1681  loss_dice_4: 0.1353  loss_ce_5: 0.1497  loss_mask_5: 0.1739  loss_dice_5: 0.1394  loss_ce_6: 0.1508  loss_mask_6: 0.1729  loss_dice_6: 0.1348  loss_ce_7: 0.151  loss_mask_7: 0.1703  loss_dice_7: 0.1348  loss_ce_8: 0.1514  loss_mask_8: 0.171  loss_dice_8: 0.1423  time: 0.5691  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:19:54] d2.utils.events INFO:  eta: 3:05:06  iter: 11819  total_loss: 4.462  loss_ce: 0.1062  loss_mask: 0.1861  loss_dice: 0.1463  loss_ce_0: 0.05484  loss_mask_0: 0.1799  loss_dice_0: 0.1426  loss_ce_1: 0.1079  loss_mask_1: 0.1918  loss_dice_1: 0.1477  loss_ce_2: 0.1072  loss_mask_2: 0.195  loss_dice_2: 0.1509  loss_ce_3: 0.1074  loss_mask_3: 0.1931  loss_dice_3: 0.1477  loss_ce_4: 0.1077  loss_mask_4: 0.183  loss_dice_4: 0.1416  loss_ce_5: 0.1078  loss_mask_5: 0.195  loss_dice_5: 0.1455  loss_ce_6: 0.1072  loss_mask_6: 0.189  loss_dice_6: 0.1458  loss_ce_7: 0.1068  loss_mask_7: 0.1921  loss_dice_7: 0.1475  loss_ce_8: 0.1066  loss_mask_8: 0.188  loss_dice_8: 0.1438  time: 0.5692  data_time: 0.0026  lr: 1e-05  max_mem: 2811M
[07/11 13:20:06] d2.utils.events INFO:  eta: 3:05:22  iter: 11839  total_loss: 4.809  loss_ce: 0.1286  loss_mask: 0.1521  loss_dice: 0.1385  loss_ce_0: 0.06449  loss_mask_0: 0.1483  loss_dice_0: 0.1435  loss_ce_1: 0.1283  loss_mask_1: 0.1448  loss_dice_1: 0.1441  loss_ce_2: 0.1284  loss_mask_2: 0.1467  loss_dice_2: 0.1385  loss_ce_3: 0.1285  loss_mask_3: 0.1363  loss_dice_3: 0.1404  loss_ce_4: 0.1284  loss_mask_4: 0.1403  loss_dice_4: 0.1384  loss_ce_5: 0.1284  loss_mask_5: 0.1512  loss_dice_5: 0.1426  loss_ce_6: 0.1285  loss_mask_6: 0.1408  loss_dice_6: 0.1403  loss_ce_7: 0.1286  loss_mask_7: 0.1453  loss_dice_7: 0.144  loss_ce_8: 0.1286  loss_mask_8: 0.1387  loss_dice_8: 0.1407  time: 0.5693  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 13:20:19] d2.utils.events INFO:  eta: 3:05:29  iter: 11859  total_loss: 4.617  loss_ce: 0.1286  loss_mask: 0.1854  loss_dice: 0.1466  loss_ce_0: 0.06447  loss_mask_0: 0.191  loss_dice_0: 0.1415  loss_ce_1: 0.1284  loss_mask_1: 0.1853  loss_dice_1: 0.1395  loss_ce_2: 0.1285  loss_mask_2: 0.1843  loss_dice_2: 0.1434  loss_ce_3: 0.1285  loss_mask_3: 0.1947  loss_dice_3: 0.1437  loss_ce_4: 0.1284  loss_mask_4: 0.1912  loss_dice_4: 0.1437  loss_ce_5: 0.1284  loss_mask_5: 0.1976  loss_dice_5: 0.1448  loss_ce_6: 0.1285  loss_mask_6: 0.192  loss_dice_6: 0.145  loss_ce_7: 0.1285  loss_mask_7: 0.1907  loss_dice_7: 0.1435  loss_ce_8: 0.1284  loss_mask_8: 0.186  loss_dice_8: 0.1392  time: 0.5695  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 13:20:32] d2.utils.events INFO:  eta: 3:05:52  iter: 11879  total_loss: 4.516  loss_ce: 0.1062  loss_mask: 0.2067  loss_dice: 0.1442  loss_ce_0: 0.05487  loss_mask_0: 0.2017  loss_dice_0: 0.1513  loss_ce_1: 0.1072  loss_mask_1: 0.197  loss_dice_1: 0.1412  loss_ce_2: 0.1067  loss_mask_2: 0.2096  loss_dice_2: 0.1496  loss_ce_3: 0.1068  loss_mask_3: 0.2024  loss_dice_3: 0.144  loss_ce_4: 0.1072  loss_mask_4: 0.1991  loss_dice_4: 0.1459  loss_ce_5: 0.1073  loss_mask_5: 0.2066  loss_dice_5: 0.1434  loss_ce_6: 0.1067  loss_mask_6: 0.1938  loss_dice_6: 0.1443  loss_ce_7: 0.1065  loss_mask_7: 0.1986  loss_dice_7: 0.1455  loss_ce_8: 0.1065  loss_mask_8: 0.2117  loss_dice_8: 0.1467  time: 0.5696  data_time: 0.0025  lr: 1e-05  max_mem: 2811M
[07/11 13:20:44] d2.utils.events INFO:  eta: 3:06:13  iter: 11899  total_loss: 4.509  loss_ce: 0.1047  loss_mask: 0.2104  loss_dice: 0.1369  loss_ce_0: 0.05453  loss_mask_0: 0.2073  loss_dice_0: 0.1299  loss_ce_1: 0.105  loss_mask_1: 0.2014  loss_dice_1: 0.1327  loss_ce_2: 0.1047  loss_mask_2: 0.2081  loss_dice_2: 0.1345  loss_ce_3: 0.1047  loss_mask_3: 0.2123  loss_dice_3: 0.1316  loss_ce_4: 0.1051  loss_mask_4: 0.2068  loss_dice_4: 0.1341  loss_ce_5: 0.1051  loss_mask_5: 0.2081  loss_dice_5: 0.1359  loss_ce_6: 0.1049  loss_mask_6: 0.2125  loss_dice_6: 0.139  loss_ce_7: 0.1046  loss_mask_7: 0.201  loss_dice_7: 0.1344  loss_ce_8: 0.1048  loss_mask_8: 0.2095  loss_dice_8: 0.1371  time: 0.5697  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 13:20:57] d2.utils.events INFO:  eta: 3:06:20  iter: 11919  total_loss: 4.719  loss_ce: 0.1292  loss_mask: 0.2091  loss_dice: 0.1438  loss_ce_0: 0.0646  loss_mask_0: 0.2118  loss_dice_0: 0.1472  loss_ce_1: 0.1292  loss_mask_1: 0.2013  loss_dice_1: 0.1423  loss_ce_2: 0.1293  loss_mask_2: 0.207  loss_dice_2: 0.1489  loss_ce_3: 0.1292  loss_mask_3: 0.2121  loss_dice_3: 0.1476  loss_ce_4: 0.1291  loss_mask_4: 0.2055  loss_dice_4: 0.1402  loss_ce_5: 0.1291  loss_mask_5: 0.2103  loss_dice_5: 0.1471  loss_ce_6: 0.1292  loss_mask_6: 0.2128  loss_dice_6: 0.1486  loss_ce_7: 0.1292  loss_mask_7: 0.205  loss_dice_7: 0.1402  loss_ce_8: 0.1291  loss_mask_8: 0.2049  loss_dice_8: 0.1398  time: 0.5698  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:21:10] d2.utils.events INFO:  eta: 3:06:41  iter: 11939  total_loss: 4.64  loss_ce: 0.1025  loss_mask: 0.1884  loss_dice: 0.1553  loss_ce_0: 0.05409  loss_mask_0: 0.1936  loss_dice_0: 0.1543  loss_ce_1: 0.1021  loss_mask_1: 0.1968  loss_dice_1: 0.1565  loss_ce_2: 0.1019  loss_mask_2: 0.1885  loss_dice_2: 0.1542  loss_ce_3: 0.1019  loss_mask_3: 0.1835  loss_dice_3: 0.1562  loss_ce_4: 0.1023  loss_mask_4: 0.1909  loss_dice_4: 0.1567  loss_ce_5: 0.1023  loss_mask_5: 0.1899  loss_dice_5: 0.1544  loss_ce_6: 0.1023  loss_mask_6: 0.1861  loss_dice_6: 0.1609  loss_ce_7: 0.1021  loss_mask_7: 0.1911  loss_dice_7: 0.1539  loss_ce_8: 0.1025  loss_mask_8: 0.1988  loss_dice_8: 0.1599  time: 0.5699  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:21:22] d2.utils.events INFO:  eta: 3:06:53  iter: 11959  total_loss: 4.705  loss_ce: 0.1016  loss_mask: 0.1838  loss_dice: 0.1531  loss_ce_0: 0.05391  loss_mask_0: 0.1944  loss_dice_0: 0.1555  loss_ce_1: 0.1008  loss_mask_1: 0.1958  loss_dice_1: 0.1568  loss_ce_2: 0.1008  loss_mask_2: 0.1865  loss_dice_2: 0.1571  loss_ce_3: 0.1008  loss_mask_3: 0.1862  loss_dice_3: 0.1544  loss_ce_4: 0.1011  loss_mask_4: 0.1838  loss_dice_4: 0.1472  loss_ce_5: 0.1012  loss_mask_5: 0.1939  loss_dice_5: 0.151  loss_ce_6: 0.1012  loss_mask_6: 0.1941  loss_dice_6: 0.1565  loss_ce_7: 0.1012  loss_mask_7: 0.1853  loss_dice_7: 0.1566  loss_ce_8: 0.1014  loss_mask_8: 0.191  loss_dice_8: 0.1579  time: 0.5700  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 13:21:35] d2.utils.events INFO:  eta: 3:07:00  iter: 11979  total_loss: 4.99  loss_ce: 0.1008  loss_mask: 0.2316  loss_dice: 0.1427  loss_ce_0: 0.05374  loss_mask_0: 0.2365  loss_dice_0: 0.1412  loss_ce_1: 0.1  loss_mask_1: 0.2254  loss_dice_1: 0.1484  loss_ce_2: 0.09998  loss_mask_2: 0.2341  loss_dice_2: 0.1457  loss_ce_3: 0.0999  loss_mask_3: 0.2276  loss_dice_3: 0.1406  loss_ce_4: 0.1003  loss_mask_4: 0.2395  loss_dice_4: 0.1441  loss_ce_5: 0.1001  loss_mask_5: 0.2359  loss_dice_5: 0.1434  loss_ce_6: 0.1003  loss_mask_6: 0.2337  loss_dice_6: 0.1442  loss_ce_7: 0.1003  loss_mask_7: 0.2374  loss_dice_7: 0.144  loss_ce_8: 0.1006  loss_mask_8: 0.2288  loss_dice_8: 0.1427  time: 0.5701  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 13:21:48] d2.utils.events INFO:  eta: 3:07:03  iter: 11999  total_loss: 4.545  loss_ce: 0.1004  loss_mask: 0.1585  loss_dice: 0.1561  loss_ce_0: 0.05363  loss_mask_0: 0.1669  loss_dice_0: 0.1608  loss_ce_1: 0.09955  loss_mask_1: 0.1637  loss_dice_1: 0.1563  loss_ce_2: 0.09957  loss_mask_2: 0.178  loss_dice_2: 0.1536  loss_ce_3: 0.09949  loss_mask_3: 0.1809  loss_dice_3: 0.1489  loss_ce_4: 0.09985  loss_mask_4: 0.1748  loss_dice_4: 0.149  loss_ce_5: 0.09971  loss_mask_5: 0.1713  loss_dice_5: 0.1559  loss_ce_6: 0.09987  loss_mask_6: 0.1756  loss_dice_6: 0.1479  loss_ce_7: 0.09976  loss_mask_7: 0.169  loss_dice_7: 0.151  loss_ce_8: 0.1001  loss_mask_8: 0.1683  loss_dice_8: 0.152  time: 0.5702  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 13:22:00] d2.utils.events INFO:  eta: 3:07:11  iter: 12019  total_loss: 4.903  loss_ce: 0.09995  loss_mask: 0.1557  loss_dice: 0.1761  loss_ce_0: 0.05356  loss_mask_0: 0.1511  loss_dice_0: 0.1807  loss_ce_1: 0.09918  loss_mask_1: 0.1694  loss_dice_1: 0.1803  loss_ce_2: 0.09927  loss_mask_2: 0.1557  loss_dice_2: 0.1753  loss_ce_3: 0.09908  loss_mask_3: 0.1577  loss_dice_3: 0.1758  loss_ce_4: 0.09948  loss_mask_4: 0.1545  loss_dice_4: 0.1735  loss_ce_5: 0.09934  loss_mask_5: 0.1561  loss_dice_5: 0.1829  loss_ce_6: 0.09934  loss_mask_6: 0.1541  loss_dice_6: 0.1751  loss_ce_7: 0.09946  loss_mask_7: 0.1581  loss_dice_7: 0.1741  loss_ce_8: 0.09987  loss_mask_8: 0.157  loss_dice_8: 0.1714  time: 0.5703  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:22:13] d2.utils.events INFO:  eta: 3:07:31  iter: 12039  total_loss: 4.602  loss_ce: 0.09968  loss_mask: 0.2246  loss_dice: 0.1405  loss_ce_0: 0.05352  loss_mask_0: 0.2315  loss_dice_0: 0.1368  loss_ce_1: 0.09893  loss_mask_1: 0.2227  loss_dice_1: 0.1389  loss_ce_2: 0.09901  loss_mask_2: 0.2282  loss_dice_2: 0.1374  loss_ce_3: 0.09886  loss_mask_3: 0.2345  loss_dice_3: 0.1423  loss_ce_4: 0.09919  loss_mask_4: 0.2287  loss_dice_4: 0.1435  loss_ce_5: 0.09897  loss_mask_5: 0.2424  loss_dice_5: 0.1418  loss_ce_6: 0.09919  loss_mask_6: 0.2337  loss_dice_6: 0.1414  loss_ce_7: 0.09916  loss_mask_7: 0.2272  loss_dice_7: 0.142  loss_ce_8: 0.09942  loss_mask_8: 0.233  loss_dice_8: 0.1375  time: 0.5704  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:22:26] d2.utils.events INFO:  eta: 3:07:44  iter: 12059  total_loss: 4.779  loss_ce: 0.09916  loss_mask: 0.2077  loss_dice: 0.1463  loss_ce_0: 0.05336  loss_mask_0: 0.2  loss_dice_0: 0.1453  loss_ce_1: 0.09823  loss_mask_1: 0.2017  loss_dice_1: 0.143  loss_ce_2: 0.09841  loss_mask_2: 0.1982  loss_dice_2: 0.1448  loss_ce_3: 0.0983  loss_mask_3: 0.1979  loss_dice_3: 0.1458  loss_ce_4: 0.09859  loss_mask_4: 0.1959  loss_dice_4: 0.1416  loss_ce_5: 0.09833  loss_mask_5: 0.2012  loss_dice_5: 0.1462  loss_ce_6: 0.09852  loss_mask_6: 0.2065  loss_dice_6: 0.1493  loss_ce_7: 0.09864  loss_mask_7: 0.2035  loss_dice_7: 0.1471  loss_ce_8: 0.09893  loss_mask_8: 0.2069  loss_dice_8: 0.1474  time: 0.5705  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:22:38] d2.utils.events INFO:  eta: 3:07:46  iter: 12079  total_loss: 4.491  loss_ce: 0.0986  loss_mask: 0.1554  loss_dice: 0.161  loss_ce_0: 0.05322  loss_mask_0: 0.1509  loss_dice_0: 0.1573  loss_ce_1: 0.09767  loss_mask_1: 0.1456  loss_dice_1: 0.1549  loss_ce_2: 0.09787  loss_mask_2: 0.1499  loss_dice_2: 0.1584  loss_ce_3: 0.09771  loss_mask_3: 0.1421  loss_dice_3: 0.159  loss_ce_4: 0.098  loss_mask_4: 0.1477  loss_dice_4: 0.1573  loss_ce_5: 0.09766  loss_mask_5: 0.1454  loss_dice_5: 0.1591  loss_ce_6: 0.09789  loss_mask_6: 0.1399  loss_dice_6: 0.1558  loss_ce_7: 0.09804  loss_mask_7: 0.1456  loss_dice_7: 0.1568  loss_ce_8: 0.0983  loss_mask_8: 0.1483  loss_dice_8: 0.1634  time: 0.5706  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:22:51] d2.utils.events INFO:  eta: 3:07:54  iter: 12099  total_loss: 4.728  loss_ce: 0.1302  loss_mask: 0.1953  loss_dice: 0.1493  loss_ce_0: 0.06477  loss_mask_0: 0.2066  loss_dice_0: 0.1467  loss_ce_1: 0.1303  loss_mask_1: 0.2005  loss_dice_1: 0.1443  loss_ce_2: 0.1304  loss_mask_2: 0.1862  loss_dice_2: 0.1457  loss_ce_3: 0.1304  loss_mask_3: 0.2008  loss_dice_3: 0.1414  loss_ce_4: 0.1304  loss_mask_4: 0.2053  loss_dice_4: 0.1433  loss_ce_5: 0.1304  loss_mask_5: 0.1964  loss_dice_5: 0.1414  loss_ce_6: 0.1304  loss_mask_6: 0.2023  loss_dice_6: 0.1496  loss_ce_7: 0.1303  loss_mask_7: 0.2043  loss_dice_7: 0.1502  loss_ce_8: 0.1303  loss_mask_8: 0.2052  loss_dice_8: 0.1431  time: 0.5707  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:23:04] d2.utils.events INFO:  eta: 3:08:08  iter: 12119  total_loss: 4.73  loss_ce: 0.1617  loss_mask: 0.2119  loss_dice: 0.1372  loss_ce_0: 0.07629  loss_mask_0: 0.2056  loss_dice_0: 0.1396  loss_ce_1: 0.1624  loss_mask_1: 0.1993  loss_dice_1: 0.1284  loss_ce_2: 0.1621  loss_mask_2: 0.2058  loss_dice_2: 0.136  loss_ce_3: 0.1623  loss_mask_3: 0.2103  loss_dice_3: 0.1368  loss_ce_4: 0.1621  loss_mask_4: 0.2051  loss_dice_4: 0.1374  loss_ce_5: 0.1625  loss_mask_5: 0.2166  loss_dice_5: 0.1348  loss_ce_6: 0.1622  loss_mask_6: 0.2115  loss_dice_6: 0.1395  loss_ce_7: 0.1622  loss_mask_7: 0.2115  loss_dice_7: 0.1388  loss_ce_8: 0.162  loss_mask_8: 0.2189  loss_dice_8: 0.1348  time: 0.5708  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:23:16] d2.utils.events INFO:  eta: 3:08:08  iter: 12139  total_loss: 4.303  loss_ce: 0.09863  loss_mask: 0.1323  loss_dice: 0.1442  loss_ce_0: 0.05325  loss_mask_0: 0.1375  loss_dice_0: 0.1483  loss_ce_1: 0.09847  loss_mask_1: 0.13  loss_dice_1: 0.138  loss_ce_2: 0.09839  loss_mask_2: 0.1362  loss_dice_2: 0.1486  loss_ce_3: 0.09829  loss_mask_3: 0.1322  loss_dice_3: 0.1414  loss_ce_4: 0.09852  loss_mask_4: 0.1297  loss_dice_4: 0.1463  loss_ce_5: 0.09818  loss_mask_5: 0.1236  loss_dice_5: 0.135  loss_ce_6: 0.09837  loss_mask_6: 0.1326  loss_dice_6: 0.1434  loss_ce_7: 0.09841  loss_mask_7: 0.1335  loss_dice_7: 0.1414  loss_ce_8: 0.09841  loss_mask_8: 0.1329  loss_dice_8: 0.141  time: 0.5709  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:23:29] d2.utils.events INFO:  eta: 3:08:09  iter: 12159  total_loss: 4.594  loss_ce: 0.09912  loss_mask: 0.1598  loss_dice: 0.1629  loss_ce_0: 0.05338  loss_mask_0: 0.1555  loss_dice_0: 0.1654  loss_ce_1: 0.09927  loss_mask_1: 0.156  loss_dice_1: 0.162  loss_ce_2: 0.09921  loss_mask_2: 0.1613  loss_dice_2: 0.1703  loss_ce_3: 0.09912  loss_mask_3: 0.1513  loss_dice_3: 0.157  loss_ce_4: 0.09926  loss_mask_4: 0.1598  loss_dice_4: 0.1677  loss_ce_5: 0.09915  loss_mask_5: 0.1613  loss_dice_5: 0.1639  loss_ce_6: 0.09912  loss_mask_6: 0.1636  loss_dice_6: 0.1615  loss_ce_7: 0.09901  loss_mask_7: 0.1495  loss_dice_7: 0.1665  loss_ce_8: 0.09897  loss_mask_8: 0.1524  loss_dice_8: 0.1625  time: 0.5710  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:23:42] d2.utils.events INFO:  eta: 3:08:01  iter: 12179  total_loss: 4.357  loss_ce: 0.09916  loss_mask: 0.1733  loss_dice: 0.1393  loss_ce_0: 0.05338  loss_mask_0: 0.1694  loss_dice_0: 0.1363  loss_ce_1: 0.0991  loss_mask_1: 0.1663  loss_dice_1: 0.1322  loss_ce_2: 0.09919  loss_mask_2: 0.1623  loss_dice_2: 0.1305  loss_ce_3: 0.09915  loss_mask_3: 0.1722  loss_dice_3: 0.1356  loss_ce_4: 0.09925  loss_mask_4: 0.1753  loss_dice_4: 0.1359  loss_ce_5: 0.09915  loss_mask_5: 0.1791  loss_dice_5: 0.1362  loss_ce_6: 0.09908  loss_mask_6: 0.1709  loss_dice_6: 0.1344  loss_ce_7: 0.09901  loss_mask_7: 0.1692  loss_dice_7: 0.1325  loss_ce_8: 0.09901  loss_mask_8: 0.1719  loss_dice_8: 0.1312  time: 0.5711  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:23:54] d2.utils.events INFO:  eta: 3:07:44  iter: 12199  total_loss: 4.764  loss_ce: 0.1601  loss_mask: 0.1959  loss_dice: 0.1357  loss_ce_0: 0.07581  loss_mask_0: 0.21  loss_dice_0: 0.1381  loss_ce_1: 0.1595  loss_mask_1: 0.2118  loss_dice_1: 0.1367  loss_ce_2: 0.1598  loss_mask_2: 0.2089  loss_dice_2: 0.1365  loss_ce_3: 0.16  loss_mask_3: 0.2111  loss_dice_3: 0.1366  loss_ce_4: 0.1597  loss_mask_4: 0.2023  loss_dice_4: 0.134  loss_ce_5: 0.1599  loss_mask_5: 0.2014  loss_dice_5: 0.1388  loss_ce_6: 0.1599  loss_mask_6: 0.215  loss_dice_6: 0.1385  loss_ce_7: 0.1601  loss_mask_7: 0.2083  loss_dice_7: 0.1351  loss_ce_8: 0.1602  loss_mask_8: 0.197  loss_dice_8: 0.1362  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:24:07] d2.utils.events INFO:  eta: 3:07:27  iter: 12219  total_loss: 4.563  loss_ce: 0.09945  loss_mask: 0.2049  loss_dice: 0.1458  loss_ce_0: 0.05346  loss_mask_0: 0.1978  loss_dice_0: 0.1417  loss_ce_1: 0.09955  loss_mask_1: 0.1961  loss_dice_1: 0.1468  loss_ce_2: 0.09962  loss_mask_2: 0.2029  loss_dice_2: 0.149  loss_ce_3: 0.09953  loss_mask_3: 0.197  loss_dice_3: 0.1459  loss_ce_4: 0.09964  loss_mask_4: 0.2  loss_dice_4: 0.1441  loss_ce_5: 0.09964  loss_mask_5: 0.2054  loss_dice_5: 0.1515  loss_ce_6: 0.09957  loss_mask_6: 0.1947  loss_dice_6: 0.1479  loss_ce_7: 0.09949  loss_mask_7: 0.195  loss_dice_7: 0.1487  loss_ce_8: 0.09934  loss_mask_8: 0.203  loss_dice_8: 0.1441  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:24:19] d2.utils.events INFO:  eta: 3:07:08  iter: 12239  total_loss: 4.718  loss_ce: 0.09845  loss_mask: 0.2074  loss_dice: 0.1558  loss_ce_0: 0.05315  loss_mask_0: 0.2064  loss_dice_0: 0.1514  loss_ce_1: 0.09828  loss_mask_1: 0.2033  loss_dice_1: 0.1563  loss_ce_2: 0.09818  loss_mask_2: 0.2042  loss_dice_2: 0.147  loss_ce_3: 0.09811  loss_mask_3: 0.2088  loss_dice_3: 0.1533  loss_ce_4: 0.09829  loss_mask_4: 0.195  loss_dice_4: 0.1561  loss_ce_5: 0.09815  loss_mask_5: 0.2011  loss_dice_5: 0.1519  loss_ce_6: 0.09822  loss_mask_6: 0.2108  loss_dice_6: 0.1448  loss_ce_7: 0.0983  loss_mask_7: 0.2106  loss_dice_7: 0.1427  loss_ce_8: 0.09826  loss_mask_8: 0.2  loss_dice_8: 0.153  time: 0.5714  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:24:32] d2.utils.events INFO:  eta: 3:07:02  iter: 12259  total_loss: 4.597  loss_ce: 0.0977  loss_mask: 0.1736  loss_dice: 0.1507  loss_ce_0: 0.05299  loss_mask_0: 0.1778  loss_dice_0: 0.1509  loss_ce_1: 0.09719  loss_mask_1: 0.1689  loss_dice_1: 0.1481  loss_ce_2: 0.09728  loss_mask_2: 0.1718  loss_dice_2: 0.1488  loss_ce_3: 0.09726  loss_mask_3: 0.1875  loss_dice_3: 0.1552  loss_ce_4: 0.0974  loss_mask_4: 0.1856  loss_dice_4: 0.1507  loss_ce_5: 0.09722  loss_mask_5: 0.1803  loss_dice_5: 0.1563  loss_ce_6: 0.0973  loss_mask_6: 0.172  loss_dice_6: 0.1463  loss_ce_7: 0.09755  loss_mask_7: 0.1799  loss_dice_7: 0.1542  loss_ce_8: 0.09745  loss_mask_8: 0.1788  loss_dice_8: 0.1519  time: 0.5715  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:24:45] d2.utils.events INFO:  eta: 3:06:53  iter: 12279  total_loss: 4.696  loss_ce: 0.1635  loss_mask: 0.1867  loss_dice: 0.1392  loss_ce_0: 0.07668  loss_mask_0: 0.189  loss_dice_0: 0.1432  loss_ce_1: 0.1643  loss_mask_1: 0.187  loss_dice_1: 0.1345  loss_ce_2: 0.1642  loss_mask_2: 0.1873  loss_dice_2: 0.1439  loss_ce_3: 0.1643  loss_mask_3: 0.1916  loss_dice_3: 0.1413  loss_ce_4: 0.1641  loss_mask_4: 0.1882  loss_dice_4: 0.1357  loss_ce_5: 0.1643  loss_mask_5: 0.1896  loss_dice_5: 0.1428  loss_ce_6: 0.164  loss_mask_6: 0.1869  loss_dice_6: 0.1367  loss_ce_7: 0.164  loss_mask_7: 0.1864  loss_dice_7: 0.1418  loss_ce_8: 0.1639  loss_mask_8: 0.192  loss_dice_8: 0.1453  time: 0.5716  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:24:57] d2.utils.events INFO:  eta: 3:06:40  iter: 12299  total_loss: 4.882  loss_ce: 0.13  loss_mask: 0.2163  loss_dice: 0.1427  loss_ce_0: 0.06468  loss_mask_0: 0.2135  loss_dice_0: 0.1399  loss_ce_1: 0.1299  loss_mask_1: 0.2163  loss_dice_1: 0.1394  loss_ce_2: 0.13  loss_mask_2: 0.204  loss_dice_2: 0.1397  loss_ce_3: 0.13  loss_mask_3: 0.2061  loss_dice_3: 0.1417  loss_ce_4: 0.1299  loss_mask_4: 0.2091  loss_dice_4: 0.1347  loss_ce_5: 0.13  loss_mask_5: 0.2082  loss_dice_5: 0.1424  loss_ce_6: 0.13  loss_mask_6: 0.2133  loss_dice_6: 0.1367  loss_ce_7: 0.13  loss_mask_7: 0.2061  loss_dice_7: 0.1412  loss_ce_8: 0.1301  loss_mask_8: 0.2055  loss_dice_8: 0.1387  time: 0.5717  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:25:10] d2.utils.events INFO:  eta: 3:06:27  iter: 12319  total_loss: 4.523  loss_ce: 0.09845  loss_mask: 0.1838  loss_dice: 0.1234  loss_ce_0: 0.05317  loss_mask_0: 0.1973  loss_dice_0: 0.1305  loss_ce_1: 0.09865  loss_mask_1: 0.1889  loss_dice_1: 0.1222  loss_ce_2: 0.09862  loss_mask_2: 0.1853  loss_dice_2: 0.126  loss_ce_3: 0.09856  loss_mask_3: 0.1958  loss_dice_3: 0.13  loss_ce_4: 0.09863  loss_mask_4: 0.1888  loss_dice_4: 0.1311  loss_ce_5: 0.09852  loss_mask_5: 0.1884  loss_dice_5: 0.1294  loss_ce_6: 0.09841  loss_mask_6: 0.1892  loss_dice_6: 0.1308  loss_ce_7: 0.09856  loss_mask_7: 0.1856  loss_dice_7: 0.1306  loss_ce_8: 0.09841  loss_mask_8: 0.1887  loss_dice_8: 0.1312  time: 0.5718  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:25:23] d2.utils.events INFO:  eta: 3:06:15  iter: 12339  total_loss: 4.425  loss_ce: 0.13  loss_mask: 0.1698  loss_dice: 0.1377  loss_ce_0: 0.06466  loss_mask_0: 0.1563  loss_dice_0: 0.1393  loss_ce_1: 0.1302  loss_mask_1: 0.1611  loss_dice_1: 0.1384  loss_ce_2: 0.13  loss_mask_2: 0.1672  loss_dice_2: 0.1351  loss_ce_3: 0.13  loss_mask_3: 0.1679  loss_dice_3: 0.1381  loss_ce_4: 0.13  loss_mask_4: 0.1557  loss_dice_4: 0.1361  loss_ce_5: 0.13  loss_mask_5: 0.1631  loss_dice_5: 0.1416  loss_ce_6: 0.13  loss_mask_6: 0.1605  loss_dice_6: 0.1434  loss_ce_7: 0.1299  loss_mask_7: 0.1622  loss_dice_7: 0.1432  loss_ce_8: 0.13  loss_mask_8: 0.1516  loss_dice_8: 0.143  time: 0.5719  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:25:35] d2.utils.events INFO:  eta: 3:06:02  iter: 12359  total_loss: 4.558  loss_ce: 0.1299  loss_mask: 0.1854  loss_dice: 0.1413  loss_ce_0: 0.06465  loss_mask_0: 0.1818  loss_dice_0: 0.1336  loss_ce_1: 0.1296  loss_mask_1: 0.183  loss_dice_1: 0.1359  loss_ce_2: 0.1298  loss_mask_2: 0.1802  loss_dice_2: 0.1342  loss_ce_3: 0.1298  loss_mask_3: 0.1771  loss_dice_3: 0.1397  loss_ce_4: 0.1297  loss_mask_4: 0.1851  loss_dice_4: 0.1478  loss_ce_5: 0.1299  loss_mask_5: 0.1798  loss_dice_5: 0.1393  loss_ce_6: 0.1298  loss_mask_6: 0.1766  loss_dice_6: 0.1424  loss_ce_7: 0.1298  loss_mask_7: 0.1793  loss_dice_7: 0.1359  loss_ce_8: 0.1299  loss_mask_8: 0.1823  loss_dice_8: 0.1411  time: 0.5720  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:25:48] d2.utils.events INFO:  eta: 3:05:50  iter: 12379  total_loss: 4.852  loss_ce: 0.1594  loss_mask: 0.1991  loss_dice: 0.1448  loss_ce_0: 0.07564  loss_mask_0: 0.1966  loss_dice_0: 0.1401  loss_ce_1: 0.1582  loss_mask_1: 0.2003  loss_dice_1: 0.1444  loss_ce_2: 0.1586  loss_mask_2: 0.2001  loss_dice_2: 0.1476  loss_ce_3: 0.1587  loss_mask_3: 0.1989  loss_dice_3: 0.1414  loss_ce_4: 0.1587  loss_mask_4: 0.1922  loss_dice_4: 0.1435  loss_ce_5: 0.1586  loss_mask_5: 0.1865  loss_dice_5: 0.143  loss_ce_6: 0.1588  loss_mask_6: 0.1965  loss_dice_6: 0.1505  loss_ce_7: 0.1591  loss_mask_7: 0.2025  loss_dice_7: 0.1449  loss_ce_8: 0.1594  loss_mask_8: 0.1889  loss_dice_8: 0.1467  time: 0.5721  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:26:01] d2.utils.events INFO:  eta: 3:05:38  iter: 12399  total_loss: 4.85  loss_ce: 0.101  loss_mask: 0.2066  loss_dice: 0.1645  loss_ce_0: 0.05379  loss_mask_0: 0.2096  loss_dice_0: 0.1627  loss_ce_1: 0.1022  loss_mask_1: 0.1892  loss_dice_1: 0.1694  loss_ce_2: 0.1019  loss_mask_2: 0.2041  loss_dice_2: 0.1652  loss_ce_3: 0.1018  loss_mask_3: 0.2108  loss_dice_3: 0.169  loss_ce_4: 0.1019  loss_mask_4: 0.2102  loss_dice_4: 0.166  loss_ce_5: 0.102  loss_mask_5: 0.2009  loss_dice_5: 0.1643  loss_ce_6: 0.1017  loss_mask_6: 0.203  loss_dice_6: 0.1649  loss_ce_7: 0.1014  loss_mask_7: 0.2104  loss_dice_7: 0.1636  loss_ce_8: 0.101  loss_mask_8: 0.2033  loss_dice_8: 0.1667  time: 0.5721  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:26:13] d2.utils.events INFO:  eta: 3:05:26  iter: 12419  total_loss: 4.579  loss_ce: 0.101  loss_mask: 0.1621  loss_dice: 0.1643  loss_ce_0: 0.05375  loss_mask_0: 0.1629  loss_dice_0: 0.1583  loss_ce_1: 0.1019  loss_mask_1: 0.1622  loss_dice_1: 0.1586  loss_ce_2: 0.1017  loss_mask_2: 0.1608  loss_dice_2: 0.1621  loss_ce_3: 0.1017  loss_mask_3: 0.1607  loss_dice_3: 0.1624  loss_ce_4: 0.1017  loss_mask_4: 0.16  loss_dice_4: 0.1639  loss_ce_5: 0.1019  loss_mask_5: 0.1636  loss_dice_5: 0.163  loss_ce_6: 0.1016  loss_mask_6: 0.1714  loss_dice_6: 0.1654  loss_ce_7: 0.1013  loss_mask_7: 0.1599  loss_dice_7: 0.1607  loss_ce_8: 0.101  loss_mask_8: 0.1635  loss_dice_8: 0.1633  time: 0.5722  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:26:26] d2.utils.events INFO:  eta: 3:05:13  iter: 12439  total_loss: 4.671  loss_ce: 0.1596  loss_mask: 0.2101  loss_dice: 0.117  loss_ce_0: 0.07574  loss_mask_0: 0.2158  loss_dice_0: 0.1226  loss_ce_1: 0.1592  loss_mask_1: 0.2194  loss_dice_1: 0.1213  loss_ce_2: 0.1594  loss_mask_2: 0.2155  loss_dice_2: 0.1235  loss_ce_3: 0.1594  loss_mask_3: 0.2168  loss_dice_3: 0.1236  loss_ce_4: 0.1592  loss_mask_4: 0.2164  loss_dice_4: 0.122  loss_ce_5: 0.1591  loss_mask_5: 0.2089  loss_dice_5: 0.1191  loss_ce_6: 0.1593  loss_mask_6: 0.2257  loss_dice_6: 0.122  loss_ce_7: 0.1595  loss_mask_7: 0.2197  loss_dice_7: 0.1228  loss_ce_8: 0.1597  loss_mask_8: 0.2149  loss_dice_8: 0.1241  time: 0.5723  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:26:38] d2.utils.events INFO:  eta: 3:05:00  iter: 12459  total_loss: 4.297  loss_ce: 0.1014  loss_mask: 0.1776  loss_dice: 0.1364  loss_ce_0: 0.05387  loss_mask_0: 0.1747  loss_dice_0: 0.1337  loss_ce_1: 0.1023  loss_mask_1: 0.1732  loss_dice_1: 0.1283  loss_ce_2: 0.102  loss_mask_2: 0.1821  loss_dice_2: 0.1317  loss_ce_3: 0.1019  loss_mask_3: 0.1691  loss_dice_3: 0.1316  loss_ce_4: 0.1021  loss_mask_4: 0.1737  loss_dice_4: 0.1345  loss_ce_5: 0.1023  loss_mask_5: 0.1804  loss_dice_5: 0.1333  loss_ce_6: 0.102  loss_mask_6: 0.1687  loss_dice_6: 0.1278  loss_ce_7: 0.1017  loss_mask_7: 0.1702  loss_dice_7: 0.1306  loss_ce_8: 0.1015  loss_mask_8: 0.1766  loss_dice_8: 0.1334  time: 0.5724  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:26:51] d2.utils.events INFO:  eta: 3:04:49  iter: 12479  total_loss: 4.443  loss_ce: 0.1019  loss_mask: 0.1969  loss_dice: 0.1204  loss_ce_0: 0.05394  loss_mask_0: 0.2059  loss_dice_0: 0.1267  loss_ce_1: 0.1029  loss_mask_1: 0.2185  loss_dice_1: 0.1299  loss_ce_2: 0.1025  loss_mask_2: 0.2069  loss_dice_2: 0.1247  loss_ce_3: 0.1025  loss_mask_3: 0.1986  loss_dice_3: 0.1252  loss_ce_4: 0.1027  loss_mask_4: 0.2029  loss_dice_4: 0.1242  loss_ce_5: 0.1028  loss_mask_5: 0.2093  loss_dice_5: 0.1251  loss_ce_6: 0.1026  loss_mask_6: 0.2091  loss_dice_6: 0.1236  loss_ce_7: 0.1022  loss_mask_7: 0.2049  loss_dice_7: 0.1237  loss_ce_8: 0.102  loss_mask_8: 0.1999  loss_dice_8: 0.1211  time: 0.5725  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:27:04] d2.utils.events INFO:  eta: 3:04:35  iter: 12499  total_loss: 4.552  loss_ce: 0.1017  loss_mask: 0.1925  loss_dice: 0.1473  loss_ce_0: 0.05393  loss_mask_0: 0.203  loss_dice_0: 0.1398  loss_ce_1: 0.1026  loss_mask_1: 0.2133  loss_dice_1: 0.1443  loss_ce_2: 0.1023  loss_mask_2: 0.1963  loss_dice_2: 0.1428  loss_ce_3: 0.1023  loss_mask_3: 0.2006  loss_dice_3: 0.1371  loss_ce_4: 0.1024  loss_mask_4: 0.2031  loss_dice_4: 0.1426  loss_ce_5: 0.1026  loss_mask_5: 0.1986  loss_dice_5: 0.1357  loss_ce_6: 0.1023  loss_mask_6: 0.1877  loss_dice_6: 0.1404  loss_ce_7: 0.1021  loss_mask_7: 0.2048  loss_dice_7: 0.1398  loss_ce_8: 0.1018  loss_mask_8: 0.1973  loss_dice_8: 0.1371  time: 0.5726  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:27:16] d2.utils.events INFO:  eta: 3:04:24  iter: 12519  total_loss: 4.457  loss_ce: 0.1013  loss_mask: 0.2044  loss_dice: 0.1297  loss_ce_0: 0.05383  loss_mask_0: 0.2086  loss_dice_0: 0.1231  loss_ce_1: 0.1019  loss_mask_1: 0.2084  loss_dice_1: 0.1245  loss_ce_2: 0.1016  loss_mask_2: 0.2022  loss_dice_2: 0.1235  loss_ce_3: 0.1016  loss_mask_3: 0.2023  loss_dice_3: 0.1187  loss_ce_4: 0.1019  loss_mask_4: 0.2094  loss_dice_4: 0.124  loss_ce_5: 0.1019  loss_mask_5: 0.2042  loss_dice_5: 0.1216  loss_ce_6: 0.1017  loss_mask_6: 0.2013  loss_dice_6: 0.1222  loss_ce_7: 0.1015  loss_mask_7: 0.2011  loss_dice_7: 0.1216  loss_ce_8: 0.1013  loss_mask_8: 0.1979  loss_dice_8: 0.1224  time: 0.5727  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:27:29] d2.utils.events INFO:  eta: 3:04:12  iter: 12539  total_loss: 4.509  loss_ce: 0.1013  loss_mask: 0.1717  loss_dice: 0.1345  loss_ce_0: 0.05384  loss_mask_0: 0.1663  loss_dice_0: 0.1358  loss_ce_1: 0.1019  loss_mask_1: 0.1789  loss_dice_1: 0.1397  loss_ce_2: 0.1016  loss_mask_2: 0.1718  loss_dice_2: 0.1379  loss_ce_3: 0.1016  loss_mask_3: 0.1754  loss_dice_3: 0.1393  loss_ce_4: 0.1018  loss_mask_4: 0.1785  loss_dice_4: 0.1341  loss_ce_5: 0.1019  loss_mask_5: 0.1741  loss_dice_5: 0.1361  loss_ce_6: 0.1017  loss_mask_6: 0.1756  loss_dice_6: 0.1375  loss_ce_7: 0.1015  loss_mask_7: 0.1713  loss_dice_7: 0.1401  loss_ce_8: 0.1014  loss_mask_8: 0.1771  loss_dice_8: 0.1386  time: 0.5728  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:27:42] d2.utils.events INFO:  eta: 3:04:02  iter: 12559  total_loss: 5.013  loss_ce: 0.1011  loss_mask: 0.1896  loss_dice: 0.1783  loss_ce_0: 0.05378  loss_mask_0: 0.1933  loss_dice_0: 0.1871  loss_ce_1: 0.1014  loss_mask_1: 0.1963  loss_dice_1: 0.1842  loss_ce_2: 0.1012  loss_mask_2: 0.1929  loss_dice_2: 0.1803  loss_ce_3: 0.1012  loss_mask_3: 0.1873  loss_dice_3: 0.181  loss_ce_4: 0.1015  loss_mask_4: 0.1919  loss_dice_4: 0.1822  loss_ce_5: 0.1015  loss_mask_5: 0.1991  loss_dice_5: 0.1796  loss_ce_6: 0.1014  loss_mask_6: 0.188  loss_dice_6: 0.1811  loss_ce_7: 0.1012  loss_mask_7: 0.1909  loss_dice_7: 0.1791  loss_ce_8: 0.1011  loss_mask_8: 0.1974  loss_dice_8: 0.1763  time: 0.5729  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:27:54] d2.utils.events INFO:  eta: 3:03:50  iter: 12579  total_loss: 4.618  loss_ce: 0.1006  loss_mask: 0.1764  loss_dice: 0.1418  loss_ce_0: 0.05364  loss_mask_0: 0.1713  loss_dice_0: 0.1456  loss_ce_1: 0.1007  loss_mask_1: 0.1758  loss_dice_1: 0.1432  loss_ce_2: 0.1005  loss_mask_2: 0.1788  loss_dice_2: 0.1437  loss_ce_3: 0.1004  loss_mask_3: 0.1787  loss_dice_3: 0.1452  loss_ce_4: 0.1007  loss_mask_4: 0.1743  loss_dice_4: 0.144  loss_ce_5: 0.1007  loss_mask_5: 0.1781  loss_dice_5: 0.141  loss_ce_6: 0.1006  loss_mask_6: 0.181  loss_dice_6: 0.1413  loss_ce_7: 0.1005  loss_mask_7: 0.1709  loss_dice_7: 0.1472  loss_ce_8: 0.1005  loss_mask_8: 0.1761  loss_dice_8: 0.1434  time: 0.5730  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:28:07] d2.utils.events INFO:  eta: 3:03:36  iter: 12599  total_loss: 4.427  loss_ce: 0.1296  loss_mask: 0.1765  loss_dice: 0.1666  loss_ce_0: 0.06451  loss_mask_0: 0.1741  loss_dice_0: 0.1661  loss_ce_1: 0.1297  loss_mask_1: 0.166  loss_dice_1: 0.1654  loss_ce_2: 0.1296  loss_mask_2: 0.1614  loss_dice_2: 0.1748  loss_ce_3: 0.1296  loss_mask_3: 0.1662  loss_dice_3: 0.1723  loss_ce_4: 0.1296  loss_mask_4: 0.1751  loss_dice_4: 0.1635  loss_ce_5: 0.1295  loss_mask_5: 0.1634  loss_dice_5: 0.1661  loss_ce_6: 0.1297  loss_mask_6: 0.1692  loss_dice_6: 0.1638  loss_ce_7: 0.1295  loss_mask_7: 0.1684  loss_dice_7: 0.1619  loss_ce_8: 0.1297  loss_mask_8: 0.1599  loss_dice_8: 0.1693  time: 0.5731  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:28:20] d2.utils.events INFO:  eta: 3:03:18  iter: 12619  total_loss: 5.265  loss_ce: 0.1004  loss_mask: 0.2394  loss_dice: 0.146  loss_ce_0: 0.05362  loss_mask_0: 0.2428  loss_dice_0: 0.147  loss_ce_1: 0.1007  loss_mask_1: 0.2352  loss_dice_1: 0.1393  loss_ce_2: 0.1004  loss_mask_2: 0.2356  loss_dice_2: 0.1393  loss_ce_3: 0.1003  loss_mask_3: 0.2376  loss_dice_3: 0.1449  loss_ce_4: 0.1007  loss_mask_4: 0.2399  loss_dice_4: 0.1457  loss_ce_5: 0.1005  loss_mask_5: 0.2408  loss_dice_5: 0.148  loss_ce_6: 0.1006  loss_mask_6: 0.2315  loss_dice_6: 0.144  loss_ce_7: 0.1004  loss_mask_7: 0.24  loss_dice_7: 0.144  loss_ce_8: 0.1004  loss_mask_8: 0.2403  loss_dice_8: 0.1466  time: 0.5732  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:28:32] d2.utils.events INFO:  eta: 3:03:06  iter: 12639  total_loss: 4.609  loss_ce: 0.1297  loss_mask: 0.1648  loss_dice: 0.1494  loss_ce_0: 0.06454  loss_mask_0: 0.1707  loss_dice_0: 0.1494  loss_ce_1: 0.1295  loss_mask_1: 0.1757  loss_dice_1: 0.1482  loss_ce_2: 0.1297  loss_mask_2: 0.175  loss_dice_2: 0.1492  loss_ce_3: 0.1297  loss_mask_3: 0.1718  loss_dice_3: 0.1513  loss_ce_4: 0.1296  loss_mask_4: 0.1726  loss_dice_4: 0.148  loss_ce_5: 0.1296  loss_mask_5: 0.1693  loss_dice_5: 0.1491  loss_ce_6: 0.1295  loss_mask_6: 0.1781  loss_dice_6: 0.1495  loss_ce_7: 0.1296  loss_mask_7: 0.1784  loss_dice_7: 0.1546  loss_ce_8: 0.1297  loss_mask_8: 0.1737  loss_dice_8: 0.1498  time: 0.5733  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:28:45] d2.utils.events INFO:  eta: 3:02:54  iter: 12659  total_loss: 4.454  loss_ce: 0.1295  loss_mask: 0.1777  loss_dice: 0.1346  loss_ce_0: 0.06449  loss_mask_0: 0.1772  loss_dice_0: 0.1388  loss_ce_1: 0.1295  loss_mask_1: 0.1839  loss_dice_1: 0.1385  loss_ce_2: 0.1294  loss_mask_2: 0.1741  loss_dice_2: 0.1348  loss_ce_3: 0.1294  loss_mask_3: 0.1794  loss_dice_3: 0.1335  loss_ce_4: 0.1294  loss_mask_4: 0.188  loss_dice_4: 0.1383  loss_ce_5: 0.1294  loss_mask_5: 0.1806  loss_dice_5: 0.1395  loss_ce_6: 0.1295  loss_mask_6: 0.1832  loss_dice_6: 0.1387  loss_ce_7: 0.1295  loss_mask_7: 0.1834  loss_dice_7: 0.1382  loss_ce_8: 0.1295  loss_mask_8: 0.1808  loss_dice_8: 0.1362  time: 0.5734  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:28:57] d2.utils.events INFO:  eta: 3:02:40  iter: 12679  total_loss: 4.478  loss_ce: 0.1007  loss_mask: 0.2001  loss_dice: 0.1371  loss_ce_0: 0.05367  loss_mask_0: 0.1965  loss_dice_0: 0.1346  loss_ce_1: 0.1011  loss_mask_1: 0.1934  loss_dice_1: 0.1345  loss_ce_2: 0.1008  loss_mask_2: 0.2006  loss_dice_2: 0.1375  loss_ce_3: 0.1007  loss_mask_3: 0.1932  loss_dice_3: 0.1387  loss_ce_4: 0.1011  loss_mask_4: 0.1947  loss_dice_4: 0.1368  loss_ce_5: 0.101  loss_mask_5: 0.1892  loss_dice_5: 0.1368  loss_ce_6: 0.1008  loss_mask_6: 0.2002  loss_dice_6: 0.1376  loss_ce_7: 0.1008  loss_mask_7: 0.1895  loss_dice_7: 0.132  loss_ce_8: 0.1006  loss_mask_8: 0.1954  loss_dice_8: 0.1384  time: 0.5734  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:29:10] d2.utils.events INFO:  eta: 3:02:27  iter: 12699  total_loss: 4.671  loss_ce: 0.09986  loss_mask: 0.1814  loss_dice: 0.1482  loss_ce_0: 0.05349  loss_mask_0: 0.1821  loss_dice_0: 0.1547  loss_ce_1: 0.09977  loss_mask_1: 0.1844  loss_dice_1: 0.1501  loss_ce_2: 0.09971  loss_mask_2: 0.1787  loss_dice_2: 0.151  loss_ce_3: 0.09963  loss_mask_3: 0.1788  loss_dice_3: 0.1552  loss_ce_4: 0.09993  loss_mask_4: 0.1695  loss_dice_4: 0.1489  loss_ce_5: 0.09982  loss_mask_5: 0.1753  loss_dice_5: 0.1491  loss_ce_6: 0.09986  loss_mask_6: 0.1752  loss_dice_6: 0.1485  loss_ce_7: 0.09986  loss_mask_7: 0.1947  loss_dice_7: 0.156  loss_ce_8: 0.09979  loss_mask_8: 0.1837  loss_dice_8: 0.1492  time: 0.5735  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:29:23] d2.utils.events INFO:  eta: 3:02:14  iter: 12719  total_loss: 5.194  loss_ce: 0.09848  loss_mask: 0.1858  loss_dice: 0.1686  loss_ce_0: 0.05312  loss_mask_0: 0.1796  loss_dice_0: 0.1713  loss_ce_1: 0.098  loss_mask_1: 0.1768  loss_dice_1: 0.1743  loss_ce_2: 0.09794  loss_mask_2: 0.18  loss_dice_2: 0.1656  loss_ce_3: 0.09792  loss_mask_3: 0.1816  loss_dice_3: 0.1722  loss_ce_4: 0.09814  loss_mask_4: 0.1901  loss_dice_4: 0.1641  loss_ce_5: 0.09792  loss_mask_5: 0.179  loss_dice_5: 0.1674  loss_ce_6: 0.09803  loss_mask_6: 0.1772  loss_dice_6: 0.1696  loss_ce_7: 0.09811  loss_mask_7: 0.1758  loss_dice_7: 0.1707  loss_ce_8: 0.09829  loss_mask_8: 0.1736  loss_dice_8: 0.1664  time: 0.5736  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:29:35] d2.utils.events INFO:  eta: 3:02:00  iter: 12739  total_loss: 4.593  loss_ce: 0.09755  loss_mask: 0.1419  loss_dice: 0.1448  loss_ce_0: 0.05291  loss_mask_0: 0.1389  loss_dice_0: 0.1455  loss_ce_1: 0.09687  loss_mask_1: 0.1407  loss_dice_1: 0.1429  loss_ce_2: 0.09689  loss_mask_2: 0.1452  loss_dice_2: 0.143  loss_ce_3: 0.09685  loss_mask_3: 0.1497  loss_dice_3: 0.1453  loss_ce_4: 0.0971  loss_mask_4: 0.1408  loss_dice_4: 0.1476  loss_ce_5: 0.09685  loss_mask_5: 0.1464  loss_dice_5: 0.1456  loss_ce_6: 0.09696  loss_mask_6: 0.1463  loss_dice_6: 0.1481  loss_ce_7: 0.09725  loss_mask_7: 0.1347  loss_dice_7: 0.1453  loss_ce_8: 0.09733  loss_mask_8: 0.1432  loss_dice_8: 0.1494  time: 0.5737  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:29:48] d2.utils.events INFO:  eta: 3:01:46  iter: 12759  total_loss: 4.504  loss_ce: 0.09652  loss_mask: 0.1873  loss_dice: 0.139  loss_ce_0: 0.05268  loss_mask_0: 0.1804  loss_dice_0: 0.134  loss_ce_1: 0.09546  loss_mask_1: 0.1853  loss_dice_1: 0.1311  loss_ce_2: 0.09572  loss_mask_2: 0.1846  loss_dice_2: 0.1332  loss_ce_3: 0.09561  loss_mask_3: 0.1797  loss_dice_3: 0.1358  loss_ce_4: 0.09593  loss_mask_4: 0.1813  loss_dice_4: 0.1279  loss_ce_5: 0.09557  loss_mask_5: 0.1773  loss_dice_5: 0.1313  loss_ce_6: 0.09586  loss_mask_6: 0.1832  loss_dice_6: 0.1336  loss_ce_7: 0.09608  loss_mask_7: 0.1826  loss_dice_7: 0.131  loss_ce_8: 0.09623  loss_mask_8: 0.1853  loss_dice_8: 0.1358  time: 0.5738  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:30:01] d2.utils.events INFO:  eta: 3:01:32  iter: 12779  total_loss: 4.743  loss_ce: 0.09652  loss_mask: 0.1479  loss_dice: 0.1565  loss_ce_0: 0.05266  loss_mask_0: 0.1568  loss_dice_0: 0.1648  loss_ce_1: 0.0963  loss_mask_1: 0.146  loss_dice_1: 0.1528  loss_ce_2: 0.09613  loss_mask_2: 0.143  loss_dice_2: 0.1642  loss_ce_3: 0.09604  loss_mask_3: 0.142  loss_dice_3: 0.1621  loss_ce_4: 0.0963  loss_mask_4: 0.1384  loss_dice_4: 0.1549  loss_ce_5: 0.09589  loss_mask_5: 0.1416  loss_dice_5: 0.1582  loss_ce_6: 0.09611  loss_mask_6: 0.1461  loss_dice_6: 0.1573  loss_ce_7: 0.0963  loss_mask_7: 0.1353  loss_dice_7: 0.1583  loss_ce_8: 0.09634  loss_mask_8: 0.1438  loss_dice_8: 0.1648  time: 0.5739  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:30:13] d2.utils.events INFO:  eta: 3:01:21  iter: 12799  total_loss: 4.378  loss_ce: 0.09623  loss_mask: 0.1694  loss_dice: 0.1404  loss_ce_0: 0.05256  loss_mask_0: 0.1717  loss_dice_0: 0.1317  loss_ce_1: 0.0955  loss_mask_1: 0.1654  loss_dice_1: 0.1323  loss_ce_2: 0.09577  loss_mask_2: 0.1759  loss_dice_2: 0.1344  loss_ce_3: 0.09568  loss_mask_3: 0.17  loss_dice_3: 0.1393  loss_ce_4: 0.09582  loss_mask_4: 0.1724  loss_dice_4: 0.1383  loss_ce_5: 0.09553  loss_mask_5: 0.1806  loss_dice_5: 0.1381  loss_ce_6: 0.09579  loss_mask_6: 0.1754  loss_dice_6: 0.1421  loss_ce_7: 0.09601  loss_mask_7: 0.1681  loss_dice_7: 0.1382  loss_ce_8: 0.09597  loss_mask_8: 0.1697  loss_dice_8: 0.1354  time: 0.5740  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:30:26] d2.utils.events INFO:  eta: 3:01:11  iter: 12819  total_loss: 4.561  loss_ce: 0.09532  loss_mask: 0.1883  loss_dice: 0.1493  loss_ce_0: 0.05238  loss_mask_0: 0.1947  loss_dice_0: 0.1478  loss_ce_1: 0.09472  loss_mask_1: 0.1934  loss_dice_1: 0.1499  loss_ce_2: 0.09475  loss_mask_2: 0.2013  loss_dice_2: 0.1492  loss_ce_3: 0.09466  loss_mask_3: 0.189  loss_dice_3: 0.1467  loss_ce_4: 0.09488  loss_mask_4: 0.1842  loss_dice_4: 0.1467  loss_ce_5: 0.09455  loss_mask_5: 0.2026  loss_dice_5: 0.153  loss_ce_6: 0.09481  loss_mask_6: 0.1928  loss_dice_6: 0.1529  loss_ce_7: 0.09506  loss_mask_7: 0.194  loss_dice_7: 0.1447  loss_ce_8: 0.09506  loss_mask_8: 0.1976  loss_dice_8: 0.1515  time: 0.5741  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:30:39] d2.utils.events INFO:  eta: 3:00:51  iter: 12839  total_loss: 4.509  loss_ce: 0.1307  loss_mask: 0.163  loss_dice: 0.1358  loss_ce_0: 0.06472  loss_mask_0: 0.1535  loss_dice_0: 0.134  loss_ce_1: 0.1307  loss_mask_1: 0.1664  loss_dice_1: 0.1431  loss_ce_2: 0.1308  loss_mask_2: 0.1595  loss_dice_2: 0.1393  loss_ce_3: 0.1308  loss_mask_3: 0.1607  loss_dice_3: 0.1366  loss_ce_4: 0.1308  loss_mask_4: 0.1648  loss_dice_4: 0.131  loss_ce_5: 0.131  loss_mask_5: 0.1648  loss_dice_5: 0.1338  loss_ce_6: 0.1308  loss_mask_6: 0.1631  loss_dice_6: 0.1367  loss_ce_7: 0.1309  loss_mask_7: 0.153  loss_dice_7: 0.1322  loss_ce_8: 0.1308  loss_mask_8: 0.1639  loss_dice_8: 0.1431  time: 0.5742  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:30:51] d2.utils.events INFO:  eta: 3:00:34  iter: 12859  total_loss: 4.787  loss_ce: 0.09437  loss_mask: 0.1651  loss_dice: 0.1802  loss_ce_0: 0.05214  loss_mask_0: 0.1621  loss_dice_0: 0.1754  loss_ce_1: 0.09371  loss_mask_1: 0.1634  loss_dice_1: 0.185  loss_ce_2: 0.09387  loss_mask_2: 0.1651  loss_dice_2: 0.1713  loss_ce_3: 0.0938  loss_mask_3: 0.1647  loss_dice_3: 0.1788  loss_ce_4: 0.09398  loss_mask_4: 0.1629  loss_dice_4: 0.1769  loss_ce_5: 0.09369  loss_mask_5: 0.1632  loss_dice_5: 0.1738  loss_ce_6: 0.09391  loss_mask_6: 0.1671  loss_dice_6: 0.1784  loss_ce_7: 0.09416  loss_mask_7: 0.1613  loss_dice_7: 0.1676  loss_ce_8: 0.09412  loss_mask_8: 0.166  loss_dice_8: 0.1785  time: 0.5743  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:31:04] d2.utils.events INFO:  eta: 3:00:26  iter: 12879  total_loss: 4.625  loss_ce: 0.1691  loss_mask: 0.1796  loss_dice: 0.1378  loss_ce_0: 0.07774  loss_mask_0: 0.1794  loss_dice_0: 0.1358  loss_ce_1: 0.1702  loss_mask_1: 0.171  loss_dice_1: 0.1349  loss_ce_2: 0.17  loss_mask_2: 0.1716  loss_dice_2: 0.1299  loss_ce_3: 0.1703  loss_mask_3: 0.176  loss_dice_3: 0.1355  loss_ce_4: 0.17  loss_mask_4: 0.1666  loss_dice_4: 0.1333  loss_ce_5: 0.1705  loss_mask_5: 0.1722  loss_dice_5: 0.1344  loss_ce_6: 0.17  loss_mask_6: 0.1694  loss_dice_6: 0.1306  loss_ce_7: 0.1696  loss_mask_7: 0.1727  loss_dice_7: 0.1329  loss_ce_8: 0.1696  loss_mask_8: 0.1698  loss_dice_8: 0.1334  time: 0.5744  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:31:17] d2.utils.events INFO:  eta: 3:00:13  iter: 12899  total_loss: 4.757  loss_ce: 0.09477  loss_mask: 0.2008  loss_dice: 0.1443  loss_ce_0: 0.05218  loss_mask_0: 0.2063  loss_dice_0: 0.1417  loss_ce_1: 0.09468  loss_mask_1: 0.2063  loss_dice_1: 0.1379  loss_ce_2: 0.09474  loss_mask_2: 0.2098  loss_dice_2: 0.1392  loss_ce_3: 0.09463  loss_mask_3: 0.2105  loss_dice_3: 0.1311  loss_ce_4: 0.09477  loss_mask_4: 0.2033  loss_dice_4: 0.1344  loss_ce_5: 0.09444  loss_mask_5: 0.2023  loss_dice_5: 0.1348  loss_ce_6: 0.09466  loss_mask_6: 0.2006  loss_dice_6: 0.1373  loss_ce_7: 0.09473  loss_mask_7: 0.2013  loss_dice_7: 0.1391  loss_ce_8: 0.09463  loss_mask_8: 0.2045  loss_dice_8: 0.1407  time: 0.5744  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:31:29] d2.utils.events INFO:  eta: 3:00:05  iter: 12919  total_loss: 4.512  loss_ce: 0.09521  loss_mask: 0.2  loss_dice: 0.1365  loss_ce_0: 0.05223  loss_mask_0: 0.2  loss_dice_0: 0.13  loss_ce_1: 0.09535  loss_mask_1: 0.195  loss_dice_1: 0.1411  loss_ce_2: 0.09533  loss_mask_2: 0.21  loss_dice_2: 0.1369  loss_ce_3: 0.0952  loss_mask_3: 0.2049  loss_dice_3: 0.1355  loss_ce_4: 0.09524  loss_mask_4: 0.202  loss_dice_4: 0.1392  loss_ce_5: 0.09506  loss_mask_5: 0.2005  loss_dice_5: 0.1365  loss_ce_6: 0.0951  loss_mask_6: 0.2104  loss_dice_6: 0.1358  loss_ce_7: 0.09524  loss_mask_7: 0.2103  loss_dice_7: 0.1338  loss_ce_8: 0.09506  loss_mask_8: 0.1981  loss_dice_8: 0.1383  time: 0.5745  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:31:42] d2.utils.events INFO:  eta: 2:59:55  iter: 12939  total_loss: 4.579  loss_ce: 0.1661  loss_mask: 0.1207  loss_dice: 0.1597  loss_ce_0: 0.07722  loss_mask_0: 0.1192  loss_dice_0: 0.152  loss_ce_1: 0.1662  loss_mask_1: 0.1242  loss_dice_1: 0.1638  loss_ce_2: 0.1661  loss_mask_2: 0.1238  loss_dice_2: 0.1601  loss_ce_3: 0.1662  loss_mask_3: 0.1223  loss_dice_3: 0.1697  loss_ce_4: 0.166  loss_mask_4: 0.1149  loss_dice_4: 0.1618  loss_ce_5: 0.1665  loss_mask_5: 0.122  loss_dice_5: 0.1563  loss_ce_6: 0.1662  loss_mask_6: 0.1237  loss_dice_6: 0.1534  loss_ce_7: 0.1661  loss_mask_7: 0.1208  loss_dice_7: 0.1581  loss_ce_8: 0.1664  loss_mask_8: 0.1156  loss_dice_8: 0.1625  time: 0.5746  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:31:55] d2.utils.events INFO:  eta: 2:59:42  iter: 12959  total_loss: 4.698  loss_ce: 0.1646  loss_mask: 0.205  loss_dice: 0.122  loss_ce_0: 0.07687  loss_mask_0: 0.2051  loss_dice_0: 0.1257  loss_ce_1: 0.164  loss_mask_1: 0.1988  loss_dice_1: 0.1221  loss_ce_2: 0.1641  loss_mask_2: 0.1985  loss_dice_2: 0.1236  loss_ce_3: 0.1642  loss_mask_3: 0.2035  loss_dice_3: 0.1217  loss_ce_4: 0.1641  loss_mask_4: 0.1974  loss_dice_4: 0.1261  loss_ce_5: 0.1643  loss_mask_5: 0.2073  loss_dice_5: 0.1213  loss_ce_6: 0.1644  loss_mask_6: 0.1955  loss_dice_6: 0.1259  loss_ce_7: 0.1643  loss_mask_7: 0.2047  loss_dice_7: 0.128  loss_ce_8: 0.1643  loss_mask_8: 0.2071  loss_dice_8: 0.1235  time: 0.5747  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:32:07] d2.utils.events INFO:  eta: 2:59:30  iter: 12979  total_loss: 4.488  loss_ce: 0.1299  loss_mask: 0.1636  loss_dice: 0.1479  loss_ce_0: 0.06453  loss_mask_0: 0.1595  loss_dice_0: 0.1419  loss_ce_1: 0.1297  loss_mask_1: 0.156  loss_dice_1: 0.1417  loss_ce_2: 0.1297  loss_mask_2: 0.163  loss_dice_2: 0.1454  loss_ce_3: 0.1297  loss_mask_3: 0.157  loss_dice_3: 0.142  loss_ce_4: 0.1296  loss_mask_4: 0.1595  loss_dice_4: 0.1498  loss_ce_5: 0.1297  loss_mask_5: 0.1605  loss_dice_5: 0.1464  loss_ce_6: 0.1297  loss_mask_6: 0.1615  loss_dice_6: 0.1454  loss_ce_7: 0.1297  loss_mask_7: 0.1571  loss_dice_7: 0.144  loss_ce_8: 0.1298  loss_mask_8: 0.1569  loss_dice_8: 0.1394  time: 0.5748  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:32:20] d2.utils.events INFO:  eta: 2:59:20  iter: 12999  total_loss: 4.879  loss_ce: 0.1588  loss_mask: 0.1955  loss_dice: 0.1568  loss_ce_0: 0.07555  loss_mask_0: 0.1898  loss_dice_0: 0.1516  loss_ce_1: 0.1565  loss_mask_1: 0.1973  loss_dice_1: 0.1558  loss_ce_2: 0.1571  loss_mask_2: 0.1924  loss_dice_2: 0.159  loss_ce_3: 0.1571  loss_mask_3: 0.1923  loss_dice_3: 0.1523  loss_ce_4: 0.1572  loss_mask_4: 0.1968  loss_dice_4: 0.148  loss_ce_5: 0.1568  loss_mask_5: 0.1935  loss_dice_5: 0.1533  loss_ce_6: 0.1574  loss_mask_6: 0.1937  loss_dice_6: 0.1521  loss_ce_7: 0.1579  loss_mask_7: 0.1938  loss_dice_7: 0.1562  loss_ce_8: 0.1585  loss_mask_8: 0.1896  loss_dice_8: 0.1531  time: 0.5749  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:32:33] d2.utils.events INFO:  eta: 2:59:06  iter: 13019  total_loss: 4.301  loss_ce: 0.1575  loss_mask: 0.1501  loss_dice: 0.1371  loss_ce_0: 0.07523  loss_mask_0: 0.1599  loss_dice_0: 0.139  loss_ce_1: 0.1554  loss_mask_1: 0.1642  loss_dice_1: 0.1325  loss_ce_2: 0.1559  loss_mask_2: 0.1511  loss_dice_2: 0.1405  loss_ce_3: 0.1559  loss_mask_3: 0.1637  loss_dice_3: 0.1438  loss_ce_4: 0.1559  loss_mask_4: 0.1506  loss_dice_4: 0.1366  loss_ce_5: 0.1556  loss_mask_5: 0.1568  loss_dice_5: 0.1411  loss_ce_6: 0.1562  loss_mask_6: 0.1592  loss_dice_6: 0.127  loss_ce_7: 0.1566  loss_mask_7: 0.1584  loss_dice_7: 0.1386  loss_ce_8: 0.1573  loss_mask_8: 0.1595  loss_dice_8: 0.1363  time: 0.5750  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:32:45] d2.utils.events INFO:  eta: 2:58:50  iter: 13039  total_loss: 4.834  loss_ce: 0.1027  loss_mask: 0.2352  loss_dice: 0.1564  loss_ce_0: 0.05393  loss_mask_0: 0.2364  loss_dice_0: 0.1581  loss_ce_1: 0.1044  loss_mask_1: 0.2363  loss_dice_1: 0.1597  loss_ce_2: 0.104  loss_mask_2: 0.2371  loss_dice_2: 0.1593  loss_ce_3: 0.104  loss_mask_3: 0.2269  loss_dice_3: 0.1635  loss_ce_4: 0.1041  loss_mask_4: 0.2275  loss_dice_4: 0.1586  loss_ce_5: 0.1043  loss_mask_5: 0.2381  loss_dice_5: 0.1604  loss_ce_6: 0.1038  loss_mask_6: 0.2316  loss_dice_6: 0.1612  loss_ce_7: 0.1034  loss_mask_7: 0.2278  loss_dice_7: 0.1607  loss_ce_8: 0.1029  loss_mask_8: 0.2319  loss_dice_8: 0.16  time: 0.5751  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:32:58] d2.utils.events INFO:  eta: 2:58:39  iter: 13059  total_loss: 4.503  loss_ce: 0.1026  loss_mask: 0.2109  loss_dice: 0.1531  loss_ce_0: 0.05389  loss_mask_0: 0.2027  loss_dice_0: 0.1451  loss_ce_1: 0.104  loss_mask_1: 0.2086  loss_dice_1: 0.1467  loss_ce_2: 0.1036  loss_mask_2: 0.1999  loss_dice_2: 0.1524  loss_ce_3: 0.1036  loss_mask_3: 0.2046  loss_dice_3: 0.1466  loss_ce_4: 0.1037  loss_mask_4: 0.2056  loss_dice_4: 0.1488  loss_ce_5: 0.104  loss_mask_5: 0.204  loss_dice_5: 0.1527  loss_ce_6: 0.1035  loss_mask_6: 0.2086  loss_dice_6: 0.1486  loss_ce_7: 0.1032  loss_mask_7: 0.1982  loss_dice_7: 0.1469  loss_ce_8: 0.1027  loss_mask_8: 0.2068  loss_dice_8: 0.1465  time: 0.5751  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:33:10] d2.utils.events INFO:  eta: 2:58:27  iter: 13079  total_loss: 4.742  loss_ce: 0.1015  loss_mask: 0.1942  loss_dice: 0.1324  loss_ce_0: 0.05364  loss_mask_0: 0.196  loss_dice_0: 0.1247  loss_ce_1: 0.1025  loss_mask_1: 0.2132  loss_dice_1: 0.1276  loss_ce_2: 0.1021  loss_mask_2: 0.2099  loss_dice_2: 0.1226  loss_ce_3: 0.1021  loss_mask_3: 0.1981  loss_dice_3: 0.1259  loss_ce_4: 0.1024  loss_mask_4: 0.1964  loss_dice_4: 0.1218  loss_ce_5: 0.1025  loss_mask_5: 0.1989  loss_dice_5: 0.1279  loss_ce_6: 0.1021  loss_mask_6: 0.2024  loss_dice_6: 0.1301  loss_ce_7: 0.1018  loss_mask_7: 0.2039  loss_dice_7: 0.1308  loss_ce_8: 0.1016  loss_mask_8: 0.206  loss_dice_8: 0.1298  time: 0.5752  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:33:23] d2.utils.events INFO:  eta: 2:58:12  iter: 13099  total_loss: 5.032  loss_ce: 0.101  loss_mask: 0.198  loss_dice: 0.1482  loss_ce_0: 0.0535  loss_mask_0: 0.1923  loss_dice_0: 0.1489  loss_ce_1: 0.1018  loss_mask_1: 0.2021  loss_dice_1: 0.1517  loss_ce_2: 0.1012  loss_mask_2: 0.1928  loss_dice_2: 0.1514  loss_ce_3: 0.1013  loss_mask_3: 0.1859  loss_dice_3: 0.1538  loss_ce_4: 0.1015  loss_mask_4: 0.1918  loss_dice_4: 0.149  loss_ce_5: 0.1016  loss_mask_5: 0.1998  loss_dice_5: 0.1503  loss_ce_6: 0.1014  loss_mask_6: 0.1963  loss_dice_6: 0.1547  loss_ce_7: 0.1011  loss_mask_7: 0.1907  loss_dice_7: 0.1463  loss_ce_8: 0.1011  loss_mask_8: 0.2016  loss_dice_8: 0.1495  time: 0.5753  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:33:36] d2.utils.events INFO:  eta: 2:58:00  iter: 13119  total_loss: 4.223  loss_ce: 0.1575  loss_mask: 0.1373  loss_dice: 0.1432  loss_ce_0: 0.07521  loss_mask_0: 0.1281  loss_dice_0: 0.1588  loss_ce_1: 0.157  loss_mask_1: 0.1273  loss_dice_1: 0.1591  loss_ce_2: 0.1572  loss_mask_2: 0.1363  loss_dice_2: 0.1493  loss_ce_3: 0.1572  loss_mask_3: 0.1406  loss_dice_3: 0.1499  loss_ce_4: 0.1569  loss_mask_4: 0.1252  loss_dice_4: 0.1598  loss_ce_5: 0.1565  loss_mask_5: 0.1305  loss_dice_5: 0.1602  loss_ce_6: 0.1572  loss_mask_6: 0.1272  loss_dice_6: 0.1595  loss_ce_7: 0.1573  loss_mask_7: 0.1285  loss_dice_7: 0.15  loss_ce_8: 0.1576  loss_mask_8: 0.138  loss_dice_8: 0.1582  time: 0.5754  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:33:48] d2.utils.events INFO:  eta: 2:57:46  iter: 13139  total_loss: 4.48  loss_ce: 0.1557  loss_mask: 0.1963  loss_dice: 0.128  loss_ce_0: 0.07476  loss_mask_0: 0.2009  loss_dice_0: 0.1269  loss_ce_1: 0.1542  loss_mask_1: 0.1961  loss_dice_1: 0.1314  loss_ce_2: 0.1549  loss_mask_2: 0.2009  loss_dice_2: 0.1342  loss_ce_3: 0.1549  loss_mask_3: 0.1906  loss_dice_3: 0.13  loss_ce_4: 0.1545  loss_mask_4: 0.1987  loss_dice_4: 0.1305  loss_ce_5: 0.1544  loss_mask_5: 0.2003  loss_dice_5: 0.1314  loss_ce_6: 0.1548  loss_mask_6: 0.1953  loss_dice_6: 0.1274  loss_ce_7: 0.1552  loss_mask_7: 0.202  loss_dice_7: 0.1331  loss_ce_8: 0.1555  loss_mask_8: 0.1978  loss_dice_8: 0.1266  time: 0.5755  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:34:01] d2.utils.events INFO:  eta: 2:57:34  iter: 13159  total_loss: 4.915  loss_ce: 0.1032  loss_mask: 0.2058  loss_dice: 0.1418  loss_ce_0: 0.05408  loss_mask_0: 0.203  loss_dice_0: 0.1364  loss_ce_1: 0.1042  loss_mask_1: 0.1957  loss_dice_1: 0.1381  loss_ce_2: 0.1038  loss_mask_2: 0.2055  loss_dice_2: 0.138  loss_ce_3: 0.1038  loss_mask_3: 0.1999  loss_dice_3: 0.142  loss_ce_4: 0.1041  loss_mask_4: 0.2151  loss_dice_4: 0.1426  loss_ce_5: 0.1042  loss_mask_5: 0.2052  loss_dice_5: 0.1404  loss_ce_6: 0.1039  loss_mask_6: 0.2093  loss_dice_6: 0.1368  loss_ce_7: 0.1036  loss_mask_7: 0.2029  loss_dice_7: 0.1396  loss_ce_8: 0.1034  loss_mask_8: 0.2065  loss_dice_8: 0.1429  time: 0.5756  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:34:13] d2.utils.events INFO:  eta: 2:57:20  iter: 13179  total_loss: 4.566  loss_ce: 0.1551  loss_mask: 0.2036  loss_dice: 0.1258  loss_ce_0: 0.07457  loss_mask_0: 0.2059  loss_dice_0: 0.1274  loss_ce_1: 0.154  loss_mask_1: 0.2036  loss_dice_1: 0.1197  loss_ce_2: 0.1544  loss_mask_2: 0.2033  loss_dice_2: 0.1235  loss_ce_3: 0.1544  loss_mask_3: 0.2057  loss_dice_3: 0.123  loss_ce_4: 0.1541  loss_mask_4: 0.2062  loss_dice_4: 0.1303  loss_ce_5: 0.1538  loss_mask_5: 0.2042  loss_dice_5: 0.1248  loss_ce_6: 0.1543  loss_mask_6: 0.2108  loss_dice_6: 0.1225  loss_ce_7: 0.1547  loss_mask_7: 0.2004  loss_dice_7: 0.1257  loss_ce_8: 0.1549  loss_mask_8: 0.1962  loss_dice_8: 0.127  time: 0.5756  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:34:25] d2.utils.events INFO:  eta: 2:56:55  iter: 13199  total_loss: 4.954  loss_ce: 0.1035  loss_mask: 0.1998  loss_dice: 0.166  loss_ce_0: 0.05414  loss_mask_0: 0.2049  loss_dice_0: 0.1686  loss_ce_1: 0.1046  loss_mask_1: 0.2059  loss_dice_1: 0.167  loss_ce_2: 0.1041  loss_mask_2: 0.2047  loss_dice_2: 0.1613  loss_ce_3: 0.104  loss_mask_3: 0.1847  loss_dice_3: 0.1625  loss_ce_4: 0.1043  loss_mask_4: 0.2028  loss_dice_4: 0.1702  loss_ce_5: 0.1045  loss_mask_5: 0.1941  loss_dice_5: 0.168  loss_ce_6: 0.1041  loss_mask_6: 0.194  loss_dice_6: 0.1638  loss_ce_7: 0.1038  loss_mask_7: 0.2063  loss_dice_7: 0.1672  loss_ce_8: 0.1037  loss_mask_8: 0.207  loss_dice_8: 0.1609  time: 0.5756  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:34:36] d2.utils.events INFO:  eta: 2:56:36  iter: 13219  total_loss: 4.348  loss_ce: 0.1034  loss_mask: 0.1864  loss_dice: 0.1248  loss_ce_0: 0.05415  loss_mask_0: 0.1768  loss_dice_0: 0.1252  loss_ce_1: 0.1041  loss_mask_1: 0.1896  loss_dice_1: 0.1279  loss_ce_2: 0.1037  loss_mask_2: 0.1832  loss_dice_2: 0.124  loss_ce_3: 0.1037  loss_mask_3: 0.1811  loss_dice_3: 0.1217  loss_ce_4: 0.104  loss_mask_4: 0.185  loss_dice_4: 0.12  loss_ce_5: 0.1042  loss_mask_5: 0.1807  loss_dice_5: 0.1269  loss_ce_6: 0.1038  loss_mask_6: 0.185  loss_dice_6: 0.1263  loss_ce_7: 0.1036  loss_mask_7: 0.1835  loss_dice_7: 0.1221  loss_ce_8: 0.1035  loss_mask_8: 0.1814  loss_dice_8: 0.1306  time: 0.5756  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:34:48] d2.utils.events INFO:  eta: 2:56:11  iter: 13239  total_loss: 4.726  loss_ce: 0.1031  loss_mask: 0.1492  loss_dice: 0.1637  loss_ce_0: 0.05407  loss_mask_0: 0.1601  loss_dice_0: 0.1668  loss_ce_1: 0.1033  loss_mask_1: 0.1646  loss_dice_1: 0.1662  loss_ce_2: 0.1033  loss_mask_2: 0.1599  loss_dice_2: 0.1639  loss_ce_3: 0.1033  loss_mask_3: 0.1625  loss_dice_3: 0.1697  loss_ce_4: 0.1033  loss_mask_4: 0.1601  loss_dice_4: 0.1673  loss_ce_5: 0.1037  loss_mask_5: 0.1535  loss_dice_5: 0.1601  loss_ce_6: 0.1034  loss_mask_6: 0.1647  loss_dice_6: 0.1698  loss_ce_7: 0.103  loss_mask_7: 0.17  loss_dice_7: 0.1637  loss_ce_8: 0.1032  loss_mask_8: 0.1666  loss_dice_8: 0.1625  time: 0.5756  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:34:59] d2.utils.events INFO:  eta: 2:55:44  iter: 13259  total_loss: 4.608  loss_ce: 0.1015  loss_mask: 0.1842  loss_dice: 0.1589  loss_ce_0: 0.05371  loss_mask_0: 0.1679  loss_dice_0: 0.1519  loss_ce_1: 0.1015  loss_mask_1: 0.169  loss_dice_1: 0.1543  loss_ce_2: 0.1012  loss_mask_2: 0.1751  loss_dice_2: 0.1551  loss_ce_3: 0.1013  loss_mask_3: 0.1728  loss_dice_3: 0.1578  loss_ce_4: 0.1016  loss_mask_4: 0.1742  loss_dice_4: 0.1612  loss_ce_5: 0.1016  loss_mask_5: 0.1827  loss_dice_5: 0.1585  loss_ce_6: 0.1014  loss_mask_6: 0.1713  loss_dice_6: 0.1544  loss_ce_7: 0.1014  loss_mask_7: 0.1719  loss_dice_7: 0.1561  loss_ce_8: 0.1015  loss_mask_8: 0.1705  loss_dice_8: 0.1539  time: 0.5756  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:35:10] d2.utils.events INFO:  eta: 2:55:19  iter: 13279  total_loss: 4.646  loss_ce: 0.1011  loss_mask: 0.174  loss_dice: 0.1418  loss_ce_0: 0.05362  loss_mask_0: 0.1758  loss_dice_0: 0.1355  loss_ce_1: 0.1007  loss_mask_1: 0.1685  loss_dice_1: 0.1369  loss_ce_2: 0.1006  loss_mask_2: 0.1762  loss_dice_2: 0.1384  loss_ce_3: 0.1007  loss_mask_3: 0.1826  loss_dice_3: 0.135  loss_ce_4: 0.1009  loss_mask_4: 0.1793  loss_dice_4: 0.1384  loss_ce_5: 0.101  loss_mask_5: 0.1783  loss_dice_5: 0.1349  loss_ce_6: 0.1009  loss_mask_6: 0.1753  loss_dice_6: 0.1347  loss_ce_7: 0.1009  loss_mask_7: 0.1758  loss_dice_7: 0.1393  loss_ce_8: 0.1011  loss_mask_8: 0.1759  loss_dice_8: 0.1332  time: 0.5756  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:35:22] d2.utils.events INFO:  eta: 2:55:01  iter: 13299  total_loss: 4.693  loss_ce: 0.1003  loss_mask: 0.185  loss_dice: 0.1443  loss_ce_0: 0.05347  loss_mask_0: 0.1823  loss_dice_0: 0.1352  loss_ce_1: 0.1002  loss_mask_1: 0.1951  loss_dice_1: 0.1459  loss_ce_2: 0.09995  loss_mask_2: 0.1903  loss_dice_2: 0.1477  loss_ce_3: 0.09992  loss_mask_3: 0.1848  loss_dice_3: 0.1492  loss_ce_4: 0.1004  loss_mask_4: 0.1894  loss_dice_4: 0.153  loss_ce_5: 0.1001  loss_mask_5: 0.1929  loss_dice_5: 0.1459  loss_ce_6: 0.1002  loss_mask_6: 0.1829  loss_dice_6: 0.1367  loss_ce_7: 0.1002  loss_mask_7: 0.1848  loss_dice_7: 0.1491  loss_ce_8: 0.1003  loss_mask_8: 0.1869  loss_dice_8: 0.1373  time: 0.5756  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:35:33] d2.utils.events INFO:  eta: 2:54:39  iter: 13319  total_loss: 4.853  loss_ce: 0.1005  loss_mask: 0.1852  loss_dice: 0.1496  loss_ce_0: 0.05352  loss_mask_0: 0.1821  loss_dice_0: 0.1488  loss_ce_1: 0.1003  loss_mask_1: 0.1876  loss_dice_1: 0.147  loss_ce_2: 0.1002  loss_mask_2: 0.1891  loss_dice_2: 0.1457  loss_ce_3: 0.1003  loss_mask_3: 0.1896  loss_dice_3: 0.154  loss_ce_4: 0.1005  loss_mask_4: 0.1846  loss_dice_4: 0.1497  loss_ce_5: 0.1003  loss_mask_5: 0.1899  loss_dice_5: 0.1494  loss_ce_6: 0.1004  loss_mask_6: 0.1933  loss_dice_6: 0.1555  loss_ce_7: 0.1004  loss_mask_7: 0.1986  loss_dice_7: 0.1466  loss_ce_8: 0.1003  loss_mask_8: 0.1867  loss_dice_8: 0.1505  time: 0.5756  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:35:45] d2.utils.events INFO:  eta: 2:54:15  iter: 13339  total_loss: 4.648  loss_ce: 0.1594  loss_mask: 0.17  loss_dice: 0.1454  loss_ce_0: 0.07543  loss_mask_0: 0.1548  loss_dice_0: 0.1417  loss_ce_1: 0.1598  loss_mask_1: 0.1635  loss_dice_1: 0.1504  loss_ce_2: 0.1598  loss_mask_2: 0.1582  loss_dice_2: 0.142  loss_ce_3: 0.1599  loss_mask_3: 0.1763  loss_dice_3: 0.1514  loss_ce_4: 0.1596  loss_mask_4: 0.1617  loss_dice_4: 0.1502  loss_ce_5: 0.1596  loss_mask_5: 0.1649  loss_dice_5: 0.1446  loss_ce_6: 0.1598  loss_mask_6: 0.1665  loss_dice_6: 0.1461  loss_ce_7: 0.1597  loss_mask_7: 0.151  loss_dice_7: 0.1472  loss_ce_8: 0.1596  loss_mask_8: 0.1572  loss_dice_8: 0.1524  time: 0.5756  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:35:56] d2.utils.events INFO:  eta: 2:53:43  iter: 13359  total_loss: 4.598  loss_ce: 0.09982  loss_mask: 0.2014  loss_dice: 0.1679  loss_ce_0: 0.05335  loss_mask_0: 0.2106  loss_dice_0: 0.1639  loss_ce_1: 0.09956  loss_mask_1: 0.1963  loss_dice_1: 0.1607  loss_ce_2: 0.09952  loss_mask_2: 0.1976  loss_dice_2: 0.17  loss_ce_3: 0.09948  loss_mask_3: 0.1995  loss_dice_3: 0.1692  loss_ce_4: 0.09974  loss_mask_4: 0.1958  loss_dice_4: 0.1699  loss_ce_5: 0.09959  loss_mask_5: 0.1994  loss_dice_5: 0.1649  loss_ce_6: 0.09959  loss_mask_6: 0.1964  loss_dice_6: 0.1656  loss_ce_7: 0.09963  loss_mask_7: 0.1946  loss_dice_7: 0.1698  loss_ce_8: 0.09967  loss_mask_8: 0.2055  loss_dice_8: 0.1615  time: 0.5756  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:36:08] d2.utils.events INFO:  eta: 2:53:12  iter: 13379  total_loss: 4.521  loss_ce: 0.09814  loss_mask: 0.1663  loss_dice: 0.1551  loss_ce_0: 0.05296  loss_mask_0: 0.1572  loss_dice_0: 0.1451  loss_ce_1: 0.09758  loss_mask_1: 0.1643  loss_dice_1: 0.1507  loss_ce_2: 0.09743  loss_mask_2: 0.164  loss_dice_2: 0.154  loss_ce_3: 0.0975  loss_mask_3: 0.1655  loss_dice_3: 0.1544  loss_ce_4: 0.09772  loss_mask_4: 0.1539  loss_dice_4: 0.1569  loss_ce_5: 0.09732  loss_mask_5: 0.1655  loss_dice_5: 0.1577  loss_ce_6: 0.09751  loss_mask_6: 0.1577  loss_dice_6: 0.1494  loss_ce_7: 0.09791  loss_mask_7: 0.158  loss_dice_7: 0.1524  loss_ce_8: 0.09797  loss_mask_8: 0.154  loss_dice_8: 0.1464  time: 0.5755  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:36:19] d2.utils.events INFO:  eta: 2:52:52  iter: 13399  total_loss: 4.304  loss_ce: 0.1624  loss_mask: 0.1917  loss_dice: 0.119  loss_ce_0: 0.07607  loss_mask_0: 0.1906  loss_dice_0: 0.1257  loss_ce_1: 0.1632  loss_mask_1: 0.1864  loss_dice_1: 0.1279  loss_ce_2: 0.1631  loss_mask_2: 0.2006  loss_dice_2: 0.1219  loss_ce_3: 0.163  loss_mask_3: 0.2011  loss_dice_3: 0.1252  loss_ce_4: 0.1628  loss_mask_4: 0.1925  loss_dice_4: 0.124  loss_ce_5: 0.1633  loss_mask_5: 0.1804  loss_dice_5: 0.1213  loss_ce_6: 0.163  loss_mask_6: 0.1806  loss_dice_6: 0.1238  loss_ce_7: 0.1626  loss_mask_7: 0.1891  loss_dice_7: 0.1233  loss_ce_8: 0.1624  loss_mask_8: 0.1911  loss_dice_8: 0.1221  time: 0.5755  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:36:30] d2.utils.events INFO:  eta: 2:52:18  iter: 13419  total_loss: 4.513  loss_ce: 0.1299  loss_mask: 0.1698  loss_dice: 0.1476  loss_ce_0: 0.06441  loss_mask_0: 0.1694  loss_dice_0: 0.1388  loss_ce_1: 0.1299  loss_mask_1: 0.17  loss_dice_1: 0.1441  loss_ce_2: 0.13  loss_mask_2: 0.1742  loss_dice_2: 0.1496  loss_ce_3: 0.1299  loss_mask_3: 0.1672  loss_dice_3: 0.1434  loss_ce_4: 0.1299  loss_mask_4: 0.1732  loss_dice_4: 0.1464  loss_ce_5: 0.1302  loss_mask_5: 0.1674  loss_dice_5: 0.144  loss_ce_6: 0.13  loss_mask_6: 0.1649  loss_dice_6: 0.1448  loss_ce_7: 0.13  loss_mask_7: 0.1671  loss_dice_7: 0.1476  loss_ce_8: 0.13  loss_mask_8: 0.1654  loss_dice_8: 0.1438  time: 0.5755  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:36:42] d2.utils.events INFO:  eta: 2:51:48  iter: 13439  total_loss: 4.242  loss_ce: 0.09884  loss_mask: 0.141  loss_dice: 0.1464  loss_ce_0: 0.05314  loss_mask_0: 0.1483  loss_dice_0: 0.1511  loss_ce_1: 0.09884  loss_mask_1: 0.1384  loss_dice_1: 0.1405  loss_ce_2: 0.09877  loss_mask_2: 0.1396  loss_dice_2: 0.1464  loss_ce_3: 0.09877  loss_mask_3: 0.1411  loss_dice_3: 0.1411  loss_ce_4: 0.09891  loss_mask_4: 0.1446  loss_dice_4: 0.15  loss_ce_5: 0.09884  loss_mask_5: 0.1406  loss_dice_5: 0.1421  loss_ce_6: 0.09873  loss_mask_6: 0.1451  loss_dice_6: 0.1471  loss_ce_7: 0.09884  loss_mask_7: 0.1484  loss_dice_7: 0.1422  loss_ce_8: 0.09866  loss_mask_8: 0.141  loss_dice_8: 0.1404  time: 0.5755  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:36:53] d2.utils.events INFO:  eta: 2:51:17  iter: 13459  total_loss: 4.485  loss_ce: 0.09766  loss_mask: 0.1731  loss_dice: 0.1307  loss_ce_0: 0.05284  loss_mask_0: 0.1731  loss_dice_0: 0.1337  loss_ce_1: 0.09721  loss_mask_1: 0.1716  loss_dice_1: 0.1343  loss_ce_2: 0.09734  loss_mask_2: 0.1747  loss_dice_2: 0.1411  loss_ce_3: 0.09732  loss_mask_3: 0.1712  loss_dice_3: 0.1306  loss_ce_4: 0.09739  loss_mask_4: 0.1729  loss_dice_4: 0.1371  loss_ce_5: 0.0971  loss_mask_5: 0.172  loss_dice_5: 0.1354  loss_ce_6: 0.09725  loss_mask_6: 0.1788  loss_dice_6: 0.1342  loss_ce_7: 0.09751  loss_mask_7: 0.1705  loss_dice_7: 0.1315  loss_ce_8: 0.09754  loss_mask_8: 0.1721  loss_dice_8: 0.1333  time: 0.5755  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:37:05] d2.utils.events INFO:  eta: 2:50:36  iter: 13479  total_loss: 4.887  loss_ce: 0.09673  loss_mask: 0.2245  loss_dice: 0.1384  loss_ce_0: 0.05258  loss_mask_0: 0.2201  loss_dice_0: 0.1418  loss_ce_1: 0.09628  loss_mask_1: 0.2207  loss_dice_1: 0.1384  loss_ce_2: 0.09629  loss_mask_2: 0.217  loss_dice_2: 0.1409  loss_ce_3: 0.09626  loss_mask_3: 0.2236  loss_dice_3: 0.1376  loss_ce_4: 0.09644  loss_mask_4: 0.2208  loss_dice_4: 0.1377  loss_ce_5: 0.09611  loss_mask_5: 0.211  loss_dice_5: 0.141  loss_ce_6: 0.09629  loss_mask_6: 0.2199  loss_dice_6: 0.1425  loss_ce_7: 0.09655  loss_mask_7: 0.2243  loss_dice_7: 0.1385  loss_ce_8: 0.09655  loss_mask_8: 0.2177  loss_dice_8: 0.1427  time: 0.5755  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:37:16] d2.utils.events INFO:  eta: 2:50:05  iter: 13499  total_loss: 4.392  loss_ce: 0.09644  loss_mask: 0.2006  loss_dice: 0.1252  loss_ce_0: 0.05251  loss_mask_0: 0.1908  loss_dice_0: 0.1243  loss_ce_1: 0.09631  loss_mask_1: 0.1943  loss_dice_1: 0.1254  loss_ce_2: 0.09609  loss_mask_2: 0.1885  loss_dice_2: 0.1219  loss_ce_3: 0.09603  loss_mask_3: 0.2048  loss_dice_3: 0.128  loss_ce_4: 0.09625  loss_mask_4: 0.1972  loss_dice_4: 0.1235  loss_ce_5: 0.09596  loss_mask_5: 0.201  loss_dice_5: 0.1254  loss_ce_6: 0.09607  loss_mask_6: 0.1833  loss_dice_6: 0.129  loss_ce_7: 0.09633  loss_mask_7: 0.1883  loss_dice_7: 0.1242  loss_ce_8: 0.09629  loss_mask_8: 0.2009  loss_dice_8: 0.1219  time: 0.5755  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:37:28] d2.utils.events INFO:  eta: 2:49:24  iter: 13519  total_loss: 4.662  loss_ce: 0.09633  loss_mask: 0.1849  loss_dice: 0.1457  loss_ce_0: 0.05248  loss_mask_0: 0.1899  loss_dice_0: 0.1457  loss_ce_1: 0.09617  loss_mask_1: 0.1839  loss_dice_1: 0.1439  loss_ce_2: 0.0962  loss_mask_2: 0.1829  loss_dice_2: 0.1478  loss_ce_3: 0.09611  loss_mask_3: 0.1838  loss_dice_3: 0.1491  loss_ce_4: 0.09618  loss_mask_4: 0.1822  loss_dice_4: 0.1444  loss_ce_5: 0.096  loss_mask_5: 0.1775  loss_dice_5: 0.1487  loss_ce_6: 0.09607  loss_mask_6: 0.1906  loss_dice_6: 0.1416  loss_ce_7: 0.09622  loss_mask_7: 0.1875  loss_dice_7: 0.1473  loss_ce_8: 0.09618  loss_mask_8: 0.1806  loss_dice_8: 0.1431  time: 0.5755  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:37:39] d2.utils.events INFO:  eta: 2:48:56  iter: 13539  total_loss: 4.592  loss_ce: 0.0964  loss_mask: 0.2013  loss_dice: 0.1537  loss_ce_0: 0.05247  loss_mask_0: 0.1844  loss_dice_0: 0.1548  loss_ce_1: 0.09622  loss_mask_1: 0.1912  loss_dice_1: 0.1562  loss_ce_2: 0.0962  loss_mask_2: 0.1999  loss_dice_2: 0.1482  loss_ce_3: 0.09618  loss_mask_3: 0.1953  loss_dice_3: 0.1544  loss_ce_4: 0.09625  loss_mask_4: 0.2026  loss_dice_4: 0.1544  loss_ce_5: 0.09603  loss_mask_5: 0.1906  loss_dice_5: 0.1544  loss_ce_6: 0.09615  loss_mask_6: 0.1972  loss_dice_6: 0.1518  loss_ce_7: 0.09633  loss_mask_7: 0.1905  loss_dice_7: 0.1632  loss_ce_8: 0.09622  loss_mask_8: 0.1996  loss_dice_8: 0.1525  time: 0.5755  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:37:50] d2.utils.events INFO:  eta: 2:48:09  iter: 13559  total_loss: 4.66  loss_ce: 0.09604  loss_mask: 0.1899  loss_dice: 0.1421  loss_ce_0: 0.05237  loss_mask_0: 0.1945  loss_dice_0: 0.1413  loss_ce_1: 0.09589  loss_mask_1: 0.1964  loss_dice_1: 0.1373  loss_ce_2: 0.09585  loss_mask_2: 0.198  loss_dice_2: 0.1422  loss_ce_3: 0.09578  loss_mask_3: 0.1989  loss_dice_3: 0.1459  loss_ce_4: 0.09589  loss_mask_4: 0.1978  loss_dice_4: 0.1458  loss_ce_5: 0.09567  loss_mask_5: 0.1959  loss_dice_5: 0.1404  loss_ce_6: 0.09582  loss_mask_6: 0.2006  loss_dice_6: 0.1464  loss_ce_7: 0.096  loss_mask_7: 0.1889  loss_dice_7: 0.149  loss_ce_8: 0.09589  loss_mask_8: 0.1904  loss_dice_8: 0.1342  time: 0.5755  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:38:02] d2.utils.events INFO:  eta: 2:47:25  iter: 13579  total_loss: 4.222  loss_ce: 0.1645  loss_mask: 0.1424  loss_dice: 0.1385  loss_ce_0: 0.07662  loss_mask_0: 0.1505  loss_dice_0: 0.1355  loss_ce_1: 0.1643  loss_mask_1: 0.146  loss_dice_1: 0.1413  loss_ce_2: 0.1646  loss_mask_2: 0.1515  loss_dice_2: 0.1363  loss_ce_3: 0.1646  loss_mask_3: 0.1434  loss_dice_3: 0.1398  loss_ce_4: 0.1645  loss_mask_4: 0.1397  loss_dice_4: 0.1394  loss_ce_5: 0.1648  loss_mask_5: 0.1425  loss_dice_5: 0.1408  loss_ce_6: 0.1647  loss_mask_6: 0.142  loss_dice_6: 0.1369  loss_ce_7: 0.1644  loss_mask_7: 0.1443  loss_dice_7: 0.1311  loss_ce_8: 0.1647  loss_mask_8: 0.1462  loss_dice_8: 0.1381  time: 0.5755  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:38:13] d2.utils.events INFO:  eta: 2:46:44  iter: 13599  total_loss: 4.655  loss_ce: 0.1611  loss_mask: 0.1921  loss_dice: 0.1383  loss_ce_0: 0.07584  loss_mask_0: 0.1984  loss_dice_0: 0.1354  loss_ce_1: 0.1601  loss_mask_1: 0.1978  loss_dice_1: 0.1379  loss_ce_2: 0.1605  loss_mask_2: 0.1974  loss_dice_2: 0.1337  loss_ce_3: 0.1604  loss_mask_3: 0.2031  loss_dice_3: 0.1345  loss_ce_4: 0.1602  loss_mask_4: 0.1853  loss_dice_4: 0.1342  loss_ce_5: 0.1601  loss_mask_5: 0.1956  loss_dice_5: 0.1384  loss_ce_6: 0.1605  loss_mask_6: 0.1981  loss_dice_6: 0.1394  loss_ce_7: 0.1605  loss_mask_7: 0.1908  loss_dice_7: 0.136  loss_ce_8: 0.1612  loss_mask_8: 0.1906  loss_dice_8: 0.1302  time: 0.5755  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:38:25] d2.utils.events INFO:  eta: 2:46:00  iter: 13619  total_loss: 4.579  loss_ce: 0.09884  loss_mask: 0.1756  loss_dice: 0.1617  loss_ce_0: 0.05302  loss_mask_0: 0.1815  loss_dice_0: 0.1644  loss_ce_1: 0.09951  loss_mask_1: 0.1868  loss_dice_1: 0.174  loss_ce_2: 0.09925  loss_mask_2: 0.1876  loss_dice_2: 0.1636  loss_ce_3: 0.09929  loss_mask_3: 0.1832  loss_dice_3: 0.1601  loss_ce_4: 0.0994  loss_mask_4: 0.1855  loss_dice_4: 0.1676  loss_ce_5: 0.09944  loss_mask_5: 0.1774  loss_dice_5: 0.163  loss_ce_6: 0.09921  loss_mask_6: 0.1769  loss_dice_6: 0.1666  loss_ce_7: 0.09918  loss_mask_7: 0.1904  loss_dice_7: 0.1616  loss_ce_8: 0.09877  loss_mask_8: 0.1872  loss_dice_8: 0.166  time: 0.5754  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:38:35] d2.utils.events INFO:  eta: 2:45:20  iter: 13639  total_loss: 4.555  loss_ce: 0.1299  loss_mask: 0.1687  loss_dice: 0.1336  loss_ce_0: 0.06442  loss_mask_0: 0.1784  loss_dice_0: 0.1345  loss_ce_1: 0.13  loss_mask_1: 0.1755  loss_dice_1: 0.1386  loss_ce_2: 0.1299  loss_mask_2: 0.1574  loss_dice_2: 0.1315  loss_ce_3: 0.1299  loss_mask_3: 0.1653  loss_dice_3: 0.132  loss_ce_4: 0.1299  loss_mask_4: 0.168  loss_dice_4: 0.1333  loss_ce_5: 0.1299  loss_mask_5: 0.1676  loss_dice_5: 0.132  loss_ce_6: 0.13  loss_mask_6: 0.1746  loss_dice_6: 0.1336  loss_ce_7: 0.1299  loss_mask_7: 0.1784  loss_dice_7: 0.1376  loss_ce_8: 0.13  loss_mask_8: 0.1667  loss_dice_8: 0.1279  time: 0.5754  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:38:45] d2.utils.events INFO:  eta: 2:44:35  iter: 13659  total_loss: 4.507  loss_ce: 0.09921  loss_mask: 0.1866  loss_dice: 0.1391  loss_ce_0: 0.05308  loss_mask_0: 0.189  loss_dice_0: 0.1443  loss_ce_1: 0.09981  loss_mask_1: 0.1851  loss_dice_1: 0.1418  loss_ce_2: 0.09964  loss_mask_2: 0.1969  loss_dice_2: 0.1392  loss_ce_3: 0.09962  loss_mask_3: 0.1877  loss_dice_3: 0.1416  loss_ce_4: 0.09977  loss_mask_4: 0.184  loss_dice_4: 0.1332  loss_ce_5: 0.09981  loss_mask_5: 0.1877  loss_dice_5: 0.1409  loss_ce_6: 0.09955  loss_mask_6: 0.1874  loss_dice_6: 0.1419  loss_ce_7: 0.09955  loss_mask_7: 0.1776  loss_dice_7: 0.1516  loss_ce_8: 0.09914  loss_mask_8: 0.1909  loss_dice_8: 0.141  time: 0.5753  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:38:56] d2.utils.events INFO:  eta: 2:43:57  iter: 13679  total_loss: 4.736  loss_ce: 0.09839  loss_mask: 0.2171  loss_dice: 0.1433  loss_ce_0: 0.05289  loss_mask_0: 0.2106  loss_dice_0: 0.142  loss_ce_1: 0.09871  loss_mask_1: 0.2167  loss_dice_1: 0.1459  loss_ce_2: 0.0985  loss_mask_2: 0.2179  loss_dice_2: 0.144  loss_ce_3: 0.09854  loss_mask_3: 0.2206  loss_dice_3: 0.1425  loss_ce_4: 0.0988  loss_mask_4: 0.2128  loss_dice_4: 0.1486  loss_ce_5: 0.09884  loss_mask_5: 0.2102  loss_dice_5: 0.1422  loss_ce_6: 0.09846  loss_mask_6: 0.2181  loss_dice_6: 0.1476  loss_ce_7: 0.09869  loss_mask_7: 0.2183  loss_dice_7: 0.1389  loss_ce_8: 0.09835  loss_mask_8: 0.2232  loss_dice_8: 0.1377  time: 0.5752  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:39:06] d2.utils.events INFO:  eta: 2:43:08  iter: 13699  total_loss: 5.113  loss_ce: 0.1623  loss_mask: 0.1903  loss_dice: 0.1434  loss_ce_0: 0.0762  loss_mask_0: 0.1895  loss_dice_0: 0.1356  loss_ce_1: 0.162  loss_mask_1: 0.1808  loss_dice_1: 0.1385  loss_ce_2: 0.1623  loss_mask_2: 0.1954  loss_dice_2: 0.1384  loss_ce_3: 0.1622  loss_mask_3: 0.1969  loss_dice_3: 0.1434  loss_ce_4: 0.1621  loss_mask_4: 0.184  loss_dice_4: 0.1344  loss_ce_5: 0.1621  loss_mask_5: 0.1811  loss_dice_5: 0.1334  loss_ce_6: 0.1624  loss_mask_6: 0.1943  loss_dice_6: 0.1399  loss_ce_7: 0.1622  loss_mask_7: 0.1835  loss_dice_7: 0.1435  loss_ce_8: 0.1624  loss_mask_8: 0.1935  loss_dice_8: 0.1393  time: 0.5751  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:39:16] d2.utils.events INFO:  eta: 2:42:21  iter: 13719  total_loss: 4.973  loss_ce: 0.09776  loss_mask: 0.17  loss_dice: 0.1697  loss_ce_0: 0.05267  loss_mask_0: 0.1701  loss_dice_0: 0.1627  loss_ce_1: 0.09774  loss_mask_1: 0.1659  loss_dice_1: 0.1695  loss_ce_2: 0.09776  loss_mask_2: 0.1692  loss_dice_2: 0.1699  loss_ce_3: 0.09779  loss_mask_3: 0.1677  loss_dice_3: 0.1703  loss_ce_4: 0.09776  loss_mask_4: 0.1542  loss_dice_4: 0.1651  loss_ce_5: 0.09787  loss_mask_5: 0.1713  loss_dice_5: 0.1641  loss_ce_6: 0.09765  loss_mask_6: 0.1681  loss_dice_6: 0.1638  loss_ce_7: 0.09787  loss_mask_7: 0.1767  loss_dice_7: 0.1597  loss_ce_8: 0.09765  loss_mask_8: 0.1733  loss_dice_8: 0.1635  time: 0.5750  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:39:27] d2.utils.events INFO:  eta: 2:41:37  iter: 13739  total_loss: 4.533  loss_ce: 0.09688  loss_mask: 0.1561  loss_dice: 0.1385  loss_ce_0: 0.05248  loss_mask_0: 0.1572  loss_dice_0: 0.1466  loss_ce_1: 0.09699  loss_mask_1: 0.1577  loss_dice_1: 0.1469  loss_ce_2: 0.09676  loss_mask_2: 0.1659  loss_dice_2: 0.1452  loss_ce_3: 0.09676  loss_mask_3: 0.1565  loss_dice_3: 0.1456  loss_ce_4: 0.09695  loss_mask_4: 0.1565  loss_dice_4: 0.1451  loss_ce_5: 0.09665  loss_mask_5: 0.1614  loss_dice_5: 0.146  loss_ce_6: 0.09669  loss_mask_6: 0.1632  loss_dice_6: 0.1426  loss_ce_7: 0.09684  loss_mask_7: 0.161  loss_dice_7: 0.1457  loss_ce_8: 0.0968  loss_mask_8: 0.1604  loss_dice_8: 0.1431  time: 0.5749  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:39:37] d2.utils.events INFO:  eta: 2:40:35  iter: 13759  total_loss: 4.744  loss_ce: 0.1636  loss_mask: 0.1709  loss_dice: 0.1321  loss_ce_0: 0.07646  loss_mask_0: 0.1627  loss_dice_0: 0.1291  loss_ce_1: 0.1638  loss_mask_1: 0.1694  loss_dice_1: 0.1339  loss_ce_2: 0.1638  loss_mask_2: 0.1657  loss_dice_2: 0.1274  loss_ce_3: 0.1637  loss_mask_3: 0.1608  loss_dice_3: 0.1295  loss_ce_4: 0.1638  loss_mask_4: 0.1666  loss_dice_4: 0.1318  loss_ce_5: 0.1636  loss_mask_5: 0.1678  loss_dice_5: 0.1335  loss_ce_6: 0.1638  loss_mask_6: 0.1657  loss_dice_6: 0.1271  loss_ce_7: 0.1638  loss_mask_7: 0.1655  loss_dice_7: 0.1294  loss_ce_8: 0.1638  loss_mask_8: 0.1616  loss_dice_8: 0.1273  time: 0.5748  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:39:47] d2.utils.events INFO:  eta: 2:39:39  iter: 13779  total_loss: 4.517  loss_ce: 0.09758  loss_mask: 0.1865  loss_dice: 0.1593  loss_ce_0: 0.05262  loss_mask_0: 0.1934  loss_dice_0: 0.1628  loss_ce_1: 0.09763  loss_mask_1: 0.1825  loss_dice_1: 0.1524  loss_ce_2: 0.09761  loss_mask_2: 0.1951  loss_dice_2: 0.1665  loss_ce_3: 0.09761  loss_mask_3: 0.1905  loss_dice_3: 0.1576  loss_ce_4: 0.09776  loss_mask_4: 0.1822  loss_dice_4: 0.1583  loss_ce_5: 0.09765  loss_mask_5: 0.1894  loss_dice_5: 0.1632  loss_ce_6: 0.0975  loss_mask_6: 0.1862  loss_dice_6: 0.1572  loss_ce_7: 0.09769  loss_mask_7: 0.1891  loss_dice_7: 0.1573  loss_ce_8: 0.09747  loss_mask_8: 0.1821  loss_dice_8: 0.1666  time: 0.5748  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:39:58] d2.utils.events INFO:  eta: 2:38:41  iter: 13799  total_loss: 4.668  loss_ce: 0.1626  loss_mask: 0.1371  loss_dice: 0.1542  loss_ce_0: 0.07626  loss_mask_0: 0.1401  loss_dice_0: 0.1493  loss_ce_1: 0.1621  loss_mask_1: 0.1443  loss_dice_1: 0.1486  loss_ce_2: 0.1623  loss_mask_2: 0.146  loss_dice_2: 0.1538  loss_ce_3: 0.1624  loss_mask_3: 0.1428  loss_dice_3: 0.1506  loss_ce_4: 0.1622  loss_mask_4: 0.1397  loss_dice_4: 0.1479  loss_ce_5: 0.1623  loss_mask_5: 0.1428  loss_dice_5: 0.152  loss_ce_6: 0.1625  loss_mask_6: 0.1405  loss_dice_6: 0.1511  loss_ce_7: 0.1624  loss_mask_7: 0.148  loss_dice_7: 0.1513  loss_ce_8: 0.1627  loss_mask_8: 0.1444  loss_dice_8: 0.1616  time: 0.5747  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:40:08] d2.utils.events INFO:  eta: 2:37:52  iter: 13819  total_loss: 4.692  loss_ce: 0.09836  loss_mask: 0.2173  loss_dice: 0.1267  loss_ce_0: 0.0528  loss_mask_0: 0.2208  loss_dice_0: 0.1357  loss_ce_1: 0.09895  loss_mask_1: 0.2184  loss_dice_1: 0.1325  loss_ce_2: 0.09869  loss_mask_2: 0.215  loss_dice_2: 0.1319  loss_ce_3: 0.09872  loss_mask_3: 0.2226  loss_dice_3: 0.1327  loss_ce_4: 0.09884  loss_mask_4: 0.22  loss_dice_4: 0.1379  loss_ce_5: 0.09884  loss_mask_5: 0.2155  loss_dice_5: 0.1267  loss_ce_6: 0.09869  loss_mask_6: 0.2295  loss_dice_6: 0.1322  loss_ce_7: 0.09854  loss_mask_7: 0.2238  loss_dice_7: 0.1269  loss_ce_8: 0.09836  loss_mask_8: 0.2262  loss_dice_8: 0.136  time: 0.5746  data_time: 0.0021  lr: 1e-05  max_mem: 2811M
[07/11 13:40:19] d2.utils.events INFO:  eta: 2:37:12  iter: 13839  total_loss: 4.575  loss_ce: 0.09865  loss_mask: 0.1845  loss_dice: 0.1307  loss_ce_0: 0.05288  loss_mask_0: 0.1858  loss_dice_0: 0.1298  loss_ce_1: 0.09923  loss_mask_1: 0.1894  loss_dice_1: 0.1229  loss_ce_2: 0.09897  loss_mask_2: 0.1904  loss_dice_2: 0.1293  loss_ce_3: 0.09898  loss_mask_3: 0.1889  loss_dice_3: 0.1279  loss_ce_4: 0.0991  loss_mask_4: 0.1915  loss_dice_4: 0.1308  loss_ce_5: 0.09928  loss_mask_5: 0.1829  loss_dice_5: 0.1302  loss_ce_6: 0.09899  loss_mask_6: 0.1883  loss_dice_6: 0.1303  loss_ce_7: 0.09891  loss_mask_7: 0.1975  loss_dice_7: 0.1295  loss_ce_8: 0.09865  loss_mask_8: 0.1856  loss_dice_8: 0.1251  time: 0.5745  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:40:29] d2.utils.events INFO:  eta: 2:36:25  iter: 13859  total_loss: 4.425  loss_ce: 0.09821  loss_mask: 0.1632  loss_dice: 0.1406  loss_ce_0: 0.05273  loss_mask_0: 0.1812  loss_dice_0: 0.1302  loss_ce_1: 0.09834  loss_mask_1: 0.1725  loss_dice_1: 0.134  loss_ce_2: 0.09832  loss_mask_2: 0.1731  loss_dice_2: 0.1369  loss_ce_3: 0.09831  loss_mask_3: 0.1654  loss_dice_3: 0.1328  loss_ce_4: 0.09839  loss_mask_4: 0.1701  loss_dice_4: 0.136  loss_ce_5: 0.09846  loss_mask_5: 0.1676  loss_dice_5: 0.1354  loss_ce_6: 0.09835  loss_mask_6: 0.1718  loss_dice_6: 0.1379  loss_ce_7: 0.09835  loss_mask_7: 0.168  loss_dice_7: 0.1356  loss_ce_8: 0.09817  loss_mask_8: 0.1712  loss_dice_8: 0.1382  time: 0.5744  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:40:39] d2.utils.events INFO:  eta: 2:35:39  iter: 13879  total_loss: 4.87  loss_ce: 0.1638  loss_mask: 0.1524  loss_dice: 0.1714  loss_ce_0: 0.07652  loss_mask_0: 0.1491  loss_dice_0: 0.1743  loss_ce_1: 0.1643  loss_mask_1: 0.1566  loss_dice_1: 0.1764  loss_ce_2: 0.164  loss_mask_2: 0.1514  loss_dice_2: 0.1776  loss_ce_3: 0.164  loss_mask_3: 0.153  loss_dice_3: 0.1689  loss_ce_4: 0.164  loss_mask_4: 0.147  loss_dice_4: 0.1679  loss_ce_5: 0.164  loss_mask_5: 0.1489  loss_dice_5: 0.1718  loss_ce_6: 0.164  loss_mask_6: 0.1463  loss_dice_6: 0.1768  loss_ce_7: 0.1641  loss_mask_7: 0.15  loss_dice_7: 0.1766  loss_ce_8: 0.1635  loss_mask_8: 0.1518  loss_dice_8: 0.1741  time: 0.5744  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:40:50] d2.utils.events INFO:  eta: 2:34:51  iter: 13899  total_loss: 4.272  loss_ce: 0.09607  loss_mask: 0.1947  loss_dice: 0.1378  loss_ce_0: 0.05226  loss_mask_0: 0.1878  loss_dice_0: 0.1325  loss_ce_1: 0.09578  loss_mask_1: 0.1902  loss_dice_1: 0.1368  loss_ce_2: 0.09581  loss_mask_2: 0.192  loss_dice_2: 0.1366  loss_ce_3: 0.09578  loss_mask_3: 0.1859  loss_dice_3: 0.1377  loss_ce_4: 0.09588  loss_mask_4: 0.1973  loss_dice_4: 0.1353  loss_ce_5: 0.09567  loss_mask_5: 0.1943  loss_dice_5: 0.1357  loss_ce_6: 0.09581  loss_mask_6: 0.1919  loss_dice_6: 0.1356  loss_ce_7: 0.09603  loss_mask_7: 0.1844  loss_dice_7: 0.1376  loss_ce_8: 0.096  loss_mask_8: 0.1974  loss_dice_8: 0.1364  time: 0.5743  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:41:00] d2.utils.events INFO:  eta: 2:34:18  iter: 13919  total_loss: 4.457  loss_ce: 0.09509  loss_mask: 0.1551  loss_dice: 0.1593  loss_ce_0: 0.052  loss_mask_0: 0.1617  loss_dice_0: 0.1584  loss_ce_1: 0.09474  loss_mask_1: 0.1621  loss_dice_1: 0.1667  loss_ce_2: 0.09476  loss_mask_2: 0.1639  loss_dice_2: 0.1602  loss_ce_3: 0.09469  loss_mask_3: 0.1692  loss_dice_3: 0.1683  loss_ce_4: 0.09483  loss_mask_4: 0.1511  loss_dice_4: 0.1631  loss_ce_5: 0.09461  loss_mask_5: 0.1651  loss_dice_5: 0.1602  loss_ce_6: 0.09472  loss_mask_6: 0.1623  loss_dice_6: 0.16  loss_ce_7: 0.09501  loss_mask_7: 0.1691  loss_dice_7: 0.1624  loss_ce_8: 0.09494  loss_mask_8: 0.1642  loss_dice_8: 0.1629  time: 0.5742  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:41:10] d2.utils.events INFO:  eta: 2:32:55  iter: 13939  total_loss: 4.323  loss_ce: 0.0948  loss_mask: 0.1741  loss_dice: 0.1385  loss_ce_0: 0.05192  loss_mask_0: 0.1707  loss_dice_0: 0.1402  loss_ce_1: 0.09424  loss_mask_1: 0.1708  loss_dice_1: 0.1429  loss_ce_2: 0.0944  loss_mask_2: 0.1711  loss_dice_2: 0.1382  loss_ce_3: 0.0944  loss_mask_3: 0.1736  loss_dice_3: 0.1357  loss_ce_4: 0.09442  loss_mask_4: 0.1728  loss_dice_4: 0.1396  loss_ce_5: 0.09425  loss_mask_5: 0.1728  loss_dice_5: 0.1433  loss_ce_6: 0.0944  loss_mask_6: 0.1751  loss_dice_6: 0.1395  loss_ce_7: 0.09465  loss_mask_7: 0.1719  loss_dice_7: 0.1354  loss_ce_8: 0.09465  loss_mask_8: 0.1763  loss_dice_8: 0.1373  time: 0.5741  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:41:21] d2.utils.events INFO:  eta: 2:32:01  iter: 13959  total_loss: 4.541  loss_ce: 0.1682  loss_mask: 0.1881  loss_dice: 0.1287  loss_ce_0: 0.07752  loss_mask_0: 0.1787  loss_dice_0: 0.123  loss_ce_1: 0.1684  loss_mask_1: 0.1744  loss_dice_1: 0.129  loss_ce_2: 0.1687  loss_mask_2: 0.1878  loss_dice_2: 0.1285  loss_ce_3: 0.1687  loss_mask_3: 0.1785  loss_dice_3: 0.1232  loss_ce_4: 0.1684  loss_mask_4: 0.1811  loss_dice_4: 0.1214  loss_ce_5: 0.1691  loss_mask_5: 0.1841  loss_dice_5: 0.1339  loss_ce_6: 0.1687  loss_mask_6: 0.1802  loss_dice_6: 0.1315  loss_ce_7: 0.1683  loss_mask_7: 0.1789  loss_dice_7: 0.1275  loss_ce_8: 0.1684  loss_mask_8: 0.177  loss_dice_8: 0.124  time: 0.5740  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:41:31] d2.utils.events INFO:  eta: 2:31:01  iter: 13979  total_loss: 4.911  loss_ce: 0.1666  loss_mask: 0.1738  loss_dice: 0.1197  loss_ce_0: 0.0772  loss_mask_0: 0.1753  loss_dice_0: 0.1169  loss_ce_1: 0.1663  loss_mask_1: 0.1704  loss_dice_1: 0.1196  loss_ce_2: 0.1666  loss_mask_2: 0.1723  loss_dice_2: 0.1165  loss_ce_3: 0.1666  loss_mask_3: 0.1743  loss_dice_3: 0.1153  loss_ce_4: 0.1665  loss_mask_4: 0.1706  loss_dice_4: 0.1158  loss_ce_5: 0.1669  loss_mask_5: 0.1759  loss_dice_5: 0.1191  loss_ce_6: 0.1666  loss_mask_6: 0.1654  loss_dice_6: 0.1187  loss_ce_7: 0.1666  loss_mask_7: 0.1732  loss_dice_7: 0.1146  loss_ce_8: 0.1668  loss_mask_8: 0.1773  loss_dice_8: 0.119  time: 0.5739  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:41:42] d2.utils.events INFO:  eta: 2:29:45  iter: 13999  total_loss: 4.536  loss_ce: 0.1304  loss_mask: 0.1625  loss_dice: 0.1631  loss_ce_0: 0.06448  loss_mask_0: 0.174  loss_dice_0: 0.1611  loss_ce_1: 0.1303  loss_mask_1: 0.173  loss_dice_1: 0.1582  loss_ce_2: 0.1304  loss_mask_2: 0.1772  loss_dice_2: 0.1704  loss_ce_3: 0.1303  loss_mask_3: 0.1786  loss_dice_3: 0.1573  loss_ce_4: 0.1303  loss_mask_4: 0.1624  loss_dice_4: 0.1647  loss_ce_5: 0.1303  loss_mask_5: 0.1714  loss_dice_5: 0.1635  loss_ce_6: 0.1304  loss_mask_6: 0.1789  loss_dice_6: 0.1652  loss_ce_7: 0.1304  loss_mask_7: 0.1721  loss_dice_7: 0.1624  loss_ce_8: 0.1302  loss_mask_8: 0.1753  loss_dice_8: 0.1604  time: 0.5739  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:41:52] d2.utils.events INFO:  eta: 2:28:42  iter: 14019  total_loss: 4.338  loss_ce: 0.09622  loss_mask: 0.1977  loss_dice: 0.1389  loss_ce_0: 0.0522  loss_mask_0: 0.1902  loss_dice_0: 0.1396  loss_ce_1: 0.09642  loss_mask_1: 0.1849  loss_dice_1: 0.1419  loss_ce_2: 0.09643  loss_mask_2: 0.1838  loss_dice_2: 0.1429  loss_ce_3: 0.09647  loss_mask_3: 0.1796  loss_dice_3: 0.1394  loss_ce_4: 0.09636  loss_mask_4: 0.179  loss_dice_4: 0.1386  loss_ce_5: 0.09647  loss_mask_5: 0.1865  loss_dice_5: 0.1435  loss_ce_6: 0.09632  loss_mask_6: 0.191  loss_dice_6: 0.1454  loss_ce_7: 0.09636  loss_mask_7: 0.1887  loss_dice_7: 0.1414  loss_ce_8: 0.09611  loss_mask_8: 0.193  loss_dice_8: 0.1435  time: 0.5738  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:42:02] d2.utils.events INFO:  eta: 2:27:46  iter: 14039  total_loss: 4.982  loss_ce: 0.1654  loss_mask: 0.2011  loss_dice: 0.1519  loss_ce_0: 0.07692  loss_mask_0: 0.2099  loss_dice_0: 0.1551  loss_ce_1: 0.1653  loss_mask_1: 0.2019  loss_dice_1: 0.1585  loss_ce_2: 0.1651  loss_mask_2: 0.2002  loss_dice_2: 0.1565  loss_ce_3: 0.1651  loss_mask_3: 0.2052  loss_dice_3: 0.1535  loss_ce_4: 0.1653  loss_mask_4: 0.1964  loss_dice_4: 0.1568  loss_ce_5: 0.1653  loss_mask_5: 0.2026  loss_dice_5: 0.1522  loss_ce_6: 0.1654  loss_mask_6: 0.196  loss_dice_6: 0.1484  loss_ce_7: 0.1652  loss_mask_7: 0.1947  loss_dice_7: 0.1484  loss_ce_8: 0.1655  loss_mask_8: 0.2036  loss_dice_8: 0.1542  time: 0.5737  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:42:13] d2.utils.events INFO:  eta: 2:27:06  iter: 14059  total_loss: 4.563  loss_ce: 0.09611  loss_mask: 0.1973  loss_dice: 0.1395  loss_ce_0: 0.05214  loss_mask_0: 0.1926  loss_dice_0: 0.137  loss_ce_1: 0.09636  loss_mask_1: 0.1994  loss_dice_1: 0.1371  loss_ce_2: 0.09629  loss_mask_2: 0.204  loss_dice_2: 0.1358  loss_ce_3: 0.09625  loss_mask_3: 0.1987  loss_dice_3: 0.1334  loss_ce_4: 0.09629  loss_mask_4: 0.2062  loss_dice_4: 0.1368  loss_ce_5: 0.09632  loss_mask_5: 0.2033  loss_dice_5: 0.1364  loss_ce_6: 0.09618  loss_mask_6: 0.1856  loss_dice_6: 0.1432  loss_ce_7: 0.09625  loss_mask_7: 0.2051  loss_dice_7: 0.1407  loss_ce_8: 0.09603  loss_mask_8: 0.202  loss_dice_8: 0.1416  time: 0.5736  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:42:23] d2.utils.events INFO:  eta: 2:26:26  iter: 14079  total_loss: 4.596  loss_ce: 0.09567  loss_mask: 0.1911  loss_dice: 0.1431  loss_ce_0: 0.05202  loss_mask_0: 0.1889  loss_dice_0: 0.1428  loss_ce_1: 0.09572  loss_mask_1: 0.1861  loss_dice_1: 0.1445  loss_ce_2: 0.09567  loss_mask_2: 0.1998  loss_dice_2: 0.1452  loss_ce_3: 0.0957  loss_mask_3: 0.1982  loss_dice_3: 0.1442  loss_ce_4: 0.0957  loss_mask_4: 0.195  loss_dice_4: 0.1426  loss_ce_5: 0.09574  loss_mask_5: 0.1922  loss_dice_5: 0.1423  loss_ce_6: 0.09556  loss_mask_6: 0.1863  loss_dice_6: 0.1493  loss_ce_7: 0.09581  loss_mask_7: 0.1912  loss_dice_7: 0.1441  loss_ce_8: 0.09552  loss_mask_8: 0.194  loss_dice_8: 0.1449  time: 0.5736  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:42:34] d2.utils.events INFO:  eta: 2:25:30  iter: 14099  total_loss: 4.346  loss_ce: 0.1649  loss_mask: 0.1605  loss_dice: 0.1444  loss_ce_0: 0.07689  loss_mask_0: 0.1632  loss_dice_0: 0.1436  loss_ce_1: 0.1644  loss_mask_1: 0.1652  loss_dice_1: 0.1467  loss_ce_2: 0.1646  loss_mask_2: 0.1625  loss_dice_2: 0.1391  loss_ce_3: 0.1647  loss_mask_3: 0.1692  loss_dice_3: 0.1406  loss_ce_4: 0.1647  loss_mask_4: 0.1517  loss_dice_4: 0.1431  loss_ce_5: 0.1647  loss_mask_5: 0.1523  loss_dice_5: 0.142  loss_ce_6: 0.1648  loss_mask_6: 0.158  loss_dice_6: 0.1397  loss_ce_7: 0.1647  loss_mask_7: 0.16  loss_dice_7: 0.1385  loss_ce_8: 0.1648  loss_mask_8: 0.1604  loss_dice_8: 0.1392  time: 0.5735  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:42:44] d2.utils.events INFO:  eta: 2:24:48  iter: 14119  total_loss: 4.51  loss_ce: 0.1625  loss_mask: 0.1404  loss_dice: 0.1582  loss_ce_0: 0.07634  loss_mask_0: 0.138  loss_dice_0: 0.1591  loss_ce_1: 0.1619  loss_mask_1: 0.1331  loss_dice_1: 0.1532  loss_ce_2: 0.1619  loss_mask_2: 0.1364  loss_dice_2: 0.1485  loss_ce_3: 0.1618  loss_mask_3: 0.1363  loss_dice_3: 0.1553  loss_ce_4: 0.1618  loss_mask_4: 0.1337  loss_dice_4: 0.1541  loss_ce_5: 0.1618  loss_mask_5: 0.1409  loss_dice_5: 0.1506  loss_ce_6: 0.1621  loss_mask_6: 0.1446  loss_dice_6: 0.156  loss_ce_7: 0.1621  loss_mask_7: 0.1332  loss_dice_7: 0.1534  loss_ce_8: 0.1626  loss_mask_8: 0.1415  loss_dice_8: 0.155  time: 0.5734  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:42:55] d2.utils.events INFO:  eta: 2:24:07  iter: 14139  total_loss: 4.483  loss_ce: 0.1599  loss_mask: 0.1887  loss_dice: 0.1347  loss_ce_0: 0.07578  loss_mask_0: 0.1933  loss_dice_0: 0.1268  loss_ce_1: 0.1584  loss_mask_1: 0.1919  loss_dice_1: 0.1307  loss_ce_2: 0.1587  loss_mask_2: 0.1936  loss_dice_2: 0.1332  loss_ce_3: 0.1587  loss_mask_3: 0.1941  loss_dice_3: 0.1313  loss_ce_4: 0.1586  loss_mask_4: 0.1955  loss_dice_4: 0.1317  loss_ce_5: 0.1584  loss_mask_5: 0.1897  loss_dice_5: 0.1366  loss_ce_6: 0.1589  loss_mask_6: 0.1853  loss_dice_6: 0.1336  loss_ce_7: 0.1593  loss_mask_7: 0.1897  loss_dice_7: 0.141  loss_ce_8: 0.1598  loss_mask_8: 0.1958  loss_dice_8: 0.1332  time: 0.5733  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:43:05] d2.utils.events INFO:  eta: 2:23:27  iter: 14159  total_loss: 4.738  loss_ce: 0.1003  loss_mask: 0.1714  loss_dice: 0.1399  loss_ce_0: 0.05306  loss_mask_0: 0.1696  loss_dice_0: 0.1377  loss_ce_1: 0.1017  loss_mask_1: 0.1734  loss_dice_1: 0.143  loss_ce_2: 0.1013  loss_mask_2: 0.1778  loss_dice_2: 0.1413  loss_ce_3: 0.1012  loss_mask_3: 0.1744  loss_dice_3: 0.1378  loss_ce_4: 0.1014  loss_mask_4: 0.1711  loss_dice_4: 0.1432  loss_ce_5: 0.1015  loss_mask_5: 0.1712  loss_dice_5: 0.1418  loss_ce_6: 0.1011  loss_mask_6: 0.1682  loss_dice_6: 0.1345  loss_ce_7: 0.1008  loss_mask_7: 0.1819  loss_dice_7: 0.1432  loss_ce_8: 0.1004  loss_mask_8: 0.1658  loss_dice_8: 0.142  time: 0.5733  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:43:15] d2.utils.events INFO:  eta: 2:23:01  iter: 14179  total_loss: 4.368  loss_ce: 0.1003  loss_mask: 0.1781  loss_dice: 0.1332  loss_ce_0: 0.05301  loss_mask_0: 0.1799  loss_dice_0: 0.1268  loss_ce_1: 0.1014  loss_mask_1: 0.1767  loss_dice_1: 0.1304  loss_ce_2: 0.101  loss_mask_2: 0.1866  loss_dice_2: 0.1282  loss_ce_3: 0.1009  loss_mask_3: 0.1766  loss_dice_3: 0.1307  loss_ce_4: 0.1011  loss_mask_4: 0.1797  loss_dice_4: 0.1222  loss_ce_5: 0.1012  loss_mask_5: 0.1785  loss_dice_5: 0.1285  loss_ce_6: 0.101  loss_mask_6: 0.1829  loss_dice_6: 0.1262  loss_ce_7: 0.1006  loss_mask_7: 0.1867  loss_dice_7: 0.1284  loss_ce_8: 0.1003  loss_mask_8: 0.1806  loss_dice_8: 0.1267  time: 0.5732  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:43:26] d2.utils.events INFO:  eta: 2:22:22  iter: 14199  total_loss: 5.084  loss_ce: 0.09977  loss_mask: 0.2201  loss_dice: 0.1673  loss_ce_0: 0.05291  loss_mask_0: 0.2086  loss_dice_0: 0.1638  loss_ce_1: 0.1004  loss_mask_1: 0.2112  loss_dice_1: 0.1612  loss_ce_2: 0.1001  loss_mask_2: 0.2192  loss_dice_2: 0.1674  loss_ce_3: 0.1002  loss_mask_3: 0.221  loss_dice_3: 0.1614  loss_ce_4: 0.1002  loss_mask_4: 0.2096  loss_dice_4: 0.1667  loss_ce_5: 0.1003  loss_mask_5: 0.2149  loss_dice_5: 0.1703  loss_ce_6: 0.1002  loss_mask_6: 0.2065  loss_dice_6: 0.1644  loss_ce_7: 0.1  loss_mask_7: 0.2209  loss_dice_7: 0.1668  loss_ce_8: 0.09973  loss_mask_8: 0.2066  loss_dice_8: 0.1613  time: 0.5731  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:43:36] d2.utils.events INFO:  eta: 2:21:53  iter: 14219  total_loss: 4.653  loss_ce: 0.1615  loss_mask: 0.198  loss_dice: 0.1317  loss_ce_0: 0.0761  loss_mask_0: 0.206  loss_dice_0: 0.1317  loss_ce_1: 0.1612  loss_mask_1: 0.2116  loss_dice_1: 0.1344  loss_ce_2: 0.1616  loss_mask_2: 0.1986  loss_dice_2: 0.1293  loss_ce_3: 0.1615  loss_mask_3: 0.2147  loss_dice_3: 0.129  loss_ce_4: 0.1613  loss_mask_4: 0.2021  loss_dice_4: 0.1309  loss_ce_5: 0.1611  loss_mask_5: 0.2003  loss_dice_5: 0.1357  loss_ce_6: 0.1613  loss_mask_6: 0.2058  loss_dice_6: 0.1323  loss_ce_7: 0.1615  loss_mask_7: 0.2087  loss_dice_7: 0.1302  loss_ce_8: 0.1615  loss_mask_8: 0.2033  loss_dice_8: 0.1287  time: 0.5730  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:43:47] d2.utils.events INFO:  eta: 2:21:17  iter: 14239  total_loss: 4.593  loss_ce: 0.1584  loss_mask: 0.1944  loss_dice: 0.1212  loss_ce_0: 0.07544  loss_mask_0: 0.1893  loss_dice_0: 0.1284  loss_ce_1: 0.1571  loss_mask_1: 0.1931  loss_dice_1: 0.136  loss_ce_2: 0.1578  loss_mask_2: 0.1904  loss_dice_2: 0.1281  loss_ce_3: 0.1577  loss_mask_3: 0.1923  loss_dice_3: 0.1306  loss_ce_4: 0.1574  loss_mask_4: 0.1946  loss_dice_4: 0.129  loss_ce_5: 0.1574  loss_mask_5: 0.1921  loss_dice_5: 0.132  loss_ce_6: 0.1576  loss_mask_6: 0.1886  loss_dice_6: 0.1301  loss_ce_7: 0.158  loss_mask_7: 0.1808  loss_dice_7: 0.1293  loss_ce_8: 0.1584  loss_mask_8: 0.1873  loss_dice_8: 0.1281  time: 0.5730  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:43:57] d2.utils.events INFO:  eta: 2:20:50  iter: 14259  total_loss: 4.496  loss_ce: 0.1542  loss_mask: 0.1929  loss_dice: 0.1332  loss_ce_0: 0.07447  loss_mask_0: 0.1965  loss_dice_0: 0.1348  loss_ce_1: 0.1523  loss_mask_1: 0.2053  loss_dice_1: 0.1342  loss_ce_2: 0.1529  loss_mask_2: 0.2014  loss_dice_2: 0.1333  loss_ce_3: 0.1529  loss_mask_3: 0.1944  loss_dice_3: 0.1353  loss_ce_4: 0.1527  loss_mask_4: 0.2022  loss_dice_4: 0.1333  loss_ce_5: 0.1525  loss_mask_5: 0.1988  loss_dice_5: 0.1293  loss_ce_6: 0.1529  loss_mask_6: 0.1871  loss_dice_6: 0.1341  loss_ce_7: 0.1537  loss_mask_7: 0.2001  loss_dice_7: 0.1363  loss_ce_8: 0.154  loss_mask_8: 0.1992  loss_dice_8: 0.1338  time: 0.5729  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:44:08] d2.utils.events INFO:  eta: 2:20:24  iter: 14279  total_loss: 4.788  loss_ce: 0.1042  loss_mask: 0.1806  loss_dice: 0.1791  loss_ce_0: 0.05389  loss_mask_0: 0.1859  loss_dice_0: 0.1808  loss_ce_1: 0.1057  loss_mask_1: 0.1802  loss_dice_1: 0.1823  loss_ce_2: 0.1052  loss_mask_2: 0.1872  loss_dice_2: 0.1763  loss_ce_3: 0.1053  loss_mask_3: 0.1812  loss_dice_3: 0.1865  loss_ce_4: 0.1055  loss_mask_4: 0.1806  loss_dice_4: 0.1784  loss_ce_5: 0.1057  loss_mask_5: 0.1846  loss_dice_5: 0.1827  loss_ce_6: 0.1052  loss_mask_6: 0.1834  loss_dice_6: 0.1813  loss_ce_7: 0.1048  loss_mask_7: 0.1822  loss_dice_7: 0.1785  loss_ce_8: 0.1043  loss_mask_8: 0.1859  loss_dice_8: 0.1826  time: 0.5728  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:44:18] d2.utils.events INFO:  eta: 2:20:01  iter: 14299  total_loss: 4.553  loss_ce: 0.104  loss_mask: 0.1923  loss_dice: 0.1558  loss_ce_0: 0.05389  loss_mask_0: 0.2  loss_dice_0: 0.1501  loss_ce_1: 0.1051  loss_mask_1: 0.1843  loss_dice_1: 0.1519  loss_ce_2: 0.1047  loss_mask_2: 0.1934  loss_dice_2: 0.1493  loss_ce_3: 0.1047  loss_mask_3: 0.1958  loss_dice_3: 0.1455  loss_ce_4: 0.1049  loss_mask_4: 0.1886  loss_dice_4: 0.1469  loss_ce_5: 0.1051  loss_mask_5: 0.1899  loss_dice_5: 0.1471  loss_ce_6: 0.1047  loss_mask_6: 0.1891  loss_dice_6: 0.1463  loss_ce_7: 0.1043  loss_mask_7: 0.188  loss_dice_7: 0.15  loss_ce_8: 0.1041  loss_mask_8: 0.1923  loss_dice_8: 0.1483  time: 0.5727  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:44:28] d2.utils.events INFO:  eta: 2:19:14  iter: 14319  total_loss: 4.952  loss_ce: 0.1031  loss_mask: 0.204  loss_dice: 0.139  loss_ce_0: 0.05369  loss_mask_0: 0.2104  loss_dice_0: 0.1345  loss_ce_1: 0.104  loss_mask_1: 0.2152  loss_dice_1: 0.1337  loss_ce_2: 0.1036  loss_mask_2: 0.2103  loss_dice_2: 0.1355  loss_ce_3: 0.1036  loss_mask_3: 0.2023  loss_dice_3: 0.1352  loss_ce_4: 0.1038  loss_mask_4: 0.2055  loss_dice_4: 0.1366  loss_ce_5: 0.1039  loss_mask_5: 0.2022  loss_dice_5: 0.1351  loss_ce_6: 0.1036  loss_mask_6: 0.21  loss_dice_6: 0.1371  loss_ce_7: 0.1033  loss_mask_7: 0.2044  loss_dice_7: 0.1347  loss_ce_8: 0.1032  loss_mask_8: 0.1994  loss_dice_8: 0.1362  time: 0.5727  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:44:39] d2.utils.events INFO:  eta: 2:18:41  iter: 14339  total_loss: 4.348  loss_ce: 0.1023  loss_mask: 0.1694  loss_dice: 0.1544  loss_ce_0: 0.05351  loss_mask_0: 0.1681  loss_dice_0: 0.1545  loss_ce_1: 0.1026  loss_mask_1: 0.1727  loss_dice_1: 0.1569  loss_ce_2: 0.1023  loss_mask_2: 0.1612  loss_dice_2: 0.1545  loss_ce_3: 0.1023  loss_mask_3: 0.1665  loss_dice_3: 0.1565  loss_ce_4: 0.1027  loss_mask_4: 0.158  loss_dice_4: 0.1591  loss_ce_5: 0.1027  loss_mask_5: 0.1693  loss_dice_5: 0.1464  loss_ce_6: 0.1025  loss_mask_6: 0.1707  loss_dice_6: 0.1504  loss_ce_7: 0.1023  loss_mask_7: 0.1754  loss_dice_7: 0.1554  loss_ce_8: 0.1022  loss_mask_8: 0.1724  loss_dice_8: 0.1597  time: 0.5726  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:44:49] d2.utils.events INFO:  eta: 2:18:01  iter: 14359  total_loss: 4.599  loss_ce: 0.102  loss_mask: 0.2154  loss_dice: 0.1521  loss_ce_0: 0.05347  loss_mask_0: 0.2135  loss_dice_0: 0.147  loss_ce_1: 0.1023  loss_mask_1: 0.216  loss_dice_1: 0.1504  loss_ce_2: 0.102  loss_mask_2: 0.2119  loss_dice_2: 0.1417  loss_ce_3: 0.102  loss_mask_3: 0.2173  loss_dice_3: 0.1544  loss_ce_4: 0.1023  loss_mask_4: 0.2152  loss_dice_4: 0.1498  loss_ce_5: 0.1023  loss_mask_5: 0.2174  loss_dice_5: 0.1481  loss_ce_6: 0.1023  loss_mask_6: 0.2118  loss_dice_6: 0.1509  loss_ce_7: 0.102  loss_mask_7: 0.2145  loss_dice_7: 0.1431  loss_ce_8: 0.102  loss_mask_8: 0.2148  loss_dice_8: 0.1439  time: 0.5725  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:44:59] d2.utils.events INFO:  eta: 2:17:34  iter: 14379  total_loss: 4.334  loss_ce: 0.1017  loss_mask: 0.1824  loss_dice: 0.1279  loss_ce_0: 0.05343  loss_mask_0: 0.1824  loss_dice_0: 0.1289  loss_ce_1: 0.1019  loss_mask_1: 0.171  loss_dice_1: 0.132  loss_ce_2: 0.1016  loss_mask_2: 0.1779  loss_dice_2: 0.1313  loss_ce_3: 0.1017  loss_mask_3: 0.1806  loss_dice_3: 0.1316  loss_ce_4: 0.1019  loss_mask_4: 0.1749  loss_dice_4: 0.1306  loss_ce_5: 0.1019  loss_mask_5: 0.1792  loss_dice_5: 0.1265  loss_ce_6: 0.1019  loss_mask_6: 0.1738  loss_dice_6: 0.1292  loss_ce_7: 0.1017  loss_mask_7: 0.1834  loss_dice_7: 0.127  loss_ce_8: 0.1017  loss_mask_8: 0.1712  loss_dice_8: 0.1304  time: 0.5724  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:45:10] d2.utils.events INFO:  eta: 2:17:01  iter: 14399  total_loss: 4.618  loss_ce: 0.1013  loss_mask: 0.1803  loss_dice: 0.1344  loss_ce_0: 0.05337  loss_mask_0: 0.1809  loss_dice_0: 0.1383  loss_ce_1: 0.1015  loss_mask_1: 0.1847  loss_dice_1: 0.1358  loss_ce_2: 0.1011  loss_mask_2: 0.1722  loss_dice_2: 0.1348  loss_ce_3: 0.1012  loss_mask_3: 0.1888  loss_dice_3: 0.1363  loss_ce_4: 0.1015  loss_mask_4: 0.1811  loss_dice_4: 0.135  loss_ce_5: 0.1015  loss_mask_5: 0.1872  loss_dice_5: 0.134  loss_ce_6: 0.1014  loss_mask_6: 0.1769  loss_dice_6: 0.1339  loss_ce_7: 0.1013  loss_mask_7: 0.1884  loss_dice_7: 0.1357  loss_ce_8: 0.1013  loss_mask_8: 0.1813  loss_dice_8: 0.135  time: 0.5723  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:45:20] d2.utils.events INFO:  eta: 2:16:24  iter: 14419  total_loss: 4.779  loss_ce: 0.1014  loss_mask: 0.2197  loss_dice: 0.1521  loss_ce_0: 0.05342  loss_mask_0: 0.2285  loss_dice_0: 0.1529  loss_ce_1: 0.1014  loss_mask_1: 0.2232  loss_dice_1: 0.1532  loss_ce_2: 0.1014  loss_mask_2: 0.2237  loss_dice_2: 0.1486  loss_ce_3: 0.1014  loss_mask_3: 0.2313  loss_dice_3: 0.157  loss_ce_4: 0.1016  loss_mask_4: 0.2154  loss_dice_4: 0.1552  loss_ce_5: 0.1015  loss_mask_5: 0.2327  loss_dice_5: 0.1524  loss_ce_6: 0.1015  loss_mask_6: 0.2291  loss_dice_6: 0.1523  loss_ce_7: 0.1014  loss_mask_7: 0.2276  loss_dice_7: 0.1527  loss_ce_8: 0.1014  loss_mask_8: 0.2227  loss_dice_8: 0.156  time: 0.5723  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:45:30] d2.utils.events INFO:  eta: 2:15:57  iter: 14439  total_loss: 4.516  loss_ce: 0.1294  loss_mask: 0.1873  loss_dice: 0.1656  loss_ce_0: 0.06416  loss_mask_0: 0.1916  loss_dice_0: 0.1722  loss_ce_1: 0.1294  loss_mask_1: 0.1871  loss_dice_1: 0.1534  loss_ce_2: 0.1293  loss_mask_2: 0.1933  loss_dice_2: 0.1518  loss_ce_3: 0.1293  loss_mask_3: 0.1895  loss_dice_3: 0.1601  loss_ce_4: 0.1292  loss_mask_4: 0.1933  loss_dice_4: 0.1531  loss_ce_5: 0.1293  loss_mask_5: 0.1855  loss_dice_5: 0.1506  loss_ce_6: 0.1294  loss_mask_6: 0.1853  loss_dice_6: 0.1629  loss_ce_7: 0.1293  loss_mask_7: 0.1963  loss_dice_7: 0.1559  loss_ce_8: 0.1293  loss_mask_8: 0.1976  loss_dice_8: 0.1596  time: 0.5722  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:45:41] d2.utils.events INFO:  eta: 2:15:06  iter: 14459  total_loss: 4.552  loss_ce: 0.1022  loss_mask: 0.1799  loss_dice: 0.1487  loss_ce_0: 0.0536  loss_mask_0: 0.181  loss_dice_0: 0.154  loss_ce_1: 0.1025  loss_mask_1: 0.1819  loss_dice_1: 0.1546  loss_ce_2: 0.1022  loss_mask_2: 0.179  loss_dice_2: 0.1609  loss_ce_3: 0.1023  loss_mask_3: 0.1765  loss_dice_3: 0.1517  loss_ce_4: 0.1025  loss_mask_4: 0.1917  loss_dice_4: 0.1545  loss_ce_5: 0.1025  loss_mask_5: 0.1849  loss_dice_5: 0.1551  loss_ce_6: 0.1023  loss_mask_6: 0.1738  loss_dice_6: 0.1632  loss_ce_7: 0.1022  loss_mask_7: 0.1762  loss_dice_7: 0.1503  loss_ce_8: 0.1022  loss_mask_8: 0.179  loss_dice_8: 0.1496  time: 0.5721  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:45:51] d2.utils.events INFO:  eta: 2:14:42  iter: 14479  total_loss: 4.82  loss_ce: 0.1026  loss_mask: 0.1951  loss_dice: 0.1543  loss_ce_0: 0.05371  loss_mask_0: 0.1903  loss_dice_0: 0.154  loss_ce_1: 0.103  loss_mask_1: 0.196  loss_dice_1: 0.1584  loss_ce_2: 0.1028  loss_mask_2: 0.1979  loss_dice_2: 0.1639  loss_ce_3: 0.1029  loss_mask_3: 0.1883  loss_dice_3: 0.1497  loss_ce_4: 0.103  loss_mask_4: 0.1898  loss_dice_4: 0.1578  loss_ce_5: 0.103  loss_mask_5: 0.1986  loss_dice_5: 0.1542  loss_ce_6: 0.1029  loss_mask_6: 0.1945  loss_dice_6: 0.1623  loss_ce_7: 0.1027  loss_mask_7: 0.1929  loss_dice_7: 0.1639  loss_ce_8: 0.1027  loss_mask_8: 0.1861  loss_dice_8: 0.1511  time: 0.5720  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:46:02] d2.utils.events INFO:  eta: 2:14:18  iter: 14499  total_loss: 4.968  loss_ce: 0.1024  loss_mask: 0.1892  loss_dice: 0.1424  loss_ce_0: 0.05365  loss_mask_0: 0.1851  loss_dice_0: 0.135  loss_ce_1: 0.103  loss_mask_1: 0.1902  loss_dice_1: 0.14  loss_ce_2: 0.1026  loss_mask_2: 0.1869  loss_dice_2: 0.1437  loss_ce_3: 0.1026  loss_mask_3: 0.201  loss_dice_3: 0.1476  loss_ce_4: 0.1028  loss_mask_4: 0.1904  loss_dice_4: 0.1399  loss_ce_5: 0.1027  loss_mask_5: 0.1913  loss_dice_5: 0.1366  loss_ce_6: 0.1027  loss_mask_6: 0.1916  loss_dice_6: 0.1403  loss_ce_7: 0.1023  loss_mask_7: 0.1924  loss_dice_7: 0.1382  loss_ce_8: 0.1024  loss_mask_8: 0.1872  loss_dice_8: 0.1362  time: 0.5720  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:46:12] d2.utils.events INFO:  eta: 2:13:57  iter: 14519  total_loss: 4.526  loss_ce: 0.1022  loss_mask: 0.1956  loss_dice: 0.1476  loss_ce_0: 0.05365  loss_mask_0: 0.194  loss_dice_0: 0.151  loss_ce_1: 0.1025  loss_mask_1: 0.1922  loss_dice_1: 0.1498  loss_ce_2: 0.1022  loss_mask_2: 0.1895  loss_dice_2: 0.1498  loss_ce_3: 0.1022  loss_mask_3: 0.2035  loss_dice_3: 0.1516  loss_ce_4: 0.1025  loss_mask_4: 0.2005  loss_dice_4: 0.1497  loss_ce_5: 0.1024  loss_mask_5: 0.1958  loss_dice_5: 0.1512  loss_ce_6: 0.1023  loss_mask_6: 0.1911  loss_dice_6: 0.1486  loss_ce_7: 0.1022  loss_mask_7: 0.2046  loss_dice_7: 0.1513  loss_ce_8: 0.1022  loss_mask_8: 0.2006  loss_dice_8: 0.1476  time: 0.5719  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:46:22] d2.utils.events INFO:  eta: 2:13:24  iter: 14539  total_loss: 4.539  loss_ce: 0.1017  loss_mask: 0.1855  loss_dice: 0.1606  loss_ce_0: 0.05354  loss_mask_0: 0.1834  loss_dice_0: 0.1633  loss_ce_1: 0.1016  loss_mask_1: 0.178  loss_dice_1: 0.1587  loss_ce_2: 0.1016  loss_mask_2: 0.1803  loss_dice_2: 0.1575  loss_ce_3: 0.1017  loss_mask_3: 0.1795  loss_dice_3: 0.1594  loss_ce_4: 0.1019  loss_mask_4: 0.1757  loss_dice_4: 0.1562  loss_ce_5: 0.1017  loss_mask_5: 0.1789  loss_dice_5: 0.1609  loss_ce_6: 0.1017  loss_mask_6: 0.1849  loss_dice_6: 0.1538  loss_ce_7: 0.1017  loss_mask_7: 0.1854  loss_dice_7: 0.1597  loss_ce_8: 0.1017  loss_mask_8: 0.1803  loss_dice_8: 0.163  time: 0.5718  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:46:33] d2.utils.events INFO:  eta: 2:13:07  iter: 14559  total_loss: 5.153  loss_ce: 0.1008  loss_mask: 0.2018  loss_dice: 0.1582  loss_ce_0: 0.05333  loss_mask_0: 0.1997  loss_dice_0: 0.1561  loss_ce_1: 0.1008  loss_mask_1: 0.1999  loss_dice_1: 0.1563  loss_ce_2: 0.1006  loss_mask_2: 0.2136  loss_dice_2: 0.1568  loss_ce_3: 0.1006  loss_mask_3: 0.2076  loss_dice_3: 0.1569  loss_ce_4: 0.1009  loss_mask_4: 0.2097  loss_dice_4: 0.155  loss_ce_5: 0.1007  loss_mask_5: 0.1933  loss_dice_5: 0.1562  loss_ce_6: 0.1008  loss_mask_6: 0.2081  loss_dice_6: 0.1562  loss_ce_7: 0.1007  loss_mask_7: 0.2116  loss_dice_7: 0.1601  loss_ce_8: 0.1007  loss_mask_8: 0.206  loss_dice_8: 0.1578  time: 0.5717  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 13:46:43] d2.utils.events INFO:  eta: 2:12:33  iter: 14579  total_loss: 4.813  loss_ce: 0.1571  loss_mask: 0.2051  loss_dice: 0.1386  loss_ce_0: 0.07477  loss_mask_0: 0.2089  loss_dice_0: 0.141  loss_ce_1: 0.1567  loss_mask_1: 0.203  loss_dice_1: 0.1405  loss_ce_2: 0.1569  loss_mask_2: 0.2134  loss_dice_2: 0.1359  loss_ce_3: 0.157  loss_mask_3: 0.2102  loss_dice_3: 0.1399  loss_ce_4: 0.1567  loss_mask_4: 0.2034  loss_dice_4: 0.1414  loss_ce_5: 0.1569  loss_mask_5: 0.2047  loss_dice_5: 0.1396  loss_ce_6: 0.1568  loss_mask_6: 0.2056  loss_dice_6: 0.1399  loss_ce_7: 0.1571  loss_mask_7: 0.1958  loss_dice_7: 0.1397  loss_ce_8: 0.1571  loss_mask_8: 0.2106  loss_dice_8: 0.1444  time: 0.5717  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:46:54] d2.utils.events INFO:  eta: 2:12:15  iter: 14599  total_loss: 4.47  loss_ce: 0.1019  loss_mask: 0.1826  loss_dice: 0.1483  loss_ce_0: 0.0536  loss_mask_0: 0.1736  loss_dice_0: 0.1458  loss_ce_1: 0.1021  loss_mask_1: 0.177  loss_dice_1: 0.1461  loss_ce_2: 0.1019  loss_mask_2: 0.1845  loss_dice_2: 0.1418  loss_ce_3: 0.1019  loss_mask_3: 0.1821  loss_dice_3: 0.1448  loss_ce_4: 0.1021  loss_mask_4: 0.1788  loss_dice_4: 0.1458  loss_ce_5: 0.102  loss_mask_5: 0.176  loss_dice_5: 0.1426  loss_ce_6: 0.102  loss_mask_6: 0.1777  loss_dice_6: 0.1478  loss_ce_7: 0.1019  loss_mask_7: 0.187  loss_dice_7: 0.1374  loss_ce_8: 0.1018  loss_mask_8: 0.1834  loss_dice_8: 0.1431  time: 0.5716  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:47:04] d2.utils.events INFO:  eta: 2:11:47  iter: 14619  total_loss: 4.57  loss_ce: 0.1006  loss_mask: 0.1802  loss_dice: 0.1598  loss_ce_0: 0.0533  loss_mask_0: 0.1818  loss_dice_0: 0.1591  loss_ce_1: 0.1005  loss_mask_1: 0.1803  loss_dice_1: 0.1593  loss_ce_2: 0.1004  loss_mask_2: 0.1866  loss_dice_2: 0.1633  loss_ce_3: 0.1004  loss_mask_3: 0.1905  loss_dice_3: 0.1576  loss_ce_4: 0.1007  loss_mask_4: 0.1789  loss_dice_4: 0.1658  loss_ce_5: 0.1005  loss_mask_5: 0.1851  loss_dice_5: 0.1652  loss_ce_6: 0.1005  loss_mask_6: 0.1905  loss_dice_6: 0.1643  loss_ce_7: 0.1005  loss_mask_7: 0.1874  loss_dice_7: 0.1631  loss_ce_8: 0.1005  loss_mask_8: 0.1835  loss_dice_8: 0.1585  time: 0.5715  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:47:14] d2.utils.events INFO:  eta: 2:11:29  iter: 14639  total_loss: 4.682  loss_ce: 0.09902  loss_mask: 0.2099  loss_dice: 0.141  loss_ce_0: 0.053  loss_mask_0: 0.2089  loss_dice_0: 0.1402  loss_ce_1: 0.09848  loss_mask_1: 0.2041  loss_dice_1: 0.1416  loss_ce_2: 0.09846  loss_mask_2: 0.2092  loss_dice_2: 0.1422  loss_ce_3: 0.09849  loss_mask_3: 0.21  loss_dice_3: 0.1442  loss_ce_4: 0.09879  loss_mask_4: 0.2171  loss_dice_4: 0.1424  loss_ce_5: 0.09853  loss_mask_5: 0.2129  loss_dice_5: 0.1424  loss_ce_6: 0.09872  loss_mask_6: 0.2122  loss_dice_6: 0.143  loss_ce_7: 0.09876  loss_mask_7: 0.1989  loss_dice_7: 0.1383  loss_ce_8: 0.09891  loss_mask_8: 0.2152  loss_dice_8: 0.1382  time: 0.5715  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:47:25] d2.utils.events INFO:  eta: 2:11:19  iter: 14659  total_loss: 4.706  loss_ce: 0.1299  loss_mask: 0.2083  loss_dice: 0.1394  loss_ce_0: 0.06423  loss_mask_0: 0.2015  loss_dice_0: 0.1364  loss_ce_1: 0.1298  loss_mask_1: 0.1999  loss_dice_1: 0.1358  loss_ce_2: 0.1299  loss_mask_2: 0.2112  loss_dice_2: 0.1351  loss_ce_3: 0.13  loss_mask_3: 0.205  loss_dice_3: 0.1324  loss_ce_4: 0.1299  loss_mask_4: 0.2086  loss_dice_4: 0.1329  loss_ce_5: 0.13  loss_mask_5: 0.2108  loss_dice_5: 0.134  loss_ce_6: 0.1299  loss_mask_6: 0.2041  loss_dice_6: 0.1385  loss_ce_7: 0.13  loss_mask_7: 0.2051  loss_dice_7: 0.1346  loss_ce_8: 0.1299  loss_mask_8: 0.2058  loss_dice_8: 0.1331  time: 0.5714  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:47:35] d2.utils.events INFO:  eta: 2:11:09  iter: 14679  total_loss: 4.693  loss_ce: 0.09857  loss_mask: 0.1966  loss_dice: 0.1532  loss_ce_0: 0.05289  loss_mask_0: 0.1968  loss_dice_0: 0.1481  loss_ce_1: 0.09846  loss_mask_1: 0.1918  loss_dice_1: 0.1574  loss_ce_2: 0.09829  loss_mask_2: 0.1968  loss_dice_2: 0.1465  loss_ce_3: 0.09827  loss_mask_3: 0.203  loss_dice_3: 0.1521  loss_ce_4: 0.09857  loss_mask_4: 0.2041  loss_dice_4: 0.1499  loss_ce_5: 0.09831  loss_mask_5: 0.1991  loss_dice_5: 0.1513  loss_ce_6: 0.09838  loss_mask_6: 0.196  loss_dice_6: 0.1452  loss_ce_7: 0.09853  loss_mask_7: 0.1952  loss_dice_7: 0.1511  loss_ce_8: 0.0985  loss_mask_8: 0.2004  loss_dice_8: 0.1525  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:47:45] d2.utils.events INFO:  eta: 2:11:01  iter: 14699  total_loss: 4.734  loss_ce: 0.09846  loss_mask: 0.2011  loss_dice: 0.1395  loss_ce_0: 0.05289  loss_mask_0: 0.1976  loss_dice_0: 0.1343  loss_ce_1: 0.09824  loss_mask_1: 0.2085  loss_dice_1: 0.1333  loss_ce_2: 0.09822  loss_mask_2: 0.1988  loss_dice_2: 0.1336  loss_ce_3: 0.09816  loss_mask_3: 0.2001  loss_dice_3: 0.1335  loss_ce_4: 0.09831  loss_mask_4: 0.2031  loss_dice_4: 0.1336  loss_ce_5: 0.09816  loss_mask_5: 0.2044  loss_dice_5: 0.1309  loss_ce_6: 0.09831  loss_mask_6: 0.2009  loss_dice_6: 0.1374  loss_ce_7: 0.09831  loss_mask_7: 0.2046  loss_dice_7: 0.1303  loss_ce_8: 0.09835  loss_mask_8: 0.1949  loss_dice_8: 0.1366  time: 0.5712  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:47:56] d2.utils.events INFO:  eta: 2:10:58  iter: 14719  total_loss: 4.441  loss_ce: 0.09826  loss_mask: 0.1674  loss_dice: 0.1469  loss_ce_0: 0.05278  loss_mask_0: 0.1721  loss_dice_0: 0.1425  loss_ce_1: 0.09757  loss_mask_1: 0.1734  loss_dice_1: 0.1411  loss_ce_2: 0.09785  loss_mask_2: 0.1689  loss_dice_2: 0.142  loss_ce_3: 0.09788  loss_mask_3: 0.1616  loss_dice_3: 0.1415  loss_ce_4: 0.09792  loss_mask_4: 0.1721  loss_dice_4: 0.1416  loss_ce_5: 0.09781  loss_mask_5: 0.1708  loss_dice_5: 0.1463  loss_ce_6: 0.09796  loss_mask_6: 0.1729  loss_dice_6: 0.1382  loss_ce_7: 0.0981  loss_mask_7: 0.1759  loss_dice_7: 0.1465  loss_ce_8: 0.09818  loss_mask_8: 0.1627  loss_dice_8: 0.1442  time: 0.5712  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:48:06] d2.utils.events INFO:  eta: 2:10:50  iter: 14739  total_loss: 4.789  loss_ce: 0.09755  loss_mask: 0.1736  loss_dice: 0.1685  loss_ce_0: 0.05256  loss_mask_0: 0.1662  loss_dice_0: 0.1733  loss_ce_1: 0.09707  loss_mask_1: 0.1807  loss_dice_1: 0.1722  loss_ce_2: 0.09716  loss_mask_2: 0.1791  loss_dice_2: 0.1822  loss_ce_3: 0.09714  loss_mask_3: 0.1802  loss_dice_3: 0.1723  loss_ce_4: 0.09725  loss_mask_4: 0.1773  loss_dice_4: 0.1776  loss_ce_5: 0.09692  loss_mask_5: 0.1683  loss_dice_5: 0.1615  loss_ce_6: 0.09718  loss_mask_6: 0.1698  loss_dice_6: 0.168  loss_ce_7: 0.09737  loss_mask_7: 0.1774  loss_dice_7: 0.1619  loss_ce_8: 0.09759  loss_mask_8: 0.1717  loss_dice_8: 0.1639  time: 0.5711  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:48:16] d2.utils.events INFO:  eta: 2:10:37  iter: 14759  total_loss: 4.802  loss_ce: 0.09689  loss_mask: 0.1647  loss_dice: 0.1591  loss_ce_0: 0.05246  loss_mask_0: 0.1661  loss_dice_0: 0.1565  loss_ce_1: 0.09632  loss_mask_1: 0.175  loss_dice_1: 0.1508  loss_ce_2: 0.09645  loss_mask_2: 0.1789  loss_dice_2: 0.1613  loss_ce_3: 0.09645  loss_mask_3: 0.1687  loss_dice_3: 0.1602  loss_ce_4: 0.09656  loss_mask_4: 0.1679  loss_dice_4: 0.1598  loss_ce_5: 0.09619  loss_mask_5: 0.163  loss_dice_5: 0.1663  loss_ce_6: 0.09652  loss_mask_6: 0.1709  loss_dice_6: 0.1547  loss_ce_7: 0.0967  loss_mask_7: 0.1709  loss_dice_7: 0.1514  loss_ce_8: 0.09682  loss_mask_8: 0.1648  loss_dice_8: 0.1556  time: 0.5710  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:48:27] d2.utils.events INFO:  eta: 2:10:17  iter: 14779  total_loss: 4.775  loss_ce: 0.09605  loss_mask: 0.1768  loss_dice: 0.154  loss_ce_0: 0.05222  loss_mask_0: 0.1853  loss_dice_0: 0.1482  loss_ce_1: 0.09556  loss_mask_1: 0.1851  loss_dice_1: 0.1536  loss_ce_2: 0.09553  loss_mask_2: 0.1817  loss_dice_2: 0.1454  loss_ce_3: 0.09535  loss_mask_3: 0.1752  loss_dice_3: 0.1507  loss_ce_4: 0.09561  loss_mask_4: 0.1767  loss_dice_4: 0.1551  loss_ce_5: 0.09517  loss_mask_5: 0.179  loss_dice_5: 0.1471  loss_ce_6: 0.09565  loss_mask_6: 0.1823  loss_dice_6: 0.1541  loss_ce_7: 0.09575  loss_mask_7: 0.1812  loss_dice_7: 0.1499  loss_ce_8: 0.09597  loss_mask_8: 0.1809  loss_dice_8: 0.1518  time: 0.5709  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 13:48:37] d2.utils.events INFO:  eta: 2:10:09  iter: 14799  total_loss: 4.569  loss_ce: 0.09597  loss_mask: 0.2036  loss_dice: 0.1521  loss_ce_0: 0.05222  loss_mask_0: 0.2035  loss_dice_0: 0.1519  loss_ce_1: 0.09543  loss_mask_1: 0.2083  loss_dice_1: 0.1533  loss_ce_2: 0.09555  loss_mask_2: 0.2047  loss_dice_2: 0.1482  loss_ce_3: 0.09542  loss_mask_3: 0.2099  loss_dice_3: 0.1508  loss_ce_4: 0.09564  loss_mask_4: 0.2153  loss_dice_4: 0.1543  loss_ce_5: 0.09532  loss_mask_5: 0.2062  loss_dice_5: 0.1473  loss_ce_6: 0.09557  loss_mask_6: 0.2151  loss_dice_6: 0.1444  loss_ce_7: 0.09579  loss_mask_7: 0.2011  loss_dice_7: 0.1481  loss_ce_8: 0.09586  loss_mask_8: 0.2135  loss_dice_8: 0.1484  time: 0.5709  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:48:47] d2.utils.events INFO:  eta: 2:09:59  iter: 14819  total_loss: 4.544  loss_ce: 0.09543  loss_mask: 0.1731  loss_dice: 0.1504  loss_ce_0: 0.05207  loss_mask_0: 0.162  loss_dice_0: 0.1534  loss_ce_1: 0.09472  loss_mask_1: 0.1662  loss_dice_1: 0.1507  loss_ce_2: 0.09492  loss_mask_2: 0.1631  loss_dice_2: 0.1557  loss_ce_3: 0.09481  loss_mask_3: 0.1708  loss_dice_3: 0.1565  loss_ce_4: 0.09495  loss_mask_4: 0.163  loss_dice_4: 0.1573  loss_ce_5: 0.09459  loss_mask_5: 0.1756  loss_dice_5: 0.1571  loss_ce_6: 0.09499  loss_mask_6: 0.1693  loss_dice_6: 0.1555  loss_ce_7: 0.09521  loss_mask_7: 0.1648  loss_dice_7: 0.1499  loss_ce_8: 0.09528  loss_mask_8: 0.1783  loss_dice_8: 0.1545  time: 0.5708  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:48:58] d2.utils.events INFO:  eta: 2:09:51  iter: 14839  total_loss: 4.523  loss_ce: 0.0947  loss_mask: 0.1567  loss_dice: 0.1587  loss_ce_0: 0.0519  loss_mask_0: 0.1644  loss_dice_0: 0.161  loss_ce_1: 0.09389  loss_mask_1: 0.165  loss_dice_1: 0.1696  loss_ce_2: 0.09404  loss_mask_2: 0.1607  loss_dice_2: 0.1549  loss_ce_3: 0.09391  loss_mask_3: 0.1665  loss_dice_3: 0.1581  loss_ce_4: 0.09416  loss_mask_4: 0.1639  loss_dice_4: 0.1602  loss_ce_5: 0.09369  loss_mask_5: 0.1636  loss_dice_5: 0.1556  loss_ce_6: 0.09409  loss_mask_6: 0.1563  loss_dice_6: 0.1562  loss_ce_7: 0.09445  loss_mask_7: 0.1621  loss_dice_7: 0.158  loss_ce_8: 0.09449  loss_mask_8: 0.1604  loss_dice_8: 0.1527  time: 0.5707  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:49:08] d2.utils.events INFO:  eta: 2:09:43  iter: 14859  total_loss: 4.383  loss_ce: 0.09413  loss_mask: 0.1625  loss_dice: 0.1298  loss_ce_0: 0.05174  loss_mask_0: 0.1717  loss_dice_0: 0.1301  loss_ce_1: 0.09332  loss_mask_1: 0.1689  loss_dice_1: 0.1334  loss_ce_2: 0.09353  loss_mask_2: 0.1717  loss_dice_2: 0.1345  loss_ce_3: 0.09337  loss_mask_3: 0.1672  loss_dice_3: 0.1313  loss_ce_4: 0.09351  loss_mask_4: 0.1671  loss_dice_4: 0.13  loss_ce_5: 0.09316  loss_mask_5: 0.1638  loss_dice_5: 0.1295  loss_ce_6: 0.09359  loss_mask_6: 0.1695  loss_dice_6: 0.1353  loss_ce_7: 0.0938  loss_mask_7: 0.1727  loss_dice_7: 0.1346  loss_ce_8: 0.09391  loss_mask_8: 0.1678  loss_dice_8: 0.1297  time: 0.5707  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:49:19] d2.utils.events INFO:  eta: 2:09:35  iter: 14879  total_loss: 4.611  loss_ce: 0.09316  loss_mask: 0.2039  loss_dice: 0.1487  loss_ce_0: 0.05149  loss_mask_0: 0.2061  loss_dice_0: 0.1557  loss_ce_1: 0.09199  loss_mask_1: 0.2019  loss_dice_1: 0.1563  loss_ce_2: 0.0924  loss_mask_2: 0.205  loss_dice_2: 0.1579  loss_ce_3: 0.0922  loss_mask_3: 0.1963  loss_dice_3: 0.1546  loss_ce_4: 0.09231  loss_mask_4: 0.2059  loss_dice_4: 0.1554  loss_ce_5: 0.09206  loss_mask_5: 0.1926  loss_dice_5: 0.1587  loss_ce_6: 0.09255  loss_mask_6: 0.212  loss_dice_6: 0.1548  loss_ce_7: 0.09273  loss_mask_7: 0.1995  loss_dice_7: 0.1562  loss_ce_8: 0.09291  loss_mask_8: 0.1991  loss_dice_8: 0.1549  time: 0.5706  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:49:29] d2.utils.events INFO:  eta: 2:09:23  iter: 14899  total_loss: 4.702  loss_ce: 0.1702  loss_mask: 0.186  loss_dice: 0.1372  loss_ce_0: 0.07773  loss_mask_0: 0.1778  loss_dice_0: 0.1365  loss_ce_1: 0.1715  loss_mask_1: 0.1886  loss_dice_1: 0.1377  loss_ce_2: 0.1711  loss_mask_2: 0.1769  loss_dice_2: 0.1376  loss_ce_3: 0.1714  loss_mask_3: 0.1818  loss_dice_3: 0.1412  loss_ce_4: 0.1713  loss_mask_4: 0.1749  loss_dice_4: 0.1426  loss_ce_5: 0.1715  loss_mask_5: 0.1821  loss_dice_5: 0.1353  loss_ce_6: 0.1711  loss_mask_6: 0.1842  loss_dice_6: 0.138  loss_ce_7: 0.1708  loss_mask_7: 0.1749  loss_dice_7: 0.1383  loss_ce_8: 0.1705  loss_mask_8: 0.1854  loss_dice_8: 0.1369  time: 0.5705  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:49:39] d2.utils.events INFO:  eta: 2:09:12  iter: 14919  total_loss: 4.508  loss_ce: 0.09302  loss_mask: 0.1678  loss_dice: 0.1315  loss_ce_0: 0.05143  loss_mask_0: 0.1662  loss_dice_0: 0.1345  loss_ce_1: 0.09282  loss_mask_1: 0.1706  loss_dice_1: 0.1332  loss_ce_2: 0.09285  loss_mask_2: 0.1761  loss_dice_2: 0.1369  loss_ce_3: 0.09273  loss_mask_3: 0.1647  loss_dice_3: 0.1305  loss_ce_4: 0.09276  loss_mask_4: 0.1723  loss_dice_4: 0.1312  loss_ce_5: 0.09248  loss_mask_5: 0.165  loss_dice_5: 0.1343  loss_ce_6: 0.09273  loss_mask_6: 0.1633  loss_dice_6: 0.1287  loss_ce_7: 0.09308  loss_mask_7: 0.1632  loss_dice_7: 0.1305  loss_ce_8: 0.09287  loss_mask_8: 0.1745  loss_dice_8: 0.1298  time: 0.5704  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:49:50] d2.utils.events INFO:  eta: 2:08:55  iter: 14939  total_loss: 4.551  loss_ce: 0.1312  loss_mask: 0.1471  loss_dice: 0.1786  loss_ce_0: 0.06447  loss_mask_0: 0.1502  loss_dice_0: 0.1741  loss_ce_1: 0.1312  loss_mask_1: 0.1516  loss_dice_1: 0.1755  loss_ce_2: 0.1313  loss_mask_2: 0.151  loss_dice_2: 0.179  loss_ce_3: 0.1313  loss_mask_3: 0.147  loss_dice_3: 0.1716  loss_ce_4: 0.1313  loss_mask_4: 0.1531  loss_dice_4: 0.1756  loss_ce_5: 0.1314  loss_mask_5: 0.1385  loss_dice_5: 0.1678  loss_ce_6: 0.1313  loss_mask_6: 0.152  loss_dice_6: 0.1708  loss_ce_7: 0.1313  loss_mask_7: 0.1495  loss_dice_7: 0.1758  loss_ce_8: 0.1313  loss_mask_8: 0.1547  loss_dice_8: 0.1723  time: 0.5704  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:50:01] d2.utils.events INFO:  eta: 2:08:45  iter: 14959  total_loss: 4.968  loss_ce: 0.1309  loss_mask: 0.1989  loss_dice: 0.1457  loss_ce_0: 0.06446  loss_mask_0: 0.1991  loss_dice_0: 0.1474  loss_ce_1: 0.1307  loss_mask_1: 0.2077  loss_dice_1: 0.1515  loss_ce_2: 0.1308  loss_mask_2: 0.2077  loss_dice_2: 0.148  loss_ce_3: 0.1309  loss_mask_3: 0.2086  loss_dice_3: 0.1474  loss_ce_4: 0.1309  loss_mask_4: 0.2034  loss_dice_4: 0.1503  loss_ce_5: 0.1309  loss_mask_5: 0.1979  loss_dice_5: 0.1521  loss_ce_6: 0.1309  loss_mask_6: 0.2078  loss_dice_6: 0.1443  loss_ce_7: 0.131  loss_mask_7: 0.2112  loss_dice_7: 0.1461  loss_ce_8: 0.1309  loss_mask_8: 0.2192  loss_dice_8: 0.1489  time: 0.5703  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:50:12] d2.utils.events INFO:  eta: 2:08:44  iter: 14979  total_loss: 4.722  loss_ce: 0.1309  loss_mask: 0.1567  loss_dice: 0.159  loss_ce_0: 0.06444  loss_mask_0: 0.1596  loss_dice_0: 0.163  loss_ce_1: 0.1308  loss_mask_1: 0.1594  loss_dice_1: 0.1569  loss_ce_2: 0.1308  loss_mask_2: 0.159  loss_dice_2: 0.1571  loss_ce_3: 0.1309  loss_mask_3: 0.1603  loss_dice_3: 0.1561  loss_ce_4: 0.1309  loss_mask_4: 0.1633  loss_dice_4: 0.1587  loss_ce_5: 0.1309  loss_mask_5: 0.159  loss_dice_5: 0.1626  loss_ce_6: 0.1309  loss_mask_6: 0.1619  loss_dice_6: 0.1574  loss_ce_7: 0.1309  loss_mask_7: 0.1626  loss_dice_7: 0.1582  loss_ce_8: 0.1314  loss_mask_8: 0.166  loss_dice_8: 0.1628  time: 0.5703  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:50:23] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256to256_instance/model_0014999.pth
[07/11 13:50:24] d2.data.datasets.coco INFO: Loaded 754 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_val2017.json
[07/11 13:50:24] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/11 13:50:24] d2.data.common INFO: Serializing 754 elements to byte tensors and concatenating them all ...
[07/11 13:50:24] d2.data.common INFO: Serialized dataset takes 2.49 MiB
[07/11 13:50:24] d2.evaluation.evaluator INFO: Start inference on 754 batches
[07/11 13:50:29] d2.evaluation.evaluator INFO: Inference done 11/754. Dataloading: 0.0005 s/iter. Inference: 0.1414 s/iter. Eval: 0.2861 s/iter. Total: 0.4279 s/iter. ETA=0:05:17
[07/11 13:50:34] d2.evaluation.evaluator INFO: Inference done 23/754. Dataloading: 0.0006 s/iter. Inference: 0.1381 s/iter. Eval: 0.3020 s/iter. Total: 0.4408 s/iter. ETA=0:05:22
[07/11 13:50:39] d2.evaluation.evaluator INFO: Inference done 35/754. Dataloading: 0.0007 s/iter. Inference: 0.1395 s/iter. Eval: 0.3027 s/iter. Total: 0.4430 s/iter. ETA=0:05:18
[07/11 13:50:45] d2.evaluation.evaluator INFO: Inference done 47/754. Dataloading: 0.0007 s/iter. Inference: 0.1401 s/iter. Eval: 0.3033 s/iter. Total: 0.4442 s/iter. ETA=0:05:14
[07/11 13:50:50] d2.evaluation.evaluator INFO: Inference done 59/754. Dataloading: 0.0007 s/iter. Inference: 0.1407 s/iter. Eval: 0.3011 s/iter. Total: 0.4427 s/iter. ETA=0:05:07
[07/11 13:50:55] d2.evaluation.evaluator INFO: Inference done 71/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.3022 s/iter. Total: 0.4431 s/iter. ETA=0:05:02
[07/11 13:51:01] d2.evaluation.evaluator INFO: Inference done 83/754. Dataloading: 0.0007 s/iter. Inference: 0.1398 s/iter. Eval: 0.3028 s/iter. Total: 0.4434 s/iter. ETA=0:04:57
[07/11 13:51:06] d2.evaluation.evaluator INFO: Inference done 95/754. Dataloading: 0.0007 s/iter. Inference: 0.1406 s/iter. Eval: 0.3015 s/iter. Total: 0.4429 s/iter. ETA=0:04:51
[07/11 13:51:11] d2.evaluation.evaluator INFO: Inference done 107/754. Dataloading: 0.0007 s/iter. Inference: 0.1405 s/iter. Eval: 0.3026 s/iter. Total: 0.4439 s/iter. ETA=0:04:47
[07/11 13:51:17] d2.evaluation.evaluator INFO: Inference done 119/754. Dataloading: 0.0007 s/iter. Inference: 0.1395 s/iter. Eval: 0.3040 s/iter. Total: 0.4443 s/iter. ETA=0:04:42
[07/11 13:51:22] d2.evaluation.evaluator INFO: Inference done 131/754. Dataloading: 0.0007 s/iter. Inference: 0.1392 s/iter. Eval: 0.3039 s/iter. Total: 0.4439 s/iter. ETA=0:04:36
[07/11 13:51:27] d2.evaluation.evaluator INFO: Inference done 143/754. Dataloading: 0.0007 s/iter. Inference: 0.1393 s/iter. Eval: 0.3038 s/iter. Total: 0.4439 s/iter. ETA=0:04:31
[07/11 13:51:32] d2.evaluation.evaluator INFO: Inference done 154/754. Dataloading: 0.0008 s/iter. Inference: 0.1389 s/iter. Eval: 0.3053 s/iter. Total: 0.4450 s/iter. ETA=0:04:27
[07/11 13:51:38] d2.evaluation.evaluator INFO: Inference done 166/754. Dataloading: 0.0008 s/iter. Inference: 0.1385 s/iter. Eval: 0.3054 s/iter. Total: 0.4447 s/iter. ETA=0:04:21
[07/11 13:51:43] d2.evaluation.evaluator INFO: Inference done 178/754. Dataloading: 0.0008 s/iter. Inference: 0.1385 s/iter. Eval: 0.3045 s/iter. Total: 0.4438 s/iter. ETA=0:04:15
[07/11 13:51:48] d2.evaluation.evaluator INFO: Inference done 190/754. Dataloading: 0.0008 s/iter. Inference: 0.1380 s/iter. Eval: 0.3048 s/iter. Total: 0.4436 s/iter. ETA=0:04:10
[07/11 13:51:53] d2.evaluation.evaluator INFO: Inference done 202/754. Dataloading: 0.0008 s/iter. Inference: 0.1376 s/iter. Eval: 0.3050 s/iter. Total: 0.4434 s/iter. ETA=0:04:04
[07/11 13:51:59] d2.evaluation.evaluator INFO: Inference done 214/754. Dataloading: 0.0008 s/iter. Inference: 0.1374 s/iter. Eval: 0.3046 s/iter. Total: 0.4428 s/iter. ETA=0:03:59
[07/11 13:52:04] d2.evaluation.evaluator INFO: Inference done 225/754. Dataloading: 0.0008 s/iter. Inference: 0.1376 s/iter. Eval: 0.3050 s/iter. Total: 0.4435 s/iter. ETA=0:03:54
[07/11 13:52:09] d2.evaluation.evaluator INFO: Inference done 236/754. Dataloading: 0.0008 s/iter. Inference: 0.1378 s/iter. Eval: 0.3054 s/iter. Total: 0.4440 s/iter. ETA=0:03:50
[07/11 13:52:14] d2.evaluation.evaluator INFO: Inference done 248/754. Dataloading: 0.0008 s/iter. Inference: 0.1377 s/iter. Eval: 0.3050 s/iter. Total: 0.4435 s/iter. ETA=0:03:44
[07/11 13:52:19] d2.evaluation.evaluator INFO: Inference done 260/754. Dataloading: 0.0008 s/iter. Inference: 0.1374 s/iter. Eval: 0.3051 s/iter. Total: 0.4433 s/iter. ETA=0:03:39
[07/11 13:52:24] d2.evaluation.evaluator INFO: Inference done 272/754. Dataloading: 0.0008 s/iter. Inference: 0.1371 s/iter. Eval: 0.3043 s/iter. Total: 0.4423 s/iter. ETA=0:03:33
[07/11 13:52:29] d2.evaluation.evaluator INFO: Inference done 283/754. Dataloading: 0.0008 s/iter. Inference: 0.1376 s/iter. Eval: 0.3048 s/iter. Total: 0.4433 s/iter. ETA=0:03:28
[07/11 13:52:35] d2.evaluation.evaluator INFO: Inference done 295/754. Dataloading: 0.0008 s/iter. Inference: 0.1374 s/iter. Eval: 0.3052 s/iter. Total: 0.4435 s/iter. ETA=0:03:23
[07/11 13:52:40] d2.evaluation.evaluator INFO: Inference done 307/754. Dataloading: 0.0008 s/iter. Inference: 0.1377 s/iter. Eval: 0.3052 s/iter. Total: 0.4438 s/iter. ETA=0:03:18
[07/11 13:52:45] d2.evaluation.evaluator INFO: Inference done 319/754. Dataloading: 0.0008 s/iter. Inference: 0.1381 s/iter. Eval: 0.3048 s/iter. Total: 0.4438 s/iter. ETA=0:03:13
[07/11 13:52:51] d2.evaluation.evaluator INFO: Inference done 331/754. Dataloading: 0.0008 s/iter. Inference: 0.1379 s/iter. Eval: 0.3049 s/iter. Total: 0.4437 s/iter. ETA=0:03:07
[07/11 13:52:56] d2.evaluation.evaluator INFO: Inference done 343/754. Dataloading: 0.0008 s/iter. Inference: 0.1378 s/iter. Eval: 0.3051 s/iter. Total: 0.4438 s/iter. ETA=0:03:02
[07/11 13:53:01] d2.evaluation.evaluator INFO: Inference done 355/754. Dataloading: 0.0008 s/iter. Inference: 0.1379 s/iter. Eval: 0.3044 s/iter. Total: 0.4432 s/iter. ETA=0:02:56
[07/11 13:53:06] d2.evaluation.evaluator INFO: Inference done 366/754. Dataloading: 0.0008 s/iter. Inference: 0.1381 s/iter. Eval: 0.3046 s/iter. Total: 0.4436 s/iter. ETA=0:02:52
[07/11 13:53:12] d2.evaluation.evaluator INFO: Inference done 378/754. Dataloading: 0.0008 s/iter. Inference: 0.1379 s/iter. Eval: 0.3046 s/iter. Total: 0.4434 s/iter. ETA=0:02:46
[07/11 13:53:17] d2.evaluation.evaluator INFO: Inference done 390/754. Dataloading: 0.0008 s/iter. Inference: 0.1378 s/iter. Eval: 0.3048 s/iter. Total: 0.4435 s/iter. ETA=0:02:41
[07/11 13:53:22] d2.evaluation.evaluator INFO: Inference done 402/754. Dataloading: 0.0008 s/iter. Inference: 0.1378 s/iter. Eval: 0.3044 s/iter. Total: 0.4431 s/iter. ETA=0:02:35
[07/11 13:53:27] d2.evaluation.evaluator INFO: Inference done 414/754. Dataloading: 0.0008 s/iter. Inference: 0.1380 s/iter. Eval: 0.3041 s/iter. Total: 0.4429 s/iter. ETA=0:02:30
[07/11 13:53:33] d2.evaluation.evaluator INFO: Inference done 426/754. Dataloading: 0.0008 s/iter. Inference: 0.1380 s/iter. Eval: 0.3043 s/iter. Total: 0.4432 s/iter. ETA=0:02:25
[07/11 13:53:38] d2.evaluation.evaluator INFO: Inference done 438/754. Dataloading: 0.0008 s/iter. Inference: 0.1379 s/iter. Eval: 0.3044 s/iter. Total: 0.4431 s/iter. ETA=0:02:20
[07/11 13:53:43] d2.evaluation.evaluator INFO: Inference done 450/754. Dataloading: 0.0008 s/iter. Inference: 0.1379 s/iter. Eval: 0.3043 s/iter. Total: 0.4431 s/iter. ETA=0:02:14
[07/11 13:53:49] d2.evaluation.evaluator INFO: Inference done 462/754. Dataloading: 0.0008 s/iter. Inference: 0.1382 s/iter. Eval: 0.3042 s/iter. Total: 0.4432 s/iter. ETA=0:02:09
[07/11 13:53:54] d2.evaluation.evaluator INFO: Inference done 474/754. Dataloading: 0.0008 s/iter. Inference: 0.1381 s/iter. Eval: 0.3044 s/iter. Total: 0.4434 s/iter. ETA=0:02:04
[07/11 13:53:59] d2.evaluation.evaluator INFO: Inference done 486/754. Dataloading: 0.0008 s/iter. Inference: 0.1384 s/iter. Eval: 0.3042 s/iter. Total: 0.4434 s/iter. ETA=0:01:58
[07/11 13:54:04] d2.evaluation.evaluator INFO: Inference done 497/754. Dataloading: 0.0008 s/iter. Inference: 0.1385 s/iter. Eval: 0.3044 s/iter. Total: 0.4437 s/iter. ETA=0:01:54
[07/11 13:54:10] d2.evaluation.evaluator INFO: Inference done 509/754. Dataloading: 0.0008 s/iter. Inference: 0.1384 s/iter. Eval: 0.3044 s/iter. Total: 0.4437 s/iter. ETA=0:01:48
[07/11 13:54:15] d2.evaluation.evaluator INFO: Inference done 521/754. Dataloading: 0.0008 s/iter. Inference: 0.1383 s/iter. Eval: 0.3043 s/iter. Total: 0.4435 s/iter. ETA=0:01:43
[07/11 13:54:20] d2.evaluation.evaluator INFO: Inference done 532/754. Dataloading: 0.0008 s/iter. Inference: 0.1386 s/iter. Eval: 0.3046 s/iter. Total: 0.4440 s/iter. ETA=0:01:38
[07/11 13:54:25] d2.evaluation.evaluator INFO: Inference done 544/754. Dataloading: 0.0008 s/iter. Inference: 0.1385 s/iter. Eval: 0.3044 s/iter. Total: 0.4438 s/iter. ETA=0:01:33
[07/11 13:54:31] d2.evaluation.evaluator INFO: Inference done 556/754. Dataloading: 0.0008 s/iter. Inference: 0.1387 s/iter. Eval: 0.3043 s/iter. Total: 0.4439 s/iter. ETA=0:01:27
[07/11 13:54:36] d2.evaluation.evaluator INFO: Inference done 567/754. Dataloading: 0.0008 s/iter. Inference: 0.1387 s/iter. Eval: 0.3046 s/iter. Total: 0.4442 s/iter. ETA=0:01:23
[07/11 13:54:41] d2.evaluation.evaluator INFO: Inference done 579/754. Dataloading: 0.0008 s/iter. Inference: 0.1390 s/iter. Eval: 0.3047 s/iter. Total: 0.4445 s/iter. ETA=0:01:17
[07/11 13:54:46] d2.evaluation.evaluator INFO: Inference done 591/754. Dataloading: 0.0008 s/iter. Inference: 0.1389 s/iter. Eval: 0.3044 s/iter. Total: 0.4442 s/iter. ETA=0:01:12
[07/11 13:54:52] d2.evaluation.evaluator INFO: Inference done 603/754. Dataloading: 0.0008 s/iter. Inference: 0.1391 s/iter. Eval: 0.3043 s/iter. Total: 0.4443 s/iter. ETA=0:01:07
[07/11 13:54:57] d2.evaluation.evaluator INFO: Inference done 615/754. Dataloading: 0.0008 s/iter. Inference: 0.1392 s/iter. Eval: 0.3043 s/iter. Total: 0.4444 s/iter. ETA=0:01:01
[07/11 13:55:02] d2.evaluation.evaluator INFO: Inference done 627/754. Dataloading: 0.0008 s/iter. Inference: 0.1393 s/iter. Eval: 0.3040 s/iter. Total: 0.4442 s/iter. ETA=0:00:56
[07/11 13:55:08] d2.evaluation.evaluator INFO: Inference done 639/754. Dataloading: 0.0008 s/iter. Inference: 0.1390 s/iter. Eval: 0.3041 s/iter. Total: 0.4440 s/iter. ETA=0:00:51
[07/11 13:55:13] d2.evaluation.evaluator INFO: Inference done 651/754. Dataloading: 0.0008 s/iter. Inference: 0.1390 s/iter. Eval: 0.3040 s/iter. Total: 0.4439 s/iter. ETA=0:00:45
[07/11 13:55:18] d2.evaluation.evaluator INFO: Inference done 663/754. Dataloading: 0.0008 s/iter. Inference: 0.1389 s/iter. Eval: 0.3037 s/iter. Total: 0.4435 s/iter. ETA=0:00:40
[07/11 13:55:23] d2.evaluation.evaluator INFO: Inference done 675/754. Dataloading: 0.0008 s/iter. Inference: 0.1388 s/iter. Eval: 0.3038 s/iter. Total: 0.4435 s/iter. ETA=0:00:35
[07/11 13:55:28] d2.evaluation.evaluator INFO: Inference done 687/754. Dataloading: 0.0008 s/iter. Inference: 0.1388 s/iter. Eval: 0.3036 s/iter. Total: 0.4433 s/iter. ETA=0:00:29
[07/11 13:55:34] d2.evaluation.evaluator INFO: Inference done 698/754. Dataloading: 0.0008 s/iter. Inference: 0.1390 s/iter. Eval: 0.3038 s/iter. Total: 0.4437 s/iter. ETA=0:00:24
[07/11 13:55:39] d2.evaluation.evaluator INFO: Inference done 709/754. Dataloading: 0.0008 s/iter. Inference: 0.1391 s/iter. Eval: 0.3042 s/iter. Total: 0.4441 s/iter. ETA=0:00:19
[07/11 13:55:44] d2.evaluation.evaluator INFO: Inference done 721/754. Dataloading: 0.0008 s/iter. Inference: 0.1391 s/iter. Eval: 0.3039 s/iter. Total: 0.4439 s/iter. ETA=0:00:14
[07/11 13:55:49] d2.evaluation.evaluator INFO: Inference done 732/754. Dataloading: 0.0008 s/iter. Inference: 0.1392 s/iter. Eval: 0.3040 s/iter. Total: 0.4441 s/iter. ETA=0:00:09
[07/11 13:55:54] d2.evaluation.evaluator INFO: Inference done 744/754. Dataloading: 0.0008 s/iter. Inference: 0.1391 s/iter. Eval: 0.3042 s/iter. Total: 0.4441 s/iter. ETA=0:00:04
[07/11 13:55:59] d2.evaluation.evaluator INFO: Total inference time: 0:05:32.688746 (0.444177 s / iter per device, on 1 devices)
[07/11 13:55:59] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:44 (0.139232 s / iter per device, on 1 devices)
[07/11 13:56:00] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/11 13:56:00] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/11 13:56:01] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/11 13:56:03] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/11 13:56:03] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 13:56:03] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/11 13:56:12] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 65.877 | 65.877 | 65.877 |  nan  |  nan  | 65.877 |
[07/11 13:56:12] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 13:56:12] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 78.836 | defect     | 52.918 |
[07/11 13:56:12] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_new in csv format:
[07/11 13:56:12] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/11 13:56:12] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 13:56:12] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/11 13:56:12] d2.evaluation.testing INFO: copypaste: Task: segm
[07/11 13:56:12] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 13:56:12] d2.evaluation.testing INFO: copypaste: 65.8768,65.8768,65.8768,nan,nan,65.8768
[07/11 13:56:12] d2.utils.events INFO:  eta: 2:08:54  iter: 14999  total_loss: 4.47  loss_ce: 0.1671  loss_mask: 0.1929  loss_dice: 0.1254  loss_ce_0: 0.07714  loss_mask_0: 0.1919  loss_dice_0: 0.1308  loss_ce_1: 0.1664  loss_mask_1: 0.1926  loss_dice_1: 0.1264  loss_ce_2: 0.1664  loss_mask_2: 0.1944  loss_dice_2: 0.1281  loss_ce_3: 0.1666  loss_mask_3: 0.193  loss_dice_3: 0.1284  loss_ce_4: 0.1666  loss_mask_4: 0.1853  loss_dice_4: 0.1319  loss_ce_5: 0.1669  loss_mask_5: 0.1956  loss_dice_5: 0.1346  loss_ce_6: 0.1668  loss_mask_6: 0.1922  loss_dice_6: 0.1311  loss_ce_7: 0.1667  loss_mask_7: 0.1925  loss_dice_7: 0.1353  loss_ce_8: 0.167  loss_mask_8: 0.1955  loss_dice_8: 0.1299  time: 0.5703  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:56:12] d2.engine.hooks INFO: Not saving as latest eval score for total_loss is 4.77388, not better than best score 4.45740 @ iteration 4999.
[07/11 13:56:23] d2.utils.events INFO:  eta: 2:08:53  iter: 15019  total_loss: 4.838  loss_ce: 0.1651  loss_mask: 0.1705  loss_dice: 0.1499  loss_ce_0: 0.0767  loss_mask_0: 0.1773  loss_dice_0: 0.1509  loss_ce_1: 0.1638  loss_mask_1: 0.1716  loss_dice_1: 0.1544  loss_ce_2: 0.164  loss_mask_2: 0.1742  loss_dice_2: 0.1527  loss_ce_3: 0.1641  loss_mask_3: 0.1733  loss_dice_3: 0.1521  loss_ce_4: 0.1642  loss_mask_4: 0.1777  loss_dice_4: 0.1526  loss_ce_5: 0.1642  loss_mask_5: 0.1765  loss_dice_5: 0.1501  loss_ce_6: 0.1645  loss_mask_6: 0.1729  loss_dice_6: 0.151  loss_ce_7: 0.1645  loss_mask_7: 0.1765  loss_dice_7: 0.153  loss_ce_8: 0.1644  loss_mask_8: 0.1768  loss_dice_8: 0.1534  time: 0.5703  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:56:35] d2.utils.events INFO:  eta: 2:09:02  iter: 15039  total_loss: 4.448  loss_ce: 0.0967  loss_mask: 0.1729  loss_dice: 0.1392  loss_ce_0: 0.05218  loss_mask_0: 0.1703  loss_dice_0: 0.146  loss_ce_1: 0.09785  loss_mask_1: 0.1672  loss_dice_1: 0.1435  loss_ce_2: 0.09768  loss_mask_2: 0.1726  loss_dice_2: 0.1438  loss_ce_3: 0.09765  loss_mask_3: 0.1621  loss_dice_3: 0.1469  loss_ce_4: 0.09758  loss_mask_4: 0.1738  loss_dice_4: 0.145  loss_ce_5: 0.09755  loss_mask_5: 0.1678  loss_dice_5: 0.1477  loss_ce_6: 0.09729  loss_mask_6: 0.1673  loss_dice_6: 0.1468  loss_ce_7: 0.09725  loss_mask_7: 0.1771  loss_dice_7: 0.1464  loss_ce_8: 0.09689  loss_mask_8: 0.1707  loss_dice_8: 0.145  time: 0.5703  data_time: 0.0034  lr: 1e-05  max_mem: 2811M
[07/11 13:56:46] d2.utils.events INFO:  eta: 2:09:13  iter: 15059  total_loss: 4.732  loss_ce: 0.1627  loss_mask: 0.1869  loss_dice: 0.1385  loss_ce_0: 0.07617  loss_mask_0: 0.1862  loss_dice_0: 0.131  loss_ce_1: 0.1611  loss_mask_1: 0.1881  loss_dice_1: 0.1357  loss_ce_2: 0.1614  loss_mask_2: 0.1846  loss_dice_2: 0.1286  loss_ce_3: 0.1614  loss_mask_3: 0.1858  loss_dice_3: 0.1352  loss_ce_4: 0.1614  loss_mask_4: 0.189  loss_dice_4: 0.1379  loss_ce_5: 0.1613  loss_mask_5: 0.1898  loss_dice_5: 0.1366  loss_ce_6: 0.1617  loss_mask_6: 0.2001  loss_dice_6: 0.1356  loss_ce_7: 0.1619  loss_mask_7: 0.1792  loss_dice_7: 0.1395  loss_ce_8: 0.1625  loss_mask_8: 0.1872  loss_dice_8: 0.1371  time: 0.5703  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:56:58] d2.utils.events INFO:  eta: 2:09:14  iter: 15079  total_loss: 4.587  loss_ce: 0.09874  loss_mask: 0.1924  loss_dice: 0.1412  loss_ce_0: 0.05265  loss_mask_0: 0.1994  loss_dice_0: 0.1325  loss_ce_1: 0.1003  loss_mask_1: 0.1906  loss_dice_1: 0.137  loss_ce_2: 0.09997  loss_mask_2: 0.2086  loss_dice_2: 0.1392  loss_ce_3: 0.09997  loss_mask_3: 0.1894  loss_dice_3: 0.1375  loss_ce_4: 0.09997  loss_mask_4: 0.2001  loss_dice_4: 0.1334  loss_ce_5: 0.1  loss_mask_5: 0.2019  loss_dice_5: 0.1328  loss_ce_6: 0.09967  loss_mask_6: 0.2014  loss_dice_6: 0.1405  loss_ce_7: 0.09944  loss_mask_7: 0.1908  loss_dice_7: 0.1371  loss_ce_8: 0.099  loss_mask_8: 0.1901  loss_dice_8: 0.1311  time: 0.5703  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:57:09] d2.utils.events INFO:  eta: 2:09:12  iter: 15099  total_loss: 4.916  loss_ce: 0.13  loss_mask: 0.2304  loss_dice: 0.1438  loss_ce_0: 0.06424  loss_mask_0: 0.2187  loss_dice_0: 0.1403  loss_ce_1: 0.1302  loss_mask_1: 0.214  loss_dice_1: 0.1398  loss_ce_2: 0.13  loss_mask_2: 0.2181  loss_dice_2: 0.1452  loss_ce_3: 0.1299  loss_mask_3: 0.2211  loss_dice_3: 0.1423  loss_ce_4: 0.13  loss_mask_4: 0.2412  loss_dice_4: 0.1462  loss_ce_5: 0.1299  loss_mask_5: 0.2196  loss_dice_5: 0.1415  loss_ce_6: 0.1299  loss_mask_6: 0.2227  loss_dice_6: 0.1444  loss_ce_7: 0.1299  loss_mask_7: 0.2219  loss_dice_7: 0.1413  loss_ce_8: 0.13  loss_mask_8: 0.2277  loss_dice_8: 0.1482  time: 0.5703  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:57:21] d2.utils.events INFO:  eta: 2:09:27  iter: 15119  total_loss: 4.838  loss_ce: 0.09755  loss_mask: 0.1971  loss_dice: 0.1438  loss_ce_0: 0.05234  loss_mask_0: 0.1857  loss_dice_0: 0.1464  loss_ce_1: 0.09822  loss_mask_1: 0.2035  loss_dice_1: 0.1452  loss_ce_2: 0.0979  loss_mask_2: 0.2075  loss_dice_2: 0.1473  loss_ce_3: 0.09795  loss_mask_3: 0.197  loss_dice_3: 0.1502  loss_ce_4: 0.09814  loss_mask_4: 0.2015  loss_dice_4: 0.1489  loss_ce_5: 0.09818  loss_mask_5: 0.2004  loss_dice_5: 0.1498  loss_ce_6: 0.09792  loss_mask_6: 0.1952  loss_dice_6: 0.1477  loss_ce_7: 0.09788  loss_mask_7: 0.2034  loss_dice_7: 0.1527  loss_ce_8: 0.09766  loss_mask_8: 0.2033  loss_dice_8: 0.1492  time: 0.5703  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:57:32] d2.utils.events INFO:  eta: 2:09:41  iter: 15139  total_loss: 4.593  loss_ce: 0.1625  loss_mask: 0.1507  loss_dice: 0.1627  loss_ce_0: 0.07621  loss_mask_0: 0.1508  loss_dice_0: 0.1635  loss_ce_1: 0.1614  loss_mask_1: 0.1533  loss_dice_1: 0.1594  loss_ce_2: 0.162  loss_mask_2: 0.1584  loss_dice_2: 0.1616  loss_ce_3: 0.162  loss_mask_3: 0.1472  loss_dice_3: 0.1646  loss_ce_4: 0.1618  loss_mask_4: 0.1497  loss_dice_4: 0.1651  loss_ce_5: 0.1619  loss_mask_5: 0.1534  loss_dice_5: 0.1614  loss_ce_6: 0.1618  loss_mask_6: 0.1544  loss_dice_6: 0.1624  loss_ce_7: 0.1623  loss_mask_7: 0.1532  loss_dice_7: 0.1579  loss_ce_8: 0.1625  loss_mask_8: 0.1581  loss_dice_8: 0.1647  time: 0.5703  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:57:43] d2.utils.events INFO:  eta: 2:09:46  iter: 15159  total_loss: 4.585  loss_ce: 0.09796  loss_mask: 0.1885  loss_dice: 0.1483  loss_ce_0: 0.05243  loss_mask_0: 0.1909  loss_dice_0: 0.1454  loss_ce_1: 0.0985  loss_mask_1: 0.181  loss_dice_1: 0.15  loss_ce_2: 0.09825  loss_mask_2: 0.1823  loss_dice_2: 0.1448  loss_ce_3: 0.09836  loss_mask_3: 0.1945  loss_dice_3: 0.1454  loss_ce_4: 0.09844  loss_mask_4: 0.186  loss_dice_4: 0.1408  loss_ce_5: 0.09855  loss_mask_5: 0.185  loss_dice_5: 0.1507  loss_ce_6: 0.09829  loss_mask_6: 0.1938  loss_dice_6: 0.1496  loss_ce_7: 0.09821  loss_mask_7: 0.1877  loss_dice_7: 0.1433  loss_ce_8: 0.09803  loss_mask_8: 0.1875  loss_dice_8: 0.1478  time: 0.5703  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 13:57:55] d2.utils.events INFO:  eta: 2:09:46  iter: 15179  total_loss: 4.539  loss_ce: 0.09836  loss_mask: 0.1816  loss_dice: 0.1486  loss_ce_0: 0.0525  loss_mask_0: 0.1722  loss_dice_0: 0.1516  loss_ce_1: 0.09905  loss_mask_1: 0.1782  loss_dice_1: 0.1581  loss_ce_2: 0.09877  loss_mask_2: 0.1805  loss_dice_2: 0.1448  loss_ce_3: 0.09881  loss_mask_3: 0.1694  loss_dice_3: 0.1544  loss_ce_4: 0.09899  loss_mask_4: 0.1774  loss_dice_4: 0.1571  loss_ce_5: 0.09903  loss_mask_5: 0.1692  loss_dice_5: 0.1584  loss_ce_6: 0.09881  loss_mask_6: 0.1805  loss_dice_6: 0.1558  loss_ce_7: 0.09862  loss_mask_7: 0.1723  loss_dice_7: 0.1489  loss_ce_8: 0.09851  loss_mask_8: 0.1721  loss_dice_8: 0.1544  time: 0.5703  data_time: 0.0023  lr: 1e-05  max_mem: 2811M
[07/11 13:58:06] d2.utils.events INFO:  eta: 2:09:52  iter: 15199  total_loss: 4.546  loss_ce: 0.09836  loss_mask: 0.2035  loss_dice: 0.1375  loss_ce_0: 0.05249  loss_mask_0: 0.2117  loss_dice_0: 0.1396  loss_ce_1: 0.09905  loss_mask_1: 0.203  loss_dice_1: 0.1357  loss_ce_2: 0.09864  loss_mask_2: 0.2029  loss_dice_2: 0.1409  loss_ce_3: 0.09873  loss_mask_3: 0.2063  loss_dice_3: 0.1451  loss_ce_4: 0.09895  loss_mask_4: 0.1999  loss_dice_4: 0.1347  loss_ce_5: 0.09888  loss_mask_5: 0.2055  loss_dice_5: 0.1409  loss_ce_6: 0.09866  loss_mask_6: 0.1964  loss_dice_6: 0.137  loss_ce_7: 0.09858  loss_mask_7: 0.2029  loss_dice_7: 0.1387  loss_ce_8: 0.0984  loss_mask_8: 0.1959  loss_dice_8: 0.1371  time: 0.5703  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:58:18] d2.utils.events INFO:  eta: 2:09:57  iter: 15219  total_loss: 4.742  loss_ce: 0.1617  loss_mask: 0.1958  loss_dice: 0.1479  loss_ce_0: 0.07598  loss_mask_0: 0.1971  loss_dice_0: 0.149  loss_ce_1: 0.1609  loss_mask_1: 0.2  loss_dice_1: 0.1461  loss_ce_2: 0.1613  loss_mask_2: 0.1961  loss_dice_2: 0.1413  loss_ce_3: 0.1613  loss_mask_3: 0.1859  loss_dice_3: 0.1412  loss_ce_4: 0.161  loss_mask_4: 0.1898  loss_dice_4: 0.1378  loss_ce_5: 0.161  loss_mask_5: 0.1941  loss_dice_5: 0.1472  loss_ce_6: 0.1613  loss_mask_6: 0.1896  loss_dice_6: 0.1424  loss_ce_7: 0.1614  loss_mask_7: 0.1919  loss_dice_7: 0.143  loss_ce_8: 0.1615  loss_mask_8: 0.1949  loss_dice_8: 0.1425  time: 0.5704  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 13:58:29] d2.utils.events INFO:  eta: 2:10:09  iter: 15239  total_loss: 4.443  loss_ce: 0.09885  loss_mask: 0.1919  loss_dice: 0.1443  loss_ce_0: 0.05261  loss_mask_0: 0.1967  loss_dice_0: 0.1464  loss_ce_1: 0.09965  loss_mask_1: 0.1966  loss_dice_1: 0.1447  loss_ce_2: 0.09926  loss_mask_2: 0.1982  loss_dice_2: 0.1457  loss_ce_3: 0.09929  loss_mask_3: 0.1831  loss_dice_3: 0.1472  loss_ce_4: 0.09948  loss_mask_4: 0.1974  loss_dice_4: 0.1446  loss_ce_5: 0.09941  loss_mask_5: 0.1922  loss_dice_5: 0.1467  loss_ce_6: 0.09937  loss_mask_6: 0.1938  loss_dice_6: 0.144  loss_ce_7: 0.09907  loss_mask_7: 0.1957  loss_dice_7: 0.1459  loss_ce_8: 0.09903  loss_mask_8: 0.1832  loss_dice_8: 0.1513  time: 0.5703  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:58:41] d2.utils.events INFO:  eta: 2:10:27  iter: 15259  total_loss: 4.657  loss_ce: 0.1596  loss_mask: 0.1842  loss_dice: 0.1347  loss_ce_0: 0.07555  loss_mask_0: 0.1929  loss_dice_0: 0.1392  loss_ce_1: 0.158  loss_mask_1: 0.1946  loss_dice_1: 0.148  loss_ce_2: 0.1587  loss_mask_2: 0.1823  loss_dice_2: 0.1361  loss_ce_3: 0.1588  loss_mask_3: 0.2047  loss_dice_3: 0.1409  loss_ce_4: 0.1585  loss_mask_4: 0.1975  loss_dice_4: 0.1394  loss_ce_5: 0.1586  loss_mask_5: 0.174  loss_dice_5: 0.1437  loss_ce_6: 0.1587  loss_mask_6: 0.1906  loss_dice_6: 0.1411  loss_ce_7: 0.1592  loss_mask_7: 0.1882  loss_dice_7: 0.143  loss_ce_8: 0.1594  loss_mask_8: 0.186  loss_dice_8: 0.1321  time: 0.5703  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 13:58:52] d2.utils.events INFO:  eta: 2:10:31  iter: 15279  total_loss: 4.945  loss_ce: 0.0999  loss_mask: 0.2362  loss_dice: 0.1457  loss_ce_0: 0.05287  loss_mask_0: 0.2321  loss_dice_0: 0.1434  loss_ce_1: 0.1007  loss_mask_1: 0.2379  loss_dice_1: 0.1432  loss_ce_2: 0.1005  loss_mask_2: 0.2302  loss_dice_2: 0.1413  loss_ce_3: 0.1006  loss_mask_3: 0.2383  loss_dice_3: 0.1457  loss_ce_4: 0.1006  loss_mask_4: 0.2423  loss_dice_4: 0.1467  loss_ce_5: 0.1006  loss_mask_5: 0.2291  loss_dice_5: 0.1435  loss_ce_6: 0.1004  loss_mask_6: 0.2413  loss_dice_6: 0.1498  loss_ce_7: 0.1002  loss_mask_7: 0.2296  loss_dice_7: 0.1442  loss_ce_8: 0.1  loss_mask_8: 0.2325  loss_dice_8: 0.139  time: 0.5703  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:59:04] d2.utils.events INFO:  eta: 2:10:55  iter: 15299  total_loss: 4.683  loss_ce: 0.1596  loss_mask: 0.1563  loss_dice: 0.1537  loss_ce_0: 0.07546  loss_mask_0: 0.15  loss_dice_0: 0.1538  loss_ce_1: 0.1589  loss_mask_1: 0.1558  loss_dice_1: 0.1572  loss_ce_2: 0.1592  loss_mask_2: 0.1584  loss_dice_2: 0.1612  loss_ce_3: 0.1591  loss_mask_3: 0.16  loss_dice_3: 0.164  loss_ce_4: 0.1588  loss_mask_4: 0.1524  loss_dice_4: 0.1588  loss_ce_5: 0.1588  loss_mask_5: 0.1485  loss_dice_5: 0.1504  loss_ce_6: 0.1592  loss_mask_6: 0.1526  loss_dice_6: 0.1551  loss_ce_7: 0.1592  loss_mask_7: 0.1486  loss_dice_7: 0.1526  loss_ce_8: 0.1591  loss_mask_8: 0.1609  loss_dice_8: 0.1589  time: 0.5704  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 13:59:17] d2.utils.events INFO:  eta: 2:11:17  iter: 15319  total_loss: 4.974  loss_ce: 0.1294  loss_mask: 0.1729  loss_dice: 0.1666  loss_ce_0: 0.0641  loss_mask_0: 0.1735  loss_dice_0: 0.1678  loss_ce_1: 0.1292  loss_mask_1: 0.1801  loss_dice_1: 0.1653  loss_ce_2: 0.1293  loss_mask_2: 0.1759  loss_dice_2: 0.1743  loss_ce_3: 0.1293  loss_mask_3: 0.1702  loss_dice_3: 0.1681  loss_ce_4: 0.1293  loss_mask_4: 0.1673  loss_dice_4: 0.1652  loss_ce_5: 0.1293  loss_mask_5: 0.1821  loss_dice_5: 0.17  loss_ce_6: 0.1293  loss_mask_6: 0.1696  loss_dice_6: 0.1699  loss_ce_7: 0.1295  loss_mask_7: 0.181  loss_dice_7: 0.1622  loss_ce_8: 0.1297  loss_mask_8: 0.1747  loss_dice_8: 0.1618  time: 0.5705  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:59:30] d2.utils.events INFO:  eta: 2:11:37  iter: 15339  total_loss: 4.741  loss_ce: 0.1012  loss_mask: 0.2139  loss_dice: 0.1591  loss_ce_0: 0.05315  loss_mask_0: 0.203  loss_dice_0: 0.1616  loss_ce_1: 0.1023  loss_mask_1: 0.2003  loss_dice_1: 0.1683  loss_ce_2: 0.1018  loss_mask_2: 0.1974  loss_dice_2: 0.1505  loss_ce_3: 0.1018  loss_mask_3: 0.2075  loss_dice_3: 0.1535  loss_ce_4: 0.1021  loss_mask_4: 0.2057  loss_dice_4: 0.1518  loss_ce_5: 0.1021  loss_mask_5: 0.2004  loss_dice_5: 0.1535  loss_ce_6: 0.1018  loss_mask_6: 0.2001  loss_dice_6: 0.1604  loss_ce_7: 0.1016  loss_mask_7: 0.2055  loss_dice_7: 0.1552  loss_ce_8: 0.1013  loss_mask_8: 0.2085  loss_dice_8: 0.1619  time: 0.5706  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 13:59:42] d2.utils.events INFO:  eta: 2:12:11  iter: 15359  total_loss: 4.517  loss_ce: 0.1004  loss_mask: 0.1972  loss_dice: 0.1358  loss_ce_0: 0.05299  loss_mask_0: 0.1957  loss_dice_0: 0.1344  loss_ce_1: 0.1009  loss_mask_1: 0.1925  loss_dice_1: 0.1315  loss_ce_2: 0.1006  loss_mask_2: 0.2037  loss_dice_2: 0.13  loss_ce_3: 0.1007  loss_mask_3: 0.1978  loss_dice_3: 0.1378  loss_ce_4: 0.1009  loss_mask_4: 0.1925  loss_dice_4: 0.1337  loss_ce_5: 0.1009  loss_mask_5: 0.1985  loss_dice_5: 0.1368  loss_ce_6: 0.1008  loss_mask_6: 0.2021  loss_dice_6: 0.136  loss_ce_7: 0.1006  loss_mask_7: 0.19  loss_dice_7: 0.1397  loss_ce_8: 0.1005  loss_mask_8: 0.196  loss_dice_8: 0.1345  time: 0.5706  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 13:59:55] d2.utils.events INFO:  eta: 2:12:29  iter: 15379  total_loss: 4.78  loss_ce: 0.1298  loss_mask: 0.1845  loss_dice: 0.1495  loss_ce_0: 0.06415  loss_mask_0: 0.1746  loss_dice_0: 0.1456  loss_ce_1: 0.1298  loss_mask_1: 0.1861  loss_dice_1: 0.1478  loss_ce_2: 0.1298  loss_mask_2: 0.1834  loss_dice_2: 0.1518  loss_ce_3: 0.1297  loss_mask_3: 0.1753  loss_dice_3: 0.1477  loss_ce_4: 0.1297  loss_mask_4: 0.1786  loss_dice_4: 0.1471  loss_ce_5: 0.1298  loss_mask_5: 0.1795  loss_dice_5: 0.1439  loss_ce_6: 0.1298  loss_mask_6: 0.1758  loss_dice_6: 0.1525  loss_ce_7: 0.1297  loss_mask_7: 0.1765  loss_dice_7: 0.1484  loss_ce_8: 0.1298  loss_mask_8: 0.1696  loss_dice_8: 0.1466  time: 0.5707  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:00:08] d2.utils.events INFO:  eta: 2:12:52  iter: 15399  total_loss: 4.237  loss_ce: 0.09888  loss_mask: 0.1735  loss_dice: 0.1458  loss_ce_0: 0.05264  loss_mask_0: 0.1705  loss_dice_0: 0.1437  loss_ce_1: 0.0989  loss_mask_1: 0.1652  loss_dice_1: 0.1404  loss_ce_2: 0.09858  loss_mask_2: 0.1759  loss_dice_2: 0.1427  loss_ce_3: 0.09869  loss_mask_3: 0.1777  loss_dice_3: 0.1545  loss_ce_4: 0.09906  loss_mask_4: 0.172  loss_dice_4: 0.1557  loss_ce_5: 0.09888  loss_mask_5: 0.1684  loss_dice_5: 0.1457  loss_ce_6: 0.09877  loss_mask_6: 0.1652  loss_dice_6: 0.1493  loss_ce_7: 0.09885  loss_mask_7: 0.1732  loss_dice_7: 0.1503  loss_ce_8: 0.09884  loss_mask_8: 0.1659  loss_dice_8: 0.1493  time: 0.5708  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:00:20] d2.utils.events INFO:  eta: 2:13:16  iter: 15419  total_loss: 4.49  loss_ce: 0.1601  loss_mask: 0.1913  loss_dice: 0.1354  loss_ce_0: 0.07553  loss_mask_0: 0.1901  loss_dice_0: 0.1343  loss_ce_1: 0.1597  loss_mask_1: 0.1929  loss_dice_1: 0.1214  loss_ce_2: 0.1601  loss_mask_2: 0.1969  loss_dice_2: 0.1306  loss_ce_3: 0.16  loss_mask_3: 0.194  loss_dice_3: 0.1357  loss_ce_4: 0.1596  loss_mask_4: 0.1968  loss_dice_4: 0.1328  loss_ce_5: 0.1599  loss_mask_5: 0.193  loss_dice_5: 0.1324  loss_ce_6: 0.16  loss_mask_6: 0.1853  loss_dice_6: 0.1353  loss_ce_7: 0.1599  loss_mask_7: 0.1932  loss_dice_7: 0.1304  loss_ce_8: 0.1601  loss_mask_8: 0.1924  loss_dice_8: 0.1292  time: 0.5709  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:00:33] d2.utils.events INFO:  eta: 2:13:52  iter: 15439  total_loss: 4.797  loss_ce: 0.1576  loss_mask: 0.2053  loss_dice: 0.1398  loss_ce_0: 0.07498  loss_mask_0: 0.2057  loss_dice_0: 0.1377  loss_ce_1: 0.1565  loss_mask_1: 0.2022  loss_dice_1: 0.1358  loss_ce_2: 0.1569  loss_mask_2: 0.2047  loss_dice_2: 0.1379  loss_ce_3: 0.1568  loss_mask_3: 0.2025  loss_dice_3: 0.1377  loss_ce_4: 0.1566  loss_mask_4: 0.2059  loss_dice_4: 0.1424  loss_ce_5: 0.1564  loss_mask_5: 0.2003  loss_dice_5: 0.1366  loss_ce_6: 0.157  loss_mask_6: 0.199  loss_dice_6: 0.1375  loss_ce_7: 0.1573  loss_mask_7: 0.2034  loss_dice_7: 0.1367  loss_ce_8: 0.1574  loss_mask_8: 0.1986  loss_dice_8: 0.1412  time: 0.5710  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:00:46] d2.utils.events INFO:  eta: 2:14:29  iter: 15459  total_loss: 4.811  loss_ce: 0.1565  loss_mask: 0.2249  loss_dice: 0.1359  loss_ce_0: 0.07465  loss_mask_0: 0.2252  loss_dice_0: 0.1385  loss_ce_1: 0.155  loss_mask_1: 0.2246  loss_dice_1: 0.1395  loss_ce_2: 0.1554  loss_mask_2: 0.2163  loss_dice_2: 0.1345  loss_ce_3: 0.1556  loss_mask_3: 0.2188  loss_dice_3: 0.1364  loss_ce_4: 0.1553  loss_mask_4: 0.2246  loss_dice_4: 0.1356  loss_ce_5: 0.1553  loss_mask_5: 0.2191  loss_dice_5: 0.1342  loss_ce_6: 0.1557  loss_mask_6: 0.2149  loss_dice_6: 0.131  loss_ce_7: 0.156  loss_mask_7: 0.2262  loss_dice_7: 0.1294  loss_ce_8: 0.156  loss_mask_8: 0.2194  loss_dice_8: 0.1377  time: 0.5710  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:00:58] d2.utils.events INFO:  eta: 2:15:03  iter: 15479  total_loss: 4.477  loss_ce: 0.1029  loss_mask: 0.1667  loss_dice: 0.1278  loss_ce_0: 0.05355  loss_mask_0: 0.1702  loss_dice_0: 0.1309  loss_ce_1: 0.1041  loss_mask_1: 0.1691  loss_dice_1: 0.1292  loss_ce_2: 0.1036  loss_mask_2: 0.1622  loss_dice_2: 0.1305  loss_ce_3: 0.1037  loss_mask_3: 0.1617  loss_dice_3: 0.126  loss_ce_4: 0.1039  loss_mask_4: 0.1603  loss_dice_4: 0.1313  loss_ce_5: 0.1039  loss_mask_5: 0.1667  loss_dice_5: 0.1296  loss_ce_6: 0.1035  loss_mask_6: 0.1673  loss_dice_6: 0.1274  loss_ce_7: 0.1032  loss_mask_7: 0.1661  loss_dice_7: 0.1317  loss_ce_8: 0.103  loss_mask_8: 0.1615  loss_dice_8: 0.1322  time: 0.5711  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:01:10] d2.utils.events INFO:  eta: 2:15:37  iter: 15499  total_loss: 4.606  loss_ce: 0.1029  loss_mask: 0.2142  loss_dice: 0.1555  loss_ce_0: 0.05358  loss_mask_0: 0.2075  loss_dice_0: 0.1494  loss_ce_1: 0.104  loss_mask_1: 0.2139  loss_dice_1: 0.1514  loss_ce_2: 0.1035  loss_mask_2: 0.2066  loss_dice_2: 0.1492  loss_ce_3: 0.1036  loss_mask_3: 0.194  loss_dice_3: 0.1461  loss_ce_4: 0.1039  loss_mask_4: 0.2108  loss_dice_4: 0.1503  loss_ce_5: 0.1039  loss_mask_5: 0.2075  loss_dice_5: 0.1489  loss_ce_6: 0.1035  loss_mask_6: 0.2047  loss_dice_6: 0.144  loss_ce_7: 0.1033  loss_mask_7: 0.2112  loss_dice_7: 0.1499  loss_ce_8: 0.1031  loss_mask_8: 0.1968  loss_dice_8: 0.1444  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:01:22] d2.utils.events INFO:  eta: 2:15:46  iter: 15519  total_loss: 4.862  loss_ce: 0.1292  loss_mask: 0.1961  loss_dice: 0.1454  loss_ce_0: 0.064  loss_mask_0: 0.1991  loss_dice_0: 0.1466  loss_ce_1: 0.1291  loss_mask_1: 0.1859  loss_dice_1: 0.1477  loss_ce_2: 0.1292  loss_mask_2: 0.1916  loss_dice_2: 0.1528  loss_ce_3: 0.1292  loss_mask_3: 0.1876  loss_dice_3: 0.142  loss_ce_4: 0.1291  loss_mask_4: 0.1981  loss_dice_4: 0.1478  loss_ce_5: 0.1291  loss_mask_5: 0.192  loss_dice_5: 0.1421  loss_ce_6: 0.1292  loss_mask_6: 0.1928  loss_dice_6: 0.1492  loss_ce_7: 0.1293  loss_mask_7: 0.1922  loss_dice_7: 0.1452  loss_ce_8: 0.1296  loss_mask_8: 0.1918  loss_dice_8: 0.1454  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:01:33] d2.utils.events INFO:  eta: 2:15:45  iter: 15539  total_loss: 4.727  loss_ce: 0.1552  loss_mask: 0.2007  loss_dice: 0.135  loss_ce_0: 0.07432  loss_mask_0: 0.2078  loss_dice_0: 0.136  loss_ce_1: 0.1543  loss_mask_1: 0.2066  loss_dice_1: 0.134  loss_ce_2: 0.1548  loss_mask_2: 0.1958  loss_dice_2: 0.1321  loss_ce_3: 0.1545  loss_mask_3: 0.2029  loss_dice_3: 0.1385  loss_ce_4: 0.1544  loss_mask_4: 0.2014  loss_dice_4: 0.1367  loss_ce_5: 0.1541  loss_mask_5: 0.1974  loss_dice_5: 0.1414  loss_ce_6: 0.1548  loss_mask_6: 0.2064  loss_dice_6: 0.1337  loss_ce_7: 0.1548  loss_mask_7: 0.2125  loss_dice_7: 0.1379  loss_ce_8: 0.1551  loss_mask_8: 0.2027  loss_dice_8: 0.1346  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:01:45] d2.utils.events INFO:  eta: 2:16:10  iter: 15559  total_loss: 4.578  loss_ce: 0.1045  loss_mask: 0.1745  loss_dice: 0.1529  loss_ce_0: 0.05395  loss_mask_0: 0.1914  loss_dice_0: 0.1444  loss_ce_1: 0.106  loss_mask_1: 0.1882  loss_dice_1: 0.1524  loss_ce_2: 0.1054  loss_mask_2: 0.1936  loss_dice_2: 0.1561  loss_ce_3: 0.1054  loss_mask_3: 0.1912  loss_dice_3: 0.1515  loss_ce_4: 0.1056  loss_mask_4: 0.1914  loss_dice_4: 0.149  loss_ce_5: 0.1057  loss_mask_5: 0.1814  loss_dice_5: 0.1547  loss_ce_6: 0.1052  loss_mask_6: 0.1747  loss_dice_6: 0.1491  loss_ce_7: 0.1049  loss_mask_7: 0.1883  loss_dice_7: 0.1539  loss_ce_8: 0.1047  loss_mask_8: 0.182  loss_dice_8: 0.1484  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:01:56] d2.utils.events INFO:  eta: 2:16:11  iter: 15579  total_loss: 4.665  loss_ce: 0.1046  loss_mask: 0.1822  loss_dice: 0.1472  loss_ce_0: 0.054  loss_mask_0: 0.1854  loss_dice_0: 0.1432  loss_ce_1: 0.1059  loss_mask_1: 0.1873  loss_dice_1: 0.1517  loss_ce_2: 0.1053  loss_mask_2: 0.1859  loss_dice_2: 0.1466  loss_ce_3: 0.1054  loss_mask_3: 0.1915  loss_dice_3: 0.1504  loss_ce_4: 0.1057  loss_mask_4: 0.1908  loss_dice_4: 0.1442  loss_ce_5: 0.1057  loss_mask_5: 0.1886  loss_dice_5: 0.1418  loss_ce_6: 0.1053  loss_mask_6: 0.1899  loss_dice_6: 0.1458  loss_ce_7: 0.1051  loss_mask_7: 0.1875  loss_dice_7: 0.1423  loss_ce_8: 0.1048  loss_mask_8: 0.1885  loss_dice_8: 0.1485  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:02:08] d2.utils.events INFO:  eta: 2:16:22  iter: 15599  total_loss: 4.34  loss_ce: 0.1046  loss_mask: 0.1838  loss_dice: 0.152  loss_ce_0: 0.05399  loss_mask_0: 0.1763  loss_dice_0: 0.1499  loss_ce_1: 0.1053  loss_mask_1: 0.1723  loss_dice_1: 0.1496  loss_ce_2: 0.1049  loss_mask_2: 0.1789  loss_dice_2: 0.1499  loss_ce_3: 0.1051  loss_mask_3: 0.1781  loss_dice_3: 0.1511  loss_ce_4: 0.1053  loss_mask_4: 0.1747  loss_dice_4: 0.1555  loss_ce_5: 0.1054  loss_mask_5: 0.1748  loss_dice_5: 0.152  loss_ce_6: 0.1049  loss_mask_6: 0.1741  loss_dice_6: 0.1544  loss_ce_7: 0.1048  loss_mask_7: 0.1857  loss_dice_7: 0.1555  loss_ce_8: 0.1047  loss_mask_8: 0.1784  loss_dice_8: 0.1537  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:02:19] d2.utils.events INFO:  eta: 2:16:16  iter: 15619  total_loss: 4.742  loss_ce: 0.1037  loss_mask: 0.1743  loss_dice: 0.1341  loss_ce_0: 0.05378  loss_mask_0: 0.1753  loss_dice_0: 0.1311  loss_ce_1: 0.1043  loss_mask_1: 0.1786  loss_dice_1: 0.131  loss_ce_2: 0.1038  loss_mask_2: 0.1768  loss_dice_2: 0.1358  loss_ce_3: 0.1039  loss_mask_3: 0.1838  loss_dice_3: 0.1302  loss_ce_4: 0.1042  loss_mask_4: 0.1819  loss_dice_4: 0.1271  loss_ce_5: 0.1041  loss_mask_5: 0.1744  loss_dice_5: 0.1309  loss_ce_6: 0.1039  loss_mask_6: 0.1819  loss_dice_6: 0.1321  loss_ce_7: 0.1039  loss_mask_7: 0.1808  loss_dice_7: 0.1384  loss_ce_8: 0.1037  loss_mask_8: 0.1752  loss_dice_8: 0.1341  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:02:31] d2.utils.events INFO:  eta: 2:16:15  iter: 15639  total_loss: 4.576  loss_ce: 0.1292  loss_mask: 0.1896  loss_dice: 0.1354  loss_ce_0: 0.06394  loss_mask_0: 0.1849  loss_dice_0: 0.1397  loss_ce_1: 0.1289  loss_mask_1: 0.1999  loss_dice_1: 0.1321  loss_ce_2: 0.1292  loss_mask_2: 0.1874  loss_dice_2: 0.1369  loss_ce_3: 0.1292  loss_mask_3: 0.1983  loss_dice_3: 0.1363  loss_ce_4: 0.1289  loss_mask_4: 0.1832  loss_dice_4: 0.1384  loss_ce_5: 0.129  loss_mask_5: 0.1924  loss_dice_5: 0.1334  loss_ce_6: 0.1291  loss_mask_6: 0.1913  loss_dice_6: 0.1439  loss_ce_7: 0.129  loss_mask_7: 0.1939  loss_dice_7: 0.1361  loss_ce_8: 0.1291  loss_mask_8: 0.1878  loss_dice_8: 0.1399  time: 0.5712  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:02:43] d2.utils.events INFO:  eta: 2:16:17  iter: 15659  total_loss: 4.493  loss_ce: 0.1028  loss_mask: 0.1937  loss_dice: 0.1498  loss_ce_0: 0.05368  loss_mask_0: 0.1903  loss_dice_0: 0.1434  loss_ce_1: 0.1028  loss_mask_1: 0.1931  loss_dice_1: 0.1487  loss_ce_2: 0.1026  loss_mask_2: 0.1898  loss_dice_2: 0.1423  loss_ce_3: 0.1027  loss_mask_3: 0.1849  loss_dice_3: 0.1449  loss_ce_4: 0.103  loss_mask_4: 0.1928  loss_dice_4: 0.1475  loss_ce_5: 0.1029  loss_mask_5: 0.1966  loss_dice_5: 0.146  loss_ce_6: 0.1027  loss_mask_6: 0.1928  loss_dice_6: 0.1473  loss_ce_7: 0.1028  loss_mask_7: 0.1874  loss_dice_7: 0.1475  loss_ce_8: 0.1029  loss_mask_8: 0.1956  loss_dice_8: 0.1472  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:02:54] d2.utils.events INFO:  eta: 2:16:29  iter: 15679  total_loss: 4.614  loss_ce: 0.1014  loss_mask: 0.2018  loss_dice: 0.1621  loss_ce_0: 0.0534  loss_mask_0: 0.1935  loss_dice_0: 0.1614  loss_ce_1: 0.101  loss_mask_1: 0.1985  loss_dice_1: 0.1595  loss_ce_2: 0.1009  loss_mask_2: 0.2009  loss_dice_2: 0.1572  loss_ce_3: 0.101  loss_mask_3: 0.201  loss_dice_3: 0.1568  loss_ce_4: 0.1013  loss_mask_4: 0.1966  loss_dice_4: 0.1585  loss_ce_5: 0.1011  loss_mask_5: 0.2005  loss_dice_5: 0.1534  loss_ce_6: 0.1012  loss_mask_6: 0.1972  loss_dice_6: 0.1613  loss_ce_7: 0.1013  loss_mask_7: 0.2005  loss_dice_7: 0.1537  loss_ce_8: 0.1014  loss_mask_8: 0.1969  loss_dice_8: 0.1567  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:03:06] d2.utils.events INFO:  eta: 2:16:38  iter: 15699  total_loss: 5.154  loss_ce: 0.1  loss_mask: 0.2183  loss_dice: 0.1653  loss_ce_0: 0.05308  loss_mask_0: 0.2081  loss_dice_0: 0.1615  loss_ce_1: 0.09912  loss_mask_1: 0.2119  loss_dice_1: 0.1628  loss_ce_2: 0.09923  loss_mask_2: 0.2111  loss_dice_2: 0.1606  loss_ce_3: 0.09936  loss_mask_3: 0.2122  loss_dice_3: 0.1668  loss_ce_4: 0.09948  loss_mask_4: 0.2061  loss_dice_4: 0.1708  loss_ce_5: 0.09944  loss_mask_5: 0.2131  loss_dice_5: 0.1682  loss_ce_6: 0.09963  loss_mask_6: 0.209  loss_dice_6: 0.1599  loss_ce_7: 0.09959  loss_mask_7: 0.2092  loss_dice_7: 0.1638  loss_ce_8: 0.09997  loss_mask_8: 0.2188  loss_dice_8: 0.1668  time: 0.5712  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:03:17] d2.utils.events INFO:  eta: 2:16:46  iter: 15719  total_loss: 4.318  loss_ce: 0.09967  loss_mask: 0.1999  loss_dice: 0.1284  loss_ce_0: 0.053  loss_mask_0: 0.2079  loss_dice_0: 0.1304  loss_ce_1: 0.09903  loss_mask_1: 0.1992  loss_dice_1: 0.1322  loss_ce_2: 0.09906  loss_mask_2: 0.2022  loss_dice_2: 0.1277  loss_ce_3: 0.09906  loss_mask_3: 0.1989  loss_dice_3: 0.1262  loss_ce_4: 0.09929  loss_mask_4: 0.2028  loss_dice_4: 0.1346  loss_ce_5: 0.09903  loss_mask_5: 0.2057  loss_dice_5: 0.1265  loss_ce_6: 0.09933  loss_mask_6: 0.2038  loss_dice_6: 0.1308  loss_ce_7: 0.09929  loss_mask_7: 0.1979  loss_dice_7: 0.1312  loss_ce_8: 0.09952  loss_mask_8: 0.2012  loss_dice_8: 0.1312  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:03:29] d2.utils.events INFO:  eta: 2:16:52  iter: 15739  total_loss: 4.753  loss_ce: 0.1608  loss_mask: 0.2002  loss_dice: 0.137  loss_ce_0: 0.07525  loss_mask_0: 0.2164  loss_dice_0: 0.142  loss_ce_1: 0.1612  loss_mask_1: 0.2082  loss_dice_1: 0.1414  loss_ce_2: 0.1615  loss_mask_2: 0.2119  loss_dice_2: 0.1387  loss_ce_3: 0.1614  loss_mask_3: 0.2103  loss_dice_3: 0.1412  loss_ce_4: 0.1612  loss_mask_4: 0.2008  loss_dice_4: 0.1362  loss_ce_5: 0.1617  loss_mask_5: 0.208  loss_dice_5: 0.1416  loss_ce_6: 0.1612  loss_mask_6: 0.2036  loss_dice_6: 0.1409  loss_ce_7: 0.1612  loss_mask_7: 0.2098  loss_dice_7: 0.1426  loss_ce_8: 0.1609  loss_mask_8: 0.2188  loss_dice_8: 0.1434  time: 0.5712  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:03:40] d2.utils.events INFO:  eta: 2:16:56  iter: 15759  total_loss: 4.668  loss_ce: 0.1297  loss_mask: 0.1917  loss_dice: 0.1519  loss_ce_0: 0.06402  loss_mask_0: 0.1964  loss_dice_0: 0.1515  loss_ce_1: 0.1295  loss_mask_1: 0.1997  loss_dice_1: 0.1515  loss_ce_2: 0.1296  loss_mask_2: 0.2018  loss_dice_2: 0.1568  loss_ce_3: 0.1296  loss_mask_3: 0.1968  loss_dice_3: 0.1497  loss_ce_4: 0.1297  loss_mask_4: 0.1895  loss_dice_4: 0.1475  loss_ce_5: 0.1296  loss_mask_5: 0.1917  loss_dice_5: 0.1508  loss_ce_6: 0.1297  loss_mask_6: 0.1973  loss_dice_6: 0.1497  loss_ce_7: 0.1296  loss_mask_7: 0.1942  loss_dice_7: 0.1539  loss_ce_8: 0.1297  loss_mask_8: 0.2009  loss_dice_8: 0.1557  time: 0.5712  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:03:52] d2.utils.events INFO:  eta: 2:17:11  iter: 15779  total_loss: 4.705  loss_ce: 0.1005  loss_mask: 0.2136  loss_dice: 0.1408  loss_ce_0: 0.05321  loss_mask_0: 0.21  loss_dice_0: 0.1409  loss_ce_1: 0.101  loss_mask_1: 0.2054  loss_dice_1: 0.1446  loss_ce_2: 0.1007  loss_mask_2: 0.1976  loss_dice_2: 0.1427  loss_ce_3: 0.1006  loss_mask_3: 0.1967  loss_dice_3: 0.138  loss_ce_4: 0.1008  loss_mask_4: 0.2099  loss_dice_4: 0.1429  loss_ce_5: 0.1006  loss_mask_5: 0.201  loss_dice_5: 0.1387  loss_ce_6: 0.1007  loss_mask_6: 0.2097  loss_dice_6: 0.1433  loss_ce_7: 0.1005  loss_mask_7: 0.206  loss_dice_7: 0.1474  loss_ce_8: 0.1005  loss_mask_8: 0.2056  loss_dice_8: 0.1392  time: 0.5713  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:04:04] d2.utils.events INFO:  eta: 2:17:10  iter: 15799  total_loss: 4.461  loss_ce: 0.1003  loss_mask: 0.1876  loss_dice: 0.1566  loss_ce_0: 0.05318  loss_mask_0: 0.1923  loss_dice_0: 0.1583  loss_ce_1: 0.1004  loss_mask_1: 0.1846  loss_dice_1: 0.1621  loss_ce_2: 0.1003  loss_mask_2: 0.1873  loss_dice_2: 0.1535  loss_ce_3: 0.1003  loss_mask_3: 0.176  loss_dice_3: 0.1501  loss_ce_4: 0.1004  loss_mask_4: 0.1855  loss_dice_4: 0.1551  loss_ce_5: 0.1003  loss_mask_5: 0.1892  loss_dice_5: 0.1554  loss_ce_6: 0.1003  loss_mask_6: 0.1761  loss_dice_6: 0.1502  loss_ce_7: 0.1002  loss_mask_7: 0.1884  loss_dice_7: 0.1511  loss_ce_8: 0.1002  loss_mask_8: 0.1866  loss_dice_8: 0.1532  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:04:15] d2.utils.events INFO:  eta: 2:17:03  iter: 15819  total_loss: 4.324  loss_ce: 0.09933  loss_mask: 0.1614  loss_dice: 0.1524  loss_ce_0: 0.05296  loss_mask_0: 0.1591  loss_dice_0: 0.161  loss_ce_1: 0.09914  loss_mask_1: 0.1681  loss_dice_1: 0.1616  loss_ce_2: 0.09908  loss_mask_2: 0.1669  loss_dice_2: 0.1571  loss_ce_3: 0.09913  loss_mask_3: 0.1698  loss_dice_3: 0.1613  loss_ce_4: 0.09928  loss_mask_4: 0.1673  loss_dice_4: 0.1562  loss_ce_5: 0.09914  loss_mask_5: 0.1733  loss_dice_5: 0.1576  loss_ce_6: 0.09917  loss_mask_6: 0.1606  loss_dice_6: 0.1489  loss_ce_7: 0.09929  loss_mask_7: 0.1666  loss_dice_7: 0.1516  loss_ce_8: 0.09925  loss_mask_8: 0.1627  loss_dice_8: 0.157  time: 0.5713  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:04:27] d2.utils.events INFO:  eta: 2:16:51  iter: 15839  total_loss: 4.701  loss_ce: 0.1595  loss_mask: 0.142  loss_dice: 0.1428  loss_ce_0: 0.07499  loss_mask_0: 0.1335  loss_dice_0: 0.1402  loss_ce_1: 0.1597  loss_mask_1: 0.1442  loss_dice_1: 0.1473  loss_ce_2: 0.1596  loss_mask_2: 0.1464  loss_dice_2: 0.1375  loss_ce_3: 0.1596  loss_mask_3: 0.1435  loss_dice_3: 0.1417  loss_ce_4: 0.1595  loss_mask_4: 0.1323  loss_dice_4: 0.1424  loss_ce_5: 0.1597  loss_mask_5: 0.1427  loss_dice_5: 0.1462  loss_ce_6: 0.1597  loss_mask_6: 0.1402  loss_dice_6: 0.1405  loss_ce_7: 0.1595  loss_mask_7: 0.1424  loss_dice_7: 0.143  loss_ce_8: 0.1597  loss_mask_8: 0.1395  loss_dice_8: 0.1417  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:04:38] d2.utils.events INFO:  eta: 2:16:53  iter: 15859  total_loss: 4.439  loss_ce: 0.1001  loss_mask: 0.1817  loss_dice: 0.1494  loss_ce_0: 0.05313  loss_mask_0: 0.1811  loss_dice_0: 0.1527  loss_ce_1: 0.1004  loss_mask_1: 0.1777  loss_dice_1: 0.1428  loss_ce_2: 0.1003  loss_mask_2: 0.1832  loss_dice_2: 0.1451  loss_ce_3: 0.1003  loss_mask_3: 0.175  loss_dice_3: 0.1427  loss_ce_4: 0.1004  loss_mask_4: 0.1849  loss_dice_4: 0.1446  loss_ce_5: 0.1002  loss_mask_5: 0.1845  loss_dice_5: 0.1503  loss_ce_6: 0.1002  loss_mask_6: 0.1821  loss_dice_6: 0.1442  loss_ce_7: 0.1002  loss_mask_7: 0.1804  loss_dice_7: 0.1414  loss_ce_8: 0.1001  loss_mask_8: 0.1872  loss_dice_8: 0.1417  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:04:50] d2.utils.events INFO:  eta: 2:16:58  iter: 15879  total_loss: 4.689  loss_ce: 0.09914  loss_mask: 0.2135  loss_dice: 0.1278  loss_ce_0: 0.05292  loss_mask_0: 0.1945  loss_dice_0: 0.1276  loss_ce_1: 0.0992  loss_mask_1: 0.1935  loss_dice_1: 0.1292  loss_ce_2: 0.09903  loss_mask_2: 0.203  loss_dice_2: 0.1273  loss_ce_3: 0.09902  loss_mask_3: 0.2033  loss_dice_3: 0.1332  loss_ce_4: 0.09921  loss_mask_4: 0.2079  loss_dice_4: 0.1271  loss_ce_5: 0.09891  loss_mask_5: 0.1952  loss_dice_5: 0.1286  loss_ce_6: 0.09906  loss_mask_6: 0.2085  loss_dice_6: 0.1258  loss_ce_7: 0.09914  loss_mask_7: 0.1992  loss_dice_7: 0.1268  loss_ce_8: 0.0991  loss_mask_8: 0.2043  loss_dice_8: 0.1302  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:05:01] d2.utils.events INFO:  eta: 2:16:55  iter: 15899  total_loss: 4.473  loss_ce: 0.1601  loss_mask: 0.1744  loss_dice: 0.1234  loss_ce_0: 0.07508  loss_mask_0: 0.1787  loss_dice_0: 0.1188  loss_ce_1: 0.1598  loss_mask_1: 0.1779  loss_dice_1: 0.1238  loss_ce_2: 0.16  loss_mask_2: 0.1728  loss_dice_2: 0.1212  loss_ce_3: 0.1599  loss_mask_3: 0.1808  loss_dice_3: 0.1221  loss_ce_4: 0.1597  loss_mask_4: 0.1832  loss_dice_4: 0.1165  loss_ce_5: 0.1599  loss_mask_5: 0.1699  loss_dice_5: 0.1159  loss_ce_6: 0.16  loss_mask_6: 0.1877  loss_dice_6: 0.1226  loss_ce_7: 0.1599  loss_mask_7: 0.1732  loss_dice_7: 0.1169  loss_ce_8: 0.1601  loss_mask_8: 0.1743  loss_dice_8: 0.1222  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:05:13] d2.utils.events INFO:  eta: 2:16:45  iter: 15919  total_loss: 4.391  loss_ce: 0.1  loss_mask: 0.1841  loss_dice: 0.1219  loss_ce_0: 0.05309  loss_mask_0: 0.1874  loss_dice_0: 0.1229  loss_ce_1: 0.1005  loss_mask_1: 0.1921  loss_dice_1: 0.1265  loss_ce_2: 0.1003  loss_mask_2: 0.1896  loss_dice_2: 0.1225  loss_ce_3: 0.1003  loss_mask_3: 0.1968  loss_dice_3: 0.1291  loss_ce_4: 0.1004  loss_mask_4: 0.1903  loss_dice_4: 0.1204  loss_ce_5: 0.1002  loss_mask_5: 0.1961  loss_dice_5: 0.1299  loss_ce_6: 0.1002  loss_mask_6: 0.1872  loss_dice_6: 0.1255  loss_ce_7: 0.1003  loss_mask_7: 0.1957  loss_dice_7: 0.1319  loss_ce_8: 0.1  loss_mask_8: 0.1934  loss_dice_8: 0.1198  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:05:24] d2.utils.events INFO:  eta: 2:16:35  iter: 15939  total_loss: 4.855  loss_ce: 0.1001  loss_mask: 0.1696  loss_dice: 0.161  loss_ce_0: 0.0531  loss_mask_0: 0.1682  loss_dice_0: 0.1564  loss_ce_1: 0.1005  loss_mask_1: 0.1711  loss_dice_1: 0.159  loss_ce_2: 0.1003  loss_mask_2: 0.1762  loss_dice_2: 0.1602  loss_ce_3: 0.1003  loss_mask_3: 0.1728  loss_dice_3: 0.1543  loss_ce_4: 0.1004  loss_mask_4: 0.1812  loss_dice_4: 0.1595  loss_ce_5: 0.1003  loss_mask_5: 0.1691  loss_dice_5: 0.1521  loss_ce_6: 0.1002  loss_mask_6: 0.1674  loss_dice_6: 0.1557  loss_ce_7: 0.1003  loss_mask_7: 0.1798  loss_dice_7: 0.16  loss_ce_8: 0.1  loss_mask_8: 0.1662  loss_dice_8: 0.1513  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:05:36] d2.utils.events INFO:  eta: 2:16:25  iter: 15959  total_loss: 4.584  loss_ce: 0.0994  loss_mask: 0.2069  loss_dice: 0.1354  loss_ce_0: 0.05297  loss_mask_0: 0.2089  loss_dice_0: 0.1341  loss_ce_1: 0.09931  loss_mask_1: 0.2097  loss_dice_1: 0.1364  loss_ce_2: 0.09931  loss_mask_2: 0.2112  loss_dice_2: 0.1367  loss_ce_3: 0.09932  loss_mask_3: 0.2119  loss_dice_3: 0.1349  loss_ce_4: 0.09947  loss_mask_4: 0.2082  loss_dice_4: 0.1377  loss_ce_5: 0.0994  loss_mask_5: 0.2111  loss_dice_5: 0.1327  loss_ce_6: 0.0994  loss_mask_6: 0.2113  loss_dice_6: 0.134  loss_ce_7: 0.0994  loss_mask_7: 0.2032  loss_dice_7: 0.133  loss_ce_8: 0.0994  loss_mask_8: 0.1961  loss_dice_8: 0.1316  time: 0.5713  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:05:47] d2.utils.events INFO:  eta: 2:16:14  iter: 15979  total_loss: 4.585  loss_ce: 0.09773  loss_mask: 0.156  loss_dice: 0.1593  loss_ce_0: 0.05262  loss_mask_0: 0.1479  loss_dice_0: 0.1496  loss_ce_1: 0.09693  loss_mask_1: 0.1506  loss_dice_1: 0.1539  loss_ce_2: 0.09722  loss_mask_2: 0.1397  loss_dice_2: 0.1588  loss_ce_3: 0.09731  loss_mask_3: 0.1402  loss_dice_3: 0.1585  loss_ce_4: 0.09708  loss_mask_4: 0.1521  loss_dice_4: 0.1577  loss_ce_5: 0.09728  loss_mask_5: 0.1524  loss_dice_5: 0.1589  loss_ce_6: 0.09743  loss_mask_6: 0.1466  loss_dice_6: 0.1577  loss_ce_7: 0.09724  loss_mask_7: 0.1548  loss_dice_7: 0.1588  loss_ce_8: 0.09761  loss_mask_8: 0.1561  loss_dice_8: 0.1504  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:05:59] d2.utils.events INFO:  eta: 2:16:05  iter: 15999  total_loss: 4.49  loss_ce: 0.1629  loss_mask: 0.1495  loss_dice: 0.1494  loss_ce_0: 0.07574  loss_mask_0: 0.1569  loss_dice_0: 0.1458  loss_ce_1: 0.1637  loss_mask_1: 0.1561  loss_dice_1: 0.1434  loss_ce_2: 0.1635  loss_mask_2: 0.1466  loss_dice_2: 0.1423  loss_ce_3: 0.1635  loss_mask_3: 0.1567  loss_dice_3: 0.1381  loss_ce_4: 0.1633  loss_mask_4: 0.1466  loss_dice_4: 0.1433  loss_ce_5: 0.1638  loss_mask_5: 0.1459  loss_dice_5: 0.145  loss_ce_6: 0.1634  loss_mask_6: 0.1514  loss_dice_6: 0.1411  loss_ce_7: 0.1632  loss_mask_7: 0.1482  loss_dice_7: 0.144  loss_ce_8: 0.1631  loss_mask_8: 0.1537  loss_dice_8: 0.1473  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:06:10] d2.utils.events INFO:  eta: 2:15:53  iter: 16019  total_loss: 4.154  loss_ce: 0.0988  loss_mask: 0.1526  loss_dice: 0.148  loss_ce_0: 0.05282  loss_mask_0: 0.1582  loss_dice_0: 0.1538  loss_ce_1: 0.09936  loss_mask_1: 0.1565  loss_dice_1: 0.1444  loss_ce_2: 0.0991  loss_mask_2: 0.1614  loss_dice_2: 0.1539  loss_ce_3: 0.09909  loss_mask_3: 0.1605  loss_dice_3: 0.1539  loss_ce_4: 0.09932  loss_mask_4: 0.1573  loss_dice_4: 0.1439  loss_ce_5: 0.09891  loss_mask_5: 0.1681  loss_dice_5: 0.1441  loss_ce_6: 0.0991  loss_mask_6: 0.1581  loss_dice_6: 0.1428  loss_ce_7: 0.09902  loss_mask_7: 0.157  loss_dice_7: 0.1459  loss_ce_8: 0.09876  loss_mask_8: 0.1649  loss_dice_8: 0.1453  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:06:22] d2.utils.events INFO:  eta: 2:15:42  iter: 16039  total_loss: 4.431  loss_ce: 0.16  loss_mask: 0.1849  loss_dice: 0.124  loss_ce_0: 0.07514  loss_mask_0: 0.1763  loss_dice_0: 0.1223  loss_ce_1: 0.1591  loss_mask_1: 0.1807  loss_dice_1: 0.1196  loss_ce_2: 0.1595  loss_mask_2: 0.1759  loss_dice_2: 0.1241  loss_ce_3: 0.1595  loss_mask_3: 0.1812  loss_dice_3: 0.1199  loss_ce_4: 0.1593  loss_mask_4: 0.1889  loss_dice_4: 0.1201  loss_ce_5: 0.1594  loss_mask_5: 0.1818  loss_dice_5: 0.1234  loss_ce_6: 0.1596  loss_mask_6: 0.1734  loss_dice_6: 0.1199  loss_ce_7: 0.1596  loss_mask_7: 0.1772  loss_dice_7: 0.1213  loss_ce_8: 0.1599  loss_mask_8: 0.1826  loss_dice_8: 0.125  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:06:34] d2.utils.events INFO:  eta: 2:15:38  iter: 16059  total_loss: 4.458  loss_ce: 0.1011  loss_mask: 0.1874  loss_dice: 0.1281  loss_ce_0: 0.05328  loss_mask_0: 0.1882  loss_dice_0: 0.1274  loss_ce_1: 0.1022  loss_mask_1: 0.1901  loss_dice_1: 0.1242  loss_ce_2: 0.1018  loss_mask_2: 0.1902  loss_dice_2: 0.1229  loss_ce_3: 0.1018  loss_mask_3: 0.1876  loss_dice_3: 0.1315  loss_ce_4: 0.102  loss_mask_4: 0.1928  loss_dice_4: 0.126  loss_ce_5: 0.1019  loss_mask_5: 0.1893  loss_dice_5: 0.1285  loss_ce_6: 0.1016  loss_mask_6: 0.1882  loss_dice_6: 0.1295  loss_ce_7: 0.1016  loss_mask_7: 0.1928  loss_dice_7: 0.1293  loss_ce_8: 0.1012  loss_mask_8: 0.1949  loss_dice_8: 0.1286  time: 0.5713  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:06:45] d2.utils.events INFO:  eta: 2:15:30  iter: 16079  total_loss: 4.551  loss_ce: 0.1013  loss_mask: 0.2082  loss_dice: 0.1395  loss_ce_0: 0.05333  loss_mask_0: 0.2023  loss_dice_0: 0.1364  loss_ce_1: 0.102  loss_mask_1: 0.2023  loss_dice_1: 0.1323  loss_ce_2: 0.1019  loss_mask_2: 0.2023  loss_dice_2: 0.1354  loss_ce_3: 0.1019  loss_mask_3: 0.2072  loss_dice_3: 0.1386  loss_ce_4: 0.102  loss_mask_4: 0.2021  loss_dice_4: 0.1419  loss_ce_5: 0.102  loss_mask_5: 0.2051  loss_dice_5: 0.131  loss_ce_6: 0.1017  loss_mask_6: 0.2004  loss_dice_6: 0.1331  loss_ce_7: 0.1017  loss_mask_7: 0.2065  loss_dice_7: 0.1367  loss_ce_8: 0.1013  loss_mask_8: 0.2073  loss_dice_8: 0.1362  time: 0.5713  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:06:56] d2.utils.events INFO:  eta: 2:15:22  iter: 16099  total_loss: 4.682  loss_ce: 0.1576  loss_mask: 0.1388  loss_dice: 0.1459  loss_ce_0: 0.0746  loss_mask_0: 0.1338  loss_dice_0: 0.147  loss_ce_1: 0.1565  loss_mask_1: 0.1318  loss_dice_1: 0.1531  loss_ce_2: 0.1571  loss_mask_2: 0.1258  loss_dice_2: 0.1469  loss_ce_3: 0.157  loss_mask_3: 0.1322  loss_dice_3: 0.151  loss_ce_4: 0.1566  loss_mask_4: 0.1362  loss_dice_4: 0.1538  loss_ce_5: 0.1569  loss_mask_5: 0.1392  loss_dice_5: 0.1496  loss_ce_6: 0.1571  loss_mask_6: 0.1324  loss_dice_6: 0.1512  loss_ce_7: 0.157  loss_mask_7: 0.1389  loss_dice_7: 0.1478  loss_ce_8: 0.1576  loss_mask_8: 0.1368  loss_dice_8: 0.1481  time: 0.5713  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 14:07:08] d2.utils.events INFO:  eta: 2:15:12  iter: 16119  total_loss: 4.285  loss_ce: 0.1021  loss_mask: 0.1373  loss_dice: 0.1803  loss_ce_0: 0.05351  loss_mask_0: 0.1332  loss_dice_0: 0.1747  loss_ce_1: 0.1027  loss_mask_1: 0.1352  loss_dice_1: 0.1757  loss_ce_2: 0.1025  loss_mask_2: 0.1347  loss_dice_2: 0.1723  loss_ce_3: 0.1025  loss_mask_3: 0.1345  loss_dice_3: 0.184  loss_ce_4: 0.1027  loss_mask_4: 0.1327  loss_dice_4: 0.1745  loss_ce_5: 0.1027  loss_mask_5: 0.1343  loss_dice_5: 0.1708  loss_ce_6: 0.1024  loss_mask_6: 0.131  loss_dice_6: 0.1826  loss_ce_7: 0.1025  loss_mask_7: 0.1271  loss_dice_7: 0.1684  loss_ce_8: 0.1021  loss_mask_8: 0.1319  loss_dice_8: 0.1743  time: 0.5713  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 14:07:20] d2.utils.events INFO:  eta: 2:15:04  iter: 16139  total_loss: 4.411  loss_ce: 0.1294  loss_mask: 0.1666  loss_dice: 0.1358  loss_ce_0: 0.06391  loss_mask_0: 0.1641  loss_dice_0: 0.1414  loss_ce_1: 0.1296  loss_mask_1: 0.1609  loss_dice_1: 0.144  loss_ce_2: 0.1295  loss_mask_2: 0.1564  loss_dice_2: 0.1321  loss_ce_3: 0.1294  loss_mask_3: 0.1725  loss_dice_3: 0.1355  loss_ce_4: 0.1293  loss_mask_4: 0.1593  loss_dice_4: 0.1387  loss_ce_5: 0.1293  loss_mask_5: 0.1649  loss_dice_5: 0.1388  loss_ce_6: 0.1295  loss_mask_6: 0.1655  loss_dice_6: 0.1404  loss_ce_7: 0.1292  loss_mask_7: 0.1639  loss_dice_7: 0.1397  loss_ce_8: 0.1295  loss_mask_8: 0.1556  loss_dice_8: 0.1361  time: 0.5713  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 14:07:31] d2.utils.events INFO:  eta: 2:15:03  iter: 16159  total_loss: 4.638  loss_ce: 0.1022  loss_mask: 0.1948  loss_dice: 0.135  loss_ce_0: 0.05351  loss_mask_0: 0.1975  loss_dice_0: 0.1418  loss_ce_1: 0.1028  loss_mask_1: 0.1915  loss_dice_1: 0.1384  loss_ce_2: 0.1024  loss_mask_2: 0.2024  loss_dice_2: 0.1419  loss_ce_3: 0.1025  loss_mask_3: 0.1964  loss_dice_3: 0.137  loss_ce_4: 0.1027  loss_mask_4: 0.2026  loss_dice_4: 0.1413  loss_ce_5: 0.1026  loss_mask_5: 0.1913  loss_dice_5: 0.1368  loss_ce_6: 0.1024  loss_mask_6: 0.185  loss_dice_6: 0.1333  loss_ce_7: 0.1024  loss_mask_7: 0.1901  loss_dice_7: 0.1377  loss_ce_8: 0.1022  loss_mask_8: 0.1925  loss_dice_8: 0.1345  time: 0.5714  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 14:07:43] d2.utils.events INFO:  eta: 2:14:55  iter: 16179  total_loss: 4.685  loss_ce: 0.1019  loss_mask: 0.2219  loss_dice: 0.1379  loss_ce_0: 0.05345  loss_mask_0: 0.2253  loss_dice_0: 0.1346  loss_ce_1: 0.1028  loss_mask_1: 0.224  loss_dice_1: 0.1414  loss_ce_2: 0.1021  loss_mask_2: 0.2231  loss_dice_2: 0.141  loss_ce_3: 0.1021  loss_mask_3: 0.2265  loss_dice_3: 0.1459  loss_ce_4: 0.1025  loss_mask_4: 0.2186  loss_dice_4: 0.1449  loss_ce_5: 0.1023  loss_mask_5: 0.22  loss_dice_5: 0.1386  loss_ce_6: 0.1022  loss_mask_6: 0.2261  loss_dice_6: 0.1397  loss_ce_7: 0.1021  loss_mask_7: 0.2205  loss_dice_7: 0.1435  loss_ce_8: 0.1019  loss_mask_8: 0.2232  loss_dice_8: 0.1381  time: 0.5714  data_time: 0.0023  lr: 1e-05  max_mem: 2811M
[07/11 14:07:54] d2.utils.events INFO:  eta: 2:14:44  iter: 16199  total_loss: 4.63  loss_ce: 0.1292  loss_mask: 0.1708  loss_dice: 0.1472  loss_ce_0: 0.06393  loss_mask_0: 0.1682  loss_dice_0: 0.1519  loss_ce_1: 0.1286  loss_mask_1: 0.177  loss_dice_1: 0.1523  loss_ce_2: 0.129  loss_mask_2: 0.1716  loss_dice_2: 0.1455  loss_ce_3: 0.1291  loss_mask_3: 0.1664  loss_dice_3: 0.1465  loss_ce_4: 0.1288  loss_mask_4: 0.1774  loss_dice_4: 0.1456  loss_ce_5: 0.1291  loss_mask_5: 0.1777  loss_dice_5: 0.1406  loss_ce_6: 0.1291  loss_mask_6: 0.1869  loss_dice_6: 0.1438  loss_ce_7: 0.1291  loss_mask_7: 0.1785  loss_dice_7: 0.1387  loss_ce_8: 0.129  loss_mask_8: 0.1672  loss_dice_8: 0.1469  time: 0.5714  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 14:08:06] d2.utils.events INFO:  eta: 2:14:28  iter: 16219  total_loss: 4.491  loss_ce: 0.1018  loss_mask: 0.1807  loss_dice: 0.1614  loss_ce_0: 0.05343  loss_mask_0: 0.1725  loss_dice_0: 0.1577  loss_ce_1: 0.1023  loss_mask_1: 0.1805  loss_dice_1: 0.1588  loss_ce_2: 0.102  loss_mask_2: 0.1789  loss_dice_2: 0.1618  loss_ce_3: 0.102  loss_mask_3: 0.1734  loss_dice_3: 0.1604  loss_ce_4: 0.1022  loss_mask_4: 0.1803  loss_dice_4: 0.1607  loss_ce_5: 0.102  loss_mask_5: 0.1772  loss_dice_5: 0.1621  loss_ce_6: 0.102  loss_mask_6: 0.182  loss_dice_6: 0.16  loss_ce_7: 0.1019  loss_mask_7: 0.1788  loss_dice_7: 0.1656  loss_ce_8: 0.1019  loss_mask_8: 0.1749  loss_dice_8: 0.1564  time: 0.5714  data_time: 0.0024  lr: 1e-05  max_mem: 2811M
[07/11 14:08:17] d2.utils.events INFO:  eta: 2:14:18  iter: 16239  total_loss: 4.801  loss_ce: 0.1018  loss_mask: 0.1715  loss_dice: 0.1503  loss_ce_0: 0.05347  loss_mask_0: 0.1748  loss_dice_0: 0.1462  loss_ce_1: 0.1022  loss_mask_1: 0.1809  loss_dice_1: 0.149  loss_ce_2: 0.1018  loss_mask_2: 0.1805  loss_dice_2: 0.1516  loss_ce_3: 0.102  loss_mask_3: 0.1779  loss_dice_3: 0.1493  loss_ce_4: 0.1022  loss_mask_4: 0.1716  loss_dice_4: 0.1502  loss_ce_5: 0.1022  loss_mask_5: 0.1747  loss_dice_5: 0.1523  loss_ce_6: 0.1019  loss_mask_6: 0.1806  loss_dice_6: 0.1499  loss_ce_7: 0.1021  loss_mask_7: 0.178  loss_dice_7: 0.1445  loss_ce_8: 0.1026  loss_mask_8: 0.186  loss_dice_8: 0.143  time: 0.5714  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 14:08:28] d2.utils.events INFO:  eta: 2:14:09  iter: 16259  total_loss: 4.804  loss_ce: 0.1024  loss_mask: 0.1908  loss_dice: 0.1504  loss_ce_0: 0.05361  loss_mask_0: 0.1955  loss_dice_0: 0.1442  loss_ce_1: 0.1031  loss_mask_1: 0.2111  loss_dice_1: 0.1484  loss_ce_2: 0.1027  loss_mask_2: 0.1966  loss_dice_2: 0.1424  loss_ce_3: 0.1027  loss_mask_3: 0.1929  loss_dice_3: 0.1474  loss_ce_4: 0.1031  loss_mask_4: 0.2043  loss_dice_4: 0.1522  loss_ce_5: 0.1029  loss_mask_5: 0.2011  loss_dice_5: 0.1442  loss_ce_6: 0.1027  loss_mask_6: 0.2022  loss_dice_6: 0.1504  loss_ce_7: 0.1028  loss_mask_7: 0.2022  loss_dice_7: 0.143  loss_ce_8: 0.1025  loss_mask_8: 0.1932  loss_dice_8: 0.1479  time: 0.5714  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 14:08:40] d2.utils.events INFO:  eta: 2:14:09  iter: 16279  total_loss: 4.506  loss_ce: 0.103  loss_mask: 0.1959  loss_dice: 0.1376  loss_ce_0: 0.0537  loss_mask_0: 0.1972  loss_dice_0: 0.1429  loss_ce_1: 0.104  loss_mask_1: 0.1928  loss_dice_1: 0.1407  loss_ce_2: 0.1034  loss_mask_2: 0.1903  loss_dice_2: 0.1342  loss_ce_3: 0.1033  loss_mask_3: 0.1916  loss_dice_3: 0.1404  loss_ce_4: 0.1037  loss_mask_4: 0.1888  loss_dice_4: 0.1348  loss_ce_5: 0.1035  loss_mask_5: 0.1968  loss_dice_5: 0.1387  loss_ce_6: 0.1034  loss_mask_6: 0.1961  loss_dice_6: 0.1429  loss_ce_7: 0.1032  loss_mask_7: 0.1914  loss_dice_7: 0.1413  loss_ce_8: 0.103  loss_mask_8: 0.1934  loss_dice_8: 0.1418  time: 0.5714  data_time: 0.0024  lr: 1e-05  max_mem: 2811M
[07/11 14:08:52] d2.utils.events INFO:  eta: 2:13:47  iter: 16299  total_loss: 4.716  loss_ce: 0.1022  loss_mask: 0.1974  loss_dice: 0.1588  loss_ce_0: 0.05354  loss_mask_0: 0.1843  loss_dice_0: 0.1588  loss_ce_1: 0.1024  loss_mask_1: 0.2058  loss_dice_1: 0.1534  loss_ce_2: 0.1023  loss_mask_2: 0.1911  loss_dice_2: 0.1552  loss_ce_3: 0.1023  loss_mask_3: 0.1936  loss_dice_3: 0.1614  loss_ce_4: 0.1026  loss_mask_4: 0.2019  loss_dice_4: 0.1607  loss_ce_5: 0.1025  loss_mask_5: 0.2043  loss_dice_5: 0.1592  loss_ce_6: 0.1024  loss_mask_6: 0.195  loss_dice_6: 0.1668  loss_ce_7: 0.1024  loss_mask_7: 0.2023  loss_dice_7: 0.1592  loss_ce_8: 0.1023  loss_mask_8: 0.1961  loss_dice_8: 0.1623  time: 0.5714  data_time: 0.0025  lr: 1e-05  max_mem: 2811M
[07/11 14:09:03] d2.utils.events INFO:  eta: 2:13:14  iter: 16319  total_loss: 4.807  loss_ce: 0.1003  loss_mask: 0.1913  loss_dice: 0.1626  loss_ce_0: 0.05316  loss_mask_0: 0.1984  loss_dice_0: 0.1574  loss_ce_1: 0.09981  loss_mask_1: 0.1973  loss_dice_1: 0.1575  loss_ce_2: 0.09987  loss_mask_2: 0.1947  loss_dice_2: 0.1601  loss_ce_3: 0.09985  loss_mask_3: 0.1955  loss_dice_3: 0.1619  loss_ce_4: 0.1002  loss_mask_4: 0.1929  loss_dice_4: 0.1615  loss_ce_5: 0.09996  loss_mask_5: 0.2006  loss_dice_5: 0.1578  loss_ce_6: 0.1001  loss_mask_6: 0.1892  loss_dice_6: 0.1647  loss_ce_7: 0.1002  loss_mask_7: 0.1911  loss_dice_7: 0.1569  loss_ce_8: 0.1002  loss_mask_8: 0.1958  loss_dice_8: 0.1603  time: 0.5714  data_time: 0.0018  lr: 1e-05  max_mem: 2811M
[07/11 14:09:15] d2.utils.events INFO:  eta: 2:12:44  iter: 16339  total_loss: 5.005  loss_ce: 0.09854  loss_mask: 0.1243  loss_dice: 0.1921  loss_ce_0: 0.05278  loss_mask_0: 0.1226  loss_dice_0: 0.1991  loss_ce_1: 0.09782  loss_mask_1: 0.1238  loss_dice_1: 0.1901  loss_ce_2: 0.09779  loss_mask_2: 0.1251  loss_dice_2: 0.1912  loss_ce_3: 0.09783  loss_mask_3: 0.1243  loss_dice_3: 0.1946  loss_ce_4: 0.09824  loss_mask_4: 0.1222  loss_dice_4: 0.1954  loss_ce_5: 0.09794  loss_mask_5: 0.1268  loss_dice_5: 0.2062  loss_ce_6: 0.09813  loss_mask_6: 0.1205  loss_dice_6: 0.184  loss_ce_7: 0.09835  loss_mask_7: 0.1225  loss_dice_7: 0.1934  loss_ce_8: 0.09846  loss_mask_8: 0.1185  loss_dice_8: 0.1984  time: 0.5714  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 14:09:26] d2.utils.events INFO:  eta: 2:12:11  iter: 16359  total_loss: 4.994  loss_ce: 0.0985  loss_mask: 0.2223  loss_dice: 0.1539  loss_ce_0: 0.05277  loss_mask_0: 0.2213  loss_dice_0: 0.1533  loss_ce_1: 0.09789  loss_mask_1: 0.2173  loss_dice_1: 0.1648  loss_ce_2: 0.09791  loss_mask_2: 0.2295  loss_dice_2: 0.161  loss_ce_3: 0.09794  loss_mask_3: 0.2228  loss_dice_3: 0.1594  loss_ce_4: 0.09824  loss_mask_4: 0.2229  loss_dice_4: 0.1557  loss_ce_5: 0.09798  loss_mask_5: 0.2223  loss_dice_5: 0.1589  loss_ce_6: 0.09802  loss_mask_6: 0.2217  loss_dice_6: 0.159  loss_ce_7: 0.09839  loss_mask_7: 0.2285  loss_dice_7: 0.1537  loss_ce_8: 0.09835  loss_mask_8: 0.2242  loss_dice_8: 0.162  time: 0.5714  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 14:09:38] d2.utils.events INFO:  eta: 2:11:38  iter: 16379  total_loss: 4.35  loss_ce: 0.09798  loss_mask: 0.1683  loss_dice: 0.1446  loss_ce_0: 0.05266  loss_mask_0: 0.1664  loss_dice_0: 0.1421  loss_ce_1: 0.09776  loss_mask_1: 0.1672  loss_dice_1: 0.1375  loss_ce_2: 0.09763  loss_mask_2: 0.1671  loss_dice_2: 0.1405  loss_ce_3: 0.09753  loss_mask_3: 0.1585  loss_dice_3: 0.1376  loss_ce_4: 0.0979  loss_mask_4: 0.1581  loss_dice_4: 0.1426  loss_ce_5: 0.0975  loss_mask_5: 0.1594  loss_dice_5: 0.1478  loss_ce_6: 0.09765  loss_mask_6: 0.1568  loss_dice_6: 0.1413  loss_ce_7: 0.09794  loss_mask_7: 0.1637  loss_dice_7: 0.1483  loss_ce_8: 0.09787  loss_mask_8: 0.1677  loss_dice_8: 0.1452  time: 0.5714  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 14:09:51] d2.utils.events INFO:  eta: 2:11:26  iter: 16399  total_loss: 4.611  loss_ce: 0.1609  loss_mask: 0.2093  loss_dice: 0.1245  loss_ce_0: 0.07524  loss_mask_0: 0.2074  loss_dice_0: 0.1218  loss_ce_1: 0.1607  loss_mask_1: 0.2076  loss_dice_1: 0.1239  loss_ce_2: 0.1611  loss_mask_2: 0.2081  loss_dice_2: 0.1216  loss_ce_3: 0.1612  loss_mask_3: 0.2052  loss_dice_3: 0.1223  loss_ce_4: 0.1608  loss_mask_4: 0.2094  loss_dice_4: 0.1214  loss_ce_5: 0.1611  loss_mask_5: 0.2138  loss_dice_5: 0.1178  loss_ce_6: 0.161  loss_mask_6: 0.2082  loss_dice_6: 0.1228  loss_ce_7: 0.1611  loss_mask_7: 0.2168  loss_dice_7: 0.1234  loss_ce_8: 0.161  loss_mask_8: 0.2059  loss_dice_8: 0.123  time: 0.5715  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 14:10:03] d2.utils.events INFO:  eta: 2:11:12  iter: 16419  total_loss: 4.967  loss_ce: 0.09944  loss_mask: 0.1989  loss_dice: 0.1551  loss_ce_0: 0.05296  loss_mask_0: 0.2099  loss_dice_0: 0.1653  loss_ce_1: 0.09968  loss_mask_1: 0.2073  loss_dice_1: 0.1612  loss_ce_2: 0.09951  loss_mask_2: 0.2056  loss_dice_2: 0.1607  loss_ce_3: 0.09951  loss_mask_3: 0.2089  loss_dice_3: 0.1607  loss_ce_4: 0.1  loss_mask_4: 0.2134  loss_dice_4: 0.1667  loss_ce_5: 0.09943  loss_mask_5: 0.1992  loss_dice_5: 0.1642  loss_ce_6: 0.09936  loss_mask_6: 0.2085  loss_dice_6: 0.1629  loss_ce_7: 0.09977  loss_mask_7: 0.1967  loss_dice_7: 0.1635  loss_ce_8: 0.09936  loss_mask_8: 0.2012  loss_dice_8: 0.158  time: 0.5716  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:10:16] d2.utils.events INFO:  eta: 2:10:57  iter: 16439  total_loss: 4.646  loss_ce: 0.09947  loss_mask: 0.1951  loss_dice: 0.1423  loss_ce_0: 0.05296  loss_mask_0: 0.1991  loss_dice_0: 0.1378  loss_ce_1: 0.09934  loss_mask_1: 0.1911  loss_dice_1: 0.1386  loss_ce_2: 0.09953  loss_mask_2: 0.1983  loss_dice_2: 0.1379  loss_ce_3: 0.09954  loss_mask_3: 0.1995  loss_dice_3: 0.1402  loss_ce_4: 0.09951  loss_mask_4: 0.1964  loss_dice_4: 0.136  loss_ce_5: 0.09951  loss_mask_5: 0.2027  loss_dice_5: 0.1392  loss_ce_6: 0.09943  loss_mask_6: 0.1885  loss_dice_6: 0.1382  loss_ce_7: 0.09947  loss_mask_7: 0.1942  loss_dice_7: 0.1399  loss_ce_8: 0.0994  loss_mask_8: 0.1917  loss_dice_8: 0.1429  time: 0.5716  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 14:10:28] d2.utils.events INFO:  eta: 2:10:41  iter: 16459  total_loss: 4.648  loss_ce: 0.09846  loss_mask: 0.1886  loss_dice: 0.1475  loss_ce_0: 0.05273  loss_mask_0: 0.1897  loss_dice_0: 0.1452  loss_ce_1: 0.0983  loss_mask_1: 0.1892  loss_dice_1: 0.1484  loss_ce_2: 0.09828  loss_mask_2: 0.1884  loss_dice_2: 0.1504  loss_ce_3: 0.09831  loss_mask_3: 0.183  loss_dice_3: 0.1476  loss_ce_4: 0.09842  loss_mask_4: 0.1888  loss_dice_4: 0.1492  loss_ce_5: 0.09824  loss_mask_5: 0.1856  loss_dice_5: 0.1432  loss_ce_6: 0.09831  loss_mask_6: 0.1868  loss_dice_6: 0.1504  loss_ce_7: 0.09839  loss_mask_7: 0.1817  loss_dice_7: 0.1496  loss_ce_8: 0.09832  loss_mask_8: 0.1862  loss_dice_8: 0.15  time: 0.5717  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 14:10:41] d2.utils.events INFO:  eta: 2:10:27  iter: 16479  total_loss: 4.616  loss_ce: 0.1617  loss_mask: 0.1402  loss_dice: 0.1427  loss_ce_0: 0.07533  loss_mask_0: 0.1444  loss_dice_0: 0.1437  loss_ce_1: 0.1616  loss_mask_1: 0.1442  loss_dice_1: 0.1477  loss_ce_2: 0.1617  loss_mask_2: 0.1484  loss_dice_2: 0.1414  loss_ce_3: 0.1616  loss_mask_3: 0.1409  loss_dice_3: 0.144  loss_ce_4: 0.1616  loss_mask_4: 0.1429  loss_dice_4: 0.1442  loss_ce_5: 0.1619  loss_mask_5: 0.1459  loss_dice_5: 0.1422  loss_ce_6: 0.1618  loss_mask_6: 0.1448  loss_dice_6: 0.1417  loss_ce_7: 0.1616  loss_mask_7: 0.1383  loss_dice_7: 0.1407  loss_ce_8: 0.1618  loss_mask_8: 0.1432  loss_dice_8: 0.15  time: 0.5718  data_time: 0.0021  lr: 1e-05  max_mem: 2811M
[07/11 14:10:54] d2.utils.events INFO:  eta: 2:10:15  iter: 16499  total_loss: 4.275  loss_ce: 0.1  loss_mask: 0.178  loss_dice: 0.1343  loss_ce_0: 0.05308  loss_mask_0: 0.1745  loss_dice_0: 0.1411  loss_ce_1: 0.1004  loss_mask_1: 0.1814  loss_dice_1: 0.1412  loss_ce_2: 0.1004  loss_mask_2: 0.1686  loss_dice_2: 0.1321  loss_ce_3: 0.1004  loss_mask_3: 0.1714  loss_dice_3: 0.1417  loss_ce_4: 0.1004  loss_mask_4: 0.1771  loss_dice_4: 0.1399  loss_ce_5: 0.1004  loss_mask_5: 0.1774  loss_dice_5: 0.1381  loss_ce_6: 0.1002  loss_mask_6: 0.1707  loss_dice_6: 0.1371  loss_ce_7: 0.1002  loss_mask_7: 0.1822  loss_dice_7: 0.1371  loss_ce_8: 0.09996  loss_mask_8: 0.1831  loss_dice_8: 0.1398  time: 0.5719  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 14:11:07] d2.utils.events INFO:  eta: 2:10:15  iter: 16519  total_loss: 4.583  loss_ce: 0.1  loss_mask: 0.1793  loss_dice: 0.1551  loss_ce_0: 0.05305  loss_mask_0: 0.177  loss_dice_0: 0.1597  loss_ce_1: 0.1008  loss_mask_1: 0.1806  loss_dice_1: 0.1578  loss_ce_2: 0.1004  loss_mask_2: 0.1804  loss_dice_2: 0.1553  loss_ce_3: 0.1004  loss_mask_3: 0.1859  loss_dice_3: 0.1602  loss_ce_4: 0.1006  loss_mask_4: 0.1811  loss_dice_4: 0.1501  loss_ce_5: 0.1004  loss_mask_5: 0.1784  loss_dice_5: 0.158  loss_ce_6: 0.1003  loss_mask_6: 0.1779  loss_dice_6: 0.1578  loss_ce_7: 0.1004  loss_mask_7: 0.1827  loss_dice_7: 0.1605  loss_ce_8: 0.1003  loss_mask_8: 0.1737  loss_dice_8: 0.1552  time: 0.5719  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 14:11:20] d2.utils.events INFO:  eta: 2:10:15  iter: 16539  total_loss: 4.726  loss_ce: 0.1583  loss_mask: 0.2007  loss_dice: 0.1296  loss_ce_0: 0.07463  loss_mask_0: 0.1981  loss_dice_0: 0.1341  loss_ce_1: 0.1573  loss_mask_1: 0.2083  loss_dice_1: 0.1357  loss_ce_2: 0.1578  loss_mask_2: 0.2024  loss_dice_2: 0.1398  loss_ce_3: 0.1579  loss_mask_3: 0.2028  loss_dice_3: 0.1423  loss_ce_4: 0.1578  loss_mask_4: 0.2076  loss_dice_4: 0.1408  loss_ce_5: 0.1579  loss_mask_5: 0.202  loss_dice_5: 0.1394  loss_ce_6: 0.158  loss_mask_6: 0.1956  loss_dice_6: 0.1391  loss_ce_7: 0.1581  loss_mask_7: 0.203  loss_dice_7: 0.1381  loss_ce_8: 0.1583  loss_mask_8: 0.1996  loss_dice_8: 0.1355  time: 0.5720  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 14:11:32] d2.utils.events INFO:  eta: 2:10:10  iter: 16559  total_loss: 4.439  loss_ce: 0.1002  loss_mask: 0.1917  loss_dice: 0.1229  loss_ce_0: 0.0531  loss_mask_0: 0.1889  loss_dice_0: 0.1294  loss_ce_1: 0.1002  loss_mask_1: 0.1803  loss_dice_1: 0.1286  loss_ce_2: 0.1004  loss_mask_2: 0.1825  loss_dice_2: 0.126  loss_ce_3: 0.1004  loss_mask_3: 0.1834  loss_dice_3: 0.1325  loss_ce_4: 0.1004  loss_mask_4: 0.1882  loss_dice_4: 0.1254  loss_ce_5: 0.1004  loss_mask_5: 0.1859  loss_dice_5: 0.1252  loss_ce_6: 0.1003  loss_mask_6: 0.1854  loss_dice_6: 0.1253  loss_ce_7: 0.1003  loss_mask_7: 0.1891  loss_dice_7: 0.1235  loss_ce_8: 0.1003  loss_mask_8: 0.1822  loss_dice_8: 0.1255  time: 0.5721  data_time: 0.0023  lr: 1e-05  max_mem: 2811M
[07/11 14:11:45] d2.utils.events INFO:  eta: 2:10:09  iter: 16579  total_loss: 4.433  loss_ce: 0.09795  loss_mask: 0.1871  loss_dice: 0.147  loss_ce_0: 0.05258  loss_mask_0: 0.1925  loss_dice_0: 0.1406  loss_ce_1: 0.09743  loss_mask_1: 0.1955  loss_dice_1: 0.1424  loss_ce_2: 0.09752  loss_mask_2: 0.1867  loss_dice_2: 0.1397  loss_ce_3: 0.09753  loss_mask_3: 0.1951  loss_dice_3: 0.1417  loss_ce_4: 0.09768  loss_mask_4: 0.1917  loss_dice_4: 0.1444  loss_ce_5: 0.0975  loss_mask_5: 0.1892  loss_dice_5: 0.1373  loss_ce_6: 0.09765  loss_mask_6: 0.1876  loss_dice_6: 0.1419  loss_ce_7: 0.09791  loss_mask_7: 0.1898  loss_dice_7: 0.1456  loss_ce_8: 0.09787  loss_mask_8: 0.1953  loss_dice_8: 0.1416  time: 0.5722  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 14:11:58] d2.utils.events INFO:  eta: 2:10:05  iter: 16599  total_loss: 4.811  loss_ce: 0.1303  loss_mask: 0.1806  loss_dice: 0.1369  loss_ce_0: 0.06406  loss_mask_0: 0.1722  loss_dice_0: 0.1364  loss_ce_1: 0.1301  loss_mask_1: 0.1698  loss_dice_1: 0.1403  loss_ce_2: 0.1302  loss_mask_2: 0.1864  loss_dice_2: 0.1373  loss_ce_3: 0.1303  loss_mask_3: 0.182  loss_dice_3: 0.1403  loss_ce_4: 0.1302  loss_mask_4: 0.1884  loss_dice_4: 0.1413  loss_ce_5: 0.1303  loss_mask_5: 0.1928  loss_dice_5: 0.1424  loss_ce_6: 0.1303  loss_mask_6: 0.1868  loss_dice_6: 0.1358  loss_ce_7: 0.1301  loss_mask_7: 0.1789  loss_dice_7: 0.1415  loss_ce_8: 0.1302  loss_mask_8: 0.1847  loss_dice_8: 0.137  time: 0.5723  data_time: 0.0020  lr: 1e-05  max_mem: 2811M
[07/11 14:12:10] d2.utils.events INFO:  eta: 2:10:03  iter: 16619  total_loss: 4.777  loss_ce: 0.1624  loss_mask: 0.1937  loss_dice: 0.1455  loss_ce_0: 0.07551  loss_mask_0: 0.2005  loss_dice_0: 0.1434  loss_ce_1: 0.1625  loss_mask_1: 0.199  loss_dice_1: 0.1409  loss_ce_2: 0.1624  loss_mask_2: 0.1951  loss_dice_2: 0.1405  loss_ce_3: 0.1625  loss_mask_3: 0.1994  loss_dice_3: 0.137  loss_ce_4: 0.1624  loss_mask_4: 0.1985  loss_dice_4: 0.1398  loss_ce_5: 0.1623  loss_mask_5: 0.1986  loss_dice_5: 0.1417  loss_ce_6: 0.1625  loss_mask_6: 0.1926  loss_dice_6: 0.1389  loss_ce_7: 0.1624  loss_mask_7: 0.198  loss_dice_7: 0.1464  loss_ce_8: 0.1623  loss_mask_8: 0.1914  loss_dice_8: 0.1464  time: 0.5723  data_time: 0.0022  lr: 1e-05  max_mem: 2811M
[07/11 14:12:23] d2.utils.events INFO:  eta: 2:10:06  iter: 16639  total_loss: 4.273  loss_ce: 0.09824  loss_mask: 0.1655  loss_dice: 0.1556  loss_ce_0: 0.05261  loss_mask_0: 0.1712  loss_dice_0: 0.1564  loss_ce_1: 0.09839  loss_mask_1: 0.1553  loss_dice_1: 0.1512  loss_ce_2: 0.09839  loss_mask_2: 0.1629  loss_dice_2: 0.1559  loss_ce_3: 0.09842  loss_mask_3: 0.1587  loss_dice_3: 0.1504  loss_ce_4: 0.09861  loss_mask_4: 0.1658  loss_dice_4: 0.155  loss_ce_5: 0.09846  loss_mask_5: 0.1656  loss_dice_5: 0.1529  loss_ce_6: 0.09831  loss_mask_6: 0.1664  loss_dice_6: 0.1594  loss_ce_7: 0.09846  loss_mask_7: 0.1653  loss_dice_7: 0.1538  loss_ce_8: 0.09817  loss_mask_8: 0.1721  loss_dice_8: 0.157  time: 0.5724  data_time: 0.0019  lr: 1e-05  max_mem: 2811M
[07/11 14:12:36] d2.utils.events INFO:  eta: 2:10:09  iter: 16659  total_loss: 4.937  loss_ce: 0.09794  loss_mask: 0.2155  loss_dice: 0.1725  loss_ce_0: 0.05251  loss_mask_0: 0.2132  loss_dice_0: 0.1654  loss_ce_1: 0.098  loss_mask_1: 0.2094  loss_dice_1: 0.1602  loss_ce_2: 0.09783  loss_mask_2: 0.2157  loss_dice_2: 0.1698  loss_ce_3: 0.09779  loss_mask_3: 0.2072  loss_dice_3: 0.17  loss_ce_4: 0.09801  loss_mask_4: 0.213  loss_dice_4: 0.1711  loss_ce_5: 0.09772  loss_mask_5: 0.2221  loss_dice_5: 0.1693  loss_ce_6: 0.09787  loss_mask_6: 0.2069  loss_dice_6: 0.1724  loss_ce_7: 0.09791  loss_mask_7: 0.2056  loss_dice_7: 0.1639  loss_ce_8: 0.0978  loss_mask_8: 0.2044  loss_dice_8: 0.1688  time: 0.5725  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 14:12:49] d2.utils.events INFO:  eta: 2:10:12  iter: 16679  total_loss: 4.548  loss_ce: 0.1636  loss_mask: 0.1873  loss_dice: 0.1336  loss_ce_0: 0.07586  loss_mask_0: 0.1804  loss_dice_0: 0.1385  loss_ce_1: 0.1634  loss_mask_1: 0.18  loss_dice_1: 0.1293  loss_ce_2: 0.1637  loss_mask_2: 0.1934  loss_dice_2: 0.1331  loss_ce_3: 0.1639  loss_mask_3: 0.1857  loss_dice_3: 0.1363  loss_ce_4: 0.1636  loss_mask_4: 0.1868  loss_dice_4: 0.1357  loss_ce_5: 0.1641  loss_mask_5: 0.1824  loss_dice_5: 0.13  loss_ce_6: 0.1638  loss_mask_6: 0.185  loss_dice_6: 0.1347  loss_ce_7: 0.1636  loss_mask_7: 0.1745  loss_dice_7: 0.132  loss_ce_8: 0.1637  loss_mask_8: 0.1905  loss_dice_8: 0.1359  time: 0.5726  data_time: 0.0023  lr: 1e-05  max_mem: 2811M
[07/11 14:13:01] d2.utils.events INFO:  eta: 2:10:05  iter: 16699  total_loss: 4.597  loss_ce: 0.09698  loss_mask: 0.2033  loss_dice: 0.1408  loss_ce_0: 0.0523  loss_mask_0: 0.1999  loss_dice_0: 0.153  loss_ce_1: 0.097  loss_mask_1: 0.2007  loss_dice_1: 0.1445  loss_ce_2: 0.09696  loss_mask_2: 0.2082  loss_dice_2: 0.1463  loss_ce_3: 0.09687  loss_mask_3: 0.193  loss_dice_3: 0.1496  loss_ce_4: 0.09698  loss_mask_4: 0.205  loss_dice_4: 0.1551  loss_ce_5: 0.09676  loss_mask_5: 0.1984  loss_dice_5: 0.1444  loss_ce_6: 0.09687  loss_mask_6: 0.2051  loss_dice_6: 0.1499  loss_ce_7: 0.09706  loss_mask_7: 0.1926  loss_dice_7: 0.1459  loss_ce_8: 0.09695  loss_mask_8: 0.2031  loss_dice_8: 0.1458  time: 0.5726  data_time: 0.0021  lr: 1e-05  max_mem: 2811M
[07/11 14:13:14] d2.utils.events INFO:  eta: 2:10:07  iter: 16719  total_loss: 4.852  loss_ce: 0.1303  loss_mask: 0.205  loss_dice: 0.1329  loss_ce_0: 0.06407  loss_mask_0: 0.2121  loss_dice_0: 0.1326  loss_ce_1: 0.1307  loss_mask_1: 0.21  loss_dice_1: 0.1334  loss_ce_2: 0.1305  loss_mask_2: 0.2055  loss_dice_2: 0.1333  loss_ce_3: 0.1304  loss_mask_3: 0.2044  loss_dice_3: 0.1346  loss_ce_4: 0.1305  loss_mask_4: 0.2178  loss_dice_4: 0.1359  loss_ce_5: 0.1305  loss_mask_5: 0.204  loss_dice_5: 0.1363  loss_ce_6: 0.1305  loss_mask_6: 0.2117  loss_dice_6: 0.1329  loss_ce_7: 0.1304  loss_mask_7: 0.2116  loss_dice_7: 0.1345  loss_ce_8: 0.1303  loss_mask_8: 0.1988  loss_dice_8: 0.1332  time: 0.5727  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:13:27] d2.utils.events INFO:  eta: 2:10:04  iter: 16739  total_loss: 4.627  loss_ce: 0.09621  loss_mask: 0.2244  loss_dice: 0.1363  loss_ce_0: 0.05211  loss_mask_0: 0.2199  loss_dice_0: 0.1362  loss_ce_1: 0.09618  loss_mask_1: 0.2238  loss_dice_1: 0.133  loss_ce_2: 0.09616  loss_mask_2: 0.2201  loss_dice_2: 0.1359  loss_ce_3: 0.09603  loss_mask_3: 0.2198  loss_dice_3: 0.1353  loss_ce_4: 0.09606  loss_mask_4: 0.216  loss_dice_4: 0.1354  loss_ce_5: 0.09592  loss_mask_5: 0.217  loss_dice_5: 0.138  loss_ce_6: 0.09603  loss_mask_6: 0.2225  loss_dice_6: 0.1356  loss_ce_7: 0.09625  loss_mask_7: 0.2257  loss_dice_7: 0.1378  loss_ce_8: 0.09618  loss_mask_8: 0.2178  loss_dice_8: 0.1305  time: 0.5728  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:13:39] d2.utils.events INFO:  eta: 2:10:10  iter: 16759  total_loss: 4.255  loss_ce: 0.09603  loss_mask: 0.1926  loss_dice: 0.1294  loss_ce_0: 0.05201  loss_mask_0: 0.1847  loss_dice_0: 0.1275  loss_ce_1: 0.09649  loss_mask_1: 0.1896  loss_dice_1: 0.1242  loss_ce_2: 0.09621  loss_mask_2: 0.2014  loss_dice_2: 0.127  loss_ce_3: 0.0961  loss_mask_3: 0.1909  loss_dice_3: 0.123  loss_ce_4: 0.09632  loss_mask_4: 0.203  loss_dice_4: 0.1216  loss_ce_5: 0.09595  loss_mask_5: 0.1885  loss_dice_5: 0.1287  loss_ce_6: 0.09603  loss_mask_6: 0.1978  loss_dice_6: 0.1247  loss_ce_7: 0.09614  loss_mask_7: 0.1823  loss_dice_7: 0.1296  loss_ce_8: 0.09603  loss_mask_8: 0.1868  loss_dice_8: 0.1258  time: 0.5728  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:13:52] d2.utils.events INFO:  eta: 2:10:26  iter: 16779  total_loss: 4.679  loss_ce: 0.1648  loss_mask: 0.1961  loss_dice: 0.132  loss_ce_0: 0.0762  loss_mask_0: 0.1915  loss_dice_0: 0.1378  loss_ce_1: 0.164  loss_mask_1: 0.1884  loss_dice_1: 0.1397  loss_ce_2: 0.1646  loss_mask_2: 0.196  loss_dice_2: 0.1355  loss_ce_3: 0.1647  loss_mask_3: 0.2015  loss_dice_3: 0.1347  loss_ce_4: 0.1643  loss_mask_4: 0.1961  loss_dice_4: 0.144  loss_ce_5: 0.165  loss_mask_5: 0.1841  loss_dice_5: 0.1405  loss_ce_6: 0.1649  loss_mask_6: 0.1945  loss_dice_6: 0.1361  loss_ce_7: 0.1645  loss_mask_7: 0.1865  loss_dice_7: 0.1355  loss_ce_8: 0.1649  loss_mask_8: 0.1981  loss_dice_8: 0.1367  time: 0.5729  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:14:05] d2.utils.events INFO:  eta: 2:10:44  iter: 16799  total_loss: 4.563  loss_ce: 0.1629  loss_mask: 0.1661  loss_dice: 0.1297  loss_ce_0: 0.0758  loss_mask_0: 0.1783  loss_dice_0: 0.1352  loss_ce_1: 0.1627  loss_mask_1: 0.1683  loss_dice_1: 0.1339  loss_ce_2: 0.1626  loss_mask_2: 0.1693  loss_dice_2: 0.1384  loss_ce_3: 0.1626  loss_mask_3: 0.1703  loss_dice_3: 0.1351  loss_ce_4: 0.1626  loss_mask_4: 0.1691  loss_dice_4: 0.1321  loss_ce_5: 0.1626  loss_mask_5: 0.1762  loss_dice_5: 0.1352  loss_ce_6: 0.1627  loss_mask_6: 0.1692  loss_dice_6: 0.133  loss_ce_7: 0.1627  loss_mask_7: 0.1689  loss_dice_7: 0.1344  loss_ce_8: 0.163  loss_mask_8: 0.17  loss_dice_8: 0.1332  time: 0.5730  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:14:17] d2.utils.events INFO:  eta: 2:10:48  iter: 16819  total_loss: 4.686  loss_ce: 0.09861  loss_mask: 0.155  loss_dice: 0.1685  loss_ce_0: 0.05257  loss_mask_0: 0.1621  loss_dice_0: 0.1634  loss_ce_1: 0.09934  loss_mask_1: 0.1665  loss_dice_1: 0.1655  loss_ce_2: 0.09911  loss_mask_2: 0.1514  loss_dice_2: 0.1586  loss_ce_3: 0.09931  loss_mask_3: 0.1544  loss_dice_3: 0.1571  loss_ce_4: 0.09931  loss_mask_4: 0.1532  loss_dice_4: 0.1593  loss_ce_5: 0.09928  loss_mask_5: 0.1519  loss_dice_5: 0.1652  loss_ce_6: 0.09905  loss_mask_6: 0.1584  loss_dice_6: 0.1633  loss_ce_7: 0.09906  loss_mask_7: 0.149  loss_dice_7: 0.1602  loss_ce_8: 0.09861  loss_mask_8: 0.1644  loss_dice_8: 0.1674  time: 0.5730  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:14:30] d2.utils.events INFO:  eta: 2:10:55  iter: 16839  total_loss: 4.59  loss_ce: 0.09865  loss_mask: 0.2037  loss_dice: 0.1491  loss_ce_0: 0.05257  loss_mask_0: 0.1945  loss_dice_0: 0.1488  loss_ce_1: 0.09955  loss_mask_1: 0.202  loss_dice_1: 0.142  loss_ce_2: 0.09932  loss_mask_2: 0.2018  loss_dice_2: 0.1468  loss_ce_3: 0.09931  loss_mask_3: 0.2082  loss_dice_3: 0.1467  loss_ce_4: 0.0995  loss_mask_4: 0.1913  loss_dice_4: 0.1416  loss_ce_5: 0.09935  loss_mask_5: 0.1876  loss_dice_5: 0.15  loss_ce_6: 0.09913  loss_mask_6: 0.1979  loss_dice_6: 0.1484  loss_ce_7: 0.09902  loss_mask_7: 0.192  loss_dice_7: 0.1469  loss_ce_8: 0.09872  loss_mask_8: 0.1917  loss_dice_8: 0.1434  time: 0.5731  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:14:43] d2.utils.events INFO:  eta: 2:10:57  iter: 16859  total_loss: 4.539  loss_ce: 0.09921  loss_mask: 0.1971  loss_dice: 0.1297  loss_ce_0: 0.05265  loss_mask_0: 0.2059  loss_dice_0: 0.1313  loss_ce_1: 0.09986  loss_mask_1: 0.1932  loss_dice_1: 0.1305  loss_ce_2: 0.09971  loss_mask_2: 0.2023  loss_dice_2: 0.1314  loss_ce_3: 0.09973  loss_mask_3: 0.2059  loss_dice_3: 0.1317  loss_ce_4: 0.0998  loss_mask_4: 0.1939  loss_dice_4: 0.1346  loss_ce_5: 0.09988  loss_mask_5: 0.2023  loss_dice_5: 0.13  loss_ce_6: 0.09962  loss_mask_6: 0.1985  loss_dice_6: 0.1295  loss_ce_7: 0.09954  loss_mask_7: 0.1987  loss_dice_7: 0.1255  loss_ce_8: 0.09928  loss_mask_8: 0.2059  loss_dice_8: 0.1351  time: 0.5732  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:14:55] d2.utils.events INFO:  eta: 2:10:52  iter: 16879  total_loss: 4.602  loss_ce: 0.09898  loss_mask: 0.162  loss_dice: 0.1487  loss_ce_0: 0.05262  loss_mask_0: 0.1574  loss_dice_0: 0.1435  loss_ce_1: 0.09973  loss_mask_1: 0.1633  loss_dice_1: 0.1442  loss_ce_2: 0.09945  loss_mask_2: 0.1644  loss_dice_2: 0.1474  loss_ce_3: 0.09943  loss_mask_3: 0.1694  loss_dice_3: 0.1447  loss_ce_4: 0.09965  loss_mask_4: 0.1623  loss_dice_4: 0.1474  loss_ce_5: 0.09965  loss_mask_5: 0.1553  loss_dice_5: 0.1453  loss_ce_6: 0.09935  loss_mask_6: 0.1629  loss_dice_6: 0.1461  loss_ce_7: 0.09932  loss_mask_7: 0.16  loss_dice_7: 0.1456  loss_ce_8: 0.09909  loss_mask_8: 0.1572  loss_dice_8: 0.1527  time: 0.5733  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:15:08] d2.utils.events INFO:  eta: 2:10:54  iter: 16899  total_loss: 4.356  loss_ce: 0.09861  loss_mask: 0.1411  loss_dice: 0.1355  loss_ce_0: 0.05251  loss_mask_0: 0.1459  loss_dice_0: 0.1395  loss_ce_1: 0.09911  loss_mask_1: 0.1422  loss_dice_1: 0.1387  loss_ce_2: 0.09879  loss_mask_2: 0.1422  loss_dice_2: 0.1414  loss_ce_3: 0.09879  loss_mask_3: 0.144  loss_dice_3: 0.1405  loss_ce_4: 0.09898  loss_mask_4: 0.1488  loss_dice_4: 0.1422  loss_ce_5: 0.09883  loss_mask_5: 0.1494  loss_dice_5: 0.1426  loss_ce_6: 0.09891  loss_mask_6: 0.1358  loss_dice_6: 0.1331  loss_ce_7: 0.09876  loss_mask_7: 0.1424  loss_dice_7: 0.1379  loss_ce_8: 0.09861  loss_mask_8: 0.1411  loss_dice_8: 0.1353  time: 0.5733  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:15:21] d2.utils.events INFO:  eta: 2:11:05  iter: 16919  total_loss: 4.732  loss_ce: 0.1297  loss_mask: 0.2168  loss_dice: 0.127  loss_ce_0: 0.06397  loss_mask_0: 0.2088  loss_dice_0: 0.1309  loss_ce_1: 0.1292  loss_mask_1: 0.2137  loss_dice_1: 0.1299  loss_ce_2: 0.1295  loss_mask_2: 0.2111  loss_dice_2: 0.1293  loss_ce_3: 0.1296  loss_mask_3: 0.2096  loss_dice_3: 0.1293  loss_ce_4: 0.1294  loss_mask_4: 0.2074  loss_dice_4: 0.128  loss_ce_5: 0.1298  loss_mask_5: 0.2065  loss_dice_5: 0.1269  loss_ce_6: 0.1294  loss_mask_6: 0.2108  loss_dice_6: 0.1247  loss_ce_7: 0.1296  loss_mask_7: 0.2055  loss_dice_7: 0.1377  loss_ce_8: 0.1297  loss_mask_8: 0.2139  loss_dice_8: 0.1284  time: 0.5734  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:15:33] d2.utils.events INFO:  eta: 2:11:30  iter: 16939  total_loss: 4.361  loss_ce: 0.1297  loss_mask: 0.1909  loss_dice: 0.1176  loss_ce_0: 0.06396  loss_mask_0: 0.1952  loss_dice_0: 0.1166  loss_ce_1: 0.1296  loss_mask_1: 0.1986  loss_dice_1: 0.1163  loss_ce_2: 0.1296  loss_mask_2: 0.1817  loss_dice_2: 0.1168  loss_ce_3: 0.1296  loss_mask_3: 0.204  loss_dice_3: 0.1137  loss_ce_4: 0.1295  loss_mask_4: 0.185  loss_dice_4: 0.115  loss_ce_5: 0.1296  loss_mask_5: 0.1881  loss_dice_5: 0.1182  loss_ce_6: 0.1296  loss_mask_6: 0.1865  loss_dice_6: 0.1163  loss_ce_7: 0.1296  loss_mask_7: 0.1882  loss_dice_7: 0.1194  loss_ce_8: 0.1297  loss_mask_8: 0.1878  loss_dice_8: 0.1168  time: 0.5735  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:15:46] d2.utils.events INFO:  eta: 2:11:50  iter: 16959  total_loss: 4.478  loss_ce: 0.1585  loss_mask: 0.1432  loss_dice: 0.1453  loss_ce_0: 0.07494  loss_mask_0: 0.1375  loss_dice_0: 0.1401  loss_ce_1: 0.1567  loss_mask_1: 0.1533  loss_dice_1: 0.1414  loss_ce_2: 0.1576  loss_mask_2: 0.1506  loss_dice_2: 0.1482  loss_ce_3: 0.1577  loss_mask_3: 0.1425  loss_dice_3: 0.1436  loss_ce_4: 0.1573  loss_mask_4: 0.1485  loss_dice_4: 0.1465  loss_ce_5: 0.1575  loss_mask_5: 0.1596  loss_dice_5: 0.1456  loss_ce_6: 0.1577  loss_mask_6: 0.1429  loss_dice_6: 0.1428  loss_ce_7: 0.1582  loss_mask_7: 0.1459  loss_dice_7: 0.1371  loss_ce_8: 0.1584  loss_mask_8: 0.1461  loss_dice_8: 0.1398  time: 0.5735  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:15:59] d2.utils.events INFO:  eta: 2:12:09  iter: 16979  total_loss: 4.76  loss_ce: 0.1006  loss_mask: 0.2084  loss_dice: 0.1479  loss_ce_0: 0.05294  loss_mask_0: 0.2044  loss_dice_0: 0.1492  loss_ce_1: 0.1011  loss_mask_1: 0.2042  loss_dice_1: 0.1524  loss_ce_2: 0.1009  loss_mask_2: 0.2015  loss_dice_2: 0.148  loss_ce_3: 0.101  loss_mask_3: 0.1959  loss_dice_3: 0.1485  loss_ce_4: 0.101  loss_mask_4: 0.1994  loss_dice_4: 0.1422  loss_ce_5: 0.1011  loss_mask_5: 0.1981  loss_dice_5: 0.1526  loss_ce_6: 0.1009  loss_mask_6: 0.206  loss_dice_6: 0.1497  loss_ce_7: 0.1008  loss_mask_7: 0.2051  loss_dice_7: 0.151  loss_ce_8: 0.1006  loss_mask_8: 0.2033  loss_dice_8: 0.1457  time: 0.5736  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:16:11] d2.utils.events INFO:  eta: 2:12:24  iter: 16999  total_loss: 4.644  loss_ce: 0.0997  loss_mask: 0.2032  loss_dice: 0.1375  loss_ce_0: 0.05276  loss_mask_0: 0.2036  loss_dice_0: 0.1375  loss_ce_1: 0.09983  loss_mask_1: 0.21  loss_dice_1: 0.1389  loss_ce_2: 0.09973  loss_mask_2: 0.2052  loss_dice_2: 0.1388  loss_ce_3: 0.09984  loss_mask_3: 0.1972  loss_dice_3: 0.1329  loss_ce_4: 0.09988  loss_mask_4: 0.1976  loss_dice_4: 0.1376  loss_ce_5: 0.09981  loss_mask_5: 0.1999  loss_dice_5: 0.1352  loss_ce_6: 0.09988  loss_mask_6: 0.2061  loss_dice_6: 0.1375  loss_ce_7: 0.09981  loss_mask_7: 0.2081  loss_dice_7: 0.1406  loss_ce_8: 0.09969  loss_mask_8: 0.2095  loss_dice_8: 0.1384  time: 0.5737  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:16:24] d2.utils.events INFO:  eta: 2:12:31  iter: 17019  total_loss: 4.197  loss_ce: 0.1614  loss_mask: 0.1596  loss_dice: 0.129  loss_ce_0: 0.07548  loss_mask_0: 0.1535  loss_dice_0: 0.128  loss_ce_1: 0.161  loss_mask_1: 0.1516  loss_dice_1: 0.1275  loss_ce_2: 0.1614  loss_mask_2: 0.1499  loss_dice_2: 0.1272  loss_ce_3: 0.1614  loss_mask_3: 0.157  loss_dice_3: 0.1307  loss_ce_4: 0.161  loss_mask_4: 0.1573  loss_dice_4: 0.1342  loss_ce_5: 0.1615  loss_mask_5: 0.1526  loss_dice_5: 0.13  loss_ce_6: 0.1613  loss_mask_6: 0.1578  loss_dice_6: 0.1295  loss_ce_7: 0.1614  loss_mask_7: 0.1565  loss_dice_7: 0.1263  loss_ce_8: 0.1613  loss_mask_8: 0.1534  loss_dice_8: 0.1291  time: 0.5738  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:16:37] d2.utils.events INFO:  eta: 2:12:47  iter: 17039  total_loss: 4.6  loss_ce: 0.09883  loss_mask: 0.1878  loss_dice: 0.1556  loss_ce_0: 0.05254  loss_mask_0: 0.1793  loss_dice_0: 0.149  loss_ce_1: 0.09896  loss_mask_1: 0.1867  loss_dice_1: 0.1468  loss_ce_2: 0.09875  loss_mask_2: 0.193  loss_dice_2: 0.153  loss_ce_3: 0.09883  loss_mask_3: 0.1837  loss_dice_3: 0.1495  loss_ce_4: 0.09901  loss_mask_4: 0.1806  loss_dice_4: 0.1536  loss_ce_5: 0.09898  loss_mask_5: 0.1817  loss_dice_5: 0.1569  loss_ce_6: 0.09883  loss_mask_6: 0.179  loss_dice_6: 0.1563  loss_ce_7: 0.09887  loss_mask_7: 0.1879  loss_dice_7: 0.1552  loss_ce_8: 0.09876  loss_mask_8: 0.1792  loss_dice_8: 0.1497  time: 0.5738  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:16:49] d2.utils.events INFO:  eta: 2:12:49  iter: 17059  total_loss: 4.335  loss_ce: 0.1597  loss_mask: 0.1336  loss_dice: 0.1407  loss_ce_0: 0.07513  loss_mask_0: 0.1323  loss_dice_0: 0.1425  loss_ce_1: 0.1593  loss_mask_1: 0.1315  loss_dice_1: 0.1413  loss_ce_2: 0.1595  loss_mask_2: 0.1336  loss_dice_2: 0.1402  loss_ce_3: 0.1595  loss_mask_3: 0.1346  loss_dice_3: 0.1435  loss_ce_4: 0.1593  loss_mask_4: 0.1363  loss_dice_4: 0.1392  loss_ce_5: 0.1594  loss_mask_5: 0.1341  loss_dice_5: 0.1371  loss_ce_6: 0.1596  loss_mask_6: 0.1324  loss_dice_6: 0.1416  loss_ce_7: 0.1595  loss_mask_7: 0.1436  loss_dice_7: 0.1421  loss_ce_8: 0.1598  loss_mask_8: 0.1339  loss_dice_8: 0.1412  time: 0.5739  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:17:02] d2.utils.events INFO:  eta: 2:12:59  iter: 17079  total_loss: 4.355  loss_ce: 0.1294  loss_mask: 0.1847  loss_dice: 0.1199  loss_ce_0: 0.06383  loss_mask_0: 0.1802  loss_dice_0: 0.1238  loss_ce_1: 0.1297  loss_mask_1: 0.179  loss_dice_1: 0.1203  loss_ce_2: 0.1295  loss_mask_2: 0.1787  loss_dice_2: 0.1192  loss_ce_3: 0.1294  loss_mask_3: 0.1934  loss_dice_3: 0.1206  loss_ce_4: 0.1294  loss_mask_4: 0.1784  loss_dice_4: 0.1187  loss_ce_5: 0.1294  loss_mask_5: 0.1763  loss_dice_5: 0.122  loss_ce_6: 0.1295  loss_mask_6: 0.1892  loss_dice_6: 0.1198  loss_ce_7: 0.1293  loss_mask_7: 0.1865  loss_dice_7: 0.125  loss_ce_8: 0.1294  loss_mask_8: 0.1735  loss_dice_8: 0.119  time: 0.5740  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:17:15] d2.utils.events INFO:  eta: 2:13:07  iter: 17099  total_loss: 4.362  loss_ce: 0.1033  loss_mask: 0.1859  loss_dice: 0.1286  loss_ce_0: 0.05352  loss_mask_0: 0.1837  loss_dice_0: 0.1254  loss_ce_1: 0.1045  loss_mask_1: 0.194  loss_dice_1: 0.1226  loss_ce_2: 0.104  loss_mask_2: 0.1909  loss_dice_2: 0.1205  loss_ce_3: 0.104  loss_mask_3: 0.193  loss_dice_3: 0.1303  loss_ce_4: 0.1042  loss_mask_4: 0.1938  loss_dice_4: 0.1233  loss_ce_5: 0.1042  loss_mask_5: 0.1861  loss_dice_5: 0.1281  loss_ce_6: 0.1039  loss_mask_6: 0.1958  loss_dice_6: 0.1328  loss_ce_7: 0.1037  loss_mask_7: 0.1803  loss_dice_7: 0.1297  loss_ce_8: 0.1035  loss_mask_8: 0.1943  loss_dice_8: 0.1285  time: 0.5740  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:17:27] d2.utils.events INFO:  eta: 2:13:05  iter: 17119  total_loss: 4.596  loss_ce: 0.1033  loss_mask: 0.1547  loss_dice: 0.1416  loss_ce_0: 0.05353  loss_mask_0: 0.1592  loss_dice_0: 0.1414  loss_ce_1: 0.1041  loss_mask_1: 0.1668  loss_dice_1: 0.1417  loss_ce_2: 0.1037  loss_mask_2: 0.1489  loss_dice_2: 0.1391  loss_ce_3: 0.1038  loss_mask_3: 0.1601  loss_dice_3: 0.1379  loss_ce_4: 0.104  loss_mask_4: 0.157  loss_dice_4: 0.1338  loss_ce_5: 0.104  loss_mask_5: 0.1621  loss_dice_5: 0.139  loss_ce_6: 0.1038  loss_mask_6: 0.1584  loss_dice_6: 0.1323  loss_ce_7: 0.1036  loss_mask_7: 0.1686  loss_dice_7: 0.1376  loss_ce_8: 0.1033  loss_mask_8: 0.1598  loss_dice_8: 0.1367  time: 0.5741  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:17:40] d2.utils.events INFO:  eta: 2:13:11  iter: 17139  total_loss: 4.966  loss_ce: 0.1022  loss_mask: 0.1784  loss_dice: 0.1511  loss_ce_0: 0.05329  loss_mask_0: 0.1801  loss_dice_0: 0.142  loss_ce_1: 0.1023  loss_mask_1: 0.1731  loss_dice_1: 0.1453  loss_ce_2: 0.1023  loss_mask_2: 0.1817  loss_dice_2: 0.152  loss_ce_3: 0.1023  loss_mask_3: 0.1894  loss_dice_3: 0.1506  loss_ce_4: 0.1023  loss_mask_4: 0.1741  loss_dice_4: 0.1483  loss_ce_5: 0.1025  loss_mask_5: 0.1752  loss_dice_5: 0.1507  loss_ce_6: 0.1023  loss_mask_6: 0.1818  loss_dice_6: 0.1535  loss_ce_7: 0.1021  loss_mask_7: 0.1871  loss_dice_7: 0.1545  loss_ce_8: 0.1021  loss_mask_8: 0.1844  loss_dice_8: 0.1484  time: 0.5742  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:17:53] d2.utils.events INFO:  eta: 2:13:05  iter: 17159  total_loss: 4.325  loss_ce: 0.1584  loss_mask: 0.1578  loss_dice: 0.1409  loss_ce_0: 0.07477  loss_mask_0: 0.1547  loss_dice_0: 0.135  loss_ce_1: 0.1582  loss_mask_1: 0.1588  loss_dice_1: 0.1349  loss_ce_2: 0.1585  loss_mask_2: 0.1622  loss_dice_2: 0.1312  loss_ce_3: 0.1584  loss_mask_3: 0.154  loss_dice_3: 0.1413  loss_ce_4: 0.1582  loss_mask_4: 0.1644  loss_dice_4: 0.1364  loss_ce_5: 0.1583  loss_mask_5: 0.1653  loss_dice_5: 0.131  loss_ce_6: 0.1582  loss_mask_6: 0.1572  loss_dice_6: 0.1394  loss_ce_7: 0.1581  loss_mask_7: 0.1628  loss_dice_7: 0.1353  loss_ce_8: 0.1583  loss_mask_8: 0.1626  loss_dice_8: 0.135  time: 0.5742  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:18:05] d2.utils.events INFO:  eta: 2:13:06  iter: 17179  total_loss: 4.407  loss_ce: 0.1017  loss_mask: 0.1785  loss_dice: 0.1592  loss_ce_0: 0.05321  loss_mask_0: 0.1843  loss_dice_0: 0.1501  loss_ce_1: 0.1022  loss_mask_1: 0.1741  loss_dice_1: 0.1526  loss_ce_2: 0.1017  loss_mask_2: 0.1736  loss_dice_2: 0.1522  loss_ce_3: 0.1018  loss_mask_3: 0.1687  loss_dice_3: 0.1559  loss_ce_4: 0.1021  loss_mask_4: 0.1643  loss_dice_4: 0.1439  loss_ce_5: 0.1018  loss_mask_5: 0.1822  loss_dice_5: 0.1509  loss_ce_6: 0.1018  loss_mask_6: 0.1684  loss_dice_6: 0.1538  loss_ce_7: 0.1018  loss_mask_7: 0.1815  loss_dice_7: 0.1495  loss_ce_8: 0.1016  loss_mask_8: 0.1756  loss_dice_8: 0.1469  time: 0.5743  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:18:18] d2.utils.events INFO:  eta: 2:13:09  iter: 17199  total_loss: 4.393  loss_ce: 0.1293  loss_mask: 0.1609  loss_dice: 0.1596  loss_ce_0: 0.06381  loss_mask_0: 0.1437  loss_dice_0: 0.1496  loss_ce_1: 0.1296  loss_mask_1: 0.1611  loss_dice_1: 0.1528  loss_ce_2: 0.1294  loss_mask_2: 0.1433  loss_dice_2: 0.1444  loss_ce_3: 0.1293  loss_mask_3: 0.1521  loss_dice_3: 0.1578  loss_ce_4: 0.1291  loss_mask_4: 0.1544  loss_dice_4: 0.1547  loss_ce_5: 0.1293  loss_mask_5: 0.147  loss_dice_5: 0.1492  loss_ce_6: 0.1293  loss_mask_6: 0.1518  loss_dice_6: 0.1512  loss_ce_7: 0.1291  loss_mask_7: 0.15  loss_dice_7: 0.1496  loss_ce_8: 0.1293  loss_mask_8: 0.1513  loss_dice_8: 0.1502  time: 0.5744  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:18:31] d2.utils.events INFO:  eta: 2:13:22  iter: 17219  total_loss: 4.856  loss_ce: 0.1025  loss_mask: 0.2047  loss_dice: 0.1438  loss_ce_0: 0.0534  loss_mask_0: 0.2063  loss_dice_0: 0.1424  loss_ce_1: 0.1031  loss_mask_1: 0.2108  loss_dice_1: 0.1408  loss_ce_2: 0.1028  loss_mask_2: 0.2084  loss_dice_2: 0.1428  loss_ce_3: 0.1028  loss_mask_3: 0.2078  loss_dice_3: 0.143  loss_ce_4: 0.103  loss_mask_4: 0.1959  loss_dice_4: 0.1443  loss_ce_5: 0.1029  loss_mask_5: 0.2039  loss_dice_5: 0.1392  loss_ce_6: 0.1028  loss_mask_6: 0.2032  loss_dice_6: 0.1463  loss_ce_7: 0.1027  loss_mask_7: 0.2015  loss_dice_7: 0.1414  loss_ce_8: 0.1025  loss_mask_8: 0.2047  loss_dice_8: 0.1369  time: 0.5744  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:18:43] d2.utils.events INFO:  eta: 2:13:12  iter: 17239  total_loss: 4.871  loss_ce: 0.1021  loss_mask: 0.2142  loss_dice: 0.147  loss_ce_0: 0.05331  loss_mask_0: 0.2123  loss_dice_0: 0.1438  loss_ce_1: 0.1027  loss_mask_1: 0.2166  loss_dice_1: 0.1464  loss_ce_2: 0.1023  loss_mask_2: 0.2161  loss_dice_2: 0.1441  loss_ce_3: 0.1023  loss_mask_3: 0.2125  loss_dice_3: 0.1473  loss_ce_4: 0.1025  loss_mask_4: 0.2121  loss_dice_4: 0.1462  loss_ce_5: 0.1023  loss_mask_5: 0.2167  loss_dice_5: 0.1491  loss_ce_6: 0.1023  loss_mask_6: 0.2179  loss_dice_6: 0.1492  loss_ce_7: 0.1021  loss_mask_7: 0.2255  loss_dice_7: 0.1457  loss_ce_8: 0.1021  loss_mask_8: 0.2148  loss_dice_8: 0.1421  time: 0.5745  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:18:56] d2.utils.events INFO:  eta: 2:13:09  iter: 17259  total_loss: 4.692  loss_ce: 0.1018  loss_mask: 0.219  loss_dice: 0.132  loss_ce_0: 0.05329  loss_mask_0: 0.2174  loss_dice_0: 0.1333  loss_ce_1: 0.1019  loss_mask_1: 0.2211  loss_dice_1: 0.1362  loss_ce_2: 0.1017  loss_mask_2: 0.2209  loss_dice_2: 0.1357  loss_ce_3: 0.1018  loss_mask_3: 0.2225  loss_dice_3: 0.1356  loss_ce_4: 0.102  loss_mask_4: 0.2297  loss_dice_4: 0.1369  loss_ce_5: 0.1018  loss_mask_5: 0.2257  loss_dice_5: 0.1343  loss_ce_6: 0.1018  loss_mask_6: 0.2269  loss_dice_6: 0.1314  loss_ce_7: 0.1018  loss_mask_7: 0.2194  loss_dice_7: 0.1336  loss_ce_8: 0.1017  loss_mask_8: 0.221  loss_dice_8: 0.1329  time: 0.5746  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:19:09] d2.utils.events INFO:  eta: 2:12:58  iter: 17279  total_loss: 4.498  loss_ce: 0.1579  loss_mask: 0.1875  loss_dice: 0.1441  loss_ce_0: 0.07457  loss_mask_0: 0.1804  loss_dice_0: 0.1415  loss_ce_1: 0.1579  loss_mask_1: 0.1899  loss_dice_1: 0.1446  loss_ce_2: 0.1582  loss_mask_2: 0.1893  loss_dice_2: 0.1439  loss_ce_3: 0.1579  loss_mask_3: 0.1868  loss_dice_3: 0.1412  loss_ce_4: 0.1577  loss_mask_4: 0.1939  loss_dice_4: 0.1418  loss_ce_5: 0.1579  loss_mask_5: 0.1873  loss_dice_5: 0.1476  loss_ce_6: 0.158  loss_mask_6: 0.1864  loss_dice_6: 0.1396  loss_ce_7: 0.1579  loss_mask_7: 0.184  loss_dice_7: 0.1422  loss_ce_8: 0.1581  loss_mask_8: 0.1867  loss_dice_8: 0.1449  time: 0.5746  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:19:21] d2.utils.events INFO:  eta: 2:12:49  iter: 17299  total_loss: 4.528  loss_ce: 0.1013  loss_mask: 0.2171  loss_dice: 0.1653  loss_ce_0: 0.05317  loss_mask_0: 0.2213  loss_dice_0: 0.1481  loss_ce_1: 0.1012  loss_mask_1: 0.224  loss_dice_1: 0.1601  loss_ce_2: 0.1012  loss_mask_2: 0.2315  loss_dice_2: 0.1551  loss_ce_3: 0.1011  loss_mask_3: 0.2279  loss_dice_3: 0.1584  loss_ce_4: 0.1013  loss_mask_4: 0.2249  loss_dice_4: 0.1466  loss_ce_5: 0.1013  loss_mask_5: 0.2288  loss_dice_5: 0.1632  loss_ce_6: 0.1012  loss_mask_6: 0.2225  loss_dice_6: 0.1505  loss_ce_7: 0.1012  loss_mask_7: 0.2186  loss_dice_7: 0.1597  loss_ce_8: 0.1012  loss_mask_8: 0.2262  loss_dice_8: 0.1617  time: 0.5747  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:19:34] d2.utils.events INFO:  eta: 2:12:45  iter: 17319  total_loss: 4.457  loss_ce: 0.1007  loss_mask: 0.1457  loss_dice: 0.1516  loss_ce_0: 0.05306  loss_mask_0: 0.156  loss_dice_0: 0.1511  loss_ce_1: 0.1006  loss_mask_1: 0.1541  loss_dice_1: 0.147  loss_ce_2: 0.1005  loss_mask_2: 0.1531  loss_dice_2: 0.1536  loss_ce_3: 0.1005  loss_mask_3: 0.1513  loss_dice_3: 0.1548  loss_ce_4: 0.1007  loss_mask_4: 0.1562  loss_dice_4: 0.151  loss_ce_5: 0.1007  loss_mask_5: 0.1608  loss_dice_5: 0.1551  loss_ce_6: 0.1005  loss_mask_6: 0.15  loss_dice_6: 0.1502  loss_ce_7: 0.1007  loss_mask_7: 0.1581  loss_dice_7: 0.1568  loss_ce_8: 0.1006  loss_mask_8: 0.1475  loss_dice_8: 0.1506  time: 0.5748  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:19:47] d2.utils.events INFO:  eta: 2:12:54  iter: 17339  total_loss: 4.531  loss_ce: 0.1585  loss_mask: 0.1653  loss_dice: 0.145  loss_ce_0: 0.07465  loss_mask_0: 0.1682  loss_dice_0: 0.1449  loss_ce_1: 0.1588  loss_mask_1: 0.1766  loss_dice_1: 0.1454  loss_ce_2: 0.1588  loss_mask_2: 0.1716  loss_dice_2: 0.1447  loss_ce_3: 0.1587  loss_mask_3: 0.1646  loss_dice_3: 0.1393  loss_ce_4: 0.1586  loss_mask_4: 0.1647  loss_dice_4: 0.1466  loss_ce_5: 0.1585  loss_mask_5: 0.1654  loss_dice_5: 0.1422  loss_ce_6: 0.1587  loss_mask_6: 0.1583  loss_dice_6: 0.1452  loss_ce_7: 0.1586  loss_mask_7: 0.1699  loss_dice_7: 0.1415  loss_ce_8: 0.1587  loss_mask_8: 0.1637  loss_dice_8: 0.143  time: 0.5748  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:20:00] d2.utils.events INFO:  eta: 2:13:03  iter: 17359  total_loss: 4.415  loss_ce: 0.1294  loss_mask: 0.1693  loss_dice: 0.1491  loss_ce_0: 0.06381  loss_mask_0: 0.18  loss_dice_0: 0.153  loss_ce_1: 0.1291  loss_mask_1: 0.1828  loss_dice_1: 0.1586  loss_ce_2: 0.1293  loss_mask_2: 0.1853  loss_dice_2: 0.1515  loss_ce_3: 0.1292  loss_mask_3: 0.1873  loss_dice_3: 0.1563  loss_ce_4: 0.1292  loss_mask_4: 0.1812  loss_dice_4: 0.152  loss_ce_5: 0.1293  loss_mask_5: 0.1835  loss_dice_5: 0.1547  loss_ce_6: 0.1292  loss_mask_6: 0.1853  loss_dice_6: 0.1548  loss_ce_7: 0.1293  loss_mask_7: 0.1767  loss_dice_7: 0.1477  loss_ce_8: 0.1294  loss_mask_8: 0.1813  loss_dice_8: 0.1496  time: 0.5749  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:20:12] d2.utils.events INFO:  eta: 2:12:58  iter: 17379  total_loss: 4.346  loss_ce: 0.1566  loss_mask: 0.1795  loss_dice: 0.1353  loss_ce_0: 0.07424  loss_mask_0: 0.1678  loss_dice_0: 0.1337  loss_ce_1: 0.1559  loss_mask_1: 0.1655  loss_dice_1: 0.1322  loss_ce_2: 0.1562  loss_mask_2: 0.18  loss_dice_2: 0.1336  loss_ce_3: 0.1563  loss_mask_3: 0.1718  loss_dice_3: 0.1327  loss_ce_4: 0.1562  loss_mask_4: 0.1752  loss_dice_4: 0.1334  loss_ce_5: 0.1562  loss_mask_5: 0.1732  loss_dice_5: 0.1352  loss_ce_6: 0.1563  loss_mask_6: 0.1766  loss_dice_6: 0.1346  loss_ce_7: 0.1565  loss_mask_7: 0.1749  loss_dice_7: 0.1385  loss_ce_8: 0.1565  loss_mask_8: 0.1721  loss_dice_8: 0.1334  time: 0.5750  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:20:25] d2.utils.events INFO:  eta: 2:12:56  iter: 17399  total_loss: 4.436  loss_ce: 0.1038  loss_mask: 0.1726  loss_dice: 0.1256  loss_ce_0: 0.05374  loss_mask_0: 0.1776  loss_dice_0: 0.1271  loss_ce_1: 0.1047  loss_mask_1: 0.1748  loss_dice_1: 0.1277  loss_ce_2: 0.1044  loss_mask_2: 0.1796  loss_dice_2: 0.124  loss_ce_3: 0.1043  loss_mask_3: 0.1747  loss_dice_3: 0.1261  loss_ce_4: 0.1043  loss_mask_4: 0.1815  loss_dice_4: 0.1281  loss_ce_5: 0.1045  loss_mask_5: 0.1688  loss_dice_5: 0.1298  loss_ce_6: 0.1043  loss_mask_6: 0.1819  loss_dice_6: 0.1308  loss_ce_7: 0.1041  loss_mask_7: 0.1729  loss_dice_7: 0.1313  loss_ce_8: 0.1039  loss_mask_8: 0.1785  loss_dice_8: 0.1291  time: 0.5750  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:20:37] d2.utils.events INFO:  eta: 2:12:34  iter: 17419  total_loss: 4.889  loss_ce: 0.1035  loss_mask: 0.2387  loss_dice: 0.1442  loss_ce_0: 0.05368  loss_mask_0: 0.2455  loss_dice_0: 0.1484  loss_ce_1: 0.104  loss_mask_1: 0.2385  loss_dice_1: 0.1406  loss_ce_2: 0.1038  loss_mask_2: 0.2381  loss_dice_2: 0.1449  loss_ce_3: 0.1038  loss_mask_3: 0.238  loss_dice_3: 0.1471  loss_ce_4: 0.1039  loss_mask_4: 0.2438  loss_dice_4: 0.1487  loss_ce_5: 0.1039  loss_mask_5: 0.2403  loss_dice_5: 0.1499  loss_ce_6: 0.1037  loss_mask_6: 0.2366  loss_dice_6: 0.1463  loss_ce_7: 0.1036  loss_mask_7: 0.234  loss_dice_7: 0.1452  loss_ce_8: 0.1035  loss_mask_8: 0.2368  loss_dice_8: 0.1442  time: 0.5751  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:20:50] d2.utils.events INFO:  eta: 2:12:31  iter: 17439  total_loss: 4.143  loss_ce: 0.1025  loss_mask: 0.2025  loss_dice: 0.1297  loss_ce_0: 0.05349  loss_mask_0: 0.1991  loss_dice_0: 0.1247  loss_ce_1: 0.1028  loss_mask_1: 0.2036  loss_dice_1: 0.128  loss_ce_2: 0.1025  loss_mask_2: 0.2065  loss_dice_2: 0.1281  loss_ce_3: 0.1025  loss_mask_3: 0.2076  loss_dice_3: 0.1333  loss_ce_4: 0.1027  loss_mask_4: 0.1989  loss_dice_4: 0.1289  loss_ce_5: 0.1026  loss_mask_5: 0.2022  loss_dice_5: 0.1298  loss_ce_6: 0.1025  loss_mask_6: 0.2041  loss_dice_6: 0.126  loss_ce_7: 0.1025  loss_mask_7: 0.2004  loss_dice_7: 0.1298  loss_ce_8: 0.1025  loss_mask_8: 0.2004  loss_dice_8: 0.1287  time: 0.5752  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:21:03] d2.utils.events INFO:  eta: 2:12:09  iter: 17459  total_loss: 4.384  loss_ce: 0.1017  loss_mask: 0.1833  loss_dice: 0.1468  loss_ce_0: 0.05331  loss_mask_0: 0.1662  loss_dice_0: 0.1464  loss_ce_1: 0.1019  loss_mask_1: 0.1776  loss_dice_1: 0.1403  loss_ce_2: 0.1015  loss_mask_2: 0.186  loss_dice_2: 0.1451  loss_ce_3: 0.1016  loss_mask_3: 0.1808  loss_dice_3: 0.145  loss_ce_4: 0.1017  loss_mask_4: 0.178  loss_dice_4: 0.1476  loss_ce_5: 0.1017  loss_mask_5: 0.188  loss_dice_5: 0.1508  loss_ce_6: 0.1016  loss_mask_6: 0.1789  loss_dice_6: 0.1449  loss_ce_7: 0.1016  loss_mask_7: 0.1754  loss_dice_7: 0.1479  loss_ce_8: 0.1016  loss_mask_8: 0.1867  loss_dice_8: 0.1496  time: 0.5752  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:21:15] d2.utils.events INFO:  eta: 2:12:01  iter: 17479  total_loss: 4.534  loss_ce: 0.1017  loss_mask: 0.1616  loss_dice: 0.1458  loss_ce_0: 0.0533  loss_mask_0: 0.1598  loss_dice_0: 0.1483  loss_ce_1: 0.102  loss_mask_1: 0.1587  loss_dice_1: 0.1437  loss_ce_2: 0.1016  loss_mask_2: 0.1515  loss_dice_2: 0.145  loss_ce_3: 0.1016  loss_mask_3: 0.1519  loss_dice_3: 0.1465  loss_ce_4: 0.1018  loss_mask_4: 0.1607  loss_dice_4: 0.1411  loss_ce_5: 0.1016  loss_mask_5: 0.1619  loss_dice_5: 0.1402  loss_ce_6: 0.1017  loss_mask_6: 0.1555  loss_dice_6: 0.1345  loss_ce_7: 0.1016  loss_mask_7: 0.1542  loss_dice_7: 0.138  loss_ce_8: 0.1016  loss_mask_8: 0.1527  loss_dice_8: 0.143  time: 0.5753  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:21:28] d2.utils.events INFO:  eta: 2:11:43  iter: 17499  total_loss: 4.395  loss_ce: 0.1573  loss_mask: 0.1775  loss_dice: 0.118  loss_ce_0: 0.07427  loss_mask_0: 0.1828  loss_dice_0: 0.1154  loss_ce_1: 0.157  loss_mask_1: 0.1816  loss_dice_1: 0.1187  loss_ce_2: 0.1574  loss_mask_2: 0.1916  loss_dice_2: 0.1252  loss_ce_3: 0.1574  loss_mask_3: 0.1937  loss_dice_3: 0.1195  loss_ce_4: 0.157  loss_mask_4: 0.1952  loss_dice_4: 0.1207  loss_ce_5: 0.1573  loss_mask_5: 0.1974  loss_dice_5: 0.1211  loss_ce_6: 0.1573  loss_mask_6: 0.1862  loss_dice_6: 0.1195  loss_ce_7: 0.1573  loss_mask_7: 0.1887  loss_dice_7: 0.1173  loss_ce_8: 0.1574  loss_mask_8: 0.1932  loss_dice_8: 0.1234  time: 0.5754  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:21:41] d2.utils.events INFO:  eta: 2:11:31  iter: 17519  total_loss: 4.398  loss_ce: 0.102  loss_mask: 0.1872  loss_dice: 0.1307  loss_ce_0: 0.0534  loss_mask_0: 0.2002  loss_dice_0: 0.1259  loss_ce_1: 0.102  loss_mask_1: 0.1883  loss_dice_1: 0.134  loss_ce_2: 0.102  loss_mask_2: 0.1983  loss_dice_2: 0.1275  loss_ce_3: 0.102  loss_mask_3: 0.1875  loss_dice_3: 0.1301  loss_ce_4: 0.1021  loss_mask_4: 0.1977  loss_dice_4: 0.1269  loss_ce_5: 0.102  loss_mask_5: 0.1932  loss_dice_5: 0.1315  loss_ce_6: 0.102  loss_mask_6: 0.1933  loss_dice_6: 0.1237  loss_ce_7: 0.102  loss_mask_7: 0.1928  loss_dice_7: 0.1306  loss_ce_8: 0.1019  loss_mask_8: 0.1922  loss_dice_8: 0.1296  time: 0.5754  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:21:53] d2.utils.events INFO:  eta: 2:11:30  iter: 17539  total_loss: 4.36  loss_ce: 0.1294  loss_mask: 0.1318  loss_dice: 0.1364  loss_ce_0: 0.0638  loss_mask_0: 0.1405  loss_dice_0: 0.1365  loss_ce_1: 0.1293  loss_mask_1: 0.136  loss_dice_1: 0.1328  loss_ce_2: 0.1294  loss_mask_2: 0.1299  loss_dice_2: 0.139  loss_ce_3: 0.1294  loss_mask_3: 0.1389  loss_dice_3: 0.1359  loss_ce_4: 0.1295  loss_mask_4: 0.1368  loss_dice_4: 0.1296  loss_ce_5: 0.1294  loss_mask_5: 0.1276  loss_dice_5: 0.1347  loss_ce_6: 0.1294  loss_mask_6: 0.135  loss_dice_6: 0.1333  loss_ce_7: 0.1295  loss_mask_7: 0.1495  loss_dice_7: 0.1404  loss_ce_8: 0.1294  loss_mask_8: 0.1389  loss_dice_8: 0.1328  time: 0.5755  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:22:06] d2.utils.events INFO:  eta: 2:11:14  iter: 17559  total_loss: 4.743  loss_ce: 0.156  loss_mask: 0.1835  loss_dice: 0.1499  loss_ce_0: 0.07401  loss_mask_0: 0.1809  loss_dice_0: 0.1473  loss_ce_1: 0.1553  loss_mask_1: 0.1939  loss_dice_1: 0.1492  loss_ce_2: 0.1558  loss_mask_2: 0.1905  loss_dice_2: 0.1526  loss_ce_3: 0.1557  loss_mask_3: 0.1925  loss_dice_3: 0.1519  loss_ce_4: 0.1554  loss_mask_4: 0.1893  loss_dice_4: 0.1478  loss_ce_5: 0.1558  loss_mask_5: 0.1856  loss_dice_5: 0.1483  loss_ce_6: 0.1558  loss_mask_6: 0.1852  loss_dice_6: 0.1464  loss_ce_7: 0.1559  loss_mask_7: 0.197  loss_dice_7: 0.1437  loss_ce_8: 0.156  loss_mask_8: 0.1879  loss_dice_8: 0.151  time: 0.5756  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:22:19] d2.utils.events INFO:  eta: 2:10:47  iter: 17579  total_loss: 4.641  loss_ce: 0.103  loss_mask: 0.1858  loss_dice: 0.1434  loss_ce_0: 0.05362  loss_mask_0: 0.1832  loss_dice_0: 0.1388  loss_ce_1: 0.1035  loss_mask_1: 0.1852  loss_dice_1: 0.147  loss_ce_2: 0.1032  loss_mask_2: 0.1959  loss_dice_2: 0.1479  loss_ce_3: 0.1032  loss_mask_3: 0.1985  loss_dice_3: 0.149  loss_ce_4: 0.1034  loss_mask_4: 0.1943  loss_dice_4: 0.1454  loss_ce_5: 0.1032  loss_mask_5: 0.1884  loss_dice_5: 0.1372  loss_ce_6: 0.1032  loss_mask_6: 0.2004  loss_dice_6: 0.1443  loss_ce_7: 0.1031  loss_mask_7: 0.1876  loss_dice_7: 0.1395  loss_ce_8: 0.103  loss_mask_8: 0.1954  loss_dice_8: 0.1447  time: 0.5756  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:22:31] d2.utils.events INFO:  eta: 2:10:33  iter: 17599  total_loss: 4.713  loss_ce: 0.1031  loss_mask: 0.1721  loss_dice: 0.1546  loss_ce_0: 0.05368  loss_mask_0: 0.1692  loss_dice_0: 0.1535  loss_ce_1: 0.1035  loss_mask_1: 0.1768  loss_dice_1: 0.1502  loss_ce_2: 0.1032  loss_mask_2: 0.1711  loss_dice_2: 0.1532  loss_ce_3: 0.1033  loss_mask_3: 0.1849  loss_dice_3: 0.1543  loss_ce_4: 0.1033  loss_mask_4: 0.1762  loss_dice_4: 0.1475  loss_ce_5: 0.1033  loss_mask_5: 0.187  loss_dice_5: 0.1507  loss_ce_6: 0.1032  loss_mask_6: 0.1772  loss_dice_6: 0.1547  loss_ce_7: 0.1033  loss_mask_7: 0.1808  loss_dice_7: 0.1513  loss_ce_8: 0.1031  loss_mask_8: 0.1794  loss_dice_8: 0.1566  time: 0.5757  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:22:44] d2.utils.events INFO:  eta: 2:10:23  iter: 17619  total_loss: 4.425  loss_ce: 0.1292  loss_mask: 0.1846  loss_dice: 0.1395  loss_ce_0: 0.06373  loss_mask_0: 0.1823  loss_dice_0: 0.1357  loss_ce_1: 0.1294  loss_mask_1: 0.1981  loss_dice_1: 0.134  loss_ce_2: 0.1293  loss_mask_2: 0.1871  loss_dice_2: 0.1381  loss_ce_3: 0.1293  loss_mask_3: 0.1913  loss_dice_3: 0.1413  loss_ce_4: 0.1292  loss_mask_4: 0.1875  loss_dice_4: 0.1375  loss_ce_5: 0.1292  loss_mask_5: 0.1935  loss_dice_5: 0.1402  loss_ce_6: 0.1293  loss_mask_6: 0.1954  loss_dice_6: 0.14  loss_ce_7: 0.129  loss_mask_7: 0.1921  loss_dice_7: 0.1337  loss_ce_8: 0.1292  loss_mask_8: 0.179  loss_dice_8: 0.1292  time: 0.5757  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:22:57] d2.utils.events INFO:  eta: 2:10:10  iter: 17639  total_loss: 4.509  loss_ce: 0.1035  loss_mask: 0.1929  loss_dice: 0.1473  loss_ce_0: 0.05379  loss_mask_0: 0.1918  loss_dice_0: 0.146  loss_ce_1: 0.1037  loss_mask_1: 0.1952  loss_dice_1: 0.1518  loss_ce_2: 0.1036  loss_mask_2: 0.1856  loss_dice_2: 0.1489  loss_ce_3: 0.1038  loss_mask_3: 0.1972  loss_dice_3: 0.1464  loss_ce_4: 0.1037  loss_mask_4: 0.1885  loss_dice_4: 0.1527  loss_ce_5: 0.1038  loss_mask_5: 0.1924  loss_dice_5: 0.1497  loss_ce_6: 0.1037  loss_mask_6: 0.1872  loss_dice_6: 0.1519  loss_ce_7: 0.1036  loss_mask_7: 0.183  loss_dice_7: 0.1521  loss_ce_8: 0.1035  loss_mask_8: 0.1907  loss_dice_8: 0.1462  time: 0.5758  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:23:09] d2.utils.events INFO:  eta: 2:09:53  iter: 17659  total_loss: 4.831  loss_ce: 0.1291  loss_mask: 0.2183  loss_dice: 0.1324  loss_ce_0: 0.06372  loss_mask_0: 0.2197  loss_dice_0: 0.1365  loss_ce_1: 0.1287  loss_mask_1: 0.2182  loss_dice_1: 0.1361  loss_ce_2: 0.129  loss_mask_2: 0.2073  loss_dice_2: 0.1342  loss_ce_3: 0.1291  loss_mask_3: 0.2101  loss_dice_3: 0.1318  loss_ce_4: 0.1289  loss_mask_4: 0.2179  loss_dice_4: 0.1327  loss_ce_5: 0.129  loss_mask_5: 0.2091  loss_dice_5: 0.1327  loss_ce_6: 0.129  loss_mask_6: 0.2082  loss_dice_6: 0.1309  loss_ce_7: 0.1291  loss_mask_7: 0.2115  loss_dice_7: 0.1344  loss_ce_8: 0.1292  loss_mask_8: 0.2175  loss_dice_8: 0.1352  time: 0.5759  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:23:22] d2.utils.events INFO:  eta: 2:09:42  iter: 17679  total_loss: 4.712  loss_ce: 0.1291  loss_mask: 0.1744  loss_dice: 0.1405  loss_ce_0: 0.06368  loss_mask_0: 0.1758  loss_dice_0: 0.1409  loss_ce_1: 0.1288  loss_mask_1: 0.179  loss_dice_1: 0.1449  loss_ce_2: 0.1289  loss_mask_2: 0.1737  loss_dice_2: 0.1436  loss_ce_3: 0.129  loss_mask_3: 0.1699  loss_dice_3: 0.1445  loss_ce_4: 0.1288  loss_mask_4: 0.1704  loss_dice_4: 0.1446  loss_ce_5: 0.1289  loss_mask_5: 0.1706  loss_dice_5: 0.1391  loss_ce_6: 0.129  loss_mask_6: 0.1752  loss_dice_6: 0.1426  loss_ce_7: 0.1288  loss_mask_7: 0.1748  loss_dice_7: 0.1404  loss_ce_8: 0.129  loss_mask_8: 0.1694  loss_dice_8: 0.144  time: 0.5759  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:23:34] d2.utils.events INFO:  eta: 2:09:34  iter: 17699  total_loss: 4.522  loss_ce: 0.1042  loss_mask: 0.1577  loss_dice: 0.1266  loss_ce_0: 0.05394  loss_mask_0: 0.1642  loss_dice_0: 0.124  loss_ce_1: 0.1045  loss_mask_1: 0.1609  loss_dice_1: 0.1371  loss_ce_2: 0.1043  loss_mask_2: 0.1617  loss_dice_2: 0.1272  loss_ce_3: 0.1044  loss_mask_3: 0.1668  loss_dice_3: 0.1341  loss_ce_4: 0.1046  loss_mask_4: 0.1603  loss_dice_4: 0.126  loss_ce_5: 0.1045  loss_mask_5: 0.1637  loss_dice_5: 0.1279  loss_ce_6: 0.1042  loss_mask_6: 0.1572  loss_dice_6: 0.1263  loss_ce_7: 0.1043  loss_mask_7: 0.1675  loss_dice_7: 0.1372  loss_ce_8: 0.1042  loss_mask_8: 0.1701  loss_dice_8: 0.1329  time: 0.5760  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:23:47] d2.utils.events INFO:  eta: 2:09:34  iter: 17719  total_loss: 4.439  loss_ce: 0.1042  loss_mask: 0.1562  loss_dice: 0.147  loss_ce_0: 0.05395  loss_mask_0: 0.1687  loss_dice_0: 0.1512  loss_ce_1: 0.1049  loss_mask_1: 0.1634  loss_dice_1: 0.1536  loss_ce_2: 0.1044  loss_mask_2: 0.1638  loss_dice_2: 0.1503  loss_ce_3: 0.1045  loss_mask_3: 0.1628  loss_dice_3: 0.15  loss_ce_4: 0.1048  loss_mask_4: 0.16  loss_dice_4: 0.1511  loss_ce_5: 0.1046  loss_mask_5: 0.154  loss_dice_5: 0.1448  loss_ce_6: 0.1044  loss_mask_6: 0.1699  loss_dice_6: 0.1496  loss_ce_7: 0.1045  loss_mask_7: 0.1693  loss_dice_7: 0.1486  loss_ce_8: 0.1042  loss_mask_8: 0.1672  loss_dice_8: 0.1486  time: 0.5761  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:24:00] d2.utils.events INFO:  eta: 2:09:16  iter: 17739  total_loss: 4.858  loss_ce: 0.1037  loss_mask: 0.2003  loss_dice: 0.1551  loss_ce_0: 0.05388  loss_mask_0: 0.203  loss_dice_0: 0.1663  loss_ce_1: 0.1039  loss_mask_1: 0.1995  loss_dice_1: 0.1674  loss_ce_2: 0.1038  loss_mask_2: 0.1972  loss_dice_2: 0.1595  loss_ce_3: 0.1038  loss_mask_3: 0.1972  loss_dice_3: 0.1672  loss_ce_4: 0.104  loss_mask_4: 0.2022  loss_dice_4: 0.1615  loss_ce_5: 0.1038  loss_mask_5: 0.2036  loss_dice_5: 0.1666  loss_ce_6: 0.1038  loss_mask_6: 0.2045  loss_dice_6: 0.1645  loss_ce_7: 0.1037  loss_mask_7: 0.2089  loss_dice_7: 0.1678  loss_ce_8: 0.1037  loss_mask_8: 0.2099  loss_dice_8: 0.1563  time: 0.5761  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:24:12] d2.utils.events INFO:  eta: 2:09:07  iter: 17759  total_loss: 4.68  loss_ce: 0.1033  loss_mask: 0.1941  loss_dice: 0.1511  loss_ce_0: 0.05381  loss_mask_0: 0.1902  loss_dice_0: 0.1526  loss_ce_1: 0.1035  loss_mask_1: 0.1948  loss_dice_1: 0.1495  loss_ce_2: 0.1032  loss_mask_2: 0.184  loss_dice_2: 0.1499  loss_ce_3: 0.1034  loss_mask_3: 0.1942  loss_dice_3: 0.1478  loss_ce_4: 0.1037  loss_mask_4: 0.2  loss_dice_4: 0.1516  loss_ce_5: 0.1034  loss_mask_5: 0.1902  loss_dice_5: 0.1503  loss_ce_6: 0.1033  loss_mask_6: 0.1914  loss_dice_6: 0.1448  loss_ce_7: 0.1035  loss_mask_7: 0.1919  loss_dice_7: 0.1518  loss_ce_8: 0.1032  loss_mask_8: 0.1926  loss_dice_8: 0.1475  time: 0.5762  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:24:25] d2.utils.events INFO:  eta: 2:08:45  iter: 17779  total_loss: 4.871  loss_ce: 0.1552  loss_mask: 0.2175  loss_dice: 0.1477  loss_ce_0: 0.07366  loss_mask_0: 0.2162  loss_dice_0: 0.1507  loss_ce_1: 0.1546  loss_mask_1: 0.2156  loss_dice_1: 0.1474  loss_ce_2: 0.1551  loss_mask_2: 0.2129  loss_dice_2: 0.1487  loss_ce_3: 0.1551  loss_mask_3: 0.2156  loss_dice_3: 0.1485  loss_ce_4: 0.1548  loss_mask_4: 0.2139  loss_dice_4: 0.1459  loss_ce_5: 0.155  loss_mask_5: 0.2226  loss_dice_5: 0.1438  loss_ce_6: 0.1551  loss_mask_6: 0.22  loss_dice_6: 0.1421  loss_ce_7: 0.1552  loss_mask_7: 0.2209  loss_dice_7: 0.1528  loss_ce_8: 0.155  loss_mask_8: 0.2186  loss_dice_8: 0.147  time: 0.5762  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:24:38] d2.utils.events INFO:  eta: 2:08:38  iter: 17799  total_loss: 4.748  loss_ce: 0.1031  loss_mask: 0.1901  loss_dice: 0.1709  loss_ce_0: 0.05378  loss_mask_0: 0.2052  loss_dice_0: 0.1668  loss_ce_1: 0.1032  loss_mask_1: 0.1981  loss_dice_1: 0.1719  loss_ce_2: 0.103  loss_mask_2: 0.2021  loss_dice_2: 0.1742  loss_ce_3: 0.1031  loss_mask_3: 0.1943  loss_dice_3: 0.1741  loss_ce_4: 0.1033  loss_mask_4: 0.1881  loss_dice_4: 0.171  loss_ce_5: 0.1032  loss_mask_5: 0.1892  loss_dice_5: 0.1793  loss_ce_6: 0.1031  loss_mask_6: 0.193  loss_dice_6: 0.1749  loss_ce_7: 0.1031  loss_mask_7: 0.1859  loss_dice_7: 0.1769  loss_ce_8: 0.1031  loss_mask_8: 0.2027  loss_dice_8: 0.1774  time: 0.5763  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:24:50] d2.utils.events INFO:  eta: 2:08:25  iter: 17819  total_loss: 5.115  loss_ce: 0.1014  loss_mask: 0.2467  loss_dice: 0.1557  loss_ce_0: 0.0534  loss_mask_0: 0.2469  loss_dice_0: 0.157  loss_ce_1: 0.101  loss_mask_1: 0.2446  loss_dice_1: 0.1502  loss_ce_2: 0.101  loss_mask_2: 0.2485  loss_dice_2: 0.1616  loss_ce_3: 0.1011  loss_mask_3: 0.2352  loss_dice_3: 0.1533  loss_ce_4: 0.1012  loss_mask_4: 0.2389  loss_dice_4: 0.159  loss_ce_5: 0.1012  loss_mask_5: 0.2537  loss_dice_5: 0.1594  loss_ce_6: 0.1011  loss_mask_6: 0.2409  loss_dice_6: 0.1566  loss_ce_7: 0.1012  loss_mask_7: 0.248  loss_dice_7: 0.1564  loss_ce_8: 0.1014  loss_mask_8: 0.2417  loss_dice_8: 0.1566  time: 0.5764  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:25:03] d2.utils.events INFO:  eta: 2:08:09  iter: 17839  total_loss: 4.582  loss_ce: 0.0995  loss_mask: 0.2077  loss_dice: 0.1424  loss_ce_0: 0.05305  loss_mask_0: 0.2154  loss_dice_0: 0.1442  loss_ce_1: 0.09877  loss_mask_1: 0.2243  loss_dice_1: 0.1481  loss_ce_2: 0.09883  loss_mask_2: 0.2225  loss_dice_2: 0.1479  loss_ce_3: 0.09897  loss_mask_3: 0.2284  loss_dice_3: 0.1442  loss_ce_4: 0.09916  loss_mask_4: 0.2272  loss_dice_4: 0.1489  loss_ce_5: 0.0989  loss_mask_5: 0.2085  loss_dice_5: 0.1426  loss_ce_6: 0.09897  loss_mask_6: 0.226  loss_dice_6: 0.1476  loss_ce_7: 0.09935  loss_mask_7: 0.2172  loss_dice_7: 0.1477  loss_ce_8: 0.09931  loss_mask_8: 0.2186  loss_dice_8: 0.1473  time: 0.5764  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:25:15] d2.utils.events INFO:  eta: 2:07:56  iter: 17859  total_loss: 4.701  loss_ce: 0.09646  loss_mask: 0.2002  loss_dice: 0.1694  loss_ce_0: 0.05241  loss_mask_0: 0.1986  loss_dice_0: 0.1702  loss_ce_1: 0.09528  loss_mask_1: 0.2032  loss_dice_1: 0.1648  loss_ce_2: 0.09562  loss_mask_2: 0.2013  loss_dice_2: 0.1706  loss_ce_3: 0.09558  loss_mask_3: 0.1998  loss_dice_3: 0.1703  loss_ce_4: 0.09566  loss_mask_4: 0.2007  loss_dice_4: 0.1723  loss_ce_5: 0.09522  loss_mask_5: 0.2039  loss_dice_5: 0.1712  loss_ce_6: 0.09562  loss_mask_6: 0.2082  loss_dice_6: 0.1736  loss_ce_7: 0.09609  loss_mask_7: 0.1923  loss_dice_7: 0.1736  loss_ce_8: 0.09631  loss_mask_8: 0.2088  loss_dice_8: 0.1753  time: 0.5765  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:25:28] d2.utils.events INFO:  eta: 2:07:42  iter: 17879  total_loss: 4.579  loss_ce: 0.0945  loss_mask: 0.1935  loss_dice: 0.1448  loss_ce_0: 0.05195  loss_mask_0: 0.1935  loss_dice_0: 0.1435  loss_ce_1: 0.09328  loss_mask_1: 0.1869  loss_dice_1: 0.143  loss_ce_2: 0.09358  loss_mask_2: 0.1834  loss_dice_2: 0.1411  loss_ce_3: 0.09345  loss_mask_3: 0.1881  loss_dice_3: 0.1436  loss_ce_4: 0.09353  loss_mask_4: 0.1911  loss_dice_4: 0.1446  loss_ce_5: 0.09328  loss_mask_5: 0.1907  loss_dice_5: 0.1387  loss_ce_6: 0.09353  loss_mask_6: 0.1894  loss_dice_6: 0.1429  loss_ce_7: 0.09407  loss_mask_7: 0.1923  loss_dice_7: 0.1433  loss_ce_8: 0.09428  loss_mask_8: 0.2005  loss_dice_8: 0.1453  time: 0.5765  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:25:41] d2.utils.events INFO:  eta: 2:07:29  iter: 17899  total_loss: 4.43  loss_ce: 0.09299  loss_mask: 0.2168  loss_dice: 0.1492  loss_ce_0: 0.05162  loss_mask_0: 0.2057  loss_dice_0: 0.1421  loss_ce_1: 0.09146  loss_mask_1: 0.2083  loss_dice_1: 0.1428  loss_ce_2: 0.092  loss_mask_2: 0.2007  loss_dice_2: 0.1373  loss_ce_3: 0.09189  loss_mask_3: 0.2052  loss_dice_3: 0.1471  loss_ce_4: 0.09203  loss_mask_4: 0.2134  loss_dice_4: 0.1433  loss_ce_5: 0.09158  loss_mask_5: 0.2065  loss_dice_5: 0.1471  loss_ce_6: 0.09186  loss_mask_6: 0.2076  loss_dice_6: 0.1441  loss_ce_7: 0.09267  loss_mask_7: 0.2066  loss_dice_7: 0.1476  loss_ce_8: 0.09267  loss_mask_8: 0.2119  loss_dice_8: 0.1386  time: 0.5766  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:25:54] d2.utils.events INFO:  eta: 2:07:17  iter: 17919  total_loss: 4.6  loss_ce: 0.09123  loss_mask: 0.1748  loss_dice: 0.1478  loss_ce_0: 0.05118  loss_mask_0: 0.1792  loss_dice_0: 0.1459  loss_ce_1: 0.08952  loss_mask_1: 0.1816  loss_dice_1: 0.1529  loss_ce_2: 0.09021  loss_mask_2: 0.1718  loss_dice_2: 0.1456  loss_ce_3: 0.09  loss_mask_3: 0.1807  loss_dice_3: 0.1534  loss_ce_4: 0.09007  loss_mask_4: 0.1722  loss_dice_4: 0.1441  loss_ce_5: 0.08969  loss_mask_5: 0.1797  loss_dice_5: 0.1505  loss_ce_6: 0.09011  loss_mask_6: 0.175  loss_dice_6: 0.1476  loss_ce_7: 0.0908  loss_mask_7: 0.1735  loss_dice_7: 0.146  loss_ce_8: 0.091  loss_mask_8: 0.1828  loss_dice_8: 0.1406  time: 0.5767  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:26:06] d2.utils.events INFO:  eta: 2:07:03  iter: 17939  total_loss: 4.614  loss_ce: 0.0889  loss_mask: 0.1926  loss_dice: 0.1474  loss_ce_0: 0.05063  loss_mask_0: 0.1906  loss_dice_0: 0.1493  loss_ce_1: 0.08745  loss_mask_1: 0.2015  loss_dice_1: 0.1469  loss_ce_2: 0.08777  loss_mask_2: 0.1984  loss_dice_2: 0.1578  loss_ce_3: 0.08749  loss_mask_3: 0.1886  loss_dice_3: 0.152  loss_ce_4: 0.08777  loss_mask_4: 0.1907  loss_dice_4: 0.1459  loss_ce_5: 0.08715  loss_mask_5: 0.1937  loss_dice_5: 0.1521  loss_ce_6: 0.08766  loss_mask_6: 0.1952  loss_dice_6: 0.1513  loss_ce_7: 0.08848  loss_mask_7: 0.1936  loss_dice_7: 0.1484  loss_ce_8: 0.08862  loss_mask_8: 0.1907  loss_dice_8: 0.1456  time: 0.5767  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:26:19] d2.utils.events INFO:  eta: 2:06:57  iter: 17959  total_loss: 4.546  loss_ce: 0.08753  loss_mask: 0.1722  loss_dice: 0.1582  loss_ce_0: 0.05028  loss_mask_0: 0.1667  loss_dice_0: 0.1597  loss_ce_1: 0.0862  loss_mask_1: 0.1777  loss_dice_1: 0.1595  loss_ce_2: 0.08651  loss_mask_2: 0.1702  loss_dice_2: 0.1537  loss_ce_3: 0.08627  loss_mask_3: 0.1734  loss_dice_3: 0.1541  loss_ce_4: 0.08665  loss_mask_4: 0.1737  loss_dice_4: 0.1523  loss_ce_5: 0.08584  loss_mask_5: 0.1692  loss_dice_5: 0.1527  loss_ce_6: 0.08638  loss_mask_6: 0.1708  loss_dice_6: 0.1532  loss_ce_7: 0.08732  loss_mask_7: 0.1769  loss_dice_7: 0.1516  loss_ce_8: 0.08725  loss_mask_8: 0.1684  loss_dice_8: 0.1465  time: 0.5768  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:26:31] d2.utils.events INFO:  eta: 2:06:48  iter: 17979  total_loss: 4.52  loss_ce: 0.08651  loss_mask: 0.204  loss_dice: 0.1332  loss_ce_0: 0.05003  loss_mask_0: 0.2042  loss_dice_0: 0.1383  loss_ce_1: 0.08531  loss_mask_1: 0.2107  loss_dice_1: 0.1341  loss_ce_2: 0.08581  loss_mask_2: 0.2069  loss_dice_2: 0.1341  loss_ce_3: 0.0855  loss_mask_3: 0.2041  loss_dice_3: 0.1305  loss_ce_4: 0.08574  loss_mask_4: 0.2028  loss_dice_4: 0.1308  loss_ce_5: 0.08504  loss_mask_5: 0.2012  loss_dice_5: 0.1393  loss_ce_6: 0.08564  loss_mask_6: 0.2028  loss_dice_6: 0.1342  loss_ce_7: 0.08628  loss_mask_7: 0.2038  loss_dice_7: 0.1324  loss_ce_8: 0.08631  loss_mask_8: 0.2043  loss_dice_8: 0.1374  time: 0.5769  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:26:44] d2.utils.events INFO:  eta: 2:06:27  iter: 17999  total_loss: 4.679  loss_ce: 0.08635  loss_mask: 0.1761  loss_dice: 0.147  loss_ce_0: 0.04994  loss_mask_0: 0.1671  loss_dice_0: 0.1532  loss_ce_1: 0.08546  loss_mask_1: 0.1726  loss_dice_1: 0.1527  loss_ce_2: 0.08581  loss_mask_2: 0.1712  loss_dice_2: 0.1535  loss_ce_3: 0.08547  loss_mask_3: 0.1767  loss_dice_3: 0.1432  loss_ce_4: 0.08581  loss_mask_4: 0.1758  loss_dice_4: 0.1444  loss_ce_5: 0.08524  loss_mask_5: 0.1815  loss_dice_5: 0.146  loss_ce_6: 0.08554  loss_mask_6: 0.1791  loss_dice_6: 0.145  loss_ce_7: 0.08644  loss_mask_7: 0.1777  loss_dice_7: 0.1441  loss_ce_8: 0.08614  loss_mask_8: 0.1803  loss_dice_8: 0.1422  time: 0.5769  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:26:57] d2.utils.events INFO:  eta: 2:06:14  iter: 18019  total_loss: 4.418  loss_ce: 0.1329  loss_mask: 0.1758  loss_dice: 0.135  loss_ce_0: 0.0644  loss_mask_0: 0.1681  loss_dice_0: 0.1399  loss_ce_1: 0.1327  loss_mask_1: 0.1856  loss_dice_1: 0.1378  loss_ce_2: 0.1327  loss_mask_2: 0.1738  loss_dice_2: 0.1414  loss_ce_3: 0.1329  loss_mask_3: 0.1809  loss_dice_3: 0.135  loss_ce_4: 0.133  loss_mask_4: 0.1765  loss_dice_4: 0.1398  loss_ce_5: 0.133  loss_mask_5: 0.1699  loss_dice_5: 0.1377  loss_ce_6: 0.1328  loss_mask_6: 0.1792  loss_dice_6: 0.1362  loss_ce_7: 0.1329  loss_mask_7: 0.1823  loss_dice_7: 0.1315  loss_ce_8: 0.1328  loss_mask_8: 0.1732  loss_dice_8: 0.1407  time: 0.5770  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:27:09] d2.utils.events INFO:  eta: 2:06:01  iter: 18039  total_loss: 4.558  loss_ce: 0.08736  loss_mask: 0.1884  loss_dice: 0.1542  loss_ce_0: 0.04997  loss_mask_0: 0.1827  loss_dice_0: 0.1574  loss_ce_1: 0.08718  loss_mask_1: 0.1892  loss_dice_1: 0.1544  loss_ce_2: 0.08743  loss_mask_2: 0.1881  loss_dice_2: 0.1573  loss_ce_3: 0.08712  loss_mask_3: 0.1854  loss_dice_3: 0.1582  loss_ce_4: 0.08726  loss_mask_4: 0.1833  loss_dice_4: 0.1521  loss_ce_5: 0.08675  loss_mask_5: 0.1925  loss_dice_5: 0.158  loss_ce_6: 0.08702  loss_mask_6: 0.1791  loss_dice_6: 0.1576  loss_ce_7: 0.08756  loss_mask_7: 0.17  loss_dice_7: 0.1529  loss_ce_8: 0.08732  loss_mask_8: 0.1842  loss_dice_8: 0.1599  time: 0.5770  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:27:22] d2.utils.events INFO:  eta: 2:05:46  iter: 18059  total_loss: 4.781  loss_ce: 0.08685  loss_mask: 0.1526  loss_dice: 0.1621  loss_ce_0: 0.04983  loss_mask_0: 0.1524  loss_dice_0: 0.1594  loss_ce_1: 0.08658  loss_mask_1: 0.1664  loss_dice_1: 0.1697  loss_ce_2: 0.08695  loss_mask_2: 0.1587  loss_dice_2: 0.1712  loss_ce_3: 0.08665  loss_mask_3: 0.1553  loss_dice_3: 0.1633  loss_ce_4: 0.08678  loss_mask_4: 0.157  loss_dice_4: 0.1657  loss_ce_5: 0.08634  loss_mask_5: 0.1536  loss_dice_5: 0.1646  loss_ce_6: 0.08648  loss_mask_6: 0.1555  loss_dice_6: 0.1634  loss_ce_7: 0.08712  loss_mask_7: 0.1605  loss_dice_7: 0.1611  loss_ce_8: 0.08685  loss_mask_8: 0.1527  loss_dice_8: 0.1639  time: 0.5771  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:27:35] d2.utils.events INFO:  eta: 2:05:34  iter: 18079  total_loss: 4.355  loss_ce: 0.1778  loss_mask: 0.1839  loss_dice: 0.1407  loss_ce_0: 0.07898  loss_mask_0: 0.1996  loss_dice_0: 0.1364  loss_ce_1: 0.1771  loss_mask_1: 0.199  loss_dice_1: 0.139  loss_ce_2: 0.1772  loss_mask_2: 0.1898  loss_dice_2: 0.1376  loss_ce_3: 0.1776  loss_mask_3: 0.185  loss_dice_3: 0.133  loss_ce_4: 0.1775  loss_mask_4: 0.1914  loss_dice_4: 0.1362  loss_ce_5: 0.1781  loss_mask_5: 0.196  loss_dice_5: 0.138  loss_ce_6: 0.1778  loss_mask_6: 0.1949  loss_dice_6: 0.1351  loss_ce_7: 0.1774  loss_mask_7: 0.1805  loss_dice_7: 0.1331  loss_ce_8: 0.1778  loss_mask_8: 0.1886  loss_dice_8: 0.1341  time: 0.5772  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:27:47] d2.utils.events INFO:  eta: 2:05:28  iter: 18099  total_loss: 4.692  loss_ce: 0.1752  loss_mask: 0.198  loss_dice: 0.1312  loss_ce_0: 0.07855  loss_mask_0: 0.2011  loss_dice_0: 0.1358  loss_ce_1: 0.1749  loss_mask_1: 0.1965  loss_dice_1: 0.1339  loss_ce_2: 0.1743  loss_mask_2: 0.1897  loss_dice_2: 0.1307  loss_ce_3: 0.1747  loss_mask_3: 0.2072  loss_dice_3: 0.1277  loss_ce_4: 0.1748  loss_mask_4: 0.2154  loss_dice_4: 0.1334  loss_ce_5: 0.1749  loss_mask_5: 0.1977  loss_dice_5: 0.1282  loss_ce_6: 0.1751  loss_mask_6: 0.2024  loss_dice_6: 0.1321  loss_ce_7: 0.1747  loss_mask_7: 0.1979  loss_dice_7: 0.1298  loss_ce_8: 0.1751  loss_mask_8: 0.2015  loss_dice_8: 0.1318  time: 0.5772  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:28:00] d2.utils.events INFO:  eta: 2:05:20  iter: 18119  total_loss: 4.586  loss_ce: 0.1725  loss_mask: 0.1398  loss_dice: 0.1633  loss_ce_0: 0.07817  loss_mask_0: 0.1387  loss_dice_0: 0.1663  loss_ce_1: 0.171  loss_mask_1: 0.1397  loss_dice_1: 0.1602  loss_ce_2: 0.1712  loss_mask_2: 0.1476  loss_dice_2: 0.1617  loss_ce_3: 0.1712  loss_mask_3: 0.1272  loss_dice_3: 0.1527  loss_ce_4: 0.1713  loss_mask_4: 0.1481  loss_dice_4: 0.1651  loss_ce_5: 0.1716  loss_mask_5: 0.1412  loss_dice_5: 0.1593  loss_ce_6: 0.1719  loss_mask_6: 0.1392  loss_dice_6: 0.1638  loss_ce_7: 0.1714  loss_mask_7: 0.1432  loss_dice_7: 0.1649  loss_ce_8: 0.1724  loss_mask_8: 0.1334  loss_dice_8: 0.1559  time: 0.5773  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:28:13] d2.utils.events INFO:  eta: 2:04:59  iter: 18139  total_loss: 4.5  loss_ce: 0.1681  loss_mask: 0.1879  loss_dice: 0.1274  loss_ce_0: 0.07728  loss_mask_0: 0.1852  loss_dice_0: 0.1329  loss_ce_1: 0.1653  loss_mask_1: 0.1762  loss_dice_1: 0.1323  loss_ce_2: 0.1658  loss_mask_2: 0.1723  loss_dice_2: 0.1303  loss_ce_3: 0.166  loss_mask_3: 0.1699  loss_dice_3: 0.1302  loss_ce_4: 0.1662  loss_mask_4: 0.1762  loss_dice_4: 0.1308  loss_ce_5: 0.1661  loss_mask_5: 0.1746  loss_dice_5: 0.1256  loss_ce_6: 0.1667  loss_mask_6: 0.174  loss_dice_6: 0.1263  loss_ce_7: 0.167  loss_mask_7: 0.1742  loss_dice_7: 0.1276  loss_ce_8: 0.1675  loss_mask_8: 0.1778  loss_dice_8: 0.1261  time: 0.5773  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:28:25] d2.utils.events INFO:  eta: 2:04:46  iter: 18159  total_loss: 4.375  loss_ce: 0.09635  loss_mask: 0.2011  loss_dice: 0.1264  loss_ce_0: 0.05153  loss_mask_0: 0.2045  loss_dice_0: 0.1286  loss_ce_1: 0.09875  loss_mask_1: 0.1926  loss_dice_1: 0.1263  loss_ce_2: 0.09802  loss_mask_2: 0.1996  loss_dice_2: 0.1277  loss_ce_3: 0.09804  loss_mask_3: 0.1971  loss_dice_3: 0.1251  loss_ce_4: 0.09815  loss_mask_4: 0.1954  loss_dice_4: 0.1261  loss_ce_5: 0.09823  loss_mask_5: 0.1991  loss_dice_5: 0.1274  loss_ce_6: 0.09778  loss_mask_6: 0.1985  loss_dice_6: 0.1279  loss_ce_7: 0.09741  loss_mask_7: 0.192  loss_dice_7: 0.1282  loss_ce_8: 0.09675  loss_mask_8: 0.1984  loss_dice_8: 0.13  time: 0.5774  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:28:38] d2.utils.events INFO:  eta: 2:04:30  iter: 18179  total_loss: 4.526  loss_ce: 0.09675  loss_mask: 0.1659  loss_dice: 0.1534  loss_ce_0: 0.05161  loss_mask_0: 0.17  loss_dice_0: 0.1456  loss_ce_1: 0.09855  loss_mask_1: 0.1739  loss_dice_1: 0.146  loss_ce_2: 0.0981  loss_mask_2: 0.1683  loss_dice_2: 0.1543  loss_ce_3: 0.09811  loss_mask_3: 0.1737  loss_dice_3: 0.1532  loss_ce_4: 0.09822  loss_mask_4: 0.1627  loss_dice_4: 0.1479  loss_ce_5: 0.09838  loss_mask_5: 0.1688  loss_dice_5: 0.1513  loss_ce_6: 0.09789  loss_mask_6: 0.1697  loss_dice_6: 0.1569  loss_ce_7: 0.09763  loss_mask_7: 0.1784  loss_dice_7: 0.1499  loss_ce_8: 0.09705  loss_mask_8: 0.1754  loss_dice_8: 0.149  time: 0.5775  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:28:51] d2.utils.events INFO:  eta: 2:04:14  iter: 18199  total_loss: 4.517  loss_ce: 0.09675  loss_mask: 0.1915  loss_dice: 0.1383  loss_ce_0: 0.05157  loss_mask_0: 0.1822  loss_dice_0: 0.1328  loss_ce_1: 0.09849  loss_mask_1: 0.1867  loss_dice_1: 0.1371  loss_ce_2: 0.09795  loss_mask_2: 0.1897  loss_dice_2: 0.1367  loss_ce_3: 0.09793  loss_mask_3: 0.1821  loss_dice_3: 0.1313  loss_ce_4: 0.09804  loss_mask_4: 0.1922  loss_dice_4: 0.1347  loss_ce_5: 0.09815  loss_mask_5: 0.1838  loss_dice_5: 0.1331  loss_ce_6: 0.09771  loss_mask_6: 0.1976  loss_dice_6: 0.1354  loss_ce_7: 0.09749  loss_mask_7: 0.1834  loss_dice_7: 0.1348  loss_ce_8: 0.09701  loss_mask_8: 0.1852  loss_dice_8: 0.1373  time: 0.5775  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:29:03] d2.utils.events INFO:  eta: 2:03:59  iter: 18219  total_loss: 4.35  loss_ce: 0.09624  loss_mask: 0.1664  loss_dice: 0.1427  loss_ce_0: 0.05146  loss_mask_0: 0.1713  loss_dice_0: 0.1451  loss_ce_1: 0.09723  loss_mask_1: 0.167  loss_dice_1: 0.1372  loss_ce_2: 0.09695  loss_mask_2: 0.1699  loss_dice_2: 0.1419  loss_ce_3: 0.09704  loss_mask_3: 0.1708  loss_dice_3: 0.1404  loss_ce_4: 0.09719  loss_mask_4: 0.1705  loss_dice_4: 0.1406  loss_ce_5: 0.09734  loss_mask_5: 0.1665  loss_dice_5: 0.1426  loss_ce_6: 0.0969  loss_mask_6: 0.1753  loss_dice_6: 0.141  loss_ce_7: 0.09682  loss_mask_7: 0.1708  loss_dice_7: 0.1414  loss_ce_8: 0.09635  loss_mask_8: 0.1725  loss_dice_8: 0.14  time: 0.5776  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:29:16] d2.utils.events INFO:  eta: 2:03:48  iter: 18239  total_loss: 4.294  loss_ce: 0.09468  loss_mask: 0.1789  loss_dice: 0.1562  loss_ce_0: 0.05109  loss_mask_0: 0.1663  loss_dice_0: 0.1553  loss_ce_1: 0.09513  loss_mask_1: 0.1718  loss_dice_1: 0.1493  loss_ce_2: 0.09489  loss_mask_2: 0.174  loss_dice_2: 0.1586  loss_ce_3: 0.09489  loss_mask_3: 0.1676  loss_dice_3: 0.1524  loss_ce_4: 0.09504  loss_mask_4: 0.1681  loss_dice_4: 0.1519  loss_ce_5: 0.09507  loss_mask_5: 0.1808  loss_dice_5: 0.1513  loss_ce_6: 0.09478  loss_mask_6: 0.1695  loss_dice_6: 0.1554  loss_ce_7: 0.095  loss_mask_7: 0.1697  loss_dice_7: 0.1568  loss_ce_8: 0.09489  loss_mask_8: 0.1706  loss_dice_8: 0.1566  time: 0.5776  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:29:29] d2.utils.events INFO:  eta: 2:03:37  iter: 18259  total_loss: 4.766  loss_ce: 0.09407  loss_mask: 0.2204  loss_dice: 0.1465  loss_ce_0: 0.05094  loss_mask_0: 0.2143  loss_dice_0: 0.1487  loss_ce_1: 0.0945  loss_mask_1: 0.2125  loss_dice_1: 0.1376  loss_ce_2: 0.09415  loss_mask_2: 0.2103  loss_dice_2: 0.1469  loss_ce_3: 0.09413  loss_mask_3: 0.2131  loss_dice_3: 0.1526  loss_ce_4: 0.09428  loss_mask_4: 0.2085  loss_dice_4: 0.148  loss_ce_5: 0.09428  loss_mask_5: 0.2217  loss_dice_5: 0.146  loss_ce_6: 0.09417  loss_mask_6: 0.2072  loss_dice_6: 0.1409  loss_ce_7: 0.09428  loss_mask_7: 0.2074  loss_dice_7: 0.1472  loss_ce_8: 0.09421  loss_mask_8: 0.2095  loss_dice_8: 0.1437  time: 0.5777  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:29:41] d2.utils.events INFO:  eta: 2:03:28  iter: 18279  total_loss: 4.389  loss_ce: 0.09345  loss_mask: 0.1774  loss_dice: 0.1355  loss_ce_0: 0.05079  loss_mask_0: 0.1774  loss_dice_0: 0.1359  loss_ce_1: 0.0938  loss_mask_1: 0.1773  loss_dice_1: 0.132  loss_ce_2: 0.09356  loss_mask_2: 0.1803  loss_dice_2: 0.1391  loss_ce_3: 0.09349  loss_mask_3: 0.184  loss_dice_3: 0.1324  loss_ce_4: 0.0937  loss_mask_4: 0.1802  loss_dice_4: 0.1301  loss_ce_5: 0.09356  loss_mask_5: 0.184  loss_dice_5: 0.1329  loss_ce_6: 0.09352  loss_mask_6: 0.1817  loss_dice_6: 0.1298  loss_ce_7: 0.09363  loss_mask_7: 0.1838  loss_dice_7: 0.1339  loss_ce_8: 0.09356  loss_mask_8: 0.1877  loss_dice_8: 0.1275  time: 0.5778  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:29:54] d2.utils.events INFO:  eta: 2:03:17  iter: 18299  total_loss: 4.441  loss_ce: 0.09396  loss_mask: 0.1607  loss_dice: 0.1602  loss_ce_0: 0.05087  loss_mask_0: 0.155  loss_dice_0: 0.1544  loss_ce_1: 0.09435  loss_mask_1: 0.1577  loss_dice_1: 0.1559  loss_ce_2: 0.09403  loss_mask_2: 0.1588  loss_dice_2: 0.159  loss_ce_3: 0.09403  loss_mask_3: 0.1648  loss_dice_3: 0.1581  loss_ce_4: 0.0942  loss_mask_4: 0.1614  loss_dice_4: 0.1523  loss_ce_5: 0.09413  loss_mask_5: 0.156  loss_dice_5: 0.1524  loss_ce_6: 0.09403  loss_mask_6: 0.1539  loss_dice_6: 0.1501  loss_ce_7: 0.09417  loss_mask_7: 0.1651  loss_dice_7: 0.1522  loss_ce_8: 0.09399  loss_mask_8: 0.1552  loss_dice_8: 0.1559  time: 0.5778  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:30:07] d2.utils.events INFO:  eta: 2:03:12  iter: 18319  total_loss: 4.622  loss_ce: 0.09303  loss_mask: 0.1941  loss_dice: 0.1379  loss_ce_0: 0.05067  loss_mask_0: 0.1996  loss_dice_0: 0.138  loss_ce_1: 0.09299  loss_mask_1: 0.2003  loss_dice_1: 0.1376  loss_ce_2: 0.09292  loss_mask_2: 0.2065  loss_dice_2: 0.1395  loss_ce_3: 0.09292  loss_mask_3: 0.1982  loss_dice_3: 0.1395  loss_ce_4: 0.09299  loss_mask_4: 0.2027  loss_dice_4: 0.1385  loss_ce_5: 0.09299  loss_mask_5: 0.2026  loss_dice_5: 0.138  loss_ce_6: 0.09288  loss_mask_6: 0.2003  loss_dice_6: 0.1432  loss_ce_7: 0.0931  loss_mask_7: 0.2014  loss_dice_7: 0.1308  loss_ce_8: 0.09303  loss_mask_8: 0.1926  loss_dice_8: 0.1373  time: 0.5779  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:30:19] d2.utils.events INFO:  eta: 2:02:52  iter: 18339  total_loss: 4.444  loss_ce: 0.09168  loss_mask: 0.1481  loss_dice: 0.1459  loss_ce_0: 0.05031  loss_mask_0: 0.1453  loss_dice_0: 0.145  loss_ce_1: 0.09147  loss_mask_1: 0.1418  loss_dice_1: 0.1572  loss_ce_2: 0.09138  loss_mask_2: 0.1419  loss_dice_2: 0.1445  loss_ce_3: 0.09143  loss_mask_3: 0.1507  loss_dice_3: 0.1588  loss_ce_4: 0.09154  loss_mask_4: 0.1468  loss_dice_4: 0.1537  loss_ce_5: 0.09129  loss_mask_5: 0.1405  loss_dice_5: 0.144  loss_ce_6: 0.0915  loss_mask_6: 0.1426  loss_dice_6: 0.1523  loss_ce_7: 0.09168  loss_mask_7: 0.1407  loss_dice_7: 0.15  loss_ce_8: 0.09161  loss_mask_8: 0.1459  loss_dice_8: 0.1458  time: 0.5779  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:30:32] d2.utils.events INFO:  eta: 2:02:37  iter: 18359  total_loss: 4.137  loss_ce: 0.09045  loss_mask: 0.1603  loss_dice: 0.1338  loss_ce_0: 0.0501  loss_mask_0: 0.1608  loss_dice_0: 0.1327  loss_ce_1: 0.08986  loss_mask_1: 0.1711  loss_dice_1: 0.1286  loss_ce_2: 0.08983  loss_mask_2: 0.1701  loss_dice_2: 0.1364  loss_ce_3: 0.08986  loss_mask_3: 0.1676  loss_dice_3: 0.1306  loss_ce_4: 0.09014  loss_mask_4: 0.1587  loss_dice_4: 0.1316  loss_ce_5: 0.08986  loss_mask_5: 0.1703  loss_dice_5: 0.1286  loss_ce_6: 0.08993  loss_mask_6: 0.1603  loss_dice_6: 0.13  loss_ce_7: 0.09042  loss_mask_7: 0.1616  loss_dice_7: 0.1312  loss_ce_8: 0.09035  loss_mask_8: 0.1681  loss_dice_8: 0.1352  time: 0.5780  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:30:45] d2.utils.events INFO:  eta: 2:02:25  iter: 18379  total_loss: 4.262  loss_ce: 0.0891  loss_mask: 0.1544  loss_dice: 0.1404  loss_ce_0: 0.0498  loss_mask_0: 0.158  loss_dice_0: 0.1373  loss_ce_1: 0.08816  loss_mask_1: 0.1618  loss_dice_1: 0.1478  loss_ce_2: 0.08838  loss_mask_2: 0.1521  loss_dice_2: 0.1509  loss_ce_3: 0.08831  loss_mask_3: 0.1511  loss_dice_3: 0.1404  loss_ce_4: 0.08858  loss_mask_4: 0.1513  loss_dice_4: 0.1428  loss_ce_5: 0.08824  loss_mask_5: 0.1542  loss_dice_5: 0.1406  loss_ce_6: 0.08848  loss_mask_6: 0.1581  loss_dice_6: 0.1451  loss_ce_7: 0.08889  loss_mask_7: 0.152  loss_dice_7: 0.1414  loss_ce_8: 0.08893  loss_mask_8: 0.1556  loss_dice_8: 0.1418  time: 0.5781  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:30:57] d2.utils.events INFO:  eta: 2:02:10  iter: 18399  total_loss: 4.374  loss_ce: 0.1763  loss_mask: 0.171  loss_dice: 0.1422  loss_ce_0: 0.07929  loss_mask_0: 0.1728  loss_dice_0: 0.1334  loss_ce_1: 0.1771  loss_mask_1: 0.168  loss_dice_1: 0.1386  loss_ce_2: 0.177  loss_mask_2: 0.1705  loss_dice_2: 0.1381  loss_ce_3: 0.1773  loss_mask_3: 0.1714  loss_dice_3: 0.1395  loss_ce_4: 0.177  loss_mask_4: 0.1683  loss_dice_4: 0.135  loss_ce_5: 0.1773  loss_mask_5: 0.1731  loss_dice_5: 0.1373  loss_ce_6: 0.1769  loss_mask_6: 0.1713  loss_dice_6: 0.1375  loss_ce_7: 0.1765  loss_mask_7: 0.1778  loss_dice_7: 0.1364  loss_ce_8: 0.1764  loss_mask_8: 0.178  loss_dice_8: 0.1318  time: 0.5781  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:31:10] d2.utils.events INFO:  eta: 2:02:02  iter: 18419  total_loss: 4.554  loss_ce: 0.08866  loss_mask: 0.2011  loss_dice: 0.1367  loss_ce_0: 0.04964  loss_mask_0: 0.1851  loss_dice_0: 0.1456  loss_ce_1: 0.08801  loss_mask_1: 0.1865  loss_dice_1: 0.1365  loss_ce_2: 0.08819  loss_mask_2: 0.1927  loss_dice_2: 0.1405  loss_ce_3: 0.08811  loss_mask_3: 0.2006  loss_dice_3: 0.1416  loss_ce_4: 0.08824  loss_mask_4: 0.1853  loss_dice_4: 0.1442  loss_ce_5: 0.08807  loss_mask_5: 0.1927  loss_dice_5: 0.1438  loss_ce_6: 0.08821  loss_mask_6: 0.1912  loss_dice_6: 0.1413  loss_ce_7: 0.08855  loss_mask_7: 0.1915  loss_dice_7: 0.1443  loss_ce_8: 0.08855  loss_mask_8: 0.1886  loss_dice_8: 0.1367  time: 0.5782  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:31:23] d2.utils.events INFO:  eta: 2:01:49  iter: 18439  total_loss: 4.614  loss_ce: 0.1755  loss_mask: 0.1797  loss_dice: 0.1418  loss_ce_0: 0.07924  loss_mask_0: 0.1809  loss_dice_0: 0.1389  loss_ce_1: 0.1758  loss_mask_1: 0.1687  loss_dice_1: 0.1407  loss_ce_2: 0.1756  loss_mask_2: 0.1748  loss_dice_2: 0.1422  loss_ce_3: 0.1758  loss_mask_3: 0.1854  loss_dice_3: 0.1444  loss_ce_4: 0.1757  loss_mask_4: 0.1818  loss_dice_4: 0.1409  loss_ce_5: 0.1758  loss_mask_5: 0.1717  loss_dice_5: 0.1459  loss_ce_6: 0.1758  loss_mask_6: 0.1739  loss_dice_6: 0.14  loss_ce_7: 0.1754  loss_mask_7: 0.1732  loss_dice_7: 0.1437  loss_ce_8: 0.1756  loss_mask_8: 0.1804  loss_dice_8: 0.1467  time: 0.5782  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:31:35] d2.utils.events INFO:  eta: 2:01:34  iter: 18459  total_loss: 4.699  loss_ce: 0.09038  loss_mask: 0.1998  loss_dice: 0.1272  loss_ce_0: 0.0499  loss_mask_0: 0.2009  loss_dice_0: 0.125  loss_ce_1: 0.0908  loss_mask_1: 0.2021  loss_dice_1: 0.12  loss_ce_2: 0.09064  loss_mask_2: 0.1964  loss_dice_2: 0.1198  loss_ce_3: 0.09056  loss_mask_3: 0.2  loss_dice_3: 0.1273  loss_ce_4: 0.09066  loss_mask_4: 0.1985  loss_dice_4: 0.1234  loss_ce_5: 0.09042  loss_mask_5: 0.2015  loss_dice_5: 0.1243  loss_ce_6: 0.09052  loss_mask_6: 0.2061  loss_dice_6: 0.1263  loss_ce_7: 0.09056  loss_mask_7: 0.2056  loss_dice_7: 0.1209  loss_ce_8: 0.09042  loss_mask_8: 0.1964  loss_dice_8: 0.1225  time: 0.5783  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:31:48] d2.utils.events INFO:  eta: 2:01:24  iter: 18479  total_loss: 4.753  loss_ce: 0.09031  loss_mask: 0.1956  loss_dice: 0.1443  loss_ce_0: 0.04991  loss_mask_0: 0.19  loss_dice_0: 0.1461  loss_ce_1: 0.09058  loss_mask_1: 0.1798  loss_dice_1: 0.1459  loss_ce_2: 0.09045  loss_mask_2: 0.1825  loss_dice_2: 0.1458  loss_ce_3: 0.09048  loss_mask_3: 0.1811  loss_dice_3: 0.144  loss_ce_4: 0.09049  loss_mask_4: 0.1818  loss_dice_4: 0.1452  loss_ce_5: 0.09038  loss_mask_5: 0.1801  loss_dice_5: 0.153  loss_ce_6: 0.09035  loss_mask_6: 0.1918  loss_dice_6: 0.1459  loss_ce_7: 0.09052  loss_mask_7: 0.187  loss_dice_7: 0.1465  loss_ce_8: 0.09031  loss_mask_8: 0.1882  loss_dice_8: 0.1443  time: 0.5784  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:32:01] d2.utils.events INFO:  eta: 2:01:09  iter: 18499  total_loss: 4.467  loss_ce: 0.08959  loss_mask: 0.1777  loss_dice: 0.1296  loss_ce_0: 0.04974  loss_mask_0: 0.1788  loss_dice_0: 0.1325  loss_ce_1: 0.08955  loss_mask_1: 0.1736  loss_dice_1: 0.1306  loss_ce_2: 0.08965  loss_mask_2: 0.1741  loss_dice_2: 0.1304  loss_ce_3: 0.08955  loss_mask_3: 0.1766  loss_dice_3: 0.1342  loss_ce_4: 0.08962  loss_mask_4: 0.1726  loss_dice_4: 0.1341  loss_ce_5: 0.08948  loss_mask_5: 0.1673  loss_dice_5: 0.1338  loss_ce_6: 0.08951  loss_mask_6: 0.1672  loss_dice_6: 0.1299  loss_ce_7: 0.08976  loss_mask_7: 0.1778  loss_dice_7: 0.1338  loss_ce_8: 0.08958  loss_mask_8: 0.1682  loss_dice_8: 0.1398  time: 0.5784  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:32:13] d2.utils.events INFO:  eta: 2:00:56  iter: 18519  total_loss: 4.723  loss_ce: 0.08862  loss_mask: 0.1862  loss_dice: 0.1347  loss_ce_0: 0.04951  loss_mask_0: 0.1956  loss_dice_0: 0.1421  loss_ce_1: 0.08824  loss_mask_1: 0.1899  loss_dice_1: 0.1415  loss_ce_2: 0.08833  loss_mask_2: 0.1986  loss_dice_2: 0.145  loss_ce_3: 0.08834  loss_mask_3: 0.1954  loss_dice_3: 0.1457  loss_ce_4: 0.08845  loss_mask_4: 0.1913  loss_dice_4: 0.1451  loss_ce_5: 0.08828  loss_mask_5: 0.1896  loss_dice_5: 0.1453  loss_ce_6: 0.08834  loss_mask_6: 0.2017  loss_dice_6: 0.1428  loss_ce_7: 0.08865  loss_mask_7: 0.1864  loss_dice_7: 0.1494  loss_ce_8: 0.08848  loss_mask_8: 0.1844  loss_dice_8: 0.1451  time: 0.5785  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:32:26] d2.utils.events INFO:  eta: 2:00:44  iter: 18539  total_loss: 5.335  loss_ce: 0.08893  loss_mask: 0.2265  loss_dice: 0.1682  loss_ce_0: 0.0495  loss_mask_0: 0.2284  loss_dice_0: 0.1682  loss_ce_1: 0.08874  loss_mask_1: 0.2349  loss_dice_1: 0.1651  loss_ce_2: 0.08889  loss_mask_2: 0.2277  loss_dice_2: 0.1638  loss_ce_3: 0.08879  loss_mask_3: 0.2185  loss_dice_3: 0.1684  loss_ce_4: 0.08872  loss_mask_4: 0.2289  loss_dice_4: 0.1605  loss_ce_5: 0.08869  loss_mask_5: 0.224  loss_dice_5: 0.1713  loss_ce_6: 0.08876  loss_mask_6: 0.2288  loss_dice_6: 0.1693  loss_ce_7: 0.08903  loss_mask_7: 0.2346  loss_dice_7: 0.1637  loss_ce_8: 0.08915  loss_mask_8: 0.2304  loss_dice_8: 0.1752  time: 0.5785  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:32:39] d2.utils.events INFO:  eta: 2:00:32  iter: 18559  total_loss: 4.509  loss_ce: 0.08969  loss_mask: 0.1772  loss_dice: 0.1478  loss_ce_0: 0.04965  loss_mask_0: 0.1809  loss_dice_0: 0.1533  loss_ce_1: 0.09017  loss_mask_1: 0.1805  loss_dice_1: 0.1506  loss_ce_2: 0.08995  loss_mask_2: 0.1745  loss_dice_2: 0.1501  loss_ce_3: 0.09  loss_mask_3: 0.1907  loss_dice_3: 0.1579  loss_ce_4: 0.0901  loss_mask_4: 0.1707  loss_dice_4: 0.152  loss_ce_5: 0.08993  loss_mask_5: 0.1849  loss_dice_5: 0.1549  loss_ce_6: 0.08989  loss_mask_6: 0.1807  loss_dice_6: 0.1442  loss_ce_7: 0.0901  loss_mask_7: 0.1961  loss_dice_7: 0.1483  loss_ce_8: 0.08976  loss_mask_8: 0.1895  loss_dice_8: 0.1456  time: 0.5786  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:32:51] d2.utils.events INFO:  eta: 2:00:30  iter: 18579  total_loss: 4.73  loss_ce: 0.08962  loss_mask: 0.2174  loss_dice: 0.1545  loss_ce_0: 0.0496  loss_mask_0: 0.217  loss_dice_0: 0.1505  loss_ce_1: 0.08978  loss_mask_1: 0.22  loss_dice_1: 0.1533  loss_ce_2: 0.08981  loss_mask_2: 0.2168  loss_dice_2: 0.1528  loss_ce_3: 0.08969  loss_mask_3: 0.2104  loss_dice_3: 0.1543  loss_ce_4: 0.08972  loss_mask_4: 0.2069  loss_dice_4: 0.1575  loss_ce_5: 0.08962  loss_mask_5: 0.2062  loss_dice_5: 0.1522  loss_ce_6: 0.08962  loss_mask_6: 0.2127  loss_dice_6: 0.1535  loss_ce_7: 0.08976  loss_mask_7: 0.2144  loss_dice_7: 0.1521  loss_ce_8: 0.08955  loss_mask_8: 0.2156  loss_dice_8: 0.1492  time: 0.5786  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:33:04] d2.utils.events INFO:  eta: 2:00:18  iter: 18599  total_loss: 4.784  loss_ce: 0.175  loss_mask: 0.1936  loss_dice: 0.1433  loss_ce_0: 0.07947  loss_mask_0: 0.1933  loss_dice_0: 0.1458  loss_ce_1: 0.1742  loss_mask_1: 0.1993  loss_dice_1: 0.1476  loss_ce_2: 0.1746  loss_mask_2: 0.1941  loss_dice_2: 0.1389  loss_ce_3: 0.1747  loss_mask_3: 0.1919  loss_dice_3: 0.1376  loss_ce_4: 0.1745  loss_mask_4: 0.1882  loss_dice_4: 0.143  loss_ce_5: 0.1748  loss_mask_5: 0.1897  loss_dice_5: 0.1459  loss_ce_6: 0.1748  loss_mask_6: 0.1846  loss_dice_6: 0.1516  loss_ce_7: 0.1746  loss_mask_7: 0.1873  loss_dice_7: 0.1401  loss_ce_8: 0.1748  loss_mask_8: 0.1946  loss_dice_8: 0.1402  time: 0.5787  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:33:17] d2.utils.events INFO:  eta: 2:00:03  iter: 18619  total_loss: 4.562  loss_ce: 0.09031  loss_mask: 0.1952  loss_dice: 0.1415  loss_ce_0: 0.04969  loss_mask_0: 0.1889  loss_dice_0: 0.1446  loss_ce_1: 0.09075  loss_mask_1: 0.1937  loss_dice_1: 0.1434  loss_ce_2: 0.09068  loss_mask_2: 0.1895  loss_dice_2: 0.1447  loss_ce_3: 0.09059  loss_mask_3: 0.1908  loss_dice_3: 0.1416  loss_ce_4: 0.09066  loss_mask_4: 0.1949  loss_dice_4: 0.1421  loss_ce_5: 0.09052  loss_mask_5: 0.1867  loss_dice_5: 0.1414  loss_ce_6: 0.09049  loss_mask_6: 0.1947  loss_dice_6: 0.143  loss_ce_7: 0.09056  loss_mask_7: 0.1919  loss_dice_7: 0.1437  loss_ce_8: 0.09035  loss_mask_8: 0.1867  loss_dice_8: 0.1477  time: 0.5788  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:33:29] d2.utils.events INFO:  eta: 1:59:51  iter: 18639  total_loss: 4.388  loss_ce: 0.1716  loss_mask: 0.1581  loss_dice: 0.1337  loss_ce_0: 0.07879  loss_mask_0: 0.1662  loss_dice_0: 0.1376  loss_ce_1: 0.171  loss_mask_1: 0.158  loss_dice_1: 0.133  loss_ce_2: 0.1709  loss_mask_2: 0.1617  loss_dice_2: 0.1312  loss_ce_3: 0.1707  loss_mask_3: 0.1601  loss_dice_3: 0.1276  loss_ce_4: 0.1709  loss_mask_4: 0.1645  loss_dice_4: 0.1405  loss_ce_5: 0.1706  loss_mask_5: 0.1683  loss_dice_5: 0.1374  loss_ce_6: 0.1711  loss_mask_6: 0.1595  loss_dice_6: 0.136  loss_ce_7: 0.1712  loss_mask_7: 0.1592  loss_dice_7: 0.1338  loss_ce_8: 0.1717  loss_mask_8: 0.1592  loss_dice_8: 0.1325  time: 0.5788  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:33:42] d2.utils.events INFO:  eta: 1:59:39  iter: 18659  total_loss: 4.697  loss_ce: 0.168  loss_mask: 0.1937  loss_dice: 0.131  loss_ce_0: 0.07812  loss_mask_0: 0.1994  loss_dice_0: 0.132  loss_ce_1: 0.1661  loss_mask_1: 0.1911  loss_dice_1: 0.1302  loss_ce_2: 0.1666  loss_mask_2: 0.1943  loss_dice_2: 0.1283  loss_ce_3: 0.1665  loss_mask_3: 0.1835  loss_dice_3: 0.1292  loss_ce_4: 0.1665  loss_mask_4: 0.1952  loss_dice_4: 0.1347  loss_ce_5: 0.1663  loss_mask_5: 0.1981  loss_dice_5: 0.1303  loss_ce_6: 0.1671  loss_mask_6: 0.1987  loss_dice_6: 0.1271  loss_ce_7: 0.1672  loss_mask_7: 0.1902  loss_dice_7: 0.1402  loss_ce_8: 0.1679  loss_mask_8: 0.1996  loss_dice_8: 0.1316  time: 0.5789  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:33:55] d2.utils.events INFO:  eta: 1:59:25  iter: 18679  total_loss: 4.444  loss_ce: 0.09482  loss_mask: 0.1772  loss_dice: 0.1537  loss_ce_0: 0.05063  loss_mask_0: 0.1725  loss_dice_0: 0.1523  loss_ce_1: 0.09635  loss_mask_1: 0.1672  loss_dice_1: 0.1555  loss_ce_2: 0.09583  loss_mask_2: 0.1808  loss_dice_2: 0.1524  loss_ce_3: 0.09587  loss_mask_3: 0.1726  loss_dice_3: 0.149  loss_ce_4: 0.09587  loss_mask_4: 0.1681  loss_dice_4: 0.1541  loss_ce_5: 0.09602  loss_mask_5: 0.1734  loss_dice_5: 0.1543  loss_ce_6: 0.09558  loss_mask_6: 0.1689  loss_dice_6: 0.1504  loss_ce_7: 0.09543  loss_mask_7: 0.1732  loss_dice_7: 0.1518  loss_ce_8: 0.095  loss_mask_8: 0.177  loss_dice_8: 0.1561  time: 0.5789  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:34:07] d2.utils.events INFO:  eta: 1:59:12  iter: 18699  total_loss: 4.808  loss_ce: 0.09554  loss_mask: 0.1985  loss_dice: 0.1622  loss_ce_0: 0.0508  loss_mask_0: 0.2003  loss_dice_0: 0.1621  loss_ce_1: 0.0969  loss_mask_1: 0.188  loss_dice_1: 0.1607  loss_ce_2: 0.09644  loss_mask_2: 0.1894  loss_dice_2: 0.1661  loss_ce_3: 0.09653  loss_mask_3: 0.1987  loss_dice_3: 0.1696  loss_ce_4: 0.09664  loss_mask_4: 0.1887  loss_dice_4: 0.1643  loss_ce_5: 0.09682  loss_mask_5: 0.1978  loss_dice_5: 0.1662  loss_ce_6: 0.09627  loss_mask_6: 0.1969  loss_dice_6: 0.1657  loss_ce_7: 0.09609  loss_mask_7: 0.1826  loss_dice_7: 0.1651  loss_ce_8: 0.09558  loss_mask_8: 0.2045  loss_dice_8: 0.1645  time: 0.5790  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:34:20] d2.utils.events INFO:  eta: 1:58:53  iter: 18719  total_loss: 4.534  loss_ce: 0.09503  loss_mask: 0.1719  loss_dice: 0.1432  loss_ce_0: 0.05065  loss_mask_0: 0.17  loss_dice_0: 0.1393  loss_ce_1: 0.09637  loss_mask_1: 0.168  loss_dice_1: 0.1428  loss_ce_2: 0.09578  loss_mask_2: 0.1711  loss_dice_2: 0.1469  loss_ce_3: 0.09587  loss_mask_3: 0.1746  loss_dice_3: 0.1437  loss_ce_4: 0.09594  loss_mask_4: 0.1788  loss_dice_4: 0.1464  loss_ce_5: 0.09587  loss_mask_5: 0.1735  loss_dice_5: 0.1444  loss_ce_6: 0.09561  loss_mask_6: 0.1756  loss_dice_6: 0.1464  loss_ce_7: 0.09547  loss_mask_7: 0.1755  loss_dice_7: 0.1396  loss_ce_8: 0.09518  loss_mask_8: 0.1751  loss_dice_8: 0.1488  time: 0.5790  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:34:32] d2.utils.events INFO:  eta: 1:58:47  iter: 18739  total_loss: 4.753  loss_ce: 0.1671  loss_mask: 0.2032  loss_dice: 0.1369  loss_ce_0: 0.07791  loss_mask_0: 0.207  loss_dice_0: 0.1377  loss_ce_1: 0.1663  loss_mask_1: 0.2001  loss_dice_1: 0.1407  loss_ce_2: 0.1668  loss_mask_2: 0.2167  loss_dice_2: 0.1425  loss_ce_3: 0.1666  loss_mask_3: 0.2241  loss_dice_3: 0.143  loss_ce_4: 0.1665  loss_mask_4: 0.2123  loss_dice_4: 0.1404  loss_ce_5: 0.1663  loss_mask_5: 0.2178  loss_dice_5: 0.1359  loss_ce_6: 0.1668  loss_mask_6: 0.2088  loss_dice_6: 0.1376  loss_ce_7: 0.1669  loss_mask_7: 0.2175  loss_dice_7: 0.1337  loss_ce_8: 0.1671  loss_mask_8: 0.215  loss_dice_8: 0.1363  time: 0.5791  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:34:45] d2.utils.events INFO:  eta: 1:58:36  iter: 18759  total_loss: 4.608  loss_ce: 0.09453  loss_mask: 0.1879  loss_dice: 0.1413  loss_ce_0: 0.05058  loss_mask_0: 0.1908  loss_dice_0: 0.1406  loss_ce_1: 0.095  loss_mask_1: 0.184  loss_dice_1: 0.1423  loss_ce_2: 0.09482  loss_mask_2: 0.1913  loss_dice_2: 0.1412  loss_ce_3: 0.09492  loss_mask_3: 0.1969  loss_dice_3: 0.1432  loss_ce_4: 0.09493  loss_mask_4: 0.1923  loss_dice_4: 0.1386  loss_ce_5: 0.09514  loss_mask_5: 0.1911  loss_dice_5: 0.1337  loss_ce_6: 0.09478  loss_mask_6: 0.1814  loss_dice_6: 0.1415  loss_ce_7: 0.09475  loss_mask_7: 0.1902  loss_dice_7: 0.1403  loss_ce_8: 0.09449  loss_mask_8: 0.1861  loss_dice_8: 0.1435  time: 0.5792  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:34:58] d2.utils.events INFO:  eta: 1:58:27  iter: 18779  total_loss: 4.541  loss_ce: 0.09562  loss_mask: 0.1955  loss_dice: 0.1223  loss_ce_0: 0.05075  loss_mask_0: 0.2046  loss_dice_0: 0.1268  loss_ce_1: 0.09664  loss_mask_1: 0.2025  loss_dice_1: 0.1218  loss_ce_2: 0.09616  loss_mask_2: 0.2053  loss_dice_2: 0.1217  loss_ce_3: 0.09612  loss_mask_3: 0.2015  loss_dice_3: 0.1244  loss_ce_4: 0.09631  loss_mask_4: 0.1964  loss_dice_4: 0.1247  loss_ce_5: 0.09653  loss_mask_5: 0.201  loss_dice_5: 0.1261  loss_ce_6: 0.09616  loss_mask_6: 0.1967  loss_dice_6: 0.1241  loss_ce_7: 0.09591  loss_mask_7: 0.2056  loss_dice_7: 0.1223  loss_ce_8: 0.09576  loss_mask_8: 0.1939  loss_dice_8: 0.1232  time: 0.5792  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:35:10] d2.utils.events INFO:  eta: 1:58:13  iter: 18799  total_loss: 4.53  loss_ce: 0.09551  loss_mask: 0.206  loss_dice: 0.1174  loss_ce_0: 0.05075  loss_mask_0: 0.2061  loss_dice_0: 0.1196  loss_ce_1: 0.09604  loss_mask_1: 0.1971  loss_dice_1: 0.1132  loss_ce_2: 0.09578  loss_mask_2: 0.2039  loss_dice_2: 0.1179  loss_ce_3: 0.09591  loss_mask_3: 0.2043  loss_dice_3: 0.1157  loss_ce_4: 0.09616  loss_mask_4: 0.211  loss_dice_4: 0.1162  loss_ce_5: 0.09631  loss_mask_5: 0.1986  loss_dice_5: 0.116  loss_ce_6: 0.09594  loss_mask_6: 0.206  loss_dice_6: 0.1163  loss_ce_7: 0.0958  loss_mask_7: 0.2041  loss_dice_7: 0.1158  loss_ce_8: 0.09555  loss_mask_8: 0.2033  loss_dice_8: 0.117  time: 0.5793  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:35:23] d2.utils.events INFO:  eta: 1:57:57  iter: 18819  total_loss: 4.741  loss_ce: 0.1306  loss_mask: 0.2284  loss_dice: 0.137  loss_ce_0: 0.06418  loss_mask_0: 0.2277  loss_dice_0: 0.133  loss_ce_1: 0.1307  loss_mask_1: 0.2263  loss_dice_1: 0.1346  loss_ce_2: 0.1306  loss_mask_2: 0.2225  loss_dice_2: 0.141  loss_ce_3: 0.1305  loss_mask_3: 0.231  loss_dice_3: 0.1361  loss_ce_4: 0.1304  loss_mask_4: 0.2287  loss_dice_4: 0.1375  loss_ce_5: 0.1305  loss_mask_5: 0.2364  loss_dice_5: 0.1329  loss_ce_6: 0.1306  loss_mask_6: 0.2343  loss_dice_6: 0.138  loss_ce_7: 0.1306  loss_mask_7: 0.2257  loss_dice_7: 0.1367  loss_ce_8: 0.1306  loss_mask_8: 0.2267  loss_dice_8: 0.1378  time: 0.5793  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:35:36] d2.utils.events INFO:  eta: 1:57:45  iter: 18839  total_loss: 4.222  loss_ce: 0.1645  loss_mask: 0.1717  loss_dice: 0.1321  loss_ce_0: 0.0773  loss_mask_0: 0.1656  loss_dice_0: 0.1306  loss_ce_1: 0.1634  loss_mask_1: 0.1715  loss_dice_1: 0.1347  loss_ce_2: 0.164  loss_mask_2: 0.1766  loss_dice_2: 0.1333  loss_ce_3: 0.1638  loss_mask_3: 0.1735  loss_dice_3: 0.1308  loss_ce_4: 0.1634  loss_mask_4: 0.1778  loss_dice_4: 0.1295  loss_ce_5: 0.1633  loss_mask_5: 0.1767  loss_dice_5: 0.1305  loss_ce_6: 0.1637  loss_mask_6: 0.1676  loss_dice_6: 0.133  loss_ce_7: 0.164  loss_mask_7: 0.1728  loss_dice_7: 0.1316  loss_ce_8: 0.1644  loss_mask_8: 0.174  loss_dice_8: 0.1296  time: 0.5794  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:35:48] d2.utils.events INFO:  eta: 1:57:32  iter: 18859  total_loss: 4.569  loss_ce: 0.09697  loss_mask: 0.2031  loss_dice: 0.1429  loss_ce_0: 0.05113  loss_mask_0: 0.2058  loss_dice_0: 0.1443  loss_ce_1: 0.09765  loss_mask_1: 0.2058  loss_dice_1: 0.1432  loss_ce_2: 0.09732  loss_mask_2: 0.1952  loss_dice_2: 0.1509  loss_ce_3: 0.09748  loss_mask_3: 0.2004  loss_dice_3: 0.146  loss_ce_4: 0.09763  loss_mask_4: 0.2009  loss_dice_4: 0.1378  loss_ce_5: 0.09778  loss_mask_5: 0.2067  loss_dice_5: 0.1533  loss_ce_6: 0.09737  loss_mask_6: 0.2061  loss_dice_6: 0.1409  loss_ce_7: 0.09723  loss_mask_7: 0.1987  loss_dice_7: 0.1516  loss_ce_8: 0.09693  loss_mask_8: 0.2127  loss_dice_8: 0.1475  time: 0.5794  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:36:01] d2.utils.events INFO:  eta: 1:57:23  iter: 18879  total_loss: 4.429  loss_ce: 0.09613  loss_mask: 0.1398  loss_dice: 0.1594  loss_ce_0: 0.05095  loss_mask_0: 0.1398  loss_dice_0: 0.1655  loss_ce_1: 0.09653  loss_mask_1: 0.1347  loss_dice_1: 0.1677  loss_ce_2: 0.09622  loss_mask_2: 0.1417  loss_dice_2: 0.1612  loss_ce_3: 0.09642  loss_mask_3: 0.1444  loss_dice_3: 0.1611  loss_ce_4: 0.09667  loss_mask_4: 0.1453  loss_dice_4: 0.1646  loss_ce_5: 0.09675  loss_mask_5: 0.1414  loss_dice_5: 0.1575  loss_ce_6: 0.09642  loss_mask_6: 0.1442  loss_dice_6: 0.1598  loss_ce_7: 0.09634  loss_mask_7: 0.1377  loss_dice_7: 0.1624  loss_ce_8: 0.09605  loss_mask_8: 0.1459  loss_dice_8: 0.1574  time: 0.5795  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:36:14] d2.utils.events INFO:  eta: 1:57:10  iter: 18899  total_loss: 4.414  loss_ce: 0.09475  loss_mask: 0.1637  loss_dice: 0.1523  loss_ce_0: 0.05067  loss_mask_0: 0.1642  loss_dice_0: 0.1518  loss_ce_1: 0.09478  loss_mask_1: 0.1581  loss_dice_1: 0.1497  loss_ce_2: 0.09453  loss_mask_2: 0.1555  loss_dice_2: 0.1494  loss_ce_3: 0.09471  loss_mask_3: 0.1641  loss_dice_3: 0.15  loss_ce_4: 0.09467  loss_mask_4: 0.1627  loss_dice_4: 0.1551  loss_ce_5: 0.09478  loss_mask_5: 0.1606  loss_dice_5: 0.1568  loss_ce_6: 0.0946  loss_mask_6: 0.1644  loss_dice_6: 0.1523  loss_ce_7: 0.09474  loss_mask_7: 0.1603  loss_dice_7: 0.1546  loss_ce_8: 0.09464  loss_mask_8: 0.1573  loss_dice_8: 0.1527  time: 0.5795  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:36:26] d2.utils.events INFO:  eta: 1:56:57  iter: 18919  total_loss: 4.643  loss_ce: 0.09435  loss_mask: 0.1859  loss_dice: 0.1532  loss_ce_0: 0.05059  loss_mask_0: 0.1857  loss_dice_0: 0.1476  loss_ce_1: 0.0943  loss_mask_1: 0.1779  loss_dice_1: 0.1517  loss_ce_2: 0.09422  loss_mask_2: 0.1917  loss_dice_2: 0.1578  loss_ce_3: 0.09431  loss_mask_3: 0.183  loss_dice_3: 0.1477  loss_ce_4: 0.09431  loss_mask_4: 0.1928  loss_dice_4: 0.1532  loss_ce_5: 0.09442  loss_mask_5: 0.1918  loss_dice_5: 0.1565  loss_ce_6: 0.09431  loss_mask_6: 0.184  loss_dice_6: 0.1498  loss_ce_7: 0.09435  loss_mask_7: 0.1794  loss_dice_7: 0.1515  loss_ce_8: 0.09431  loss_mask_8: 0.1907  loss_dice_8: 0.1562  time: 0.5796  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:36:39] d2.utils.events INFO:  eta: 1:56:44  iter: 18939  total_loss: 4.441  loss_ce: 0.09435  loss_mask: 0.1739  loss_dice: 0.1417  loss_ce_0: 0.05059  loss_mask_0: 0.1713  loss_dice_0: 0.1448  loss_ce_1: 0.09433  loss_mask_1: 0.1696  loss_dice_1: 0.1495  loss_ce_2: 0.09415  loss_mask_2: 0.1852  loss_dice_2: 0.1487  loss_ce_3: 0.0942  loss_mask_3: 0.1803  loss_dice_3: 0.144  loss_ce_4: 0.09428  loss_mask_4: 0.1671  loss_dice_4: 0.1456  loss_ce_5: 0.09435  loss_mask_5: 0.169  loss_dice_5: 0.1458  loss_ce_6: 0.09413  loss_mask_6: 0.1753  loss_dice_6: 0.1441  loss_ce_7: 0.09435  loss_mask_7: 0.1746  loss_dice_7: 0.1481  loss_ce_8: 0.09428  loss_mask_8: 0.1771  loss_dice_8: 0.1474  time: 0.5797  data_time: 0.0017  lr: 1e-05  max_mem: 2811M
[07/11 14:36:52] d2.utils.events INFO:  eta: 1:56:32  iter: 18959  total_loss: 4.851  loss_ce: 0.09352  loss_mask: 0.2109  loss_dice: 0.1517  loss_ce_0: 0.05044  loss_mask_0: 0.2174  loss_dice_0: 0.1541  loss_ce_1: 0.0931  loss_mask_1: 0.2147  loss_dice_1: 0.1569  loss_ce_2: 0.09315  loss_mask_2: 0.2168  loss_dice_2: 0.1625  loss_ce_3: 0.0932  loss_mask_3: 0.2199  loss_dice_3: 0.1629  loss_ce_4: 0.09322  loss_mask_4: 0.2162  loss_dice_4: 0.1591  loss_ce_5: 0.09334  loss_mask_5: 0.2118  loss_dice_5: 0.1534  loss_ce_6: 0.09331  loss_mask_6: 0.2075  loss_dice_6: 0.1528  loss_ce_7: 0.09327  loss_mask_7: 0.2098  loss_dice_7: 0.161  loss_ce_8: 0.09342  loss_mask_8: 0.2172  loss_dice_8: 0.1581  time: 0.5797  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:37:04] d2.utils.events INFO:  eta: 1:56:20  iter: 18979  total_loss: 4.501  loss_ce: 0.1698  loss_mask: 0.1595  loss_dice: 0.1201  loss_ce_0: 0.07834  loss_mask_0: 0.1651  loss_dice_0: 0.1234  loss_ce_1: 0.17  loss_mask_1: 0.1617  loss_dice_1: 0.1216  loss_ce_2: 0.1702  loss_mask_2: 0.1672  loss_dice_2: 0.119  loss_ce_3: 0.1702  loss_mask_3: 0.1615  loss_dice_3: 0.1234  loss_ce_4: 0.1701  loss_mask_4: 0.1633  loss_dice_4: 0.1167  loss_ce_5: 0.1703  loss_mask_5: 0.1693  loss_dice_5: 0.1251  loss_ce_6: 0.1701  loss_mask_6: 0.1679  loss_dice_6: 0.1206  loss_ce_7: 0.1699  loss_mask_7: 0.1703  loss_dice_7: 0.1187  loss_ce_8: 0.17  loss_mask_8: 0.167  loss_dice_8: 0.122  time: 0.5798  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:37:17] d2.utils.events INFO:  eta: 1:56:08  iter: 18999  total_loss: 4.572  loss_ce: 0.09324  loss_mask: 0.1941  loss_dice: 0.1285  loss_ce_0: 0.05034  loss_mask_0: 0.1968  loss_dice_0: 0.1282  loss_ce_1: 0.0937  loss_mask_1: 0.2133  loss_dice_1: 0.1344  loss_ce_2: 0.09325  loss_mask_2: 0.1987  loss_dice_2: 0.133  loss_ce_3: 0.09316  loss_mask_3: 0.1967  loss_dice_3: 0.1392  loss_ce_4: 0.09334  loss_mask_4: 0.1989  loss_dice_4: 0.1308  loss_ce_5: 0.0932  loss_mask_5: 0.19  loss_dice_5: 0.1352  loss_ce_6: 0.0932  loss_mask_6: 0.1979  loss_dice_6: 0.1383  loss_ce_7: 0.0932  loss_mask_7: 0.1921  loss_dice_7: 0.1344  loss_ce_8: 0.0932  loss_mask_8: 0.2097  loss_dice_8: 0.1319  time: 0.5798  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:37:30] d2.utils.events INFO:  eta: 1:55:55  iter: 19019  total_loss: 4.391  loss_ce: 0.09374  loss_mask: 0.1843  loss_dice: 0.141  loss_ce_0: 0.05048  loss_mask_0: 0.1897  loss_dice_0: 0.1413  loss_ce_1: 0.09367  loss_mask_1: 0.1822  loss_dice_1: 0.1408  loss_ce_2: 0.09372  loss_mask_2: 0.1807  loss_dice_2: 0.1435  loss_ce_3: 0.09381  loss_mask_3: 0.1849  loss_dice_3: 0.1441  loss_ce_4: 0.09381  loss_mask_4: 0.1824  loss_dice_4: 0.1426  loss_ce_5: 0.09384  loss_mask_5: 0.187  loss_dice_5: 0.1395  loss_ce_6: 0.0937  loss_mask_6: 0.1852  loss_dice_6: 0.1416  loss_ce_7: 0.09384  loss_mask_7: 0.183  loss_dice_7: 0.1413  loss_ce_8: 0.09367  loss_mask_8: 0.187  loss_dice_8: 0.1425  time: 0.5799  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:37:42] d2.utils.events INFO:  eta: 1:55:43  iter: 19039  total_loss: 4.63  loss_ce: 0.0932  loss_mask: 0.1916  loss_dice: 0.1702  loss_ce_0: 0.05039  loss_mask_0: 0.1818  loss_dice_0: 0.1707  loss_ce_1: 0.0932  loss_mask_1: 0.1855  loss_dice_1: 0.173  loss_ce_2: 0.09313  loss_mask_2: 0.1798  loss_dice_2: 0.1634  loss_ce_3: 0.0932  loss_mask_3: 0.1889  loss_dice_3: 0.1759  loss_ce_4: 0.09324  loss_mask_4: 0.1786  loss_dice_4: 0.1803  loss_ce_5: 0.0932  loss_mask_5: 0.1902  loss_dice_5: 0.1707  loss_ce_6: 0.09306  loss_mask_6: 0.1913  loss_dice_6: 0.1759  loss_ce_7: 0.0932  loss_mask_7: 0.1824  loss_dice_7: 0.1641  loss_ce_8: 0.09313  loss_mask_8: 0.1869  loss_dice_8: 0.1651  time: 0.5799  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:37:55] d2.utils.events INFO:  eta: 1:55:31  iter: 19059  total_loss: 4.496  loss_ce: 0.09224  loss_mask: 0.1759  loss_dice: 0.1676  loss_ce_0: 0.05017  loss_mask_0: 0.1761  loss_dice_0: 0.1645  loss_ce_1: 0.09194  loss_mask_1: 0.1778  loss_dice_1: 0.1644  loss_ce_2: 0.09208  loss_mask_2: 0.1701  loss_dice_2: 0.1677  loss_ce_3: 0.09206  loss_mask_3: 0.1728  loss_dice_3: 0.1649  loss_ce_4: 0.09214  loss_mask_4: 0.1704  loss_dice_4: 0.1682  loss_ce_5: 0.09206  loss_mask_5: 0.182  loss_dice_5: 0.1636  loss_ce_6: 0.09203  loss_mask_6: 0.1739  loss_dice_6: 0.1678  loss_ce_7: 0.09214  loss_mask_7: 0.1643  loss_dice_7: 0.1605  loss_ce_8: 0.09221  loss_mask_8: 0.1687  loss_dice_8: 0.1679  time: 0.5800  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:38:08] d2.utils.events INFO:  eta: 1:55:24  iter: 19079  total_loss: 4.974  loss_ce: 0.1317  loss_mask: 0.202  loss_dice: 0.149  loss_ce_0: 0.06435  loss_mask_0: 0.2069  loss_dice_0: 0.1513  loss_ce_1: 0.1317  loss_mask_1: 0.2101  loss_dice_1: 0.1484  loss_ce_2: 0.132  loss_mask_2: 0.2093  loss_dice_2: 0.1444  loss_ce_3: 0.132  loss_mask_3: 0.2091  loss_dice_3: 0.1497  loss_ce_4: 0.132  loss_mask_4: 0.2158  loss_dice_4: 0.1454  loss_ce_5: 0.132  loss_mask_5: 0.2073  loss_dice_5: 0.1458  loss_ce_6: 0.1319  loss_mask_6: 0.2133  loss_dice_6: 0.1544  loss_ce_7: 0.1319  loss_mask_7: 0.2197  loss_dice_7: 0.1535  loss_ce_8: 0.1318  loss_mask_8: 0.2004  loss_dice_8: 0.155  time: 0.5800  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:38:20] d2.utils.events INFO:  eta: 1:55:12  iter: 19099  total_loss: 4.935  loss_ce: 0.1714  loss_mask: 0.2003  loss_dice: 0.1456  loss_ce_0: 0.07872  loss_mask_0: 0.1834  loss_dice_0: 0.1514  loss_ce_1: 0.1715  loss_mask_1: 0.1991  loss_dice_1: 0.1488  loss_ce_2: 0.1715  loss_mask_2: 0.1992  loss_dice_2: 0.1456  loss_ce_3: 0.1716  loss_mask_3: 0.1957  loss_dice_3: 0.1533  loss_ce_4: 0.1714  loss_mask_4: 0.1966  loss_dice_4: 0.1486  loss_ce_5: 0.1719  loss_mask_5: 0.204  loss_dice_5: 0.141  loss_ce_6: 0.1717  loss_mask_6: 0.2015  loss_dice_6: 0.1518  loss_ce_7: 0.1714  loss_mask_7: 0.191  loss_dice_7: 0.1447  loss_ce_8: 0.1714  loss_mask_8: 0.2077  loss_dice_8: 0.1502  time: 0.5801  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:38:33] d2.utils.events INFO:  eta: 1:54:58  iter: 19119  total_loss: 4.66  loss_ce: 0.1689  loss_mask: 0.1576  loss_dice: 0.1404  loss_ce_0: 0.07813  loss_mask_0: 0.1587  loss_dice_0: 0.1442  loss_ce_1: 0.1687  loss_mask_1: 0.1601  loss_dice_1: 0.1455  loss_ce_2: 0.1685  loss_mask_2: 0.1568  loss_dice_2: 0.1426  loss_ce_3: 0.1685  loss_mask_3: 0.1572  loss_dice_3: 0.1434  loss_ce_4: 0.1686  loss_mask_4: 0.1554  loss_dice_4: 0.1396  loss_ce_5: 0.1685  loss_mask_5: 0.1592  loss_dice_5: 0.1396  loss_ce_6: 0.1688  loss_mask_6: 0.1548  loss_dice_6: 0.1377  loss_ce_7: 0.1688  loss_mask_7: 0.1597  loss_dice_7: 0.1408  loss_ce_8: 0.1689  loss_mask_8: 0.1618  loss_dice_8: 0.1412  time: 0.5802  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:38:46] d2.utils.events INFO:  eta: 1:54:46  iter: 19139  total_loss: 4.688  loss_ce: 0.1677  loss_mask: 0.1721  loss_dice: 0.1356  loss_ce_0: 0.0779  loss_mask_0: 0.1739  loss_dice_0: 0.1356  loss_ce_1: 0.1662  loss_mask_1: 0.173  loss_dice_1: 0.1395  loss_ce_2: 0.167  loss_mask_2: 0.176  loss_dice_2: 0.1375  loss_ce_3: 0.167  loss_mask_3: 0.1812  loss_dice_3: 0.1421  loss_ce_4: 0.1668  loss_mask_4: 0.1801  loss_dice_4: 0.1428  loss_ce_5: 0.1671  loss_mask_5: 0.1738  loss_dice_5: 0.135  loss_ce_6: 0.1673  loss_mask_6: 0.1723  loss_dice_6: 0.1368  loss_ce_7: 0.1673  loss_mask_7: 0.1757  loss_dice_7: 0.1409  loss_ce_8: 0.1676  loss_mask_8: 0.1705  loss_dice_8: 0.1395  time: 0.5802  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:38:59] d2.utils.events INFO:  eta: 1:54:32  iter: 19159  total_loss: 4.61  loss_ce: 0.095  loss_mask: 0.2079  loss_dice: 0.1291  loss_ce_0: 0.05071  loss_mask_0: 0.2078  loss_dice_0: 0.1329  loss_ce_1: 0.09602  loss_mask_1: 0.2025  loss_dice_1: 0.134  loss_ce_2: 0.09571  loss_mask_2: 0.1995  loss_dice_2: 0.1327  loss_ce_3: 0.09569  loss_mask_3: 0.2057  loss_dice_3: 0.1313  loss_ce_4: 0.09576  loss_mask_4: 0.1983  loss_dice_4: 0.1336  loss_ce_5: 0.09572  loss_mask_5: 0.1974  loss_dice_5: 0.1361  loss_ce_6: 0.09551  loss_mask_6: 0.213  loss_dice_6: 0.1322  loss_ce_7: 0.09532  loss_mask_7: 0.2008  loss_dice_7: 0.1315  loss_ce_8: 0.09518  loss_mask_8: 0.2075  loss_dice_8: 0.1335  time: 0.5803  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:39:11] d2.utils.events INFO:  eta: 1:54:20  iter: 19179  total_loss: 4.272  loss_ce: 0.09464  loss_mask: 0.1633  loss_dice: 0.1573  loss_ce_0: 0.05063  loss_mask_0: 0.1668  loss_dice_0: 0.1579  loss_ce_1: 0.09513  loss_mask_1: 0.1672  loss_dice_1: 0.1534  loss_ce_2: 0.095  loss_mask_2: 0.175  loss_dice_2: 0.157  loss_ce_3: 0.09514  loss_mask_3: 0.1686  loss_dice_3: 0.1518  loss_ce_4: 0.095  loss_mask_4: 0.1721  loss_dice_4: 0.1581  loss_ce_5: 0.095  loss_mask_5: 0.172  loss_dice_5: 0.157  loss_ce_6: 0.09489  loss_mask_6: 0.1651  loss_dice_6: 0.1587  loss_ce_7: 0.09485  loss_mask_7: 0.1687  loss_dice_7: 0.1509  loss_ce_8: 0.09464  loss_mask_8: 0.1689  loss_dice_8: 0.1531  time: 0.5803  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:39:24] d2.utils.events INFO:  eta: 1:54:07  iter: 19199  total_loss: 4.556  loss_ce: 0.09381  loss_mask: 0.1522  loss_dice: 0.1454  loss_ce_0: 0.05044  loss_mask_0: 0.1554  loss_dice_0: 0.1362  loss_ce_1: 0.0944  loss_mask_1: 0.1626  loss_dice_1: 0.142  loss_ce_2: 0.09409  loss_mask_2: 0.1613  loss_dice_2: 0.1397  loss_ce_3: 0.09406  loss_mask_3: 0.1585  loss_dice_3: 0.1461  loss_ce_4: 0.09409  loss_mask_4: 0.1545  loss_dice_4: 0.1444  loss_ce_5: 0.09399  loss_mask_5: 0.1565  loss_dice_5: 0.1386  loss_ce_6: 0.09402  loss_mask_6: 0.16  loss_dice_6: 0.1431  loss_ce_7: 0.09388  loss_mask_7: 0.1485  loss_dice_7: 0.1324  loss_ce_8: 0.09388  loss_mask_8: 0.1522  loss_dice_8: 0.1382  time: 0.5804  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:39:37] d2.utils.events INFO:  eta: 1:53:55  iter: 19219  total_loss: 4.734  loss_ce: 0.09345  loss_mask: 0.2  loss_dice: 0.1423  loss_ce_0: 0.05038  loss_mask_0: 0.2066  loss_dice_0: 0.1521  loss_ce_1: 0.09379  loss_mask_1: 0.1959  loss_dice_1: 0.1469  loss_ce_2: 0.09358  loss_mask_2: 0.1973  loss_dice_2: 0.1435  loss_ce_3: 0.09363  loss_mask_3: 0.1954  loss_dice_3: 0.1394  loss_ce_4: 0.09363  loss_mask_4: 0.1897  loss_dice_4: 0.1484  loss_ce_5: 0.09363  loss_mask_5: 0.2091  loss_dice_5: 0.1406  loss_ce_6: 0.09356  loss_mask_6: 0.2005  loss_dice_6: 0.1416  loss_ce_7: 0.09352  loss_mask_7: 0.1999  loss_dice_7: 0.1496  loss_ce_8: 0.09349  loss_mask_8: 0.2024  loss_dice_8: 0.1445  time: 0.5804  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:39:49] d2.utils.events INFO:  eta: 1:53:42  iter: 19239  total_loss: 4.702  loss_ce: 0.09334  loss_mask: 0.2065  loss_dice: 0.133  loss_ce_0: 0.05033  loss_mask_0: 0.204  loss_dice_0: 0.1314  loss_ce_1: 0.09377  loss_mask_1: 0.2019  loss_dice_1: 0.1321  loss_ce_2: 0.09347  loss_mask_2: 0.1948  loss_dice_2: 0.1296  loss_ce_3: 0.09338  loss_mask_3: 0.1997  loss_dice_3: 0.1343  loss_ce_4: 0.09352  loss_mask_4: 0.2059  loss_dice_4: 0.1317  loss_ce_5: 0.09348  loss_mask_5: 0.1978  loss_dice_5: 0.1348  loss_ce_6: 0.09338  loss_mask_6: 0.2077  loss_dice_6: 0.1332  loss_ce_7: 0.09338  loss_mask_7: 0.2049  loss_dice_7: 0.1344  loss_ce_8: 0.09352  loss_mask_8: 0.2171  loss_dice_8: 0.1379  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:40:02] d2.utils.events INFO:  eta: 1:53:28  iter: 19259  total_loss: 4.862  loss_ce: 0.1678  loss_mask: 0.1654  loss_dice: 0.1485  loss_ce_0: 0.07794  loss_mask_0: 0.1597  loss_dice_0: 0.1488  loss_ce_1: 0.1672  loss_mask_1: 0.1676  loss_dice_1: 0.15  loss_ce_2: 0.1674  loss_mask_2: 0.1637  loss_dice_2: 0.1485  loss_ce_3: 0.1674  loss_mask_3: 0.163  loss_dice_3: 0.1408  loss_ce_4: 0.1673  loss_mask_4: 0.1605  loss_dice_4: 0.1421  loss_ce_5: 0.1675  loss_mask_5: 0.1587  loss_dice_5: 0.1482  loss_ce_6: 0.1675  loss_mask_6: 0.1565  loss_dice_6: 0.1421  loss_ce_7: 0.1676  loss_mask_7: 0.163  loss_dice_7: 0.1519  loss_ce_8: 0.1677  loss_mask_8: 0.1645  loss_dice_8: 0.1463  time: 0.5805  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:40:14] d2.utils.events INFO:  eta: 1:53:07  iter: 19279  total_loss: 4.619  loss_ce: 0.09518  loss_mask: 0.2056  loss_dice: 0.1626  loss_ce_0: 0.05077  loss_mask_0: 0.197  loss_dice_0: 0.161  loss_ce_1: 0.09576  loss_mask_1: 0.1982  loss_dice_1: 0.162  loss_ce_2: 0.09556  loss_mask_2: 0.1913  loss_dice_2: 0.1568  loss_ce_3: 0.09561  loss_mask_3: 0.1992  loss_dice_3: 0.158  loss_ce_4: 0.09561  loss_mask_4: 0.198  loss_dice_4: 0.1601  loss_ce_5: 0.09572  loss_mask_5: 0.1957  loss_dice_5: 0.1593  loss_ce_6: 0.0955  loss_mask_6: 0.1909  loss_dice_6: 0.1535  loss_ce_7: 0.09536  loss_mask_7: 0.1963  loss_dice_7: 0.1592  loss_ce_8: 0.09522  loss_mask_8: 0.1982  loss_dice_8: 0.1561  time: 0.5806  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:40:26] d2.utils.events INFO:  eta: 1:52:46  iter: 19299  total_loss: 4.494  loss_ce: 0.1663  loss_mask: 0.1455  loss_dice: 0.1449  loss_ce_0: 0.07761  loss_mask_0: 0.1493  loss_dice_0: 0.1453  loss_ce_1: 0.1655  loss_mask_1: 0.1497  loss_dice_1: 0.147  loss_ce_2: 0.166  loss_mask_2: 0.1473  loss_dice_2: 0.1485  loss_ce_3: 0.1659  loss_mask_3: 0.1462  loss_dice_3: 0.1438  loss_ce_4: 0.1658  loss_mask_4: 0.1473  loss_dice_4: 0.1421  loss_ce_5: 0.1657  loss_mask_5: 0.1495  loss_dice_5: 0.1429  loss_ce_6: 0.1659  loss_mask_6: 0.1547  loss_dice_6: 0.1485  loss_ce_7: 0.166  loss_mask_7: 0.1525  loss_dice_7: 0.141  loss_ce_8: 0.1662  loss_mask_8: 0.1475  loss_dice_8: 0.1509  time: 0.5806  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:40:37] d2.utils.events INFO:  eta: 1:52:21  iter: 19319  total_loss: 4.457  loss_ce: 0.09587  loss_mask: 0.1883  loss_dice: 0.1339  loss_ce_0: 0.0509  loss_mask_0: 0.1911  loss_dice_0: 0.1295  loss_ce_1: 0.09642  loss_mask_1: 0.1875  loss_dice_1: 0.1337  loss_ce_2: 0.09621  loss_mask_2: 0.1977  loss_dice_2: 0.1291  loss_ce_3: 0.0963  loss_mask_3: 0.1899  loss_dice_3: 0.1302  loss_ce_4: 0.09641  loss_mask_4: 0.1952  loss_dice_4: 0.1287  loss_ce_5: 0.0966  loss_mask_5: 0.1966  loss_dice_5: 0.1292  loss_ce_6: 0.09634  loss_mask_6: 0.1922  loss_dice_6: 0.1299  loss_ce_7: 0.09605  loss_mask_7: 0.1894  loss_dice_7: 0.1353  loss_ce_8: 0.09591  loss_mask_8: 0.1851  loss_dice_8: 0.1297  time: 0.5806  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:40:49] d2.utils.events INFO:  eta: 1:51:57  iter: 19339  total_loss: 4.806  loss_ce: 0.1659  loss_mask: 0.2085  loss_dice: 0.1295  loss_ce_0: 0.07749  loss_mask_0: 0.2188  loss_dice_0: 0.1311  loss_ce_1: 0.1651  loss_mask_1: 0.2158  loss_dice_1: 0.1348  loss_ce_2: 0.1656  loss_mask_2: 0.2114  loss_dice_2: 0.13  loss_ce_3: 0.1656  loss_mask_3: 0.2121  loss_dice_3: 0.1328  loss_ce_4: 0.1655  loss_mask_4: 0.2159  loss_dice_4: 0.1287  loss_ce_5: 0.1656  loss_mask_5: 0.214  loss_dice_5: 0.1375  loss_ce_6: 0.1657  loss_mask_6: 0.205  loss_dice_6: 0.1293  loss_ce_7: 0.1658  loss_mask_7: 0.2238  loss_dice_7: 0.1313  loss_ce_8: 0.1659  loss_mask_8: 0.2145  loss_dice_8: 0.1337  time: 0.5806  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:41:01] d2.utils.events INFO:  eta: 1:51:37  iter: 19359  total_loss: 4.389  loss_ce: 0.09616  loss_mask: 0.1796  loss_dice: 0.1396  loss_ce_0: 0.05093  loss_mask_0: 0.1719  loss_dice_0: 0.1415  loss_ce_1: 0.0973  loss_mask_1: 0.1863  loss_dice_1: 0.1497  loss_ce_2: 0.09662  loss_mask_2: 0.1826  loss_dice_2: 0.1481  loss_ce_3: 0.0966  loss_mask_3: 0.1748  loss_dice_3: 0.1363  loss_ce_4: 0.09685  loss_mask_4: 0.179  loss_dice_4: 0.1534  loss_ce_5: 0.09682  loss_mask_5: 0.1782  loss_dice_5: 0.1515  loss_ce_6: 0.09667  loss_mask_6: 0.1806  loss_dice_6: 0.1446  loss_ce_7: 0.09631  loss_mask_7: 0.1771  loss_dice_7: 0.1466  loss_ce_8: 0.09623  loss_mask_8: 0.1764  loss_dice_8: 0.1384  time: 0.5806  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:41:12] d2.utils.events INFO:  eta: 1:51:15  iter: 19379  total_loss: 4.488  loss_ce: 0.09587  loss_mask: 0.1856  loss_dice: 0.1544  loss_ce_0: 0.05093  loss_mask_0: 0.1948  loss_dice_0: 0.154  loss_ce_1: 0.0962  loss_mask_1: 0.1839  loss_dice_1: 0.1591  loss_ce_2: 0.09596  loss_mask_2: 0.1945  loss_dice_2: 0.1628  loss_ce_3: 0.09605  loss_mask_3: 0.1909  loss_dice_3: 0.1586  loss_ce_4: 0.09623  loss_mask_4: 0.1879  loss_dice_4: 0.1592  loss_ce_5: 0.09631  loss_mask_5: 0.1917  loss_dice_5: 0.1543  loss_ce_6: 0.09616  loss_mask_6: 0.1862  loss_dice_6: 0.1495  loss_ce_7: 0.09594  loss_mask_7: 0.1882  loss_dice_7: 0.1567  loss_ce_8: 0.09587  loss_mask_8: 0.1887  loss_dice_8: 0.1554  time: 0.5806  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:41:24] d2.utils.events INFO:  eta: 1:50:51  iter: 19399  total_loss: 4.475  loss_ce: 0.09565  loss_mask: 0.2081  loss_dice: 0.1471  loss_ce_0: 0.05088  loss_mask_0: 0.1965  loss_dice_0: 0.1464  loss_ce_1: 0.09591  loss_mask_1: 0.2006  loss_dice_1: 0.1488  loss_ce_2: 0.09569  loss_mask_2: 0.2035  loss_dice_2: 0.1468  loss_ce_3: 0.09576  loss_mask_3: 0.2  loss_dice_3: 0.1457  loss_ce_4: 0.0959  loss_mask_4: 0.214  loss_dice_4: 0.1443  loss_ce_5: 0.09587  loss_mask_5: 0.209  loss_dice_5: 0.1446  loss_ce_6: 0.09572  loss_mask_6: 0.1959  loss_dice_6: 0.1476  loss_ce_7: 0.09572  loss_mask_7: 0.2023  loss_dice_7: 0.141  loss_ce_8: 0.09565  loss_mask_8: 0.1917  loss_dice_8: 0.14  time: 0.5806  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:41:35] d2.utils.events INFO:  eta: 1:50:25  iter: 19419  total_loss: 4.421  loss_ce: 0.09507  loss_mask: 0.1908  loss_dice: 0.1428  loss_ce_0: 0.05079  loss_mask_0: 0.1888  loss_dice_0: 0.141  loss_ce_1: 0.09492  loss_mask_1: 0.19  loss_dice_1: 0.1402  loss_ce_2: 0.09496  loss_mask_2: 0.2015  loss_dice_2: 0.1443  loss_ce_3: 0.0951  loss_mask_3: 0.1937  loss_dice_3: 0.1463  loss_ce_4: 0.09496  loss_mask_4: 0.1815  loss_dice_4: 0.1459  loss_ce_5: 0.09514  loss_mask_5: 0.1949  loss_dice_5: 0.149  loss_ce_6: 0.09503  loss_mask_6: 0.197  loss_dice_6: 0.1443  loss_ce_7: 0.09507  loss_mask_7: 0.1879  loss_dice_7: 0.1469  loss_ce_8: 0.09492  loss_mask_8: 0.1887  loss_dice_8: 0.1438  time: 0.5806  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:41:47] d2.utils.events INFO:  eta: 1:49:53  iter: 19439  total_loss: 4.522  loss_ce: 0.09345  loss_mask: 0.1829  loss_dice: 0.1516  loss_ce_0: 0.05044  loss_mask_0: 0.1914  loss_dice_0: 0.1516  loss_ce_1: 0.09336  loss_mask_1: 0.1846  loss_dice_1: 0.1497  loss_ce_2: 0.09322  loss_mask_2: 0.1922  loss_dice_2: 0.1541  loss_ce_3: 0.09323  loss_mask_3: 0.1829  loss_dice_3: 0.154  loss_ce_4: 0.09331  loss_mask_4: 0.1829  loss_dice_4: 0.1514  loss_ce_5: 0.0932  loss_mask_5: 0.1813  loss_dice_5: 0.1572  loss_ce_6: 0.09327  loss_mask_6: 0.1868  loss_dice_6: 0.1549  loss_ce_7: 0.09338  loss_mask_7: 0.1876  loss_dice_7: 0.1491  loss_ce_8: 0.09345  loss_mask_8: 0.1887  loss_dice_8: 0.1445  time: 0.5806  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:41:58] d2.utils.events INFO:  eta: 1:49:24  iter: 19459  total_loss: 4.248  loss_ce: 0.1702  loss_mask: 0.1574  loss_dice: 0.1305  loss_ce_0: 0.07834  loss_mask_0: 0.1595  loss_dice_0: 0.1389  loss_ce_1: 0.1704  loss_mask_1: 0.1627  loss_dice_1: 0.1347  loss_ce_2: 0.1706  loss_mask_2: 0.1607  loss_dice_2: 0.1376  loss_ce_3: 0.1707  loss_mask_3: 0.1609  loss_dice_3: 0.1382  loss_ce_4: 0.1705  loss_mask_4: 0.1531  loss_dice_4: 0.1331  loss_ce_5: 0.1708  loss_mask_5: 0.1678  loss_dice_5: 0.1398  loss_ce_6: 0.1704  loss_mask_6: 0.1555  loss_dice_6: 0.1365  loss_ce_7: 0.1705  loss_mask_7: 0.1593  loss_dice_7: 0.1336  loss_ce_8: 0.1702  loss_mask_8: 0.1548  loss_dice_8: 0.1402  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:42:10] d2.utils.events INFO:  eta: 1:49:07  iter: 19479  total_loss: 4.662  loss_ce: 0.09203  loss_mask: 0.1613  loss_dice: 0.1506  loss_ce_0: 0.05012  loss_mask_0: 0.1656  loss_dice_0: 0.1487  loss_ce_1: 0.0915  loss_mask_1: 0.1671  loss_dice_1: 0.1541  loss_ce_2: 0.09164  loss_mask_2: 0.1672  loss_dice_2: 0.1499  loss_ce_3: 0.09167  loss_mask_3: 0.163  loss_dice_3: 0.1491  loss_ce_4: 0.09167  loss_mask_4: 0.1692  loss_dice_4: 0.1461  loss_ce_5: 0.0915  loss_mask_5: 0.1719  loss_dice_5: 0.1567  loss_ce_6: 0.09164  loss_mask_6: 0.1717  loss_dice_6: 0.1547  loss_ce_7: 0.09189  loss_mask_7: 0.1666  loss_dice_7: 0.1531  loss_ce_8: 0.09182  loss_mask_8: 0.1698  loss_dice_8: 0.1496  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:42:21] d2.utils.events INFO:  eta: 1:48:48  iter: 19499  total_loss: 4.688  loss_ce: 0.1721  loss_mask: 0.1575  loss_dice: 0.1454  loss_ce_0: 0.07873  loss_mask_0: 0.171  loss_dice_0: 0.1508  loss_ce_1: 0.1725  loss_mask_1: 0.1709  loss_dice_1: 0.1459  loss_ce_2: 0.1726  loss_mask_2: 0.1649  loss_dice_2: 0.1555  loss_ce_3: 0.1728  loss_mask_3: 0.1643  loss_dice_3: 0.1433  loss_ce_4: 0.1726  loss_mask_4: 0.1678  loss_dice_4: 0.1423  loss_ce_5: 0.1731  loss_mask_5: 0.1683  loss_dice_5: 0.1379  loss_ce_6: 0.1726  loss_mask_6: 0.1758  loss_dice_6: 0.1503  loss_ce_7: 0.1723  loss_mask_7: 0.1644  loss_dice_7: 0.1502  loss_ce_8: 0.1721  loss_mask_8: 0.1631  loss_dice_8: 0.1473  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:42:33] d2.utils.events INFO:  eta: 1:48:24  iter: 19519  total_loss: 4.809  loss_ce: 0.1314  loss_mask: 0.1761  loss_dice: 0.1655  loss_ce_0: 0.06427  loss_mask_0: 0.164  loss_dice_0: 0.1653  loss_ce_1: 0.1311  loss_mask_1: 0.1612  loss_dice_1: 0.1637  loss_ce_2: 0.1313  loss_mask_2: 0.1712  loss_dice_2: 0.1695  loss_ce_3: 0.1314  loss_mask_3: 0.165  loss_dice_3: 0.1618  loss_ce_4: 0.1314  loss_mask_4: 0.1676  loss_dice_4: 0.1659  loss_ce_5: 0.1312  loss_mask_5: 0.1625  loss_dice_5: 0.1685  loss_ce_6: 0.1313  loss_mask_6: 0.1763  loss_dice_6: 0.1711  loss_ce_7: 0.1313  loss_mask_7: 0.1635  loss_dice_7: 0.1683  loss_ce_8: 0.1305  loss_mask_8: 0.1647  loss_dice_8: 0.1689  time: 0.5805  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:42:45] d2.utils.events INFO:  eta: 1:48:06  iter: 19539  total_loss: 4.593  loss_ce: 0.1697  loss_mask: 0.2041  loss_dice: 0.1535  loss_ce_0: 0.07823  loss_mask_0: 0.1903  loss_dice_0: 0.1523  loss_ce_1: 0.1697  loss_mask_1: 0.1944  loss_dice_1: 0.1516  loss_ce_2: 0.1695  loss_mask_2: 0.1972  loss_dice_2: 0.1544  loss_ce_3: 0.1696  loss_mask_3: 0.192  loss_dice_3: 0.1535  loss_ce_4: 0.1697  loss_mask_4: 0.1834  loss_dice_4: 0.1504  loss_ce_5: 0.1695  loss_mask_5: 0.2099  loss_dice_5: 0.1546  loss_ce_6: 0.1698  loss_mask_6: 0.1872  loss_dice_6: 0.1584  loss_ce_7: 0.1697  loss_mask_7: 0.1978  loss_dice_7: 0.1515  loss_ce_8: 0.1697  loss_mask_8: 0.1908  loss_dice_8: 0.1549  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:42:56] d2.utils.events INFO:  eta: 1:47:41  iter: 19559  total_loss: 4.744  loss_ce: 0.09478  loss_mask: 0.1986  loss_dice: 0.1537  loss_ce_0: 0.05071  loss_mask_0: 0.1896  loss_dice_0: 0.162  loss_ce_1: 0.09563  loss_mask_1: 0.2023  loss_dice_1: 0.1609  loss_ce_2: 0.0955  loss_mask_2: 0.2002  loss_dice_2: 0.1586  loss_ce_3: 0.09547  loss_mask_3: 0.2012  loss_dice_3: 0.156  loss_ce_4: 0.09532  loss_mask_4: 0.1959  loss_dice_4: 0.1591  loss_ce_5: 0.0955  loss_mask_5: 0.1904  loss_dice_5: 0.1599  loss_ce_6: 0.09514  loss_mask_6: 0.1998  loss_dice_6: 0.1532  loss_ce_7: 0.09514  loss_mask_7: 0.2016  loss_dice_7: 0.1608  loss_ce_8: 0.095  loss_mask_8: 0.2006  loss_dice_8: 0.1629  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:43:08] d2.utils.events INFO:  eta: 1:47:21  iter: 19579  total_loss: 4.779  loss_ce: 0.1659  loss_mask: 0.2167  loss_dice: 0.1262  loss_ce_0: 0.07749  loss_mask_0: 0.2029  loss_dice_0: 0.1249  loss_ce_1: 0.1638  loss_mask_1: 0.2026  loss_dice_1: 0.1258  loss_ce_2: 0.1648  loss_mask_2: 0.2145  loss_dice_2: 0.1258  loss_ce_3: 0.165  loss_mask_3: 0.1927  loss_dice_3: 0.1256  loss_ce_4: 0.1647  loss_mask_4: 0.2108  loss_dice_4: 0.1245  loss_ce_5: 0.1649  loss_mask_5: 0.2083  loss_dice_5: 0.1249  loss_ce_6: 0.1651  loss_mask_6: 0.208  loss_dice_6: 0.1268  loss_ce_7: 0.1655  loss_mask_7: 0.2045  loss_dice_7: 0.1244  loss_ce_8: 0.1657  loss_mask_8: 0.214  loss_dice_8: 0.1269  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:43:19] d2.utils.events INFO:  eta: 1:46:55  iter: 19599  total_loss: 4.583  loss_ce: 0.1304  loss_mask: 0.1676  loss_dice: 0.1558  loss_ce_0: 0.06405  loss_mask_0: 0.1588  loss_dice_0: 0.1478  loss_ce_1: 0.13  loss_mask_1: 0.1625  loss_dice_1: 0.1479  loss_ce_2: 0.1303  loss_mask_2: 0.1697  loss_dice_2: 0.1554  loss_ce_3: 0.1303  loss_mask_3: 0.1585  loss_dice_3: 0.1526  loss_ce_4: 0.1303  loss_mask_4: 0.1603  loss_dice_4: 0.1521  loss_ce_5: 0.1302  loss_mask_5: 0.1709  loss_dice_5: 0.1556  loss_ce_6: 0.1304  loss_mask_6: 0.1633  loss_dice_6: 0.1608  loss_ce_7: 0.1303  loss_mask_7: 0.1694  loss_dice_7: 0.1552  loss_ce_8: 0.1304  loss_mask_8: 0.1666  loss_dice_8: 0.149  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:43:31] d2.utils.events INFO:  eta: 1:46:28  iter: 19619  total_loss: 4.546  loss_ce: 0.0966  loss_mask: 0.1974  loss_dice: 0.1416  loss_ce_0: 0.05112  loss_mask_0: 0.1945  loss_dice_0: 0.1485  loss_ce_1: 0.09723  loss_mask_1: 0.2007  loss_dice_1: 0.1464  loss_ce_2: 0.09715  loss_mask_2: 0.1907  loss_dice_2: 0.1475  loss_ce_3: 0.09733  loss_mask_3: 0.2039  loss_dice_3: 0.1445  loss_ce_4: 0.09733  loss_mask_4: 0.1995  loss_dice_4: 0.1464  loss_ce_5: 0.09748  loss_mask_5: 0.202  loss_dice_5: 0.1396  loss_ce_6: 0.09722  loss_mask_6: 0.2076  loss_dice_6: 0.1462  loss_ce_7: 0.09693  loss_mask_7: 0.1861  loss_dice_7: 0.1487  loss_ce_8: 0.09667  loss_mask_8: 0.1982  loss_dice_8: 0.1424  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:43:43] d2.utils.events INFO:  eta: 1:45:59  iter: 19639  total_loss: 4.698  loss_ce: 0.1647  loss_mask: 0.1857  loss_dice: 0.1407  loss_ce_0: 0.07713  loss_mask_0: 0.1923  loss_dice_0: 0.1345  loss_ce_1: 0.1639  loss_mask_1: 0.195  loss_dice_1: 0.1373  loss_ce_2: 0.1642  loss_mask_2: 0.1869  loss_dice_2: 0.1301  loss_ce_3: 0.1641  loss_mask_3: 0.191  loss_dice_3: 0.1431  loss_ce_4: 0.1642  loss_mask_4: 0.1867  loss_dice_4: 0.1277  loss_ce_5: 0.1639  loss_mask_5: 0.1773  loss_dice_5: 0.1366  loss_ce_6: 0.1642  loss_mask_6: 0.1863  loss_dice_6: 0.1334  loss_ce_7: 0.1646  loss_mask_7: 0.1828  loss_dice_7: 0.1407  loss_ce_8: 0.1647  loss_mask_8: 0.1793  loss_dice_8: 0.1365  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:43:54] d2.utils.events INFO:  eta: 1:45:32  iter: 19659  total_loss: 5.011  loss_ce: 0.1608  loss_mask: 0.1962  loss_dice: 0.142  loss_ce_0: 0.07627  loss_mask_0: 0.1863  loss_dice_0: 0.1404  loss_ce_1: 0.1592  loss_mask_1: 0.1865  loss_dice_1: 0.1412  loss_ce_2: 0.1596  loss_mask_2: 0.1885  loss_dice_2: 0.1368  loss_ce_3: 0.1595  loss_mask_3: 0.1913  loss_dice_3: 0.1403  loss_ce_4: 0.1595  loss_mask_4: 0.1977  loss_dice_4: 0.141  loss_ce_5: 0.159  loss_mask_5: 0.1921  loss_dice_5: 0.1448  loss_ce_6: 0.1597  loss_mask_6: 0.1925  loss_dice_6: 0.1398  loss_ce_7: 0.1602  loss_mask_7: 0.1937  loss_dice_7: 0.1471  loss_ce_8: 0.1606  loss_mask_8: 0.1837  loss_dice_8: 0.1412  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:44:06] d2.utils.events INFO:  eta: 1:45:02  iter: 19679  total_loss: 4.869  loss_ce: 0.1577  loss_mask: 0.1996  loss_dice: 0.1562  loss_ce_0: 0.07556  loss_mask_0: 0.1931  loss_dice_0: 0.1593  loss_ce_1: 0.1555  loss_mask_1: 0.1949  loss_dice_1: 0.1521  loss_ce_2: 0.1561  loss_mask_2: 0.1894  loss_dice_2: 0.1544  loss_ce_3: 0.1558  loss_mask_3: 0.1913  loss_dice_3: 0.1572  loss_ce_4: 0.156  loss_mask_4: 0.1854  loss_dice_4: 0.1531  loss_ce_5: 0.1556  loss_mask_5: 0.1847  loss_dice_5: 0.159  loss_ce_6: 0.1563  loss_mask_6: 0.1907  loss_dice_6: 0.1534  loss_ce_7: 0.1569  loss_mask_7: 0.194  loss_dice_7: 0.1582  loss_ce_8: 0.1576  loss_mask_8: 0.197  loss_dice_8: 0.1628  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:44:17] d2.utils.events INFO:  eta: 1:44:39  iter: 19699  total_loss: 4.673  loss_ce: 0.1025  loss_mask: 0.2037  loss_dice: 0.1532  loss_ce_0: 0.05243  loss_mask_0: 0.2015  loss_dice_0: 0.1537  loss_ce_1: 0.1042  loss_mask_1: 0.1948  loss_dice_1: 0.1469  loss_ce_2: 0.1036  loss_mask_2: 0.2019  loss_dice_2: 0.1482  loss_ce_3: 0.1038  loss_mask_3: 0.1984  loss_dice_3: 0.1515  loss_ce_4: 0.1036  loss_mask_4: 0.1959  loss_dice_4: 0.151  loss_ce_5: 0.1041  loss_mask_5: 0.2075  loss_dice_5: 0.1499  loss_ce_6: 0.1035  loss_mask_6: 0.1977  loss_dice_6: 0.1492  loss_ce_7: 0.1029  loss_mask_7: 0.199  loss_dice_7: 0.1531  loss_ce_8: 0.1026  loss_mask_8: 0.2037  loss_dice_8: 0.147  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:44:29] d2.utils.events INFO:  eta: 1:44:05  iter: 19719  total_loss: 4.426  loss_ce: 0.1023  loss_mask: 0.1747  loss_dice: 0.1572  loss_ce_0: 0.05236  loss_mask_0: 0.1615  loss_dice_0: 0.1565  loss_ce_1: 0.1036  loss_mask_1: 0.1728  loss_dice_1: 0.1503  loss_ce_2: 0.1032  loss_mask_2: 0.1745  loss_dice_2: 0.1532  loss_ce_3: 0.1033  loss_mask_3: 0.1693  loss_dice_3: 0.1539  loss_ce_4: 0.1033  loss_mask_4: 0.1667  loss_dice_4: 0.1505  loss_ce_5: 0.1038  loss_mask_5: 0.1752  loss_dice_5: 0.1505  loss_ce_6: 0.1032  loss_mask_6: 0.1661  loss_dice_6: 0.1525  loss_ce_7: 0.1028  loss_mask_7: 0.1687  loss_dice_7: 0.1539  loss_ce_8: 0.1024  loss_mask_8: 0.1707  loss_dice_8: 0.1534  time: 0.5805  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:44:41] d2.utils.events INFO:  eta: 1:43:35  iter: 19739  total_loss: 4.803  loss_ce: 0.1016  loss_mask: 0.1991  loss_dice: 0.1657  loss_ce_0: 0.05223  loss_mask_0: 0.1832  loss_dice_0: 0.1632  loss_ce_1: 0.1027  loss_mask_1: 0.201  loss_dice_1: 0.1621  loss_ce_2: 0.1021  loss_mask_2: 0.2014  loss_dice_2: 0.1577  loss_ce_3: 0.1022  loss_mask_3: 0.1944  loss_dice_3: 0.1623  loss_ce_4: 0.1023  loss_mask_4: 0.2044  loss_dice_4: 0.1594  loss_ce_5: 0.1025  loss_mask_5: 0.1982  loss_dice_5: 0.1579  loss_ce_6: 0.1022  loss_mask_6: 0.1983  loss_dice_6: 0.1653  loss_ce_7: 0.1018  loss_mask_7: 0.1957  loss_dice_7: 0.1589  loss_ce_8: 0.1016  loss_mask_8: 0.1897  loss_dice_8: 0.1559  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:44:52] d2.utils.events INFO:  eta: 1:43:14  iter: 19759  total_loss: 4.894  loss_ce: 0.101  loss_mask: 0.2173  loss_dice: 0.1504  loss_ce_0: 0.0521  loss_mask_0: 0.2119  loss_dice_0: 0.1481  loss_ce_1: 0.1017  loss_mask_1: 0.2134  loss_dice_1: 0.1439  loss_ce_2: 0.1013  loss_mask_2: 0.2049  loss_dice_2: 0.1425  loss_ce_3: 0.1013  loss_mask_3: 0.2084  loss_dice_3: 0.1482  loss_ce_4: 0.1013  loss_mask_4: 0.2101  loss_dice_4: 0.1506  loss_ce_5: 0.1017  loss_mask_5: 0.2101  loss_dice_5: 0.1449  loss_ce_6: 0.1014  loss_mask_6: 0.2068  loss_dice_6: 0.1484  loss_ce_7: 0.101  loss_mask_7: 0.2101  loss_dice_7: 0.148  loss_ce_8: 0.101  loss_mask_8: 0.2126  loss_dice_8: 0.1492  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:45:04] d2.utils.events INFO:  eta: 1:42:49  iter: 19779  total_loss: 4.802  loss_ce: 0.1294  loss_mask: 0.202  loss_dice: 0.1387  loss_ce_0: 0.0638  loss_mask_0: 0.198  loss_dice_0: 0.1334  loss_ce_1: 0.1294  loss_mask_1: 0.1965  loss_dice_1: 0.1363  loss_ce_2: 0.1295  loss_mask_2: 0.2024  loss_dice_2: 0.1389  loss_ce_3: 0.1295  loss_mask_3: 0.2021  loss_dice_3: 0.1332  loss_ce_4: 0.1295  loss_mask_4: 0.2065  loss_dice_4: 0.1388  loss_ce_5: 0.1294  loss_mask_5: 0.2011  loss_dice_5: 0.1391  loss_ce_6: 0.1296  loss_mask_6: 0.1984  loss_dice_6: 0.1364  loss_ce_7: 0.1295  loss_mask_7: 0.203  loss_dice_7: 0.141  loss_ce_8: 0.1298  loss_mask_8: 0.2  loss_dice_8: 0.1357  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:45:15] d2.utils.events INFO:  eta: 1:42:29  iter: 19799  total_loss: 4.701  loss_ce: 0.1554  loss_mask: 0.1626  loss_dice: 0.1487  loss_ce_0: 0.07486  loss_mask_0: 0.1641  loss_dice_0: 0.1499  loss_ce_1: 0.154  loss_mask_1: 0.1664  loss_dice_1: 0.1497  loss_ce_2: 0.1546  loss_mask_2: 0.16  loss_dice_2: 0.1559  loss_ce_3: 0.1546  loss_mask_3: 0.1716  loss_dice_3: 0.1559  loss_ce_4: 0.1545  loss_mask_4: 0.1672  loss_dice_4: 0.149  loss_ce_5: 0.1542  loss_mask_5: 0.161  loss_dice_5: 0.1452  loss_ce_6: 0.1546  loss_mask_6: 0.1625  loss_dice_6: 0.1481  loss_ce_7: 0.1553  loss_mask_7: 0.165  loss_dice_7: 0.1501  loss_ce_8: 0.1553  loss_mask_8: 0.1626  loss_dice_8: 0.14  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:45:27] d2.utils.events INFO:  eta: 1:41:58  iter: 19819  total_loss: 5.308  loss_ce: 0.1531  loss_mask: 0.2037  loss_dice: 0.1527  loss_ce_0: 0.07429  loss_mask_0: 0.2136  loss_dice_0: 0.1529  loss_ce_1: 0.1515  loss_mask_1: 0.2113  loss_dice_1: 0.1517  loss_ce_2: 0.1521  loss_mask_2: 0.2101  loss_dice_2: 0.1606  loss_ce_3: 0.1521  loss_mask_3: 0.2162  loss_dice_3: 0.1518  loss_ce_4: 0.1521  loss_mask_4: 0.2164  loss_dice_4: 0.1564  loss_ce_5: 0.1517  loss_mask_5: 0.2076  loss_dice_5: 0.1573  loss_ce_6: 0.152  loss_mask_6: 0.1999  loss_dice_6: 0.1523  loss_ce_7: 0.1528  loss_mask_7: 0.2202  loss_dice_7: 0.1535  loss_ce_8: 0.1527  loss_mask_8: 0.2151  loss_dice_8: 0.1505  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:45:39] d2.utils.events INFO:  eta: 1:41:34  iter: 19839  total_loss: 4.699  loss_ce: 0.1065  loss_mask: 0.1947  loss_dice: 0.1525  loss_ce_0: 0.05335  loss_mask_0: 0.1868  loss_dice_0: 0.1463  loss_ce_1: 0.1083  loss_mask_1: 0.1816  loss_dice_1: 0.1507  loss_ce_2: 0.1074  loss_mask_2: 0.1844  loss_dice_2: 0.1475  loss_ce_3: 0.1077  loss_mask_3: 0.1925  loss_dice_3: 0.1501  loss_ce_4: 0.1075  loss_mask_4: 0.1826  loss_dice_4: 0.1481  loss_ce_5: 0.1078  loss_mask_5: 0.1935  loss_dice_5: 0.1503  loss_ce_6: 0.1076  loss_mask_6: 0.1884  loss_dice_6: 0.1442  loss_ce_7: 0.1068  loss_mask_7: 0.1889  loss_dice_7: 0.1526  loss_ce_8: 0.107  loss_mask_8: 0.1957  loss_dice_8: 0.1519  time: 0.5805  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:45:50] d2.utils.events INFO:  eta: 1:40:57  iter: 19859  total_loss: 4.596  loss_ce: 0.1501  loss_mask: 0.1849  loss_dice: 0.1333  loss_ce_0: 0.07361  loss_mask_0: 0.1801  loss_dice_0: 0.1322  loss_ce_1: 0.1484  loss_mask_1: 0.1858  loss_dice_1: 0.1311  loss_ce_2: 0.149  loss_mask_2: 0.1802  loss_dice_2: 0.1272  loss_ce_3: 0.1489  loss_mask_3: 0.1944  loss_dice_3: 0.1359  loss_ce_4: 0.1491  loss_mask_4: 0.186  loss_dice_4: 0.1343  loss_ce_5: 0.1486  loss_mask_5: 0.1771  loss_dice_5: 0.1266  loss_ce_6: 0.1489  loss_mask_6: 0.1823  loss_dice_6: 0.1305  loss_ce_7: 0.1501  loss_mask_7: 0.1904  loss_dice_7: 0.1335  loss_ce_8: 0.1501  loss_mask_8: 0.1947  loss_dice_8: 0.1336  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:46:02] d2.utils.events INFO:  eta: 1:40:27  iter: 19879  total_loss: 4.6  loss_ce: 0.107  loss_mask: 0.1914  loss_dice: 0.1468  loss_ce_0: 0.05353  loss_mask_0: 0.1843  loss_dice_0: 0.1438  loss_ce_1: 0.1082  loss_mask_1: 0.1903  loss_dice_1: 0.1424  loss_ce_2: 0.1077  loss_mask_2: 0.1903  loss_dice_2: 0.1455  loss_ce_3: 0.1078  loss_mask_3: 0.185  loss_dice_3: 0.1404  loss_ce_4: 0.1078  loss_mask_4: 0.1928  loss_dice_4: 0.1466  loss_ce_5: 0.1082  loss_mask_5: 0.1933  loss_dice_5: 0.1445  loss_ce_6: 0.1078  loss_mask_6: 0.1962  loss_dice_6: 0.1438  loss_ce_7: 0.1072  loss_mask_7: 0.187  loss_dice_7: 0.1447  loss_ce_8: 0.107  loss_mask_8: 0.1957  loss_dice_8: 0.1423  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:46:13] d2.utils.events INFO:  eta: 1:40:03  iter: 19899  total_loss: 4.467  loss_ce: 0.1498  loss_mask: 0.1666  loss_dice: 0.1411  loss_ce_0: 0.07341  loss_mask_0: 0.1735  loss_dice_0: 0.142  loss_ce_1: 0.1485  loss_mask_1: 0.177  loss_dice_1: 0.1425  loss_ce_2: 0.1492  loss_mask_2: 0.17  loss_dice_2: 0.1419  loss_ce_3: 0.1489  loss_mask_3: 0.1737  loss_dice_3: 0.1382  loss_ce_4: 0.1489  loss_mask_4: 0.1792  loss_dice_4: 0.1402  loss_ce_5: 0.1487  loss_mask_5: 0.1797  loss_dice_5: 0.1448  loss_ce_6: 0.1489  loss_mask_6: 0.1712  loss_dice_6: 0.1386  loss_ce_7: 0.1498  loss_mask_7: 0.168  loss_dice_7: 0.1458  loss_ce_8: 0.1498  loss_mask_8: 0.1785  loss_dice_8: 0.1423  time: 0.5805  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:46:25] d2.utils.events INFO:  eta: 1:39:40  iter: 19919  total_loss: 4.619  loss_ce: 0.1464  loss_mask: 0.192  loss_dice: 0.1316  loss_ce_0: 0.07265  loss_mask_0: 0.1878  loss_dice_0: 0.1308  loss_ce_1: 0.1449  loss_mask_1: 0.191  loss_dice_1: 0.1364  loss_ce_2: 0.1455  loss_mask_2: 0.1823  loss_dice_2: 0.1284  loss_ce_3: 0.1453  loss_mask_3: 0.1876  loss_dice_3: 0.1318  loss_ce_4: 0.1452  loss_mask_4: 0.1943  loss_dice_4: 0.1318  loss_ce_5: 0.1448  loss_mask_5: 0.1836  loss_dice_5: 0.1328  loss_ce_6: 0.1452  loss_mask_6: 0.1856  loss_dice_6: 0.131  loss_ce_7: 0.1461  loss_mask_7: 0.1791  loss_dice_7: 0.1312  loss_ce_8: 0.1463  loss_mask_8: 0.1866  loss_dice_8: 0.1306  time: 0.5805  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:46:37] d2.utils.events INFO:  eta: 1:39:16  iter: 19939  total_loss: 4.857  loss_ce: 0.11  loss_mask: 0.1855  loss_dice: 0.1512  loss_ce_0: 0.05421  loss_mask_0: 0.1775  loss_dice_0: 0.1417  loss_ce_1: 0.1111  loss_mask_1: 0.1777  loss_dice_1: 0.141  loss_ce_2: 0.1106  loss_mask_2: 0.1843  loss_dice_2: 0.1432  loss_ce_3: 0.1108  loss_mask_3: 0.1877  loss_dice_3: 0.145  loss_ce_4: 0.1109  loss_mask_4: 0.1865  loss_dice_4: 0.1407  loss_ce_5: 0.111  loss_mask_5: 0.1757  loss_dice_5: 0.1435  loss_ce_6: 0.1109  loss_mask_6: 0.1879  loss_dice_6: 0.1435  loss_ce_7: 0.1101  loss_mask_7: 0.1848  loss_dice_7: 0.1449  loss_ce_8: 0.11  loss_mask_8: 0.1763  loss_dice_8: 0.1418  time: 0.5805  data_time: 0.0016  lr: 1e-05  max_mem: 2811M
[07/11 14:46:48] d2.utils.events INFO:  eta: 1:38:52  iter: 19959  total_loss: 4.426  loss_ce: 0.1073  loss_mask: 0.1611  loss_dice: 0.1488  loss_ce_0: 0.05376  loss_mask_0: 0.1526  loss_dice_0: 0.1528  loss_ce_1: 0.1076  loss_mask_1: 0.1594  loss_dice_1: 0.1546  loss_ce_2: 0.1074  loss_mask_2: 0.1589  loss_dice_2: 0.1579  loss_ce_3: 0.1076  loss_mask_3: 0.1533  loss_dice_3: 0.1503  loss_ce_4: 0.1076  loss_mask_4: 0.1488  loss_dice_4: 0.1605  loss_ce_5: 0.1076  loss_mask_5: 0.154  loss_dice_5: 0.1528  loss_ce_6: 0.1076  loss_mask_6: 0.1563  loss_dice_6: 0.1518  loss_ce_7: 0.1072  loss_mask_7: 0.1506  loss_dice_7: 0.154  loss_ce_8: 0.1072  loss_mask_8: 0.1508  loss_dice_8: 0.1484  time: 0.5805  data_time: 0.0014  lr: 1e-05  max_mem: 2811M
[07/11 14:47:00] d2.utils.events INFO:  eta: 1:38:28  iter: 19979  total_loss: 4.382  loss_ce: 0.1049  loss_mask: 0.1781  loss_dice: 0.1508  loss_ce_0: 0.05341  loss_mask_0: 0.185  loss_dice_0: 0.1495  loss_ce_1: 0.1045  loss_mask_1: 0.1765  loss_dice_1: 0.1497  loss_ce_2: 0.1044  loss_mask_2: 0.1789  loss_dice_2: 0.1484  loss_ce_3: 0.1046  loss_mask_3: 0.1674  loss_dice_3: 0.1409  loss_ce_4: 0.1047  loss_mask_4: 0.1664  loss_dice_4: 0.1451  loss_ce_5: 0.1046  loss_mask_5: 0.1805  loss_dice_5: 0.1468  loss_ce_6: 0.1048  loss_mask_6: 0.1757  loss_dice_6: 0.1468  loss_ce_7: 0.1046  loss_mask_7: 0.1757  loss_dice_7: 0.1496  loss_ce_8: 0.1047  loss_mask_8: 0.178  loss_dice_8: 0.1475  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:47:11] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256to256_instance/model_0019999.pth
[07/11 14:47:12] d2.data.datasets.coco INFO: Loaded 754 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_val2017.json
[07/11 14:47:12] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/11 14:47:12] d2.data.common INFO: Serializing 754 elements to byte tensors and concatenating them all ...
[07/11 14:47:12] d2.data.common INFO: Serialized dataset takes 2.49 MiB
[07/11 14:47:12] d2.evaluation.evaluator INFO: Start inference on 754 batches
[07/11 14:47:16] d2.evaluation.evaluator INFO: Inference done 11/754. Dataloading: 0.0004 s/iter. Inference: 0.1402 s/iter. Eval: 0.2798 s/iter. Total: 0.4205 s/iter. ETA=0:05:12
[07/11 14:47:22] d2.evaluation.evaluator INFO: Inference done 23/754. Dataloading: 0.0007 s/iter. Inference: 0.1414 s/iter. Eval: 0.2776 s/iter. Total: 0.4197 s/iter. ETA=0:05:06
[07/11 14:47:27] d2.evaluation.evaluator INFO: Inference done 35/754. Dataloading: 0.0007 s/iter. Inference: 0.1425 s/iter. Eval: 0.2761 s/iter. Total: 0.4194 s/iter. ETA=0:05:01
[07/11 14:47:32] d2.evaluation.evaluator INFO: Inference done 48/754. Dataloading: 0.0007 s/iter. Inference: 0.1425 s/iter. Eval: 0.2746 s/iter. Total: 0.4179 s/iter. ETA=0:04:55
[07/11 14:47:37] d2.evaluation.evaluator INFO: Inference done 60/754. Dataloading: 0.0008 s/iter. Inference: 0.1432 s/iter. Eval: 0.2747 s/iter. Total: 0.4188 s/iter. ETA=0:04:50
[07/11 14:47:42] d2.evaluation.evaluator INFO: Inference done 72/754. Dataloading: 0.0008 s/iter. Inference: 0.1417 s/iter. Eval: 0.2766 s/iter. Total: 0.4192 s/iter. ETA=0:04:45
[07/11 14:47:47] d2.evaluation.evaluator INFO: Inference done 85/754. Dataloading: 0.0008 s/iter. Inference: 0.1415 s/iter. Eval: 0.2771 s/iter. Total: 0.4194 s/iter. ETA=0:04:40
[07/11 14:47:53] d2.evaluation.evaluator INFO: Inference done 98/754. Dataloading: 0.0008 s/iter. Inference: 0.1413 s/iter. Eval: 0.2763 s/iter. Total: 0.4184 s/iter. ETA=0:04:34
[07/11 14:47:58] d2.evaluation.evaluator INFO: Inference done 110/754. Dataloading: 0.0008 s/iter. Inference: 0.1422 s/iter. Eval: 0.2772 s/iter. Total: 0.4202 s/iter. ETA=0:04:30
[07/11 14:48:03] d2.evaluation.evaluator INFO: Inference done 123/754. Dataloading: 0.0008 s/iter. Inference: 0.1425 s/iter. Eval: 0.2759 s/iter. Total: 0.4193 s/iter. ETA=0:04:24
[07/11 14:48:09] d2.evaluation.evaluator INFO: Inference done 135/754. Dataloading: 0.0008 s/iter. Inference: 0.1429 s/iter. Eval: 0.2766 s/iter. Total: 0.4204 s/iter. ETA=0:04:20
[07/11 14:48:14] d2.evaluation.evaluator INFO: Inference done 148/754. Dataloading: 0.0007 s/iter. Inference: 0.1428 s/iter. Eval: 0.2758 s/iter. Total: 0.4194 s/iter. ETA=0:04:14
[07/11 14:48:19] d2.evaluation.evaluator INFO: Inference done 161/754. Dataloading: 0.0007 s/iter. Inference: 0.1424 s/iter. Eval: 0.2750 s/iter. Total: 0.4182 s/iter. ETA=0:04:07
[07/11 14:48:24] d2.evaluation.evaluator INFO: Inference done 174/754. Dataloading: 0.0007 s/iter. Inference: 0.1421 s/iter. Eval: 0.2743 s/iter. Total: 0.4173 s/iter. ETA=0:04:02
[07/11 14:48:30] d2.evaluation.evaluator INFO: Inference done 187/754. Dataloading: 0.0007 s/iter. Inference: 0.1415 s/iter. Eval: 0.2745 s/iter. Total: 0.4168 s/iter. ETA=0:03:56
[07/11 14:48:35] d2.evaluation.evaluator INFO: Inference done 200/754. Dataloading: 0.0007 s/iter. Inference: 0.1414 s/iter. Eval: 0.2741 s/iter. Total: 0.4163 s/iter. ETA=0:03:50
[07/11 14:48:40] d2.evaluation.evaluator INFO: Inference done 212/754. Dataloading: 0.0007 s/iter. Inference: 0.1418 s/iter. Eval: 0.2738 s/iter. Total: 0.4164 s/iter. ETA=0:03:45
[07/11 14:48:45] d2.evaluation.evaluator INFO: Inference done 225/754. Dataloading: 0.0007 s/iter. Inference: 0.1415 s/iter. Eval: 0.2736 s/iter. Total: 0.4159 s/iter. ETA=0:03:39
[07/11 14:48:51] d2.evaluation.evaluator INFO: Inference done 237/754. Dataloading: 0.0007 s/iter. Inference: 0.1419 s/iter. Eval: 0.2735 s/iter. Total: 0.4162 s/iter. ETA=0:03:35
[07/11 14:48:56] d2.evaluation.evaluator INFO: Inference done 250/754. Dataloading: 0.0007 s/iter. Inference: 0.1417 s/iter. Eval: 0.2732 s/iter. Total: 0.4157 s/iter. ETA=0:03:29
[07/11 14:49:01] d2.evaluation.evaluator INFO: Inference done 263/754. Dataloading: 0.0007 s/iter. Inference: 0.1414 s/iter. Eval: 0.2732 s/iter. Total: 0.4154 s/iter. ETA=0:03:23
[07/11 14:49:06] d2.evaluation.evaluator INFO: Inference done 275/754. Dataloading: 0.0007 s/iter. Inference: 0.1417 s/iter. Eval: 0.2734 s/iter. Total: 0.4159 s/iter. ETA=0:03:19
[07/11 14:49:12] d2.evaluation.evaluator INFO: Inference done 288/754. Dataloading: 0.0007 s/iter. Inference: 0.1416 s/iter. Eval: 0.2733 s/iter. Total: 0.4158 s/iter. ETA=0:03:13
[07/11 14:49:17] d2.evaluation.evaluator INFO: Inference done 301/754. Dataloading: 0.0007 s/iter. Inference: 0.1416 s/iter. Eval: 0.2731 s/iter. Total: 0.4155 s/iter. ETA=0:03:08
[07/11 14:49:22] d2.evaluation.evaluator INFO: Inference done 313/754. Dataloading: 0.0007 s/iter. Inference: 0.1415 s/iter. Eval: 0.2734 s/iter. Total: 0.4158 s/iter. ETA=0:03:03
[07/11 14:49:27] d2.evaluation.evaluator INFO: Inference done 326/754. Dataloading: 0.0007 s/iter. Inference: 0.1413 s/iter. Eval: 0.2734 s/iter. Total: 0.4155 s/iter. ETA=0:02:57
[07/11 14:49:33] d2.evaluation.evaluator INFO: Inference done 339/754. Dataloading: 0.0007 s/iter. Inference: 0.1411 s/iter. Eval: 0.2735 s/iter. Total: 0.4154 s/iter. ETA=0:02:52
[07/11 14:49:38] d2.evaluation.evaluator INFO: Inference done 352/754. Dataloading: 0.0007 s/iter. Inference: 0.1410 s/iter. Eval: 0.2733 s/iter. Total: 0.4152 s/iter. ETA=0:02:46
[07/11 14:49:43] d2.evaluation.evaluator INFO: Inference done 364/754. Dataloading: 0.0007 s/iter. Inference: 0.1405 s/iter. Eval: 0.2740 s/iter. Total: 0.4154 s/iter. ETA=0:02:41
[07/11 14:49:48] d2.evaluation.evaluator INFO: Inference done 376/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2751 s/iter. Total: 0.4159 s/iter. ETA=0:02:37
[07/11 14:49:53] d2.evaluation.evaluator INFO: Inference done 388/754. Dataloading: 0.0007 s/iter. Inference: 0.1396 s/iter. Eval: 0.2760 s/iter. Total: 0.4164 s/iter. ETA=0:02:32
[07/11 14:49:59] d2.evaluation.evaluator INFO: Inference done 400/754. Dataloading: 0.0007 s/iter. Inference: 0.1394 s/iter. Eval: 0.2769 s/iter. Total: 0.4172 s/iter. ETA=0:02:27
[07/11 14:50:04] d2.evaluation.evaluator INFO: Inference done 412/754. Dataloading: 0.0007 s/iter. Inference: 0.1393 s/iter. Eval: 0.2773 s/iter. Total: 0.4173 s/iter. ETA=0:02:22
[07/11 14:50:09] d2.evaluation.evaluator INFO: Inference done 424/754. Dataloading: 0.0007 s/iter. Inference: 0.1388 s/iter. Eval: 0.2781 s/iter. Total: 0.4177 s/iter. ETA=0:02:17
[07/11 14:50:14] d2.evaluation.evaluator INFO: Inference done 436/754. Dataloading: 0.0007 s/iter. Inference: 0.1388 s/iter. Eval: 0.2782 s/iter. Total: 0.4179 s/iter. ETA=0:02:12
[07/11 14:50:19] d2.evaluation.evaluator INFO: Inference done 448/754. Dataloading: 0.0007 s/iter. Inference: 0.1389 s/iter. Eval: 0.2789 s/iter. Total: 0.4186 s/iter. ETA=0:02:08
[07/11 14:50:25] d2.evaluation.evaluator INFO: Inference done 460/754. Dataloading: 0.0007 s/iter. Inference: 0.1386 s/iter. Eval: 0.2798 s/iter. Total: 0.4192 s/iter. ETA=0:02:03
[07/11 14:50:30] d2.evaluation.evaluator INFO: Inference done 472/754. Dataloading: 0.0007 s/iter. Inference: 0.1387 s/iter. Eval: 0.2801 s/iter. Total: 0.4196 s/iter. ETA=0:01:58
[07/11 14:50:35] d2.evaluation.evaluator INFO: Inference done 484/754. Dataloading: 0.0007 s/iter. Inference: 0.1389 s/iter. Eval: 0.2805 s/iter. Total: 0.4202 s/iter. ETA=0:01:53
[07/11 14:50:40] d2.evaluation.evaluator INFO: Inference done 496/754. Dataloading: 0.0007 s/iter. Inference: 0.1385 s/iter. Eval: 0.2813 s/iter. Total: 0.4205 s/iter. ETA=0:01:48
[07/11 14:50:45] d2.evaluation.evaluator INFO: Inference done 508/754. Dataloading: 0.0008 s/iter. Inference: 0.1383 s/iter. Eval: 0.2814 s/iter. Total: 0.4205 s/iter. ETA=0:01:43
[07/11 14:50:51] d2.evaluation.evaluator INFO: Inference done 520/754. Dataloading: 0.0008 s/iter. Inference: 0.1384 s/iter. Eval: 0.2819 s/iter. Total: 0.4211 s/iter. ETA=0:01:38
[07/11 14:50:56] d2.evaluation.evaluator INFO: Inference done 532/754. Dataloading: 0.0008 s/iter. Inference: 0.1381 s/iter. Eval: 0.2825 s/iter. Total: 0.4214 s/iter. ETA=0:01:33
[07/11 14:51:01] d2.evaluation.evaluator INFO: Inference done 544/754. Dataloading: 0.0008 s/iter. Inference: 0.1380 s/iter. Eval: 0.2826 s/iter. Total: 0.4215 s/iter. ETA=0:01:28
[07/11 14:51:06] d2.evaluation.evaluator INFO: Inference done 555/754. Dataloading: 0.0008 s/iter. Inference: 0.1383 s/iter. Eval: 0.2831 s/iter. Total: 0.4223 s/iter. ETA=0:01:24
[07/11 14:51:11] d2.evaluation.evaluator INFO: Inference done 567/754. Dataloading: 0.0008 s/iter. Inference: 0.1380 s/iter. Eval: 0.2835 s/iter. Total: 0.4223 s/iter. ETA=0:01:18
[07/11 14:51:17] d2.evaluation.evaluator INFO: Inference done 579/754. Dataloading: 0.0008 s/iter. Inference: 0.1382 s/iter. Eval: 0.2836 s/iter. Total: 0.4227 s/iter. ETA=0:01:13
[07/11 14:51:22] d2.evaluation.evaluator INFO: Inference done 591/754. Dataloading: 0.0008 s/iter. Inference: 0.1382 s/iter. Eval: 0.2841 s/iter. Total: 0.4232 s/iter. ETA=0:01:08
[07/11 14:51:27] d2.evaluation.evaluator INFO: Inference done 603/754. Dataloading: 0.0008 s/iter. Inference: 0.1379 s/iter. Eval: 0.2846 s/iter. Total: 0.4233 s/iter. ETA=0:01:03
[07/11 14:51:32] d2.evaluation.evaluator INFO: Inference done 615/754. Dataloading: 0.0008 s/iter. Inference: 0.1381 s/iter. Eval: 0.2845 s/iter. Total: 0.4234 s/iter. ETA=0:00:58
[07/11 14:51:38] d2.evaluation.evaluator INFO: Inference done 627/754. Dataloading: 0.0008 s/iter. Inference: 0.1382 s/iter. Eval: 0.2847 s/iter. Total: 0.4238 s/iter. ETA=0:00:53
[07/11 14:51:43] d2.evaluation.evaluator INFO: Inference done 639/754. Dataloading: 0.0008 s/iter. Inference: 0.1379 s/iter. Eval: 0.2850 s/iter. Total: 0.4238 s/iter. ETA=0:00:48
[07/11 14:51:48] d2.evaluation.evaluator INFO: Inference done 651/754. Dataloading: 0.0008 s/iter. Inference: 0.1380 s/iter. Eval: 0.2852 s/iter. Total: 0.4241 s/iter. ETA=0:00:43
[07/11 14:51:53] d2.evaluation.evaluator INFO: Inference done 663/754. Dataloading: 0.0008 s/iter. Inference: 0.1382 s/iter. Eval: 0.2856 s/iter. Total: 0.4246 s/iter. ETA=0:00:38
[07/11 14:51:58] d2.evaluation.evaluator INFO: Inference done 675/754. Dataloading: 0.0008 s/iter. Inference: 0.1379 s/iter. Eval: 0.2859 s/iter. Total: 0.4246 s/iter. ETA=0:00:33
[07/11 14:52:04] d2.evaluation.evaluator INFO: Inference done 687/754. Dataloading: 0.0008 s/iter. Inference: 0.1379 s/iter. Eval: 0.2861 s/iter. Total: 0.4248 s/iter. ETA=0:00:28
[07/11 14:52:09] d2.evaluation.evaluator INFO: Inference done 699/754. Dataloading: 0.0008 s/iter. Inference: 0.1378 s/iter. Eval: 0.2862 s/iter. Total: 0.4248 s/iter. ETA=0:00:23
[07/11 14:52:14] d2.evaluation.evaluator INFO: Inference done 711/754. Dataloading: 0.0008 s/iter. Inference: 0.1376 s/iter. Eval: 0.2865 s/iter. Total: 0.4249 s/iter. ETA=0:00:18
[07/11 14:52:19] d2.evaluation.evaluator INFO: Inference done 722/754. Dataloading: 0.0008 s/iter. Inference: 0.1380 s/iter. Eval: 0.2866 s/iter. Total: 0.4254 s/iter. ETA=0:00:13
[07/11 14:52:24] d2.evaluation.evaluator INFO: Inference done 733/754. Dataloading: 0.0008 s/iter. Inference: 0.1383 s/iter. Eval: 0.2868 s/iter. Total: 0.4260 s/iter. ETA=0:00:08
[07/11 14:52:29] d2.evaluation.evaluator INFO: Inference done 744/754. Dataloading: 0.0008 s/iter. Inference: 0.1388 s/iter. Eval: 0.2868 s/iter. Total: 0.4265 s/iter. ETA=0:00:04
[07/11 14:52:34] d2.evaluation.evaluator INFO: Total inference time: 0:05:19.823702 (0.427001 s / iter per device, on 1 devices)
[07/11 14:52:34] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:44 (0.139085 s / iter per device, on 1 devices)
[07/11 14:52:35] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/11 14:52:35] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/11 14:52:36] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/11 14:52:38] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/11 14:52:38] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 14:52:38] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/11 14:52:47] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 66.336 | 66.336 | 66.336 |  nan  |  nan  | 66.336 |
[07/11 14:52:47] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 14:52:47] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 79.754 | defect     | 52.918 |
[07/11 14:52:47] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_new in csv format:
[07/11 14:52:47] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/11 14:52:47] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 14:52:47] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/11 14:52:47] d2.evaluation.testing INFO: copypaste: Task: segm
[07/11 14:52:47] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 14:52:47] d2.evaluation.testing INFO: copypaste: 66.3357,66.3357,66.3357,nan,nan,66.3357
[07/11 14:52:47] d2.utils.events INFO:  eta: 1:38:02  iter: 19999  total_loss: 4.537  loss_ce: 0.1291  loss_mask: 0.1606  loss_dice: 0.1413  loss_ce_0: 0.06361  loss_mask_0: 0.1702  loss_dice_0: 0.1498  loss_ce_1: 0.1295  loss_mask_1: 0.1607  loss_dice_1: 0.144  loss_ce_2: 0.1294  loss_mask_2: 0.1632  loss_dice_2: 0.1502  loss_ce_3: 0.1291  loss_mask_3: 0.1598  loss_dice_3: 0.1436  loss_ce_4: 0.1293  loss_mask_4: 0.1703  loss_dice_4: 0.1442  loss_ce_5: 0.1292  loss_mask_5: 0.1715  loss_dice_5: 0.1446  loss_ce_6: 0.1292  loss_mask_6: 0.163  loss_dice_6: 0.1423  loss_ce_7: 0.1292  loss_mask_7: 0.1677  loss_dice_7: 0.1463  loss_ce_8: 0.1293  loss_mask_8: 0.1706  loss_dice_8: 0.1443  time: 0.5805  data_time: 0.0015  lr: 1e-05  max_mem: 2811M
[07/11 14:52:47] d2.engine.hooks INFO: Not saving as latest eval score for total_loss is 5.20212, not better than best score 4.45740 @ iteration 4999.
[07/11 14:52:58] d2.utils.events INFO:  eta: 1:37:38  iter: 20019  total_loss: 4.359  loss_ce: 0.1556  loss_mask: 0.1538  loss_dice: 0.1374  loss_ce_0: 0.07419  loss_mask_0: 0.1449  loss_dice_0: 0.1366  loss_ce_1: 0.1559  loss_mask_1: 0.1475  loss_dice_1: 0.1322  loss_ce_2: 0.1563  loss_mask_2: 0.151  loss_dice_2: 0.1397  loss_ce_3: 0.1563  loss_mask_3: 0.1496  loss_dice_3: 0.1408  loss_ce_4: 0.1561  loss_mask_4: 0.1481  loss_dice_4: 0.1369  loss_ce_5: 0.1563  loss_mask_5: 0.1423  loss_dice_5: 0.1365  loss_ce_6: 0.156  loss_mask_6: 0.1499  loss_dice_6: 0.1371  loss_ce_7: 0.1561  loss_mask_7: 0.1383  loss_dice_7: 0.1421  loss_ce_8: 0.1557  loss_mask_8: 0.158  loss_dice_8: 0.1386  time: 0.5805  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 14:53:10] d2.utils.events INFO:  eta: 1:37:16  iter: 20039  total_loss: 4.643  loss_ce: 0.1555  loss_mask: 0.2005  loss_dice: 0.1419  loss_ce_0: 0.07419  loss_mask_0: 0.2077  loss_dice_0: 0.1444  loss_ce_1: 0.1556  loss_mask_1: 0.2107  loss_dice_1: 0.133  loss_ce_2: 0.1561  loss_mask_2: 0.2135  loss_dice_2: 0.1379  loss_ce_3: 0.1561  loss_mask_3: 0.2044  loss_dice_3: 0.1347  loss_ce_4: 0.1559  loss_mask_4: 0.2136  loss_dice_4: 0.1358  loss_ce_5: 0.156  loss_mask_5: 0.2131  loss_dice_5: 0.1333  loss_ce_6: 0.1558  loss_mask_6: 0.2062  loss_dice_6: 0.1366  loss_ce_7: 0.1559  loss_mask_7: 0.2063  loss_dice_7: 0.1421  loss_ce_8: 0.1555  loss_mask_8: 0.2084  loss_dice_8: 0.142  time: 0.5805  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 14:53:22] d2.utils.events INFO:  eta: 1:36:53  iter: 20059  total_loss: 4.515  loss_ce: 0.1027  loss_mask: 0.1602  loss_dice: 0.1543  loss_ce_0: 0.05304  loss_mask_0: 0.1559  loss_dice_0: 0.1546  loss_ce_1: 0.1025  loss_mask_1: 0.1568  loss_dice_1: 0.1494  loss_ce_2: 0.1022  loss_mask_2: 0.1617  loss_dice_2: 0.1556  loss_ce_3: 0.1023  loss_mask_3: 0.1648  loss_dice_3: 0.1591  loss_ce_4: 0.1024  loss_mask_4: 0.1703  loss_dice_4: 0.154  loss_ce_5: 0.1024  loss_mask_5: 0.1634  loss_dice_5: 0.1504  loss_ce_6: 0.1025  loss_mask_6: 0.1572  loss_dice_6: 0.1506  loss_ce_7: 0.1024  loss_mask_7: 0.1677  loss_dice_7: 0.1565  loss_ce_8: 0.1027  loss_mask_8: 0.1586  loss_dice_8: 0.1574  time: 0.5805  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 14:53:33] d2.utils.events INFO:  eta: 1:36:30  iter: 20079  total_loss: 4.645  loss_ce: 0.1028  loss_mask: 0.2057  loss_dice: 0.1709  loss_ce_0: 0.05306  loss_mask_0: 0.2072  loss_dice_0: 0.1702  loss_ce_1: 0.1025  loss_mask_1: 0.2167  loss_dice_1: 0.1728  loss_ce_2: 0.1022  loss_mask_2: 0.2208  loss_dice_2: 0.1662  loss_ce_3: 0.1023  loss_mask_3: 0.2035  loss_dice_3: 0.1644  loss_ce_4: 0.1025  loss_mask_4: 0.2154  loss_dice_4: 0.1745  loss_ce_5: 0.1024  loss_mask_5: 0.2108  loss_dice_5: 0.1692  loss_ce_6: 0.1025  loss_mask_6: 0.2131  loss_dice_6: 0.1734  loss_ce_7: 0.1024  loss_mask_7: 0.2134  loss_dice_7: 0.1687  loss_ce_8: 0.1027  loss_mask_8: 0.2084  loss_dice_8: 0.169  time: 0.5805  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 14:53:44] d2.utils.events INFO:  eta: 1:36:14  iter: 20099  total_loss: 4.439  loss_ce: 0.1551  loss_mask: 0.1847  loss_dice: 0.1296  loss_ce_0: 0.07414  loss_mask_0: 0.1911  loss_dice_0: 0.1347  loss_ce_1: 0.1548  loss_mask_1: 0.1868  loss_dice_1: 0.1272  loss_ce_2: 0.1555  loss_mask_2: 0.1898  loss_dice_2: 0.1264  loss_ce_3: 0.1555  loss_mask_3: 0.1885  loss_dice_3: 0.1305  loss_ce_4: 0.1552  loss_mask_4: 0.1867  loss_dice_4: 0.1245  loss_ce_5: 0.1555  loss_mask_5: 0.1865  loss_dice_5: 0.127  loss_ce_6: 0.1552  loss_mask_6: 0.2024  loss_dice_6: 0.1294  loss_ce_7: 0.1555  loss_mask_7: 0.1905  loss_dice_7: 0.1303  loss_ce_8: 0.1551  loss_mask_8: 0.1855  loss_dice_8: 0.1267  time: 0.5805  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 14:53:56] d2.utils.events INFO:  eta: 1:35:58  iter: 20119  total_loss: 4.487  loss_ce: 0.1551  loss_mask: 0.1626  loss_dice: 0.1558  loss_ce_0: 0.0741  loss_mask_0: 0.1727  loss_dice_0: 0.1575  loss_ce_1: 0.1552  loss_mask_1: 0.169  loss_dice_1: 0.1542  loss_ce_2: 0.1555  loss_mask_2: 0.1617  loss_dice_2: 0.1685  loss_ce_3: 0.1556  loss_mask_3: 0.1671  loss_dice_3: 0.1642  loss_ce_4: 0.1553  loss_mask_4: 0.1676  loss_dice_4: 0.1653  loss_ce_5: 0.1554  loss_mask_5: 0.1603  loss_dice_5: 0.1524  loss_ce_6: 0.1553  loss_mask_6: 0.1625  loss_dice_6: 0.1661  loss_ce_7: 0.1555  loss_mask_7: 0.1645  loss_dice_7: 0.1562  loss_ce_8: 0.1547  loss_mask_8: 0.1557  loss_dice_8: 0.1665  time: 0.5805  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 14:54:07] d2.utils.events INFO:  eta: 1:35:41  iter: 20139  total_loss: 4.338  loss_ce: 0.1028  loss_mask: 0.1749  loss_dice: 0.136  loss_ce_0: 0.0531  loss_mask_0: 0.1714  loss_dice_0: 0.1306  loss_ce_1: 0.1025  loss_mask_1: 0.1767  loss_dice_1: 0.1314  loss_ce_2: 0.1024  loss_mask_2: 0.178  loss_dice_2: 0.1295  loss_ce_3: 0.1024  loss_mask_3: 0.1834  loss_dice_3: 0.1264  loss_ce_4: 0.1026  loss_mask_4: 0.1809  loss_dice_4: 0.1291  loss_ce_5: 0.1025  loss_mask_5: 0.1815  loss_dice_5: 0.1262  loss_ce_6: 0.1025  loss_mask_6: 0.1807  loss_dice_6: 0.127  loss_ce_7: 0.1025  loss_mask_7: 0.1756  loss_dice_7: 0.1289  loss_ce_8: 0.1028  loss_mask_8: 0.1889  loss_dice_8: 0.1318  time: 0.5805  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 14:54:19] d2.utils.events INFO:  eta: 1:35:23  iter: 20159  total_loss: 4.582  loss_ce: 0.1553  loss_mask: 0.179  loss_dice: 0.1351  loss_ce_0: 0.07413  loss_mask_0: 0.1827  loss_dice_0: 0.1329  loss_ce_1: 0.1558  loss_mask_1: 0.181  loss_dice_1: 0.1344  loss_ce_2: 0.156  loss_mask_2: 0.1805  loss_dice_2: 0.1343  loss_ce_3: 0.156  loss_mask_3: 0.1805  loss_dice_3: 0.1337  loss_ce_4: 0.1557  loss_mask_4: 0.1756  loss_dice_4: 0.134  loss_ce_5: 0.1557  loss_mask_5: 0.177  loss_dice_5: 0.1327  loss_ce_6: 0.1556  loss_mask_6: 0.1823  loss_dice_6: 0.1345  loss_ce_7: 0.1557  loss_mask_7: 0.1864  loss_dice_7: 0.1368  loss_ce_8: 0.1554  loss_mask_8: 0.1748  loss_dice_8: 0.1329  time: 0.5805  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 14:54:31] d2.utils.events INFO:  eta: 1:35:00  iter: 20179  total_loss: 4.771  loss_ce: 0.129  loss_mask: 0.1849  loss_dice: 0.1336  loss_ce_0: 0.06359  loss_mask_0: 0.1917  loss_dice_0: 0.1342  loss_ce_1: 0.1292  loss_mask_1: 0.1989  loss_dice_1: 0.1341  loss_ce_2: 0.1293  loss_mask_2: 0.1879  loss_dice_2: 0.1319  loss_ce_3: 0.1291  loss_mask_3: 0.1953  loss_dice_3: 0.1337  loss_ce_4: 0.129  loss_mask_4: 0.1964  loss_dice_4: 0.1349  loss_ce_5: 0.1292  loss_mask_5: 0.1985  loss_dice_5: 0.1394  loss_ce_6: 0.129  loss_mask_6: 0.1908  loss_dice_6: 0.1367  loss_ce_7: 0.1291  loss_mask_7: 0.1918  loss_dice_7: 0.1341  loss_ce_8: 0.1292  loss_mask_8: 0.2007  loss_dice_8: 0.1356  time: 0.5804  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 14:54:42] d2.utils.events INFO:  eta: 1:34:41  iter: 20199  total_loss: 4.744  loss_ce: 0.1031  loss_mask: 0.1919  loss_dice: 0.1709  loss_ce_0: 0.05317  loss_mask_0: 0.174  loss_dice_0: 0.1649  loss_ce_1: 0.1027  loss_mask_1: 0.1795  loss_dice_1: 0.1609  loss_ce_2: 0.1028  loss_mask_2: 0.1847  loss_dice_2: 0.1622  loss_ce_3: 0.1029  loss_mask_3: 0.1812  loss_dice_3: 0.1702  loss_ce_4: 0.1028  loss_mask_4: 0.1948  loss_dice_4: 0.1668  loss_ce_5: 0.1028  loss_mask_5: 0.1877  loss_dice_5: 0.1668  loss_ce_6: 0.103  loss_mask_6: 0.1783  loss_dice_6: 0.1603  loss_ce_7: 0.1028  loss_mask_7: 0.181  loss_dice_7: 0.1663  loss_ce_8: 0.1031  loss_mask_8: 0.1898  loss_dice_8: 0.1694  time: 0.5804  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 14:54:54] d2.utils.events INFO:  eta: 1:34:18  iter: 20219  total_loss: 4.611  loss_ce: 0.1031  loss_mask: 0.1875  loss_dice: 0.1494  loss_ce_0: 0.05315  loss_mask_0: 0.1901  loss_dice_0: 0.1488  loss_ce_1: 0.103  loss_mask_1: 0.1957  loss_dice_1: 0.1502  loss_ce_2: 0.1027  loss_mask_2: 0.1841  loss_dice_2: 0.1507  loss_ce_3: 0.1028  loss_mask_3: 0.1949  loss_dice_3: 0.1505  loss_ce_4: 0.103  loss_mask_4: 0.1984  loss_dice_4: 0.1486  loss_ce_5: 0.1028  loss_mask_5: 0.1892  loss_dice_5: 0.1488  loss_ce_6: 0.1029  loss_mask_6: 0.1938  loss_dice_6: 0.151  loss_ce_7: 0.1028  loss_mask_7: 0.1923  loss_dice_7: 0.1501  loss_ce_8: 0.1032  loss_mask_8: 0.1891  loss_dice_8: 0.1481  time: 0.5804  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 14:55:05] d2.utils.events INFO:  eta: 1:33:57  iter: 20239  total_loss: 4.462  loss_ce: 0.129  loss_mask: 0.1964  loss_dice: 0.1244  loss_ce_0: 0.06359  loss_mask_0: 0.1992  loss_dice_0: 0.1226  loss_ce_1: 0.1289  loss_mask_1: 0.1993  loss_dice_1: 0.1254  loss_ce_2: 0.129  loss_mask_2: 0.1932  loss_dice_2: 0.1213  loss_ce_3: 0.129  loss_mask_3: 0.2042  loss_dice_3: 0.125  loss_ce_4: 0.129  loss_mask_4: 0.2029  loss_dice_4: 0.1235  loss_ce_5: 0.1291  loss_mask_5: 0.1971  loss_dice_5: 0.1242  loss_ce_6: 0.129  loss_mask_6: 0.1987  loss_dice_6: 0.1234  loss_ce_7: 0.1291  loss_mask_7: 0.1964  loss_dice_7: 0.1289  loss_ce_8: 0.129  loss_mask_8: 0.2057  loss_dice_8: 0.1273  time: 0.5804  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 14:55:17] d2.utils.events INFO:  eta: 1:33:36  iter: 20259  total_loss: 4.767  loss_ce: 0.103  loss_mask: 0.2454  loss_dice: 0.1445  loss_ce_0: 0.05315  loss_mask_0: 0.2441  loss_dice_0: 0.1498  loss_ce_1: 0.103  loss_mask_1: 0.2461  loss_dice_1: 0.1477  loss_ce_2: 0.1028  loss_mask_2: 0.2399  loss_dice_2: 0.1447  loss_ce_3: 0.1028  loss_mask_3: 0.2441  loss_dice_3: 0.1515  loss_ce_4: 0.1029  loss_mask_4: 0.2428  loss_dice_4: 0.1511  loss_ce_5: 0.1028  loss_mask_5: 0.242  loss_dice_5: 0.1511  loss_ce_6: 0.103  loss_mask_6: 0.2481  loss_dice_6: 0.1489  loss_ce_7: 0.1028  loss_mask_7: 0.2477  loss_dice_7: 0.1476  loss_ce_8: 0.103  loss_mask_8: 0.2526  loss_dice_8: 0.1465  time: 0.5804  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 14:55:28] d2.utils.events INFO:  eta: 1:33:14  iter: 20279  total_loss: 4.533  loss_ce: 0.1029  loss_mask: 0.1805  loss_dice: 0.1368  loss_ce_0: 0.05313  loss_mask_0: 0.1789  loss_dice_0: 0.1379  loss_ce_1: 0.1027  loss_mask_1: 0.1837  loss_dice_1: 0.1421  loss_ce_2: 0.1025  loss_mask_2: 0.1916  loss_dice_2: 0.14  loss_ce_3: 0.1026  loss_mask_3: 0.1808  loss_dice_3: 0.1389  loss_ce_4: 0.1028  loss_mask_4: 0.182  loss_dice_4: 0.1405  loss_ce_5: 0.1026  loss_mask_5: 0.1798  loss_dice_5: 0.139  loss_ce_6: 0.1027  loss_mask_6: 0.1797  loss_dice_6: 0.1428  loss_ce_7: 0.1026  loss_mask_7: 0.185  loss_dice_7: 0.1399  loss_ce_8: 0.1028  loss_mask_8: 0.1792  loss_dice_8: 0.1364  time: 0.5804  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 14:55:40] d2.utils.events INFO:  eta: 1:32:59  iter: 20299  total_loss: 4.716  loss_ce: 0.1029  loss_mask: 0.2141  loss_dice: 0.1418  loss_ce_0: 0.05313  loss_mask_0: 0.2061  loss_dice_0: 0.1411  loss_ce_1: 0.1027  loss_mask_1: 0.2112  loss_dice_1: 0.1402  loss_ce_2: 0.1025  loss_mask_2: 0.2088  loss_dice_2: 0.1424  loss_ce_3: 0.1025  loss_mask_3: 0.2075  loss_dice_3: 0.1394  loss_ce_4: 0.1026  loss_mask_4: 0.2078  loss_dice_4: 0.1416  loss_ce_5: 0.1025  loss_mask_5: 0.2158  loss_dice_5: 0.1381  loss_ce_6: 0.1027  loss_mask_6: 0.2169  loss_dice_6: 0.1437  loss_ce_7: 0.1026  loss_mask_7: 0.2101  loss_dice_7: 0.1405  loss_ce_8: 0.1028  loss_mask_8: 0.2051  loss_dice_8: 0.1387  time: 0.5804  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 14:55:52] d2.utils.events INFO:  eta: 1:32:45  iter: 20319  total_loss: 4.807  loss_ce: 0.1291  loss_mask: 0.2143  loss_dice: 0.1445  loss_ce_0: 0.0636  loss_mask_0: 0.2196  loss_dice_0: 0.1452  loss_ce_1: 0.129  loss_mask_1: 0.2023  loss_dice_1: 0.1424  loss_ce_2: 0.1291  loss_mask_2: 0.2159  loss_dice_2: 0.1405  loss_ce_3: 0.1291  loss_mask_3: 0.2207  loss_dice_3: 0.144  loss_ce_4: 0.1291  loss_mask_4: 0.2181  loss_dice_4: 0.1451  loss_ce_5: 0.1292  loss_mask_5: 0.2166  loss_dice_5: 0.1392  loss_ce_6: 0.129  loss_mask_6: 0.2211  loss_dice_6: 0.1427  loss_ce_7: 0.1291  loss_mask_7: 0.2145  loss_dice_7: 0.1462  loss_ce_8: 0.1291  loss_mask_8: 0.219  loss_dice_8: 0.1414  time: 0.5804  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 14:56:03] d2.utils.events INFO:  eta: 1:32:33  iter: 20339  total_loss: 4.573  loss_ce: 0.1553  loss_mask: 0.1919  loss_dice: 0.1362  loss_ce_0: 0.07405  loss_mask_0: 0.1909  loss_dice_0: 0.1344  loss_ce_1: 0.1555  loss_mask_1: 0.1925  loss_dice_1: 0.1365  loss_ce_2: 0.1558  loss_mask_2: 0.1879  loss_dice_2: 0.1366  loss_ce_3: 0.1557  loss_mask_3: 0.1908  loss_dice_3: 0.1323  loss_ce_4: 0.1555  loss_mask_4: 0.1903  loss_dice_4: 0.1337  loss_ce_5: 0.1557  loss_mask_5: 0.1941  loss_dice_5: 0.1339  loss_ce_6: 0.1555  loss_mask_6: 0.1933  loss_dice_6: 0.1361  loss_ce_7: 0.1557  loss_mask_7: 0.192  loss_dice_7: 0.1369  loss_ce_8: 0.1554  loss_mask_8: 0.1942  loss_dice_8: 0.1399  time: 0.5804  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 14:56:14] d2.utils.events INFO:  eta: 1:32:21  iter: 20359  total_loss: 5.046  loss_ce: 0.1029  loss_mask: 0.2111  loss_dice: 0.1435  loss_ce_0: 0.05313  loss_mask_0: 0.2153  loss_dice_0: 0.1442  loss_ce_1: 0.1028  loss_mask_1: 0.2147  loss_dice_1: 0.1479  loss_ce_2: 0.1026  loss_mask_2: 0.2096  loss_dice_2: 0.1431  loss_ce_3: 0.1026  loss_mask_3: 0.2126  loss_dice_3: 0.142  loss_ce_4: 0.1028  loss_mask_4: 0.2143  loss_dice_4: 0.1445  loss_ce_5: 0.1026  loss_mask_5: 0.2168  loss_dice_5: 0.1501  loss_ce_6: 0.1028  loss_mask_6: 0.2199  loss_dice_6: 0.1509  loss_ce_7: 0.1027  loss_mask_7: 0.2076  loss_dice_7: 0.1468  loss_ce_8: 0.1029  loss_mask_8: 0.2044  loss_dice_8: 0.1486  time: 0.5804  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 14:56:26] d2.utils.events INFO:  eta: 1:32:03  iter: 20379  total_loss: 4.674  loss_ce: 0.1029  loss_mask: 0.1769  loss_dice: 0.1534  loss_ce_0: 0.05314  loss_mask_0: 0.1697  loss_dice_0: 0.1527  loss_ce_1: 0.1028  loss_mask_1: 0.1739  loss_dice_1: 0.1528  loss_ce_2: 0.1026  loss_mask_2: 0.172  loss_dice_2: 0.1537  loss_ce_3: 0.1026  loss_mask_3: 0.1727  loss_dice_3: 0.1522  loss_ce_4: 0.1028  loss_mask_4: 0.1719  loss_dice_4: 0.1617  loss_ce_5: 0.1026  loss_mask_5: 0.1754  loss_dice_5: 0.1604  loss_ce_6: 0.1028  loss_mask_6: 0.1781  loss_dice_6: 0.1527  loss_ce_7: 0.1026  loss_mask_7: 0.175  loss_dice_7: 0.1588  loss_ce_8: 0.1028  loss_mask_8: 0.1743  loss_dice_8: 0.1615  time: 0.5804  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 14:56:37] d2.utils.events INFO:  eta: 1:31:51  iter: 20399  total_loss: 4.717  loss_ce: 0.1552  loss_mask: 0.1752  loss_dice: 0.1328  loss_ce_0: 0.07407  loss_mask_0: 0.1793  loss_dice_0: 0.1355  loss_ce_1: 0.1548  loss_mask_1: 0.1807  loss_dice_1: 0.1356  loss_ce_2: 0.1553  loss_mask_2: 0.1885  loss_dice_2: 0.1381  loss_ce_3: 0.1554  loss_mask_3: 0.1843  loss_dice_3: 0.1381  loss_ce_4: 0.1552  loss_mask_4: 0.1792  loss_dice_4: 0.1355  loss_ce_5: 0.1553  loss_mask_5: 0.1698  loss_dice_5: 0.1302  loss_ce_6: 0.1551  loss_mask_6: 0.177  loss_dice_6: 0.1347  loss_ce_7: 0.1555  loss_mask_7: 0.1722  loss_dice_7: 0.1355  loss_ce_8: 0.1551  loss_mask_8: 0.1762  loss_dice_8: 0.1309  time: 0.5804  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 14:56:49] d2.utils.events INFO:  eta: 1:31:40  iter: 20419  total_loss: 4.852  loss_ce: 0.1031  loss_mask: 0.205  loss_dice: 0.1516  loss_ce_0: 0.05314  loss_mask_0: 0.2032  loss_dice_0: 0.1546  loss_ce_1: 0.1034  loss_mask_1: 0.2123  loss_dice_1: 0.1513  loss_ce_2: 0.1029  loss_mask_2: 0.1996  loss_dice_2: 0.148  loss_ce_3: 0.1028  loss_mask_3: 0.198  loss_dice_3: 0.1497  loss_ce_4: 0.1031  loss_mask_4: 0.2068  loss_dice_4: 0.1558  loss_ce_5: 0.1028  loss_mask_5: 0.201  loss_dice_5: 0.1535  loss_ce_6: 0.103  loss_mask_6: 0.2011  loss_dice_6: 0.1501  loss_ce_7: 0.1028  loss_mask_7: 0.2091  loss_dice_7: 0.1537  loss_ce_8: 0.1031  loss_mask_8: 0.1998  loss_dice_8: 0.1513  time: 0.5804  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 14:57:00] d2.utils.events INFO:  eta: 1:31:31  iter: 20439  total_loss: 4.474  loss_ce: 0.1029  loss_mask: 0.1355  loss_dice: 0.1644  loss_ce_0: 0.05314  loss_mask_0: 0.1363  loss_dice_0: 0.1558  loss_ce_1: 0.1031  loss_mask_1: 0.1327  loss_dice_1: 0.154  loss_ce_2: 0.1028  loss_mask_2: 0.1326  loss_dice_2: 0.1572  loss_ce_3: 0.1028  loss_mask_3: 0.1361  loss_dice_3: 0.1613  loss_ce_4: 0.1028  loss_mask_4: 0.1397  loss_dice_4: 0.1561  loss_ce_5: 0.1027  loss_mask_5: 0.1282  loss_dice_5: 0.1585  loss_ce_6: 0.1029  loss_mask_6: 0.1302  loss_dice_6: 0.1591  loss_ce_7: 0.1027  loss_mask_7: 0.1386  loss_dice_7: 0.1664  loss_ce_8: 0.1029  loss_mask_8: 0.132  loss_dice_8: 0.1535  time: 0.5804  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 14:57:12] d2.utils.events INFO:  eta: 1:31:18  iter: 20459  total_loss: 4.268  loss_ce: 0.1028  loss_mask: 0.1718  loss_dice: 0.1352  loss_ce_0: 0.05314  loss_mask_0: 0.1704  loss_dice_0: 0.1389  loss_ce_1: 0.1023  loss_mask_1: 0.1692  loss_dice_1: 0.1361  loss_ce_2: 0.1023  loss_mask_2: 0.1701  loss_dice_2: 0.1362  loss_ce_3: 0.1024  loss_mask_3: 0.1705  loss_dice_3: 0.1341  loss_ce_4: 0.1025  loss_mask_4: 0.1717  loss_dice_4: 0.1361  loss_ce_5: 0.1023  loss_mask_5: 0.172  loss_dice_5: 0.1353  loss_ce_6: 0.1024  loss_mask_6: 0.1804  loss_dice_6: 0.1397  loss_ce_7: 0.1024  loss_mask_7: 0.1796  loss_dice_7: 0.1363  loss_ce_8: 0.1026  loss_mask_8: 0.1735  loss_dice_8: 0.1374  time: 0.5804  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 14:57:23] d2.utils.events INFO:  eta: 1:31:07  iter: 20479  total_loss: 4.564  loss_ce: 0.1028  loss_mask: 0.1509  loss_dice: 0.1636  loss_ce_0: 0.0531  loss_mask_0: 0.1576  loss_dice_0: 0.1631  loss_ce_1: 0.1029  loss_mask_1: 0.1519  loss_dice_1: 0.1652  loss_ce_2: 0.1026  loss_mask_2: 0.1505  loss_dice_2: 0.1604  loss_ce_3: 0.1025  loss_mask_3: 0.1502  loss_dice_3: 0.1629  loss_ce_4: 0.1027  loss_mask_4: 0.1396  loss_dice_4: 0.1575  loss_ce_5: 0.1025  loss_mask_5: 0.151  loss_dice_5: 0.1627  loss_ce_6: 0.1027  loss_mask_6: 0.1542  loss_dice_6: 0.1671  loss_ce_7: 0.1026  loss_mask_7: 0.1526  loss_dice_7: 0.165  loss_ce_8: 0.1028  loss_mask_8: 0.1534  loss_dice_8: 0.1651  time: 0.5803  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 14:57:34] d2.utils.events INFO:  eta: 1:30:54  iter: 20499  total_loss: 4.774  loss_ce: 0.1028  loss_mask: 0.2099  loss_dice: 0.1338  loss_ce_0: 0.0531  loss_mask_0: 0.2035  loss_dice_0: 0.1363  loss_ce_1: 0.1028  loss_mask_1: 0.2067  loss_dice_1: 0.1349  loss_ce_2: 0.1026  loss_mask_2: 0.2158  loss_dice_2: 0.1384  loss_ce_3: 0.1025  loss_mask_3: 0.2206  loss_dice_3: 0.143  loss_ce_4: 0.1027  loss_mask_4: 0.2108  loss_dice_4: 0.1423  loss_ce_5: 0.1026  loss_mask_5: 0.2103  loss_dice_5: 0.1432  loss_ce_6: 0.1026  loss_mask_6: 0.2113  loss_dice_6: 0.1417  loss_ce_7: 0.1025  loss_mask_7: 0.2117  loss_dice_7: 0.1347  loss_ce_8: 0.1028  loss_mask_8: 0.2154  loss_dice_8: 0.1432  time: 0.5803  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 14:57:46] d2.utils.events INFO:  eta: 1:30:42  iter: 20519  total_loss: 4.687  loss_ce: 0.1031  loss_mask: 0.2203  loss_dice: 0.1455  loss_ce_0: 0.0531  loss_mask_0: 0.2169  loss_dice_0: 0.1466  loss_ce_1: 0.1035  loss_mask_1: 0.2134  loss_dice_1: 0.1425  loss_ce_2: 0.103  loss_mask_2: 0.2159  loss_dice_2: 0.1463  loss_ce_3: 0.103  loss_mask_3: 0.2096  loss_dice_3: 0.149  loss_ce_4: 0.1031  loss_mask_4: 0.2138  loss_dice_4: 0.1449  loss_ce_5: 0.103  loss_mask_5: 0.2147  loss_dice_5: 0.1413  loss_ce_6: 0.1033  loss_mask_6: 0.218  loss_dice_6: 0.1489  loss_ce_7: 0.1029  loss_mask_7: 0.2149  loss_dice_7: 0.1438  loss_ce_8: 0.1031  loss_mask_8: 0.2196  loss_dice_8: 0.1356  time: 0.5803  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 14:57:57] d2.utils.events INFO:  eta: 1:30:24  iter: 20539  total_loss: 4.657  loss_ce: 0.1029  loss_mask: 0.1992  loss_dice: 0.1276  loss_ce_0: 0.0531  loss_mask_0: 0.1981  loss_dice_0: 0.1357  loss_ce_1: 0.1033  loss_mask_1: 0.2006  loss_dice_1: 0.1323  loss_ce_2: 0.1028  loss_mask_2: 0.1982  loss_dice_2: 0.1287  loss_ce_3: 0.1026  loss_mask_3: 0.2024  loss_dice_3: 0.1308  loss_ce_4: 0.1029  loss_mask_4: 0.2038  loss_dice_4: 0.1335  loss_ce_5: 0.1028  loss_mask_5: 0.2024  loss_dice_5: 0.1345  loss_ce_6: 0.103  loss_mask_6: 0.1992  loss_dice_6: 0.1322  loss_ce_7: 0.1026  loss_mask_7: 0.2009  loss_dice_7: 0.1334  loss_ce_8: 0.1029  loss_mask_8: 0.1997  loss_dice_8: 0.1263  time: 0.5803  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 14:58:09] d2.utils.events INFO:  eta: 1:30:08  iter: 20559  total_loss: 4.564  loss_ce: 0.1025  loss_mask: 0.2065  loss_dice: 0.1336  loss_ce_0: 0.05308  loss_mask_0: 0.2047  loss_dice_0: 0.1417  loss_ce_1: 0.102  loss_mask_1: 0.204  loss_dice_1: 0.1322  loss_ce_2: 0.1022  loss_mask_2: 0.2035  loss_dice_2: 0.1329  loss_ce_3: 0.1023  loss_mask_3: 0.2085  loss_dice_3: 0.1377  loss_ce_4: 0.1021  loss_mask_4: 0.2015  loss_dice_4: 0.1326  loss_ce_5: 0.1023  loss_mask_5: 0.204  loss_dice_5: 0.1398  loss_ce_6: 0.1023  loss_mask_6: 0.2075  loss_dice_6: 0.1329  loss_ce_7: 0.1022  loss_mask_7: 0.2086  loss_dice_7: 0.136  loss_ce_8: 0.1024  loss_mask_8: 0.2087  loss_dice_8: 0.1367  time: 0.5803  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 14:58:20] d2.utils.events INFO:  eta: 1:29:52  iter: 20579  total_loss: 4.883  loss_ce: 0.1292  loss_mask: 0.1568  loss_dice: 0.1471  loss_ce_0: 0.0636  loss_mask_0: 0.1549  loss_dice_0: 0.1416  loss_ce_1: 0.1292  loss_mask_1: 0.1614  loss_dice_1: 0.1443  loss_ce_2: 0.1292  loss_mask_2: 0.1625  loss_dice_2: 0.1455  loss_ce_3: 0.1292  loss_mask_3: 0.1627  loss_dice_3: 0.1494  loss_ce_4: 0.1291  loss_mask_4: 0.1633  loss_dice_4: 0.145  loss_ce_5: 0.1291  loss_mask_5: 0.1644  loss_dice_5: 0.147  loss_ce_6: 0.1292  loss_mask_6: 0.161  loss_dice_6: 0.1485  loss_ce_7: 0.1292  loss_mask_7: 0.1588  loss_dice_7: 0.1471  loss_ce_8: 0.1292  loss_mask_8: 0.1618  loss_dice_8: 0.1481  time: 0.5803  data_time: 0.0025  lr: 1e-06  max_mem: 2811M
[07/11 14:58:32] d2.utils.events INFO:  eta: 1:29:38  iter: 20599  total_loss: 5.186  loss_ce: 0.1024  loss_mask: 0.1626  loss_dice: 0.1719  loss_ce_0: 0.05305  loss_mask_0: 0.1552  loss_dice_0: 0.1817  loss_ce_1: 0.1022  loss_mask_1: 0.1642  loss_dice_1: 0.1802  loss_ce_2: 0.102  loss_mask_2: 0.1607  loss_dice_2: 0.1771  loss_ce_3: 0.1021  loss_mask_3: 0.1586  loss_dice_3: 0.1707  loss_ce_4: 0.1023  loss_mask_4: 0.1673  loss_dice_4: 0.1726  loss_ce_5: 0.1021  loss_mask_5: 0.1613  loss_dice_5: 0.1769  loss_ce_6: 0.1023  loss_mask_6: 0.1609  loss_dice_6: 0.1779  loss_ce_7: 0.1022  loss_mask_7: 0.157  loss_dice_7: 0.1764  loss_ce_8: 0.1024  loss_mask_8: 0.1593  loss_dice_8: 0.1745  time: 0.5803  data_time: 0.0030  lr: 1e-06  max_mem: 2811M
[07/11 14:58:43] d2.utils.events INFO:  eta: 1:29:24  iter: 20619  total_loss: 4.657  loss_ce: 0.1024  loss_mask: 0.2099  loss_dice: 0.1347  loss_ce_0: 0.05303  loss_mask_0: 0.187  loss_dice_0: 0.1297  loss_ce_1: 0.1022  loss_mask_1: 0.1982  loss_dice_1: 0.1375  loss_ce_2: 0.1021  loss_mask_2: 0.1927  loss_dice_2: 0.1325  loss_ce_3: 0.1021  loss_mask_3: 0.1968  loss_dice_3: 0.1303  loss_ce_4: 0.1023  loss_mask_4: 0.196  loss_dice_4: 0.1293  loss_ce_5: 0.1021  loss_mask_5: 0.2009  loss_dice_5: 0.1315  loss_ce_6: 0.1022  loss_mask_6: 0.1965  loss_dice_6: 0.1309  loss_ce_7: 0.1021  loss_mask_7: 0.1996  loss_dice_7: 0.1292  loss_ce_8: 0.1024  loss_mask_8: 0.197  loss_dice_8: 0.1331  time: 0.5803  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 14:58:55] d2.utils.events INFO:  eta: 1:29:08  iter: 20639  total_loss: 4.561  loss_ce: 0.1021  loss_mask: 0.1332  loss_dice: 0.1896  loss_ce_0: 0.05301  loss_mask_0: 0.1294  loss_dice_0: 0.1826  loss_ce_1: 0.1016  loss_mask_1: 0.1185  loss_dice_1: 0.1813  loss_ce_2: 0.1016  loss_mask_2: 0.1245  loss_dice_2: 0.1865  loss_ce_3: 0.1018  loss_mask_3: 0.1251  loss_dice_3: 0.1861  loss_ce_4: 0.1018  loss_mask_4: 0.1233  loss_dice_4: 0.1858  loss_ce_5: 0.1017  loss_mask_5: 0.1253  loss_dice_5: 0.1849  loss_ce_6: 0.1018  loss_mask_6: 0.1229  loss_dice_6: 0.1858  loss_ce_7: 0.1018  loss_mask_7: 0.1278  loss_dice_7: 0.1817  loss_ce_8: 0.1019  loss_mask_8: 0.1226  loss_dice_8: 0.1803  time: 0.5803  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 14:59:06] d2.utils.events INFO:  eta: 1:28:57  iter: 20659  total_loss: 4.654  loss_ce: 0.1021  loss_mask: 0.177  loss_dice: 0.1574  loss_ce_0: 0.05298  loss_mask_0: 0.1765  loss_dice_0: 0.1587  loss_ce_1: 0.1024  loss_mask_1: 0.1733  loss_dice_1: 0.1599  loss_ce_2: 0.102  loss_mask_2: 0.1831  loss_dice_2: 0.1621  loss_ce_3: 0.1018  loss_mask_3: 0.1905  loss_dice_3: 0.1589  loss_ce_4: 0.102  loss_mask_4: 0.1748  loss_dice_4: 0.1652  loss_ce_5: 0.1017  loss_mask_5: 0.1749  loss_dice_5: 0.1613  loss_ce_6: 0.102  loss_mask_6: 0.1711  loss_dice_6: 0.1576  loss_ce_7: 0.1019  loss_mask_7: 0.1766  loss_dice_7: 0.1591  loss_ce_8: 0.1024  loss_mask_8: 0.1796  loss_dice_8: 0.1616  time: 0.5803  data_time: 0.0029  lr: 1e-06  max_mem: 2811M
[07/11 14:59:17] d2.utils.events INFO:  eta: 1:28:40  iter: 20679  total_loss: 4.184  loss_ce: 0.1021  loss_mask: 0.1874  loss_dice: 0.1436  loss_ce_0: 0.05298  loss_mask_0: 0.1809  loss_dice_0: 0.138  loss_ce_1: 0.1021  loss_mask_1: 0.1789  loss_dice_1: 0.137  loss_ce_2: 0.1019  loss_mask_2: 0.1806  loss_dice_2: 0.1398  loss_ce_3: 0.1018  loss_mask_3: 0.1818  loss_dice_3: 0.1402  loss_ce_4: 0.102  loss_mask_4: 0.1748  loss_dice_4: 0.1421  loss_ce_5: 0.1018  loss_mask_5: 0.1774  loss_dice_5: 0.1422  loss_ce_6: 0.102  loss_mask_6: 0.1865  loss_dice_6: 0.1387  loss_ce_7: 0.1018  loss_mask_7: 0.1849  loss_dice_7: 0.1399  loss_ce_8: 0.102  loss_mask_8: 0.1828  loss_dice_8: 0.1405  time: 0.5803  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 14:59:29] d2.utils.events INFO:  eta: 1:28:33  iter: 20699  total_loss: 4.366  loss_ce: 0.1019  loss_mask: 0.2049  loss_dice: 0.1327  loss_ce_0: 0.05296  loss_mask_0: 0.1969  loss_dice_0: 0.1321  loss_ce_1: 0.1017  loss_mask_1: 0.2036  loss_dice_1: 0.1364  loss_ce_2: 0.1016  loss_mask_2: 0.1995  loss_dice_2: 0.1326  loss_ce_3: 0.1015  loss_mask_3: 0.198  loss_dice_3: 0.1321  loss_ce_4: 0.1017  loss_mask_4: 0.197  loss_dice_4: 0.136  loss_ce_5: 0.1016  loss_mask_5: 0.2068  loss_dice_5: 0.1406  loss_ce_6: 0.1018  loss_mask_6: 0.2011  loss_dice_6: 0.1337  loss_ce_7: 0.1016  loss_mask_7: 0.2063  loss_dice_7: 0.1351  loss_ce_8: 0.1019  loss_mask_8: 0.2038  loss_dice_8: 0.1367  time: 0.5803  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 14:59:40] d2.utils.events INFO:  eta: 1:28:10  iter: 20719  total_loss: 4.254  loss_ce: 0.1566  loss_mask: 0.1802  loss_dice: 0.1293  loss_ce_0: 0.0743  loss_mask_0: 0.1718  loss_dice_0: 0.13  loss_ce_1: 0.1565  loss_mask_1: 0.1633  loss_dice_1: 0.1296  loss_ce_2: 0.1569  loss_mask_2: 0.1723  loss_dice_2: 0.1316  loss_ce_3: 0.157  loss_mask_3: 0.1666  loss_dice_3: 0.131  loss_ce_4: 0.1568  loss_mask_4: 0.1632  loss_dice_4: 0.1238  loss_ce_5: 0.1571  loss_mask_5: 0.1687  loss_dice_5: 0.1254  loss_ce_6: 0.1568  loss_mask_6: 0.1727  loss_dice_6: 0.1316  loss_ce_7: 0.157  loss_mask_7: 0.1729  loss_dice_7: 0.1298  loss_ce_8: 0.1566  loss_mask_8: 0.1693  loss_dice_8: 0.1315  time: 0.5802  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 14:59:50] d2.utils.events INFO:  eta: 1:27:49  iter: 20739  total_loss: 4.404  loss_ce: 0.1019  loss_mask: 0.1854  loss_dice: 0.1384  loss_ce_0: 0.05294  loss_mask_0: 0.1892  loss_dice_0: 0.1383  loss_ce_1: 0.102  loss_mask_1: 0.1853  loss_dice_1: 0.1366  loss_ce_2: 0.1017  loss_mask_2: 0.1835  loss_dice_2: 0.141  loss_ce_3: 0.1017  loss_mask_3: 0.1779  loss_dice_3: 0.1365  loss_ce_4: 0.1018  loss_mask_4: 0.1876  loss_dice_4: 0.1406  loss_ce_5: 0.1017  loss_mask_5: 0.1802  loss_dice_5: 0.1371  loss_ce_6: 0.1018  loss_mask_6: 0.1858  loss_dice_6: 0.1383  loss_ce_7: 0.1017  loss_mask_7: 0.1828  loss_dice_7: 0.1348  loss_ce_8: 0.1022  loss_mask_8: 0.1804  loss_dice_8: 0.1359  time: 0.5801  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:00:00] d2.utils.events INFO:  eta: 1:27:28  iter: 20759  total_loss: 4.91  loss_ce: 0.1018  loss_mask: 0.1841  loss_dice: 0.1562  loss_ce_0: 0.05295  loss_mask_0: 0.1811  loss_dice_0: 0.1581  loss_ce_1: 0.1018  loss_mask_1: 0.1898  loss_dice_1: 0.1496  loss_ce_2: 0.1015  loss_mask_2: 0.189  loss_dice_2: 0.1529  loss_ce_3: 0.1015  loss_mask_3: 0.1879  loss_dice_3: 0.1526  loss_ce_4: 0.1017  loss_mask_4: 0.1908  loss_dice_4: 0.1583  loss_ce_5: 0.1015  loss_mask_5: 0.1845  loss_dice_5: 0.1526  loss_ce_6: 0.1016  loss_mask_6: 0.1857  loss_dice_6: 0.1544  loss_ce_7: 0.1016  loss_mask_7: 0.1828  loss_dice_7: 0.1545  loss_ce_8: 0.102  loss_mask_8: 0.1851  loss_dice_8: 0.1506  time: 0.5801  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:00:11] d2.utils.events INFO:  eta: 1:27:14  iter: 20779  total_loss: 4.452  loss_ce: 0.1291  loss_mask: 0.1843  loss_dice: 0.1283  loss_ce_0: 0.06362  loss_mask_0: 0.1847  loss_dice_0: 0.1309  loss_ce_1: 0.1291  loss_mask_1: 0.1788  loss_dice_1: 0.124  loss_ce_2: 0.1292  loss_mask_2: 0.1871  loss_dice_2: 0.1263  loss_ce_3: 0.1292  loss_mask_3: 0.1842  loss_dice_3: 0.1301  loss_ce_4: 0.1291  loss_mask_4: 0.1829  loss_dice_4: 0.1261  loss_ce_5: 0.1291  loss_mask_5: 0.1859  loss_dice_5: 0.1282  loss_ce_6: 0.1292  loss_mask_6: 0.1821  loss_dice_6: 0.1327  loss_ce_7: 0.1292  loss_mask_7: 0.1768  loss_dice_7: 0.1291  loss_ce_8: 0.1291  loss_mask_8: 0.1787  loss_dice_8: 0.1267  time: 0.5800  data_time: 0.0024  lr: 1e-06  max_mem: 2811M
[07/11 15:00:21] d2.utils.events INFO:  eta: 1:26:58  iter: 20799  total_loss: 4.488  loss_ce: 0.1018  loss_mask: 0.1371  loss_dice: 0.1589  loss_ce_0: 0.05295  loss_mask_0: 0.136  loss_dice_0: 0.159  loss_ce_1: 0.1014  loss_mask_1: 0.1357  loss_dice_1: 0.1581  loss_ce_2: 0.1014  loss_mask_2: 0.1344  loss_dice_2: 0.1556  loss_ce_3: 0.1014  loss_mask_3: 0.1427  loss_dice_3: 0.1549  loss_ce_4: 0.1015  loss_mask_4: 0.13  loss_dice_4: 0.1585  loss_ce_5: 0.1014  loss_mask_5: 0.133  loss_dice_5: 0.1541  loss_ce_6: 0.1015  loss_mask_6: 0.1349  loss_dice_6: 0.1595  loss_ce_7: 0.1014  loss_mask_7: 0.1295  loss_dice_7: 0.1591  loss_ce_8: 0.1016  loss_mask_8: 0.1321  loss_dice_8: 0.1624  time: 0.5800  data_time: 0.0028  lr: 1e-06  max_mem: 2811M
[07/11 15:00:32] d2.utils.events INFO:  eta: 1:26:38  iter: 20819  total_loss: 4.423  loss_ce: 0.1016  loss_mask: 0.1751  loss_dice: 0.1386  loss_ce_0: 0.05291  loss_mask_0: 0.1718  loss_dice_0: 0.1405  loss_ce_1: 0.1013  loss_mask_1: 0.1724  loss_dice_1: 0.1434  loss_ce_2: 0.1012  loss_mask_2: 0.1721  loss_dice_2: 0.1394  loss_ce_3: 0.1011  loss_mask_3: 0.1881  loss_dice_3: 0.1406  loss_ce_4: 0.1013  loss_mask_4: 0.1725  loss_dice_4: 0.1412  loss_ce_5: 0.1011  loss_mask_5: 0.1846  loss_dice_5: 0.145  loss_ce_6: 0.1013  loss_mask_6: 0.1786  loss_dice_6: 0.1395  loss_ce_7: 0.1013  loss_mask_7: 0.174  loss_dice_7: 0.1412  loss_ce_8: 0.1015  loss_mask_8: 0.1756  loss_dice_8: 0.1429  time: 0.5799  data_time: 0.0023  lr: 1e-06  max_mem: 2811M
[07/11 15:00:42] d2.utils.events INFO:  eta: 1:26:26  iter: 20839  total_loss: 4.318  loss_ce: 0.1015  loss_mask: 0.1855  loss_dice: 0.14  loss_ce_0: 0.05288  loss_mask_0: 0.1896  loss_dice_0: 0.1427  loss_ce_1: 0.1016  loss_mask_1: 0.1961  loss_dice_1: 0.1353  loss_ce_2: 0.1013  loss_mask_2: 0.1838  loss_dice_2: 0.1411  loss_ce_3: 0.1012  loss_mask_3: 0.1916  loss_dice_3: 0.1341  loss_ce_4: 0.1014  loss_mask_4: 0.1895  loss_dice_4: 0.1387  loss_ce_5: 0.1012  loss_mask_5: 0.1898  loss_dice_5: 0.1349  loss_ce_6: 0.1014  loss_mask_6: 0.1994  loss_dice_6: 0.1384  loss_ce_7: 0.1013  loss_mask_7: 0.1856  loss_dice_7: 0.1388  loss_ce_8: 0.1015  loss_mask_8: 0.1858  loss_dice_8: 0.1268  time: 0.5799  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:00:53] d2.utils.events INFO:  eta: 1:26:11  iter: 20859  total_loss: 4.437  loss_ce: 0.1013  loss_mask: 0.1942  loss_dice: 0.1388  loss_ce_0: 0.05286  loss_mask_0: 0.1949  loss_dice_0: 0.1419  loss_ce_1: 0.1011  loss_mask_1: 0.1907  loss_dice_1: 0.1353  loss_ce_2: 0.101  loss_mask_2: 0.1985  loss_dice_2: 0.1419  loss_ce_3: 0.101  loss_mask_3: 0.1857  loss_dice_3: 0.1392  loss_ce_4: 0.1011  loss_mask_4: 0.1845  loss_dice_4: 0.1381  loss_ce_5: 0.101  loss_mask_5: 0.194  loss_dice_5: 0.134  loss_ce_6: 0.1011  loss_mask_6: 0.1989  loss_dice_6: 0.133  loss_ce_7: 0.1011  loss_mask_7: 0.1932  loss_dice_7: 0.1377  loss_ce_8: 0.1014  loss_mask_8: 0.1891  loss_dice_8: 0.134  time: 0.5798  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:01:03] d2.utils.events INFO:  eta: 1:25:52  iter: 20879  total_loss: 4.635  loss_ce: 0.1012  loss_mask: 0.2004  loss_dice: 0.1478  loss_ce_0: 0.05283  loss_mask_0: 0.2035  loss_dice_0: 0.1419  loss_ce_1: 0.1008  loss_mask_1: 0.2064  loss_dice_1: 0.1429  loss_ce_2: 0.1007  loss_mask_2: 0.2006  loss_dice_2: 0.1544  loss_ce_3: 0.1007  loss_mask_3: 0.2089  loss_dice_3: 0.1403  loss_ce_4: 0.1008  loss_mask_4: 0.1938  loss_dice_4: 0.1562  loss_ce_5: 0.1007  loss_mask_5: 0.2068  loss_dice_5: 0.147  loss_ce_6: 0.1009  loss_mask_6: 0.2036  loss_dice_6: 0.1519  loss_ce_7: 0.1009  loss_mask_7: 0.2051  loss_dice_7: 0.1517  loss_ce_8: 0.1011  loss_mask_8: 0.2124  loss_dice_8: 0.1459  time: 0.5797  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 15:01:13] d2.utils.events INFO:  eta: 1:25:28  iter: 20899  total_loss: 4.932  loss_ce: 0.101  loss_mask: 0.2118  loss_dice: 0.151  loss_ce_0: 0.0528  loss_mask_0: 0.209  loss_dice_0: 0.1468  loss_ce_1: 0.1006  loss_mask_1: 0.2099  loss_dice_1: 0.1559  loss_ce_2: 0.1006  loss_mask_2: 0.2139  loss_dice_2: 0.1517  loss_ce_3: 0.1005  loss_mask_3: 0.2088  loss_dice_3: 0.1515  loss_ce_4: 0.1006  loss_mask_4: 0.2127  loss_dice_4: 0.1534  loss_ce_5: 0.1006  loss_mask_5: 0.2085  loss_dice_5: 0.1577  loss_ce_6: 0.1007  loss_mask_6: 0.2145  loss_dice_6: 0.1464  loss_ce_7: 0.1007  loss_mask_7: 0.2088  loss_dice_7: 0.1479  loss_ce_8: 0.101  loss_mask_8: 0.2125  loss_dice_8: 0.1539  time: 0.5797  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:01:23] d2.utils.events INFO:  eta: 1:25:07  iter: 20919  total_loss: 4.36  loss_ce: 0.1008  loss_mask: 0.1544  loss_dice: 0.1434  loss_ce_0: 0.05278  loss_mask_0: 0.1566  loss_dice_0: 0.1432  loss_ce_1: 0.1004  loss_mask_1: 0.1583  loss_dice_1: 0.1343  loss_ce_2: 0.1004  loss_mask_2: 0.1589  loss_dice_2: 0.1392  loss_ce_3: 0.1003  loss_mask_3: 0.1583  loss_dice_3: 0.1398  loss_ce_4: 0.1004  loss_mask_4: 0.1531  loss_dice_4: 0.1397  loss_ce_5: 0.1003  loss_mask_5: 0.1544  loss_dice_5: 0.1361  loss_ce_6: 0.1005  loss_mask_6: 0.1569  loss_dice_6: 0.1393  loss_ce_7: 0.1005  loss_mask_7: 0.1543  loss_dice_7: 0.1346  loss_ce_8: 0.1007  loss_mask_8: 0.1569  loss_dice_8: 0.1364  time: 0.5796  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:01:34] d2.utils.events INFO:  eta: 1:24:47  iter: 20939  total_loss: 4.543  loss_ce: 0.1295  loss_mask: 0.1577  loss_dice: 0.1651  loss_ce_0: 0.06365  loss_mask_0: 0.1496  loss_dice_0: 0.1559  loss_ce_1: 0.1297  loss_mask_1: 0.1595  loss_dice_1: 0.1662  loss_ce_2: 0.1298  loss_mask_2: 0.162  loss_dice_2: 0.1669  loss_ce_3: 0.1297  loss_mask_3: 0.1472  loss_dice_3: 0.155  loss_ce_4: 0.1297  loss_mask_4: 0.1552  loss_dice_4: 0.1666  loss_ce_5: 0.1297  loss_mask_5: 0.1646  loss_dice_5: 0.1626  loss_ce_6: 0.1297  loss_mask_6: 0.1602  loss_dice_6: 0.1655  loss_ce_7: 0.1295  loss_mask_7: 0.1533  loss_dice_7: 0.1623  loss_ce_8: 0.1296  loss_mask_8: 0.1607  loss_dice_8: 0.1568  time: 0.5795  data_time: 0.0027  lr: 1e-06  max_mem: 2811M
[07/11 15:01:44] d2.utils.events INFO:  eta: 1:24:20  iter: 20959  total_loss: 4.558  loss_ce: 0.1008  loss_mask: 0.1892  loss_dice: 0.1419  loss_ce_0: 0.05276  loss_mask_0: 0.1872  loss_dice_0: 0.1388  loss_ce_1: 0.1005  loss_mask_1: 0.1794  loss_dice_1: 0.133  loss_ce_2: 0.1004  loss_mask_2: 0.1889  loss_dice_2: 0.1369  loss_ce_3: 0.1005  loss_mask_3: 0.1836  loss_dice_3: 0.1363  loss_ce_4: 0.1006  loss_mask_4: 0.1887  loss_dice_4: 0.1414  loss_ce_5: 0.1004  loss_mask_5: 0.1831  loss_dice_5: 0.1378  loss_ce_6: 0.1006  loss_mask_6: 0.1812  loss_dice_6: 0.1414  loss_ce_7: 0.1005  loss_mask_7: 0.1934  loss_dice_7: 0.1402  loss_ce_8: 0.1009  loss_mask_8: 0.1892  loss_dice_8: 0.1378  time: 0.5795  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:01:54] d2.utils.events INFO:  eta: 1:23:53  iter: 20979  total_loss: 4.699  loss_ce: 0.1578  loss_mask: 0.1955  loss_dice: 0.1335  loss_ce_0: 0.07452  loss_mask_0: 0.1912  loss_dice_0: 0.131  loss_ce_1: 0.158  loss_mask_1: 0.1882  loss_dice_1: 0.1357  loss_ce_2: 0.1582  loss_mask_2: 0.1922  loss_dice_2: 0.1364  loss_ce_3: 0.1584  loss_mask_3: 0.1867  loss_dice_3: 0.1332  loss_ce_4: 0.1582  loss_mask_4: 0.1926  loss_dice_4: 0.1311  loss_ce_5: 0.1584  loss_mask_5: 0.1866  loss_dice_5: 0.1337  loss_ce_6: 0.158  loss_mask_6: 0.1858  loss_dice_6: 0.1333  loss_ce_7: 0.1583  loss_mask_7: 0.1952  loss_dice_7: 0.1376  loss_ce_8: 0.1578  loss_mask_8: 0.1897  loss_dice_8: 0.1341  time: 0.5794  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:02:04] d2.utils.events INFO:  eta: 1:23:30  iter: 20999  total_loss: 4.61  loss_ce: 0.1578  loss_mask: 0.1808  loss_dice: 0.1425  loss_ce_0: 0.07448  loss_mask_0: 0.1814  loss_dice_0: 0.1424  loss_ce_1: 0.1586  loss_mask_1: 0.1776  loss_dice_1: 0.1444  loss_ce_2: 0.1585  loss_mask_2: 0.1788  loss_dice_2: 0.1496  loss_ce_3: 0.1582  loss_mask_3: 0.1797  loss_dice_3: 0.1453  loss_ce_4: 0.1583  loss_mask_4: 0.1801  loss_dice_4: 0.1469  loss_ce_5: 0.1584  loss_mask_5: 0.1687  loss_dice_5: 0.142  loss_ce_6: 0.1582  loss_mask_6: 0.1727  loss_dice_6: 0.1449  loss_ce_7: 0.1582  loss_mask_7: 0.1695  loss_dice_7: 0.1521  loss_ce_8: 0.158  loss_mask_8: 0.1712  loss_dice_8: 0.1517  time: 0.5793  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 15:02:14] d2.utils.events INFO:  eta: 1:23:16  iter: 21019  total_loss: 4.874  loss_ce: 0.1011  loss_mask: 0.2133  loss_dice: 0.15  loss_ce_0: 0.05281  loss_mask_0: 0.219  loss_dice_0: 0.1508  loss_ce_1: 0.1009  loss_mask_1: 0.2204  loss_dice_1: 0.1532  loss_ce_2: 0.1008  loss_mask_2: 0.2235  loss_dice_2: 0.152  loss_ce_3: 0.1007  loss_mask_3: 0.2087  loss_dice_3: 0.1501  loss_ce_4: 0.1008  loss_mask_4: 0.2143  loss_dice_4: 0.1479  loss_ce_5: 0.1007  loss_mask_5: 0.215  loss_dice_5: 0.1529  loss_ce_6: 0.1008  loss_mask_6: 0.2199  loss_dice_6: 0.1499  loss_ce_7: 0.1008  loss_mask_7: 0.2209  loss_dice_7: 0.1463  loss_ce_8: 0.101  loss_mask_8: 0.2173  loss_dice_8: 0.1464  time: 0.5793  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:02:24] d2.utils.events INFO:  eta: 1:22:56  iter: 21039  total_loss: 4.253  loss_ce: 0.1576  loss_mask: 0.1739  loss_dice: 0.1464  loss_ce_0: 0.07447  loss_mask_0: 0.181  loss_dice_0: 0.1436  loss_ce_1: 0.1577  loss_mask_1: 0.1771  loss_dice_1: 0.1378  loss_ce_2: 0.1579  loss_mask_2: 0.1709  loss_dice_2: 0.1456  loss_ce_3: 0.158  loss_mask_3: 0.164  loss_dice_3: 0.1377  loss_ce_4: 0.1579  loss_mask_4: 0.1731  loss_dice_4: 0.143  loss_ce_5: 0.1581  loss_mask_5: 0.1736  loss_dice_5: 0.138  loss_ce_6: 0.1579  loss_mask_6: 0.1687  loss_dice_6: 0.1398  loss_ce_7: 0.158  loss_mask_7: 0.1727  loss_dice_7: 0.1441  loss_ce_8: 0.1577  loss_mask_8: 0.177  loss_dice_8: 0.1432  time: 0.5792  data_time: 0.0024  lr: 1e-06  max_mem: 2811M
[07/11 15:02:35] d2.utils.events INFO:  eta: 1:22:38  iter: 21059  total_loss: 4.608  loss_ce: 0.1012  loss_mask: 0.1891  loss_dice: 0.1317  loss_ce_0: 0.05282  loss_mask_0: 0.194  loss_dice_0: 0.1367  loss_ce_1: 0.101  loss_mask_1: 0.1936  loss_dice_1: 0.1379  loss_ce_2: 0.1009  loss_mask_2: 0.1986  loss_dice_2: 0.1407  loss_ce_3: 0.1009  loss_mask_3: 0.1915  loss_dice_3: 0.1385  loss_ce_4: 0.101  loss_mask_4: 0.1909  loss_dice_4: 0.1421  loss_ce_5: 0.1008  loss_mask_5: 0.1947  loss_dice_5: 0.1397  loss_ce_6: 0.101  loss_mask_6: 0.195  loss_dice_6: 0.1403  loss_ce_7: 0.101  loss_mask_7: 0.19  loss_dice_7: 0.1384  loss_ce_8: 0.1012  loss_mask_8: 0.1924  loss_dice_8: 0.1371  time: 0.5791  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:02:45] d2.utils.events INFO:  eta: 1:22:15  iter: 21079  total_loss: 4.396  loss_ce: 0.1292  loss_mask: 0.1791  loss_dice: 0.1232  loss_ce_0: 0.06364  loss_mask_0: 0.1768  loss_dice_0: 0.1206  loss_ce_1: 0.1287  loss_mask_1: 0.1827  loss_dice_1: 0.1276  loss_ce_2: 0.1291  loss_mask_2: 0.1885  loss_dice_2: 0.1283  loss_ce_3: 0.1292  loss_mask_3: 0.1764  loss_dice_3: 0.1245  loss_ce_4: 0.1291  loss_mask_4: 0.1797  loss_dice_4: 0.1291  loss_ce_5: 0.1293  loss_mask_5: 0.1776  loss_dice_5: 0.117  loss_ce_6: 0.1292  loss_mask_6: 0.1775  loss_dice_6: 0.1218  loss_ce_7: 0.1292  loss_mask_7: 0.1818  loss_dice_7: 0.1207  loss_ce_8: 0.1292  loss_mask_8: 0.1762  loss_dice_8: 0.1233  time: 0.5791  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 15:02:55] d2.utils.events INFO:  eta: 1:22:02  iter: 21099  total_loss: 4.752  loss_ce: 0.1011  loss_mask: 0.1886  loss_dice: 0.1365  loss_ce_0: 0.05285  loss_mask_0: 0.1765  loss_dice_0: 0.1409  loss_ce_1: 0.1012  loss_mask_1: 0.1851  loss_dice_1: 0.1396  loss_ce_2: 0.101  loss_mask_2: 0.1901  loss_dice_2: 0.1394  loss_ce_3: 0.1009  loss_mask_3: 0.1779  loss_dice_3: 0.1404  loss_ce_4: 0.1011  loss_mask_4: 0.1845  loss_dice_4: 0.14  loss_ce_5: 0.1009  loss_mask_5: 0.1798  loss_dice_5: 0.1418  loss_ce_6: 0.1011  loss_mask_6: 0.1852  loss_dice_6: 0.1372  loss_ce_7: 0.1009  loss_mask_7: 0.1902  loss_dice_7: 0.1386  loss_ce_8: 0.1012  loss_mask_8: 0.1791  loss_dice_8: 0.1385  time: 0.5790  data_time: 0.0036  lr: 1e-06  max_mem: 2811M
[07/11 15:03:06] d2.utils.events INFO:  eta: 1:21:45  iter: 21119  total_loss: 4.57  loss_ce: 0.1009  loss_mask: 0.1616  loss_dice: 0.1511  loss_ce_0: 0.05281  loss_mask_0: 0.1647  loss_dice_0: 0.1514  loss_ce_1: 0.1005  loss_mask_1: 0.1618  loss_dice_1: 0.1529  loss_ce_2: 0.1005  loss_mask_2: 0.178  loss_dice_2: 0.1524  loss_ce_3: 0.1005  loss_mask_3: 0.1697  loss_dice_3: 0.1503  loss_ce_4: 0.1007  loss_mask_4: 0.1675  loss_dice_4: 0.1412  loss_ce_5: 0.1005  loss_mask_5: 0.1727  loss_dice_5: 0.1556  loss_ce_6: 0.1006  loss_mask_6: 0.1663  loss_dice_6: 0.1503  loss_ce_7: 0.1007  loss_mask_7: 0.1686  loss_dice_7: 0.1569  loss_ce_8: 0.1009  loss_mask_8: 0.1696  loss_dice_8: 0.1545  time: 0.5790  data_time: 0.0023  lr: 1e-06  max_mem: 2811M
[07/11 15:03:16] d2.utils.events INFO:  eta: 1:21:23  iter: 21139  total_loss: 4.397  loss_ce: 0.101  loss_mask: 0.1537  loss_dice: 0.1507  loss_ce_0: 0.05278  loss_mask_0: 0.1519  loss_dice_0: 0.1491  loss_ce_1: 0.101  loss_mask_1: 0.1503  loss_dice_1: 0.1501  loss_ce_2: 0.1008  loss_mask_2: 0.1543  loss_dice_2: 0.1532  loss_ce_3: 0.1007  loss_mask_3: 0.1488  loss_dice_3: 0.1482  loss_ce_4: 0.1008  loss_mask_4: 0.1548  loss_dice_4: 0.1554  loss_ce_5: 0.1007  loss_mask_5: 0.1532  loss_dice_5: 0.1449  loss_ce_6: 0.1008  loss_mask_6: 0.1476  loss_dice_6: 0.1439  loss_ce_7: 0.1007  loss_mask_7: 0.1514  loss_dice_7: 0.148  loss_ce_8: 0.101  loss_mask_8: 0.1436  loss_dice_8: 0.1479  time: 0.5789  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 15:03:27] d2.utils.events INFO:  eta: 1:21:02  iter: 21159  total_loss: 4.698  loss_ce: 0.1007  loss_mask: 0.1744  loss_dice: 0.1304  loss_ce_0: 0.05276  loss_mask_0: 0.1869  loss_dice_0: 0.1381  loss_ce_1: 0.1005  loss_mask_1: 0.1716  loss_dice_1: 0.1281  loss_ce_2: 0.1005  loss_mask_2: 0.1752  loss_dice_2: 0.1317  loss_ce_3: 0.1004  loss_mask_3: 0.1797  loss_dice_3: 0.1344  loss_ce_4: 0.1005  loss_mask_4: 0.1735  loss_dice_4: 0.1299  loss_ce_5: 0.1005  loss_mask_5: 0.1736  loss_dice_5: 0.1291  loss_ce_6: 0.1005  loss_mask_6: 0.1802  loss_dice_6: 0.1271  loss_ce_7: 0.1005  loss_mask_7: 0.1752  loss_dice_7: 0.1283  loss_ce_8: 0.1011  loss_mask_8: 0.1746  loss_dice_8: 0.1343  time: 0.5788  data_time: 0.0029  lr: 1e-06  max_mem: 2811M
[07/11 15:03:37] d2.utils.events INFO:  eta: 1:20:37  iter: 21179  total_loss: 4.544  loss_ce: 0.1007  loss_mask: 0.1447  loss_dice: 0.1518  loss_ce_0: 0.05278  loss_mask_0: 0.1366  loss_dice_0: 0.1539  loss_ce_1: 0.1003  loss_mask_1: 0.1335  loss_dice_1: 0.154  loss_ce_2: 0.1004  loss_mask_2: 0.1374  loss_dice_2: 0.155  loss_ce_3: 0.1005  loss_mask_3: 0.1375  loss_dice_3: 0.1537  loss_ce_4: 0.1005  loss_mask_4: 0.1318  loss_dice_4: 0.1543  loss_ce_5: 0.1003  loss_mask_5: 0.1321  loss_dice_5: 0.1565  loss_ce_6: 0.1005  loss_mask_6: 0.1387  loss_dice_6: 0.1554  loss_ce_7: 0.1005  loss_mask_7: 0.1406  loss_dice_7: 0.1583  loss_ce_8: 0.1006  loss_mask_8: 0.1337  loss_dice_8: 0.1544  time: 0.5788  data_time: 0.0024  lr: 1e-06  max_mem: 2811M
[07/11 15:03:47] d2.utils.events INFO:  eta: 1:20:11  iter: 21199  total_loss: 4.746  loss_ce: 0.1581  loss_mask: 0.2133  loss_dice: 0.1409  loss_ce_0: 0.07451  loss_mask_0: 0.2106  loss_dice_0: 0.1367  loss_ce_1: 0.1584  loss_mask_1: 0.2118  loss_dice_1: 0.1424  loss_ce_2: 0.1585  loss_mask_2: 0.2168  loss_dice_2: 0.1414  loss_ce_3: 0.1586  loss_mask_3: 0.2189  loss_dice_3: 0.1375  loss_ce_4: 0.1582  loss_mask_4: 0.2185  loss_dice_4: 0.1403  loss_ce_5: 0.1586  loss_mask_5: 0.2077  loss_dice_5: 0.1416  loss_ce_6: 0.1584  loss_mask_6: 0.2039  loss_dice_6: 0.1358  loss_ce_7: 0.1584  loss_mask_7: 0.2082  loss_dice_7: 0.1373  loss_ce_8: 0.1579  loss_mask_8: 0.2099  loss_dice_8: 0.139  time: 0.5787  data_time: 0.0025  lr: 1e-06  max_mem: 2811M
[07/11 15:03:57] d2.utils.events INFO:  eta: 1:19:46  iter: 21219  total_loss: 4.302  loss_ce: 0.1295  loss_mask: 0.1817  loss_dice: 0.1358  loss_ce_0: 0.06364  loss_mask_0: 0.1713  loss_dice_0: 0.1356  loss_ce_1: 0.1296  loss_mask_1: 0.164  loss_dice_1: 0.1344  loss_ce_2: 0.1295  loss_mask_2: 0.1642  loss_dice_2: 0.1318  loss_ce_3: 0.1295  loss_mask_3: 0.1696  loss_dice_3: 0.1376  loss_ce_4: 0.1296  loss_mask_4: 0.1764  loss_dice_4: 0.1291  loss_ce_5: 0.1297  loss_mask_5: 0.1737  loss_dice_5: 0.1323  loss_ce_6: 0.1297  loss_mask_6: 0.1753  loss_dice_6: 0.1348  loss_ce_7: 0.1295  loss_mask_7: 0.1729  loss_dice_7: 0.1349  loss_ce_8: 0.1292  loss_mask_8: 0.172  loss_dice_8: 0.1366  time: 0.5787  data_time: 0.0035  lr: 1e-06  max_mem: 2811M
[07/11 15:04:08] d2.utils.events INFO:  eta: 1:19:23  iter: 21239  total_loss: 4.416  loss_ce: 0.1292  loss_mask: 0.1604  loss_dice: 0.1351  loss_ce_0: 0.06364  loss_mask_0: 0.1581  loss_dice_0: 0.1344  loss_ce_1: 0.1286  loss_mask_1: 0.1557  loss_dice_1: 0.1337  loss_ce_2: 0.129  loss_mask_2: 0.1638  loss_dice_2: 0.142  loss_ce_3: 0.1291  loss_mask_3: 0.1639  loss_dice_3: 0.1351  loss_ce_4: 0.129  loss_mask_4: 0.1682  loss_dice_4: 0.1403  loss_ce_5: 0.1291  loss_mask_5: 0.1683  loss_dice_5: 0.1373  loss_ce_6: 0.129  loss_mask_6: 0.1608  loss_dice_6: 0.1353  loss_ce_7: 0.1293  loss_mask_7: 0.1636  loss_dice_7: 0.1335  loss_ce_8: 0.129  loss_mask_8: 0.1639  loss_dice_8: 0.1322  time: 0.5786  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 15:04:18] d2.utils.events INFO:  eta: 1:18:43  iter: 21259  total_loss: 4.747  loss_ce: 0.1578  loss_mask: 0.1581  loss_dice: 0.1611  loss_ce_0: 0.07446  loss_mask_0: 0.168  loss_dice_0: 0.1517  loss_ce_1: 0.1583  loss_mask_1: 0.1597  loss_dice_1: 0.1495  loss_ce_2: 0.1582  loss_mask_2: 0.1575  loss_dice_2: 0.1512  loss_ce_3: 0.1582  loss_mask_3: 0.1493  loss_dice_3: 0.1535  loss_ce_4: 0.1582  loss_mask_4: 0.1679  loss_dice_4: 0.1545  loss_ce_5: 0.1582  loss_mask_5: 0.1602  loss_dice_5: 0.1519  loss_ce_6: 0.1581  loss_mask_6: 0.1602  loss_dice_6: 0.148  loss_ce_7: 0.1582  loss_mask_7: 0.1619  loss_dice_7: 0.1506  loss_ce_8: 0.1579  loss_mask_8: 0.1571  loss_dice_8: 0.1591  time: 0.5785  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:04:28] d2.utils.events INFO:  eta: 1:18:24  iter: 21279  total_loss: 4.634  loss_ce: 0.101  loss_mask: 0.2059  loss_dice: 0.1529  loss_ce_0: 0.05284  loss_mask_0: 0.2108  loss_dice_0: 0.1522  loss_ce_1: 0.1007  loss_mask_1: 0.2035  loss_dice_1: 0.1525  loss_ce_2: 0.1007  loss_mask_2: 0.2122  loss_dice_2: 0.1563  loss_ce_3: 0.1008  loss_mask_3: 0.2085  loss_dice_3: 0.1547  loss_ce_4: 0.1008  loss_mask_4: 0.2133  loss_dice_4: 0.1576  loss_ce_5: 0.1006  loss_mask_5: 0.2113  loss_dice_5: 0.1565  loss_ce_6: 0.1008  loss_mask_6: 0.2047  loss_dice_6: 0.1508  loss_ce_7: 0.1008  loss_mask_7: 0.2168  loss_dice_7: 0.1529  loss_ce_8: 0.1009  loss_mask_8: 0.21  loss_dice_8: 0.1532  time: 0.5785  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:04:38] d2.utils.events INFO:  eta: 1:17:58  iter: 21299  total_loss: 4.363  loss_ce: 0.1011  loss_mask: 0.1801  loss_dice: 0.1408  loss_ce_0: 0.05282  loss_mask_0: 0.1856  loss_dice_0: 0.1399  loss_ce_1: 0.1013  loss_mask_1: 0.174  loss_dice_1: 0.1399  loss_ce_2: 0.1011  loss_mask_2: 0.1817  loss_dice_2: 0.1429  loss_ce_3: 0.1009  loss_mask_3: 0.1786  loss_dice_3: 0.1417  loss_ce_4: 0.101  loss_mask_4: 0.1755  loss_dice_4: 0.1401  loss_ce_5: 0.1008  loss_mask_5: 0.1789  loss_dice_5: 0.1391  loss_ce_6: 0.101  loss_mask_6: 0.1807  loss_dice_6: 0.1423  loss_ce_7: 0.1009  loss_mask_7: 0.1859  loss_dice_7: 0.1436  loss_ce_8: 0.1011  loss_mask_8: 0.1833  loss_dice_8: 0.1397  time: 0.5784  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 15:04:49] d2.utils.events INFO:  eta: 1:17:39  iter: 21319  total_loss: 4.886  loss_ce: 0.1011  loss_mask: 0.1989  loss_dice: 0.147  loss_ce_0: 0.05282  loss_mask_0: 0.1986  loss_dice_0: 0.1478  loss_ce_1: 0.1013  loss_mask_1: 0.1996  loss_dice_1: 0.1427  loss_ce_2: 0.101  loss_mask_2: 0.1893  loss_dice_2: 0.1462  loss_ce_3: 0.1008  loss_mask_3: 0.1909  loss_dice_3: 0.1483  loss_ce_4: 0.101  loss_mask_4: 0.1942  loss_dice_4: 0.1487  loss_ce_5: 0.1008  loss_mask_5: 0.1894  loss_dice_5: 0.1459  loss_ce_6: 0.101  loss_mask_6: 0.1944  loss_dice_6: 0.1453  loss_ce_7: 0.1009  loss_mask_7: 0.1921  loss_dice_7: 0.1445  loss_ce_8: 0.1012  loss_mask_8: 0.1933  loss_dice_8: 0.1445  time: 0.5783  data_time: 0.0026  lr: 1e-06  max_mem: 2811M
[07/11 15:04:59] d2.utils.events INFO:  eta: 1:17:22  iter: 21339  total_loss: 4.214  loss_ce: 0.1011  loss_mask: 0.1751  loss_dice: 0.1219  loss_ce_0: 0.05282  loss_mask_0: 0.1633  loss_dice_0: 0.1189  loss_ce_1: 0.1011  loss_mask_1: 0.1762  loss_dice_1: 0.1176  loss_ce_2: 0.1009  loss_mask_2: 0.1782  loss_dice_2: 0.1216  loss_ce_3: 0.1009  loss_mask_3: 0.1724  loss_dice_3: 0.1198  loss_ce_4: 0.101  loss_mask_4: 0.1667  loss_dice_4: 0.1167  loss_ce_5: 0.1008  loss_mask_5: 0.171  loss_dice_5: 0.1179  loss_ce_6: 0.101  loss_mask_6: 0.1688  loss_dice_6: 0.1164  loss_ce_7: 0.1008  loss_mask_7: 0.1731  loss_dice_7: 0.1162  loss_ce_8: 0.1011  loss_mask_8: 0.1709  loss_dice_8: 0.1208  time: 0.5783  data_time: 0.0047  lr: 1e-06  max_mem: 2811M
[07/11 15:05:09] d2.utils.events INFO:  eta: 1:16:57  iter: 21359  total_loss: 4.187  loss_ce: 0.1009  loss_mask: 0.17  loss_dice: 0.1223  loss_ce_0: 0.0528  loss_mask_0: 0.1723  loss_dice_0: 0.1256  loss_ce_1: 0.1009  loss_mask_1: 0.1659  loss_dice_1: 0.1222  loss_ce_2: 0.1008  loss_mask_2: 0.1714  loss_dice_2: 0.1222  loss_ce_3: 0.1007  loss_mask_3: 0.177  loss_dice_3: 0.1215  loss_ce_4: 0.1008  loss_mask_4: 0.1755  loss_dice_4: 0.1232  loss_ce_5: 0.1007  loss_mask_5: 0.1738  loss_dice_5: 0.1201  loss_ce_6: 0.1008  loss_mask_6: 0.1669  loss_dice_6: 0.1248  loss_ce_7: 0.1007  loss_mask_7: 0.1642  loss_dice_7: 0.1223  loss_ce_8: 0.101  loss_mask_8: 0.1761  loss_dice_8: 0.1233  time: 0.5782  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 15:05:20] d2.utils.events INFO:  eta: 1:16:34  iter: 21379  total_loss: 4.329  loss_ce: 0.1578  loss_mask: 0.162  loss_dice: 0.1398  loss_ce_0: 0.07445  loss_mask_0: 0.1743  loss_dice_0: 0.1495  loss_ce_1: 0.1578  loss_mask_1: 0.1622  loss_dice_1: 0.139  loss_ce_2: 0.158  loss_mask_2: 0.1664  loss_dice_2: 0.1433  loss_ce_3: 0.158  loss_mask_3: 0.1645  loss_dice_3: 0.1348  loss_ce_4: 0.158  loss_mask_4: 0.1668  loss_dice_4: 0.1421  loss_ce_5: 0.158  loss_mask_5: 0.1667  loss_dice_5: 0.1401  loss_ce_6: 0.1579  loss_mask_6: 0.1633  loss_dice_6: 0.1364  loss_ce_7: 0.1581  loss_mask_7: 0.1689  loss_dice_7: 0.1435  loss_ce_8: 0.1579  loss_mask_8: 0.1691  loss_dice_8: 0.1397  time: 0.5782  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:05:30] d2.utils.events INFO:  eta: 1:16:04  iter: 21399  total_loss: 4.456  loss_ce: 0.1012  loss_mask: 0.2042  loss_dice: 0.1318  loss_ce_0: 0.05282  loss_mask_0: 0.2  loss_dice_0: 0.1351  loss_ce_1: 0.1013  loss_mask_1: 0.1965  loss_dice_1: 0.1317  loss_ce_2: 0.1011  loss_mask_2: 0.2028  loss_dice_2: 0.1334  loss_ce_3: 0.101  loss_mask_3: 0.1938  loss_dice_3: 0.1305  loss_ce_4: 0.1011  loss_mask_4: 0.1954  loss_dice_4: 0.1321  loss_ce_5: 0.1011  loss_mask_5: 0.2005  loss_dice_5: 0.1374  loss_ce_6: 0.1012  loss_mask_6: 0.1951  loss_dice_6: 0.1303  loss_ce_7: 0.101  loss_mask_7: 0.1934  loss_dice_7: 0.1296  loss_ce_8: 0.1012  loss_mask_8: 0.1994  loss_dice_8: 0.1302  time: 0.5781  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:05:40] d2.utils.events INFO:  eta: 1:15:40  iter: 21419  total_loss: 4.368  loss_ce: 0.1578  loss_mask: 0.1978  loss_dice: 0.1418  loss_ce_0: 0.07444  loss_mask_0: 0.201  loss_dice_0: 0.131  loss_ce_1: 0.158  loss_mask_1: 0.2009  loss_dice_1: 0.1381  loss_ce_2: 0.1579  loss_mask_2: 0.2035  loss_dice_2: 0.1427  loss_ce_3: 0.158  loss_mask_3: 0.2006  loss_dice_3: 0.1384  loss_ce_4: 0.158  loss_mask_4: 0.2068  loss_dice_4: 0.1398  loss_ce_5: 0.158  loss_mask_5: 0.2128  loss_dice_5: 0.1418  loss_ce_6: 0.1579  loss_mask_6: 0.1995  loss_dice_6: 0.1379  loss_ce_7: 0.1581  loss_mask_7: 0.2083  loss_dice_7: 0.1393  loss_ce_8: 0.1578  loss_mask_8: 0.1938  loss_dice_8: 0.1388  time: 0.5780  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:05:51] d2.utils.events INFO:  eta: 1:15:14  iter: 21439  total_loss: 4.611  loss_ce: 0.1011  loss_mask: 0.09661  loss_dice: 0.2026  loss_ce_0: 0.05285  loss_mask_0: 0.105  loss_dice_0: 0.1983  loss_ce_1: 0.1013  loss_mask_1: 0.1047  loss_dice_1: 0.203  loss_ce_2: 0.1009  loss_mask_2: 0.1057  loss_dice_2: 0.1984  loss_ce_3: 0.1009  loss_mask_3: 0.1031  loss_dice_3: 0.2049  loss_ce_4: 0.101  loss_mask_4: 0.105  loss_dice_4: 0.1991  loss_ce_5: 0.1009  loss_mask_5: 0.1028  loss_dice_5: 0.191  loss_ce_6: 0.101  loss_mask_6: 0.1038  loss_dice_6: 0.2019  loss_ce_7: 0.101  loss_mask_7: 0.1135  loss_dice_7: 0.2038  loss_ce_8: 0.1012  loss_mask_8: 0.1056  loss_dice_8: 0.1943  time: 0.5780  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:06:01] d2.utils.events INFO:  eta: 1:14:51  iter: 21459  total_loss: 4.972  loss_ce: 0.101  loss_mask: 0.2226  loss_dice: 0.1659  loss_ce_0: 0.05283  loss_mask_0: 0.2218  loss_dice_0: 0.1649  loss_ce_1: 0.1008  loss_mask_1: 0.2215  loss_dice_1: 0.1666  loss_ce_2: 0.1007  loss_mask_2: 0.2188  loss_dice_2: 0.155  loss_ce_3: 0.1008  loss_mask_3: 0.2247  loss_dice_3: 0.1634  loss_ce_4: 0.1008  loss_mask_4: 0.2223  loss_dice_4: 0.164  loss_ce_5: 0.1006  loss_mask_5: 0.2274  loss_dice_5: 0.1624  loss_ce_6: 0.1008  loss_mask_6: 0.2285  loss_dice_6: 0.1639  loss_ce_7: 0.1008  loss_mask_7: 0.2154  loss_dice_7: 0.1626  loss_ce_8: 0.101  loss_mask_8: 0.2143  loss_dice_8: 0.1638  time: 0.5779  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:06:11] d2.utils.events INFO:  eta: 1:14:29  iter: 21479  total_loss: 4.414  loss_ce: 0.1008  loss_mask: 0.1884  loss_dice: 0.1274  loss_ce_0: 0.05281  loss_mask_0: 0.201  loss_dice_0: 0.1289  loss_ce_1: 0.1007  loss_mask_1: 0.1939  loss_dice_1: 0.1285  loss_ce_2: 0.1006  loss_mask_2: 0.1935  loss_dice_2: 0.1314  loss_ce_3: 0.1005  loss_mask_3: 0.192  loss_dice_3: 0.1303  loss_ce_4: 0.1007  loss_mask_4: 0.1901  loss_dice_4: 0.1286  loss_ce_5: 0.1005  loss_mask_5: 0.1977  loss_dice_5: 0.1345  loss_ce_6: 0.1006  loss_mask_6: 0.1985  loss_dice_6: 0.13  loss_ce_7: 0.1005  loss_mask_7: 0.1902  loss_dice_7: 0.1288  loss_ce_8: 0.1008  loss_mask_8: 0.2003  loss_dice_8: 0.1311  time: 0.5779  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 15:06:22] d2.utils.events INFO:  eta: 1:14:05  iter: 21499  total_loss: 5.037  loss_ce: 0.1006  loss_mask: 0.2316  loss_dice: 0.1378  loss_ce_0: 0.05276  loss_mask_0: 0.2354  loss_dice_0: 0.1411  loss_ce_1: 0.1002  loss_mask_1: 0.2323  loss_dice_1: 0.1393  loss_ce_2: 0.1004  loss_mask_2: 0.2213  loss_dice_2: 0.1451  loss_ce_3: 0.1002  loss_mask_3: 0.2301  loss_dice_3: 0.1425  loss_ce_4: 0.1004  loss_mask_4: 0.2277  loss_dice_4: 0.1428  loss_ce_5: 0.1003  loss_mask_5: 0.2336  loss_dice_5: 0.1475  loss_ce_6: 0.1004  loss_mask_6: 0.2304  loss_dice_6: 0.1477  loss_ce_7: 0.1004  loss_mask_7: 0.228  loss_dice_7: 0.141  loss_ce_8: 0.1006  loss_mask_8: 0.2287  loss_dice_8: 0.1393  time: 0.5778  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:06:32] d2.utils.events INFO:  eta: 1:13:35  iter: 21519  total_loss: 4.746  loss_ce: 0.1006  loss_mask: 0.215  loss_dice: 0.142  loss_ce_0: 0.0527  loss_mask_0: 0.2233  loss_dice_0: 0.1394  loss_ce_1: 0.1006  loss_mask_1: 0.2074  loss_dice_1: 0.1393  loss_ce_2: 0.1004  loss_mask_2: 0.2129  loss_dice_2: 0.1436  loss_ce_3: 0.1003  loss_mask_3: 0.2207  loss_dice_3: 0.142  loss_ce_4: 0.1004  loss_mask_4: 0.2136  loss_dice_4: 0.1433  loss_ce_5: 0.1002  loss_mask_5: 0.2178  loss_dice_5: 0.1367  loss_ce_6: 0.1003  loss_mask_6: 0.2229  loss_dice_6: 0.1433  loss_ce_7: 0.1003  loss_mask_7: 0.2136  loss_dice_7: 0.1393  loss_ce_8: 0.1005  loss_mask_8: 0.218  loss_dice_8: 0.1443  time: 0.5778  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:06:42] d2.utils.events INFO:  eta: 1:13:04  iter: 21539  total_loss: 4.716  loss_ce: 0.1586  loss_mask: 0.2034  loss_dice: 0.1271  loss_ce_0: 0.07467  loss_mask_0: 0.2041  loss_dice_0: 0.128  loss_ce_1: 0.1588  loss_mask_1: 0.2065  loss_dice_1: 0.1237  loss_ce_2: 0.1589  loss_mask_2: 0.2122  loss_dice_2: 0.127  loss_ce_3: 0.159  loss_mask_3: 0.2128  loss_dice_3: 0.1243  loss_ce_4: 0.1589  loss_mask_4: 0.21  loss_dice_4: 0.1276  loss_ce_5: 0.1591  loss_mask_5: 0.2089  loss_dice_5: 0.1254  loss_ce_6: 0.1588  loss_mask_6: 0.1984  loss_dice_6: 0.1248  loss_ce_7: 0.1588  loss_mask_7: 0.2033  loss_dice_7: 0.1237  loss_ce_8: 0.1582  loss_mask_8: 0.2073  loss_dice_8: 0.1289  time: 0.5777  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:06:53] d2.utils.events INFO:  eta: 1:12:40  iter: 21559  total_loss: 4.607  loss_ce: 0.1586  loss_mask: 0.1318  loss_dice: 0.1497  loss_ce_0: 0.07461  loss_mask_0: 0.15  loss_dice_0: 0.1575  loss_ce_1: 0.1588  loss_mask_1: 0.1411  loss_dice_1: 0.153  loss_ce_2: 0.159  loss_mask_2: 0.1338  loss_dice_2: 0.149  loss_ce_3: 0.1588  loss_mask_3: 0.1392  loss_dice_3: 0.1477  loss_ce_4: 0.1588  loss_mask_4: 0.1411  loss_dice_4: 0.1615  loss_ce_5: 0.1589  loss_mask_5: 0.1341  loss_dice_5: 0.1513  loss_ce_6: 0.1587  loss_mask_6: 0.1312  loss_dice_6: 0.1545  loss_ce_7: 0.1588  loss_mask_7: 0.1386  loss_dice_7: 0.1623  loss_ce_8: 0.1585  loss_mask_8: 0.1425  loss_dice_8: 0.1517  time: 0.5776  data_time: 0.0025  lr: 1e-06  max_mem: 2811M
[07/11 15:07:03] d2.utils.events INFO:  eta: 1:12:14  iter: 21579  total_loss: 4.335  loss_ce: 0.1006  loss_mask: 0.1751  loss_dice: 0.1436  loss_ce_0: 0.05273  loss_mask_0: 0.1833  loss_dice_0: 0.1443  loss_ce_1: 0.1004  loss_mask_1: 0.1783  loss_dice_1: 0.1477  loss_ce_2: 0.1004  loss_mask_2: 0.174  loss_dice_2: 0.1403  loss_ce_3: 0.1003  loss_mask_3: 0.1787  loss_dice_3: 0.1399  loss_ce_4: 0.1004  loss_mask_4: 0.1838  loss_dice_4: 0.1413  loss_ce_5: 0.1003  loss_mask_5: 0.1748  loss_dice_5: 0.1447  loss_ce_6: 0.1005  loss_mask_6: 0.1813  loss_dice_6: 0.1413  loss_ce_7: 0.1004  loss_mask_7: 0.1845  loss_dice_7: 0.1452  loss_ce_8: 0.1005  loss_mask_8: 0.1791  loss_dice_8: 0.1416  time: 0.5776  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:07:13] d2.utils.events INFO:  eta: 1:11:59  iter: 21599  total_loss: 5  loss_ce: 0.1583  loss_mask: 0.156  loss_dice: 0.1465  loss_ce_0: 0.07454  loss_mask_0: 0.163  loss_dice_0: 0.1432  loss_ce_1: 0.1584  loss_mask_1: 0.1605  loss_dice_1: 0.1432  loss_ce_2: 0.1586  loss_mask_2: 0.1609  loss_dice_2: 0.1412  loss_ce_3: 0.1585  loss_mask_3: 0.1645  loss_dice_3: 0.1419  loss_ce_4: 0.1584  loss_mask_4: 0.1547  loss_dice_4: 0.1481  loss_ce_5: 0.1587  loss_mask_5: 0.166  loss_dice_5: 0.1459  loss_ce_6: 0.1585  loss_mask_6: 0.1611  loss_dice_6: 0.146  loss_ce_7: 0.1584  loss_mask_7: 0.1566  loss_dice_7: 0.1437  loss_ce_8: 0.1583  loss_mask_8: 0.1623  loss_dice_8: 0.1434  time: 0.5775  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:07:24] d2.utils.events INFO:  eta: 1:11:41  iter: 21619  total_loss: 4.439  loss_ce: 0.1007  loss_mask: 0.1822  loss_dice: 0.1411  loss_ce_0: 0.05277  loss_mask_0: 0.1902  loss_dice_0: 0.1432  loss_ce_1: 0.1006  loss_mask_1: 0.1812  loss_dice_1: 0.1386  loss_ce_2: 0.1005  loss_mask_2: 0.1888  loss_dice_2: 0.1483  loss_ce_3: 0.1005  loss_mask_3: 0.1921  loss_dice_3: 0.1431  loss_ce_4: 0.1005  loss_mask_4: 0.1866  loss_dice_4: 0.1419  loss_ce_5: 0.1004  loss_mask_5: 0.1856  loss_dice_5: 0.1374  loss_ce_6: 0.1005  loss_mask_6: 0.1943  loss_dice_6: 0.148  loss_ce_7: 0.1005  loss_mask_7: 0.1838  loss_dice_7: 0.1427  loss_ce_8: 0.1007  loss_mask_8: 0.1913  loss_dice_8: 0.1407  time: 0.5775  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:07:34] d2.utils.events INFO:  eta: 1:11:06  iter: 21639  total_loss: 4.736  loss_ce: 0.1294  loss_mask: 0.1578  loss_dice: 0.1433  loss_ce_0: 0.06366  loss_mask_0: 0.164  loss_dice_0: 0.1471  loss_ce_1: 0.1294  loss_mask_1: 0.1587  loss_dice_1: 0.1413  loss_ce_2: 0.1294  loss_mask_2: 0.161  loss_dice_2: 0.1424  loss_ce_3: 0.1294  loss_mask_3: 0.1689  loss_dice_3: 0.1445  loss_ce_4: 0.1294  loss_mask_4: 0.1585  loss_dice_4: 0.1477  loss_ce_5: 0.1294  loss_mask_5: 0.159  loss_dice_5: 0.1408  loss_ce_6: 0.1294  loss_mask_6: 0.1652  loss_dice_6: 0.1405  loss_ce_7: 0.1295  loss_mask_7: 0.1742  loss_dice_7: 0.1492  loss_ce_8: 0.1297  loss_mask_8: 0.1629  loss_dice_8: 0.1438  time: 0.5774  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:07:44] d2.utils.events INFO:  eta: 1:10:48  iter: 21659  total_loss: 4.658  loss_ce: 0.1008  loss_mask: 0.184  loss_dice: 0.1481  loss_ce_0: 0.05272  loss_mask_0: 0.1845  loss_dice_0: 0.1512  loss_ce_1: 0.1012  loss_mask_1: 0.1818  loss_dice_1: 0.156  loss_ce_2: 0.1008  loss_mask_2: 0.1695  loss_dice_2: 0.1461  loss_ce_3: 0.1007  loss_mask_3: 0.1863  loss_dice_3: 0.1512  loss_ce_4: 0.1007  loss_mask_4: 0.178  loss_dice_4: 0.1511  loss_ce_5: 0.1006  loss_mask_5: 0.1814  loss_dice_5: 0.1437  loss_ce_6: 0.1007  loss_mask_6: 0.1784  loss_dice_6: 0.1497  loss_ce_7: 0.1006  loss_mask_7: 0.1742  loss_dice_7: 0.1454  loss_ce_8: 0.1011  loss_mask_8: 0.1729  loss_dice_8: 0.1413  time: 0.5774  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 15:07:55] d2.utils.events INFO:  eta: 1:10:21  iter: 21679  total_loss: 4.647  loss_ce: 0.1008  loss_mask: 0.2105  loss_dice: 0.1387  loss_ce_0: 0.05274  loss_mask_0: 0.2154  loss_dice_0: 0.1385  loss_ce_1: 0.1011  loss_mask_1: 0.2056  loss_dice_1: 0.1411  loss_ce_2: 0.1007  loss_mask_2: 0.2122  loss_dice_2: 0.1374  loss_ce_3: 0.1006  loss_mask_3: 0.2132  loss_dice_3: 0.1449  loss_ce_4: 0.1007  loss_mask_4: 0.2099  loss_dice_4: 0.139  loss_ce_5: 0.1006  loss_mask_5: 0.2153  loss_dice_5: 0.1412  loss_ce_6: 0.1008  loss_mask_6: 0.2056  loss_dice_6: 0.1405  loss_ce_7: 0.1005  loss_mask_7: 0.2124  loss_dice_7: 0.1403  loss_ce_8: 0.1008  loss_mask_8: 0.2141  loss_dice_8: 0.1403  time: 0.5773  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:08:05] d2.utils.events INFO:  eta: 1:09:53  iter: 21699  total_loss: 4.513  loss_ce: 0.1294  loss_mask: 0.1766  loss_dice: 0.1271  loss_ce_0: 0.06365  loss_mask_0: 0.1757  loss_dice_0: 0.129  loss_ce_1: 0.1295  loss_mask_1: 0.1653  loss_dice_1: 0.1227  loss_ce_2: 0.1295  loss_mask_2: 0.1625  loss_dice_2: 0.1239  loss_ce_3: 0.1295  loss_mask_3: 0.1618  loss_dice_3: 0.1293  loss_ce_4: 0.1294  loss_mask_4: 0.1708  loss_dice_4: 0.1286  loss_ce_5: 0.1294  loss_mask_5: 0.1699  loss_dice_5: 0.1231  loss_ce_6: 0.1294  loss_mask_6: 0.1642  loss_dice_6: 0.1311  loss_ce_7: 0.1295  loss_mask_7: 0.173  loss_dice_7: 0.1292  loss_ce_8: 0.1294  loss_mask_8: 0.172  loss_dice_8: 0.1274  time: 0.5772  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:08:15] d2.utils.events INFO:  eta: 1:09:42  iter: 21719  total_loss: 4.538  loss_ce: 0.1006  loss_mask: 0.1955  loss_dice: 0.1294  loss_ce_0: 0.05276  loss_mask_0: 0.1893  loss_dice_0: 0.1309  loss_ce_1: 0.1002  loss_mask_1: 0.1989  loss_dice_1: 0.1325  loss_ce_2: 0.1004  loss_mask_2: 0.1894  loss_dice_2: 0.1321  loss_ce_3: 0.1004  loss_mask_3: 0.1871  loss_dice_3: 0.131  loss_ce_4: 0.1004  loss_mask_4: 0.1896  loss_dice_4: 0.1305  loss_ce_5: 0.1003  loss_mask_5: 0.1932  loss_dice_5: 0.1346  loss_ce_6: 0.1004  loss_mask_6: 0.1926  loss_dice_6: 0.1355  loss_ce_7: 0.1004  loss_mask_7: 0.1978  loss_dice_7: 0.1361  loss_ce_8: 0.1005  loss_mask_8: 0.196  loss_dice_8: 0.129  time: 0.5772  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:08:25] d2.utils.events INFO:  eta: 1:09:32  iter: 21739  total_loss: 4.583  loss_ce: 0.1007  loss_mask: 0.2012  loss_dice: 0.1302  loss_ce_0: 0.05274  loss_mask_0: 0.2022  loss_dice_0: 0.1297  loss_ce_1: 0.1008  loss_mask_1: 0.1978  loss_dice_1: 0.1325  loss_ce_2: 0.1006  loss_mask_2: 0.1987  loss_dice_2: 0.1311  loss_ce_3: 0.1004  loss_mask_3: 0.1937  loss_dice_3: 0.133  loss_ce_4: 0.1006  loss_mask_4: 0.2026  loss_dice_4: 0.1347  loss_ce_5: 0.1004  loss_mask_5: 0.2011  loss_dice_5: 0.1322  loss_ce_6: 0.1006  loss_mask_6: 0.1968  loss_dice_6: 0.1288  loss_ce_7: 0.1004  loss_mask_7: 0.1929  loss_dice_7: 0.1286  loss_ce_8: 0.1007  loss_mask_8: 0.1872  loss_dice_8: 0.1308  time: 0.5771  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:08:36] d2.utils.events INFO:  eta: 1:09:24  iter: 21759  total_loss: 4.747  loss_ce: 0.1005  loss_mask: 0.2036  loss_dice: 0.1432  loss_ce_0: 0.05273  loss_mask_0: 0.2044  loss_dice_0: 0.1438  loss_ce_1: 0.1004  loss_mask_1: 0.2048  loss_dice_1: 0.1459  loss_ce_2: 0.1003  loss_mask_2: 0.2092  loss_dice_2: 0.1404  loss_ce_3: 0.1002  loss_mask_3: 0.213  loss_dice_3: 0.1461  loss_ce_4: 0.1003  loss_mask_4: 0.2016  loss_dice_4: 0.144  loss_ce_5: 0.1002  loss_mask_5: 0.2011  loss_dice_5: 0.1375  loss_ce_6: 0.1003  loss_mask_6: 0.2061  loss_dice_6: 0.1484  loss_ce_7: 0.1002  loss_mask_7: 0.2106  loss_dice_7: 0.1436  loss_ce_8: 0.1004  loss_mask_8: 0.2077  loss_dice_8: 0.1484  time: 0.5771  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:08:46] d2.utils.events INFO:  eta: 1:09:12  iter: 21779  total_loss: 4.697  loss_ce: 0.1005  loss_mask: 0.1951  loss_dice: 0.1454  loss_ce_0: 0.05271  loss_mask_0: 0.1976  loss_dice_0: 0.1387  loss_ce_1: 0.1005  loss_mask_1: 0.1975  loss_dice_1: 0.1399  loss_ce_2: 0.1003  loss_mask_2: 0.1892  loss_dice_2: 0.1431  loss_ce_3: 0.1002  loss_mask_3: 0.2007  loss_dice_3: 0.1402  loss_ce_4: 0.1004  loss_mask_4: 0.204  loss_dice_4: 0.1448  loss_ce_5: 0.1002  loss_mask_5: 0.1936  loss_dice_5: 0.14  loss_ce_6: 0.1003  loss_mask_6: 0.1957  loss_dice_6: 0.1433  loss_ce_7: 0.1002  loss_mask_7: 0.1966  loss_dice_7: 0.144  loss_ce_8: 0.1005  loss_mask_8: 0.1878  loss_dice_8: 0.1444  time: 0.5770  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:08:56] d2.utils.events INFO:  eta: 1:09:03  iter: 21799  total_loss: 4.366  loss_ce: 0.1001  loss_mask: 0.1525  loss_dice: 0.1588  loss_ce_0: 0.05269  loss_mask_0: 0.1415  loss_dice_0: 0.1533  loss_ce_1: 0.09973  loss_mask_1: 0.1409  loss_dice_1: 0.1527  loss_ce_2: 0.09992  loss_mask_2: 0.1533  loss_dice_2: 0.1492  loss_ce_3: 0.09994  loss_mask_3: 0.1435  loss_dice_3: 0.1472  loss_ce_4: 0.09986  loss_mask_4: 0.1415  loss_dice_4: 0.1513  loss_ce_5: 0.09986  loss_mask_5: 0.1399  loss_dice_5: 0.1543  loss_ce_6: 0.1  loss_mask_6: 0.1514  loss_dice_6: 0.154  loss_ce_7: 0.09998  loss_mask_7: 0.1423  loss_dice_7: 0.1547  loss_ce_8: 0.09994  loss_mask_8: 0.1462  loss_dice_8: 0.1497  time: 0.5769  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:09:07] d2.utils.events INFO:  eta: 1:09:09  iter: 21819  total_loss: 4.56  loss_ce: 0.09987  loss_mask: 0.24  loss_dice: 0.1326  loss_ce_0: 0.05261  loss_mask_0: 0.2438  loss_dice_0: 0.1335  loss_ce_1: 0.09951  loss_mask_1: 0.2434  loss_dice_1: 0.1338  loss_ce_2: 0.09953  loss_mask_2: 0.2352  loss_dice_2: 0.1356  loss_ce_3: 0.0996  loss_mask_3: 0.2327  loss_dice_3: 0.1332  loss_ce_4: 0.0996  loss_mask_4: 0.2396  loss_dice_4: 0.1324  loss_ce_5: 0.09942  loss_mask_5: 0.2344  loss_dice_5: 0.1321  loss_ce_6: 0.09968  loss_mask_6: 0.2493  loss_dice_6: 0.1313  loss_ce_7: 0.09964  loss_mask_7: 0.2364  loss_dice_7: 0.1336  loss_ce_8: 0.09975  loss_mask_8: 0.2328  loss_dice_8: 0.1333  time: 0.5769  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:09:19] d2.utils.events INFO:  eta: 1:09:15  iter: 21839  total_loss: 4.903  loss_ce: 0.09972  loss_mask: 0.1804  loss_dice: 0.1475  loss_ce_0: 0.05256  loss_mask_0: 0.1851  loss_dice_0: 0.1496  loss_ce_1: 0.09947  loss_mask_1: 0.1948  loss_dice_1: 0.1458  loss_ce_2: 0.09947  loss_mask_2: 0.1792  loss_dice_2: 0.1476  loss_ce_3: 0.09941  loss_mask_3: 0.185  loss_dice_3: 0.1425  loss_ce_4: 0.09956  loss_mask_4: 0.1774  loss_dice_4: 0.1501  loss_ce_5: 0.09926  loss_mask_5: 0.1852  loss_dice_5: 0.1465  loss_ce_6: 0.09949  loss_mask_6: 0.1841  loss_dice_6: 0.1481  loss_ce_7: 0.09949  loss_mask_7: 0.1806  loss_dice_7: 0.147  loss_ce_8: 0.09968  loss_mask_8: 0.1871  loss_dice_8: 0.1442  time: 0.5769  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:09:30] d2.utils.events INFO:  eta: 1:09:13  iter: 21859  total_loss: 4.543  loss_ce: 0.1297  loss_mask: 0.1437  loss_dice: 0.1735  loss_ce_0: 0.06368  loss_mask_0: 0.1425  loss_dice_0: 0.172  loss_ce_1: 0.1297  loss_mask_1: 0.1431  loss_dice_1: 0.1701  loss_ce_2: 0.1298  loss_mask_2: 0.1359  loss_dice_2: 0.1624  loss_ce_3: 0.1297  loss_mask_3: 0.1383  loss_dice_3: 0.1645  loss_ce_4: 0.1298  loss_mask_4: 0.1409  loss_dice_4: 0.164  loss_ce_5: 0.1298  loss_mask_5: 0.1386  loss_dice_5: 0.1608  loss_ce_6: 0.1298  loss_mask_6: 0.1311  loss_dice_6: 0.1659  loss_ce_7: 0.1297  loss_mask_7: 0.1412  loss_dice_7: 0.1607  loss_ce_8: 0.1298  loss_mask_8: 0.1426  loss_dice_8: 0.1544  time: 0.5769  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:09:42] d2.utils.events INFO:  eta: 1:09:19  iter: 21879  total_loss: 4.518  loss_ce: 0.1594  loss_mask: 0.182  loss_dice: 0.1403  loss_ce_0: 0.07476  loss_mask_0: 0.1963  loss_dice_0: 0.1361  loss_ce_1: 0.1593  loss_mask_1: 0.1874  loss_dice_1: 0.1369  loss_ce_2: 0.1595  loss_mask_2: 0.187  loss_dice_2: 0.1376  loss_ce_3: 0.1596  loss_mask_3: 0.1895  loss_dice_3: 0.1343  loss_ce_4: 0.1596  loss_mask_4: 0.1811  loss_dice_4: 0.1476  loss_ce_5: 0.1599  loss_mask_5: 0.1876  loss_dice_5: 0.1431  loss_ce_6: 0.1595  loss_mask_6: 0.1906  loss_dice_6: 0.1452  loss_ce_7: 0.1598  loss_mask_7: 0.1947  loss_dice_7: 0.1315  loss_ce_8: 0.1593  loss_mask_8: 0.1881  loss_dice_8: 0.1426  time: 0.5769  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:09:53] d2.utils.events INFO:  eta: 1:09:18  iter: 21899  total_loss: 4.365  loss_ce: 0.09979  loss_mask: 0.1578  loss_dice: 0.1487  loss_ce_0: 0.05259  loss_mask_0: 0.164  loss_dice_0: 0.1422  loss_ce_1: 0.0997  loss_mask_1: 0.1599  loss_dice_1: 0.1452  loss_ce_2: 0.09962  loss_mask_2: 0.1647  loss_dice_2: 0.1441  loss_ce_3: 0.09953  loss_mask_3: 0.1622  loss_dice_3: 0.1406  loss_ce_4: 0.0996  loss_mask_4: 0.1584  loss_dice_4: 0.1403  loss_ce_5: 0.09941  loss_mask_5: 0.1526  loss_dice_5: 0.1485  loss_ce_6: 0.09968  loss_mask_6: 0.1569  loss_dice_6: 0.1425  loss_ce_7: 0.09953  loss_mask_7: 0.169  loss_dice_7: 0.1515  loss_ce_8: 0.09979  loss_mask_8: 0.1589  loss_dice_8: 0.1459  time: 0.5769  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:10:04] d2.utils.events INFO:  eta: 1:09:15  iter: 21919  total_loss: 4.411  loss_ce: 0.09994  loss_mask: 0.1769  loss_dice: 0.1153  loss_ce_0: 0.05257  loss_mask_0: 0.1746  loss_dice_0: 0.1159  loss_ce_1: 0.1004  loss_mask_1: 0.1717  loss_dice_1: 0.1131  loss_ce_2: 0.09998  loss_mask_2: 0.1705  loss_dice_2: 0.1162  loss_ce_3: 0.09986  loss_mask_3: 0.1684  loss_dice_3: 0.117  loss_ce_4: 0.09998  loss_mask_4: 0.1773  loss_dice_4: 0.1161  loss_ce_5: 0.09971  loss_mask_5: 0.1802  loss_dice_5: 0.1165  loss_ce_6: 0.1001  loss_mask_6: 0.1788  loss_dice_6: 0.1163  loss_ce_7: 0.09979  loss_mask_7: 0.1724  loss_dice_7: 0.1206  loss_ce_8: 0.1002  loss_mask_8: 0.1793  loss_dice_8: 0.1186  time: 0.5769  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:10:16] d2.utils.events INFO:  eta: 1:09:13  iter: 21939  total_loss: 4.729  loss_ce: 0.09983  loss_mask: 0.1424  loss_dice: 0.1587  loss_ce_0: 0.05258  loss_mask_0: 0.143  loss_dice_0: 0.1637  loss_ce_1: 0.09994  loss_mask_1: 0.1362  loss_dice_1: 0.161  loss_ce_2: 0.09973  loss_mask_2: 0.1355  loss_dice_2: 0.1562  loss_ce_3: 0.09964  loss_mask_3: 0.1407  loss_dice_3: 0.1564  loss_ce_4: 0.09971  loss_mask_4: 0.141  loss_dice_4: 0.1509  loss_ce_5: 0.09957  loss_mask_5: 0.1443  loss_dice_5: 0.155  loss_ce_6: 0.09971  loss_mask_6: 0.1305  loss_dice_6: 0.1507  loss_ce_7: 0.09968  loss_mask_7: 0.1247  loss_dice_7: 0.1455  loss_ce_8: 0.1002  loss_mask_8: 0.1298  loss_dice_8: 0.1541  time: 0.5769  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:10:27] d2.utils.events INFO:  eta: 1:09:16  iter: 21959  total_loss: 4.656  loss_ce: 0.1295  loss_mask: 0.1935  loss_dice: 0.1523  loss_ce_0: 0.06369  loss_mask_0: 0.2033  loss_dice_0: 0.156  loss_ce_1: 0.1293  loss_mask_1: 0.197  loss_dice_1: 0.1632  loss_ce_2: 0.1295  loss_mask_2: 0.1986  loss_dice_2: 0.1525  loss_ce_3: 0.1296  loss_mask_3: 0.1929  loss_dice_3: 0.1587  loss_ce_4: 0.1295  loss_mask_4: 0.197  loss_dice_4: 0.1541  loss_ce_5: 0.1296  loss_mask_5: 0.1905  loss_dice_5: 0.1539  loss_ce_6: 0.1294  loss_mask_6: 0.1932  loss_dice_6: 0.156  loss_ce_7: 0.1295  loss_mask_7: 0.1946  loss_dice_7: 0.153  loss_ce_8: 0.1295  loss_mask_8: 0.1925  loss_dice_8: 0.1559  time: 0.5769  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:10:38] d2.utils.events INFO:  eta: 1:09:17  iter: 21979  total_loss: 4.459  loss_ce: 0.09964  loss_mask: 0.1849  loss_dice: 0.1278  loss_ce_0: 0.05258  loss_mask_0: 0.1773  loss_dice_0: 0.1312  loss_ce_1: 0.09897  loss_mask_1: 0.1822  loss_dice_1: 0.1316  loss_ce_2: 0.0993  loss_mask_2: 0.1804  loss_dice_2: 0.1324  loss_ce_3: 0.0993  loss_mask_3: 0.1797  loss_dice_3: 0.1358  loss_ce_4: 0.09932  loss_mask_4: 0.1773  loss_dice_4: 0.1274  loss_ce_5: 0.09927  loss_mask_5: 0.1745  loss_dice_5: 0.1279  loss_ce_6: 0.09934  loss_mask_6: 0.1828  loss_dice_6: 0.1299  loss_ce_7: 0.09939  loss_mask_7: 0.1936  loss_dice_7: 0.1312  loss_ce_8: 0.0996  loss_mask_8: 0.174  loss_dice_8: 0.1331  time: 0.5769  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:10:50] d2.utils.events INFO:  eta: 1:09:30  iter: 21999  total_loss: 4.672  loss_ce: 0.09964  loss_mask: 0.2093  loss_dice: 0.1339  loss_ce_0: 0.05254  loss_mask_0: 0.2154  loss_dice_0: 0.1369  loss_ce_1: 0.09949  loss_mask_1: 0.2191  loss_dice_1: 0.1365  loss_ce_2: 0.09949  loss_mask_2: 0.2153  loss_dice_2: 0.1392  loss_ce_3: 0.09945  loss_mask_3: 0.2121  loss_dice_3: 0.1296  loss_ce_4: 0.09956  loss_mask_4: 0.2106  loss_dice_4: 0.1289  loss_ce_5: 0.09927  loss_mask_5: 0.215  loss_dice_5: 0.1311  loss_ce_6: 0.09956  loss_mask_6: 0.2142  loss_dice_6: 0.1344  loss_ce_7: 0.09945  loss_mask_7: 0.2145  loss_dice_7: 0.1308  loss_ce_8: 0.09964  loss_mask_8: 0.2088  loss_dice_8: 0.1315  time: 0.5769  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:11:01] d2.utils.events INFO:  eta: 1:09:45  iter: 22019  total_loss: 4.402  loss_ce: 0.1597  loss_mask: 0.1598  loss_dice: 0.1363  loss_ce_0: 0.07485  loss_mask_0: 0.1643  loss_dice_0: 0.1361  loss_ce_1: 0.16  loss_mask_1: 0.1626  loss_dice_1: 0.1334  loss_ce_2: 0.1601  loss_mask_2: 0.1591  loss_dice_2: 0.1352  loss_ce_3: 0.16  loss_mask_3: 0.1605  loss_dice_3: 0.1377  loss_ce_4: 0.16  loss_mask_4: 0.1594  loss_dice_4: 0.133  loss_ce_5: 0.1603  loss_mask_5: 0.1652  loss_dice_5: 0.1345  loss_ce_6: 0.1601  loss_mask_6: 0.1638  loss_dice_6: 0.1382  loss_ce_7: 0.16  loss_mask_7: 0.155  loss_dice_7: 0.1302  loss_ce_8: 0.1598  loss_mask_8: 0.1592  loss_dice_8: 0.1345  time: 0.5768  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:11:13] d2.utils.events INFO:  eta: 1:09:45  iter: 22039  total_loss: 4.514  loss_ce: 0.09953  loss_mask: 0.1669  loss_dice: 0.1511  loss_ce_0: 0.05253  loss_mask_0: 0.1687  loss_dice_0: 0.1598  loss_ce_1: 0.09897  loss_mask_1: 0.1624  loss_dice_1: 0.1553  loss_ce_2: 0.09915  loss_mask_2: 0.1658  loss_dice_2: 0.1619  loss_ce_3: 0.0993  loss_mask_3: 0.1628  loss_dice_3: 0.1526  loss_ce_4: 0.09917  loss_mask_4: 0.1618  loss_dice_4: 0.1561  loss_ce_5: 0.09911  loss_mask_5: 0.1644  loss_dice_5: 0.1543  loss_ce_6: 0.09934  loss_mask_6: 0.155  loss_dice_6: 0.1539  loss_ce_7: 0.09926  loss_mask_7: 0.1599  loss_dice_7: 0.1552  loss_ce_8: 0.09945  loss_mask_8: 0.1694  loss_dice_8: 0.1608  time: 0.5768  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:11:24] d2.utils.events INFO:  eta: 1:09:50  iter: 22059  total_loss: 4.491  loss_ce: 0.09942  loss_mask: 0.1786  loss_dice: 0.1482  loss_ce_0: 0.05251  loss_mask_0: 0.1866  loss_dice_0: 0.1419  loss_ce_1: 0.09912  loss_mask_1: 0.1799  loss_dice_1: 0.1456  loss_ce_2: 0.09908  loss_mask_2: 0.1827  loss_dice_2: 0.1434  loss_ce_3: 0.09904  loss_mask_3: 0.1903  loss_dice_3: 0.1419  loss_ce_4: 0.09911  loss_mask_4: 0.173  loss_dice_4: 0.1491  loss_ce_5: 0.09897  loss_mask_5: 0.1834  loss_dice_5: 0.1481  loss_ce_6: 0.09915  loss_mask_6: 0.1769  loss_dice_6: 0.1482  loss_ce_7: 0.09919  loss_mask_7: 0.1754  loss_dice_7: 0.1428  loss_ce_8: 0.09938  loss_mask_8: 0.1765  loss_dice_8: 0.1488  time: 0.5768  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:11:36] d2.utils.events INFO:  eta: 1:09:58  iter: 22079  total_loss: 4.593  loss_ce: 0.09927  loss_mask: 0.1898  loss_dice: 0.1302  loss_ce_0: 0.0525  loss_mask_0: 0.1941  loss_dice_0: 0.129  loss_ce_1: 0.09895  loss_mask_1: 0.1878  loss_dice_1: 0.1317  loss_ce_2: 0.09897  loss_mask_2: 0.1972  loss_dice_2: 0.1337  loss_ce_3: 0.09896  loss_mask_3: 0.1864  loss_dice_3: 0.1274  loss_ce_4: 0.09908  loss_mask_4: 0.1973  loss_dice_4: 0.1327  loss_ce_5: 0.09882  loss_mask_5: 0.195  loss_dice_5: 0.126  loss_ce_6: 0.09911  loss_mask_6: 0.1894  loss_dice_6: 0.1312  loss_ce_7: 0.09908  loss_mask_7: 0.1986  loss_dice_7: 0.1336  loss_ce_8: 0.09919  loss_mask_8: 0.1858  loss_dice_8: 0.1296  time: 0.5768  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:11:47] d2.utils.events INFO:  eta: 1:09:59  iter: 22099  total_loss: 4.225  loss_ce: 0.09915  loss_mask: 0.1749  loss_dice: 0.1406  loss_ce_0: 0.05245  loss_mask_0: 0.1628  loss_dice_0: 0.138  loss_ce_1: 0.09893  loss_mask_1: 0.166  loss_dice_1: 0.1356  loss_ce_2: 0.09872  loss_mask_2: 0.1597  loss_dice_2: 0.1356  loss_ce_3: 0.09874  loss_mask_3: 0.1657  loss_dice_3: 0.132  loss_ce_4: 0.09889  loss_mask_4: 0.1733  loss_dice_4: 0.1374  loss_ce_5: 0.09867  loss_mask_5: 0.1743  loss_dice_5: 0.1356  loss_ce_6: 0.09893  loss_mask_6: 0.1725  loss_dice_6: 0.1374  loss_ce_7: 0.09897  loss_mask_7: 0.1763  loss_dice_7: 0.1299  loss_ce_8: 0.09923  loss_mask_8: 0.1736  loss_dice_8: 0.1333  time: 0.5768  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:11:58] d2.utils.events INFO:  eta: 1:09:52  iter: 22119  total_loss: 4.528  loss_ce: 0.1601  loss_mask: 0.1836  loss_dice: 0.1503  loss_ce_0: 0.07493  loss_mask_0: 0.1785  loss_dice_0: 0.1515  loss_ce_1: 0.1598  loss_mask_1: 0.1762  loss_dice_1: 0.1533  loss_ce_2: 0.1602  loss_mask_2: 0.1859  loss_dice_2: 0.1567  loss_ce_3: 0.1603  loss_mask_3: 0.1848  loss_dice_3: 0.1528  loss_ce_4: 0.1602  loss_mask_4: 0.1793  loss_dice_4: 0.1527  loss_ce_5: 0.1605  loss_mask_5: 0.1851  loss_dice_5: 0.15  loss_ce_6: 0.1601  loss_mask_6: 0.1813  loss_dice_6: 0.1561  loss_ce_7: 0.1604  loss_mask_7: 0.1816  loss_dice_7: 0.1462  loss_ce_8: 0.16  loss_mask_8: 0.1866  loss_dice_8: 0.1486  time: 0.5768  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:12:10] d2.utils.events INFO:  eta: 1:09:53  iter: 22139  total_loss: 4.616  loss_ce: 0.09927  loss_mask: 0.2168  loss_dice: 0.1327  loss_ce_0: 0.05248  loss_mask_0: 0.2056  loss_dice_0: 0.1366  loss_ce_1: 0.09921  loss_mask_1: 0.2231  loss_dice_1: 0.1307  loss_ce_2: 0.0991  loss_mask_2: 0.2076  loss_dice_2: 0.1353  loss_ce_3: 0.09908  loss_mask_3: 0.2067  loss_dice_3: 0.1279  loss_ce_4: 0.09923  loss_mask_4: 0.2228  loss_dice_4: 0.1343  loss_ce_5: 0.09897  loss_mask_5: 0.2174  loss_dice_5: 0.1303  loss_ce_6: 0.09919  loss_mask_6: 0.214  loss_dice_6: 0.1324  loss_ce_7: 0.09912  loss_mask_7: 0.2185  loss_dice_7: 0.1301  loss_ce_8: 0.0993  loss_mask_8: 0.2152  loss_dice_8: 0.1288  time: 0.5768  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:12:21] d2.utils.events INFO:  eta: 1:09:52  iter: 22159  total_loss: 4.533  loss_ce: 0.09915  loss_mask: 0.1558  loss_dice: 0.1496  loss_ce_0: 0.05248  loss_mask_0: 0.1567  loss_dice_0: 0.1497  loss_ce_1: 0.09882  loss_mask_1: 0.1581  loss_dice_1: 0.151  loss_ce_2: 0.09893  loss_mask_2: 0.1515  loss_dice_2: 0.1505  loss_ce_3: 0.099  loss_mask_3: 0.154  loss_dice_3: 0.1481  loss_ce_4: 0.099  loss_mask_4: 0.1615  loss_dice_4: 0.1496  loss_ce_5: 0.09896  loss_mask_5: 0.1534  loss_dice_5: 0.1431  loss_ce_6: 0.09908  loss_mask_6: 0.1584  loss_dice_6: 0.1521  loss_ce_7: 0.099  loss_mask_7: 0.1521  loss_dice_7: 0.1455  loss_ce_8: 0.09912  loss_mask_8: 0.1546  loss_dice_8: 0.1536  time: 0.5768  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 15:12:33] d2.utils.events INFO:  eta: 1:09:57  iter: 22179  total_loss: 4.597  loss_ce: 0.09915  loss_mask: 0.1663  loss_dice: 0.1544  loss_ce_0: 0.05247  loss_mask_0: 0.1588  loss_dice_0: 0.1523  loss_ce_1: 0.09908  loss_mask_1: 0.1635  loss_dice_1: 0.1571  loss_ce_2: 0.09887  loss_mask_2: 0.1683  loss_dice_2: 0.1515  loss_ce_3: 0.09889  loss_mask_3: 0.1637  loss_dice_3: 0.1507  loss_ce_4: 0.099  loss_mask_4: 0.1679  loss_dice_4: 0.1544  loss_ce_5: 0.09889  loss_mask_5: 0.158  loss_dice_5: 0.1551  loss_ce_6: 0.099  loss_mask_6: 0.1564  loss_dice_6: 0.154  loss_ce_7: 0.09915  loss_mask_7: 0.1553  loss_dice_7: 0.1507  loss_ce_8: 0.09919  loss_mask_8: 0.1636  loss_dice_8: 0.1537  time: 0.5768  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:12:44] d2.utils.events INFO:  eta: 1:09:58  iter: 22199  total_loss: 4.5  loss_ce: 0.09912  loss_mask: 0.136  loss_dice: 0.1635  loss_ce_0: 0.05245  loss_mask_0: 0.1327  loss_dice_0: 0.1636  loss_ce_1: 0.09871  loss_mask_1: 0.1399  loss_dice_1: 0.1634  loss_ce_2: 0.09872  loss_mask_2: 0.1347  loss_dice_2: 0.1657  loss_ce_3: 0.09881  loss_mask_3: 0.1311  loss_dice_3: 0.1692  loss_ce_4: 0.09885  loss_mask_4: 0.1427  loss_dice_4: 0.1727  loss_ce_5: 0.09867  loss_mask_5: 0.1315  loss_dice_5: 0.1718  loss_ce_6: 0.09882  loss_mask_6: 0.128  loss_dice_6: 0.1578  loss_ce_7: 0.09896  loss_mask_7: 0.1374  loss_dice_7: 0.168  loss_ce_8: 0.099  loss_mask_8: 0.1349  loss_dice_8: 0.1645  time: 0.5768  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:12:55] d2.utils.events INFO:  eta: 1:10:07  iter: 22219  total_loss: 4.573  loss_ce: 0.09927  loss_mask: 0.1881  loss_dice: 0.1345  loss_ce_0: 0.05245  loss_mask_0: 0.1856  loss_dice_0: 0.1426  loss_ce_1: 0.09921  loss_mask_1: 0.1793  loss_dice_1: 0.138  loss_ce_2: 0.09904  loss_mask_2: 0.1903  loss_dice_2: 0.1356  loss_ce_3: 0.099  loss_mask_3: 0.1893  loss_dice_3: 0.138  loss_ce_4: 0.09919  loss_mask_4: 0.1956  loss_dice_4: 0.1396  loss_ce_5: 0.09897  loss_mask_5: 0.1896  loss_dice_5: 0.1409  loss_ce_6: 0.09919  loss_mask_6: 0.1814  loss_dice_6: 0.1392  loss_ce_7: 0.09908  loss_mask_7: 0.1924  loss_dice_7: 0.1385  loss_ce_8: 0.09923  loss_mask_8: 0.1922  loss_dice_8: 0.1425  time: 0.5768  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:13:07] d2.utils.events INFO:  eta: 1:10:13  iter: 22239  total_loss: 4.726  loss_ce: 0.1602  loss_mask: 0.2056  loss_dice: 0.1446  loss_ce_0: 0.07494  loss_mask_0: 0.2151  loss_dice_0: 0.1484  loss_ce_1: 0.1602  loss_mask_1: 0.2029  loss_dice_1: 0.1481  loss_ce_2: 0.1605  loss_mask_2: 0.2067  loss_dice_2: 0.1502  loss_ce_3: 0.1606  loss_mask_3: 0.2079  loss_dice_3: 0.1414  loss_ce_4: 0.1605  loss_mask_4: 0.2142  loss_dice_4: 0.1561  loss_ce_5: 0.1606  loss_mask_5: 0.2077  loss_dice_5: 0.1495  loss_ce_6: 0.1604  loss_mask_6: 0.2075  loss_dice_6: 0.1421  loss_ce_7: 0.1606  loss_mask_7: 0.2163  loss_dice_7: 0.1455  loss_ce_8: 0.1602  loss_mask_8: 0.2158  loss_dice_8: 0.1465  time: 0.5768  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:13:18] d2.utils.events INFO:  eta: 1:10:13  iter: 22259  total_loss: 4.693  loss_ce: 0.09912  loss_mask: 0.197  loss_dice: 0.1351  loss_ce_0: 0.05249  loss_mask_0: 0.2045  loss_dice_0: 0.1352  loss_ce_1: 0.09884  loss_mask_1: 0.1936  loss_dice_1: 0.1338  loss_ce_2: 0.09887  loss_mask_2: 0.2134  loss_dice_2: 0.1382  loss_ce_3: 0.09882  loss_mask_3: 0.2017  loss_dice_3: 0.1319  loss_ce_4: 0.09904  loss_mask_4: 0.1949  loss_dice_4: 0.1335  loss_ce_5: 0.09882  loss_mask_5: 0.2024  loss_dice_5: 0.1426  loss_ce_6: 0.09893  loss_mask_6: 0.1978  loss_dice_6: 0.1317  loss_ce_7: 0.09893  loss_mask_7: 0.2101  loss_dice_7: 0.144  loss_ce_8: 0.09912  loss_mask_8: 0.1995  loss_dice_8: 0.1349  time: 0.5768  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:13:30] d2.utils.events INFO:  eta: 1:10:23  iter: 22279  total_loss: 4.228  loss_ce: 0.1296  loss_mask: 0.1487  loss_dice: 0.1343  loss_ce_0: 0.06369  loss_mask_0: 0.1509  loss_dice_0: 0.1372  loss_ce_1: 0.1291  loss_mask_1: 0.1413  loss_dice_1: 0.1307  loss_ce_2: 0.1295  loss_mask_2: 0.1465  loss_dice_2: 0.1337  loss_ce_3: 0.1296  loss_mask_3: 0.151  loss_dice_3: 0.1372  loss_ce_4: 0.1295  loss_mask_4: 0.1471  loss_dice_4: 0.1352  loss_ce_5: 0.1295  loss_mask_5: 0.153  loss_dice_5: 0.1371  loss_ce_6: 0.1294  loss_mask_6: 0.155  loss_dice_6: 0.1349  loss_ce_7: 0.1297  loss_mask_7: 0.1498  loss_dice_7: 0.1349  loss_ce_8: 0.1293  loss_mask_8: 0.1486  loss_dice_8: 0.1334  time: 0.5768  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 15:13:41] d2.utils.events INFO:  eta: 1:10:22  iter: 22299  total_loss: 4.464  loss_ce: 0.1298  loss_mask: 0.1869  loss_dice: 0.1423  loss_ce_0: 0.06369  loss_mask_0: 0.1914  loss_dice_0: 0.138  loss_ce_1: 0.13  loss_mask_1: 0.1897  loss_dice_1: 0.1373  loss_ce_2: 0.1299  loss_mask_2: 0.1951  loss_dice_2: 0.1413  loss_ce_3: 0.1299  loss_mask_3: 0.2  loss_dice_3: 0.1388  loss_ce_4: 0.1299  loss_mask_4: 0.195  loss_dice_4: 0.1405  loss_ce_5: 0.1298  loss_mask_5: 0.1914  loss_dice_5: 0.1381  loss_ce_6: 0.1299  loss_mask_6: 0.1914  loss_dice_6: 0.1376  loss_ce_7: 0.1298  loss_mask_7: 0.1989  loss_dice_7: 0.1393  loss_ce_8: 0.1299  loss_mask_8: 0.1863  loss_dice_8: 0.1346  time: 0.5768  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:13:53] d2.utils.events INFO:  eta: 1:10:23  iter: 22319  total_loss: 4.684  loss_ce: 0.09927  loss_mask: 0.2189  loss_dice: 0.1381  loss_ce_0: 0.05249  loss_mask_0: 0.2229  loss_dice_0: 0.1331  loss_ce_1: 0.0993  loss_mask_1: 0.2153  loss_dice_1: 0.1349  loss_ce_2: 0.09913  loss_mask_2: 0.2166  loss_dice_2: 0.1308  loss_ce_3: 0.09908  loss_mask_3: 0.2179  loss_dice_3: 0.1325  loss_ce_4: 0.09919  loss_mask_4: 0.2134  loss_dice_4: 0.1338  loss_ce_5: 0.09897  loss_mask_5: 0.222  loss_dice_5: 0.1355  loss_ce_6: 0.09919  loss_mask_6: 0.2228  loss_dice_6: 0.1383  loss_ce_7: 0.09908  loss_mask_7: 0.2143  loss_dice_7: 0.1356  loss_ce_8: 0.09934  loss_mask_8: 0.2173  loss_dice_8: 0.135  time: 0.5768  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:14:04] d2.utils.events INFO:  eta: 1:10:23  iter: 22339  total_loss: 4.278  loss_ce: 0.09912  loss_mask: 0.1736  loss_dice: 0.1247  loss_ce_0: 0.05247  loss_mask_0: 0.1752  loss_dice_0: 0.1286  loss_ce_1: 0.099  loss_mask_1: 0.184  loss_dice_1: 0.1255  loss_ce_2: 0.09887  loss_mask_2: 0.1684  loss_dice_2: 0.128  loss_ce_3: 0.09882  loss_mask_3: 0.1776  loss_dice_3: 0.1268  loss_ce_4: 0.09893  loss_mask_4: 0.1759  loss_dice_4: 0.1336  loss_ce_5: 0.09882  loss_mask_5: 0.1746  loss_dice_5: 0.123  loss_ce_6: 0.09889  loss_mask_6: 0.1802  loss_dice_6: 0.1308  loss_ce_7: 0.09885  loss_mask_7: 0.1786  loss_dice_7: 0.123  loss_ce_8: 0.09912  loss_mask_8: 0.1806  loss_dice_8: 0.1289  time: 0.5767  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:14:15] d2.utils.events INFO:  eta: 1:10:20  iter: 22359  total_loss: 4.18  loss_ce: 0.09923  loss_mask: 0.1529  loss_dice: 0.1495  loss_ce_0: 0.05247  loss_mask_0: 0.1623  loss_dice_0: 0.1412  loss_ce_1: 0.09981  loss_mask_1: 0.1448  loss_dice_1: 0.145  loss_ce_2: 0.0993  loss_mask_2: 0.1653  loss_dice_2: 0.1474  loss_ce_3: 0.09911  loss_mask_3: 0.1518  loss_dice_3: 0.1534  loss_ce_4: 0.09934  loss_mask_4: 0.157  loss_dice_4: 0.146  loss_ce_5: 0.09904  loss_mask_5: 0.1495  loss_dice_5: 0.1459  loss_ce_6: 0.09923  loss_mask_6: 0.1595  loss_dice_6: 0.1396  loss_ce_7: 0.09908  loss_mask_7: 0.1581  loss_dice_7: 0.1477  loss_ce_8: 0.09923  loss_mask_8: 0.1549  loss_dice_8: 0.1461  time: 0.5767  data_time: 0.0023  lr: 1e-06  max_mem: 2811M
[07/11 15:14:27] d2.utils.events INFO:  eta: 1:10:16  iter: 22379  total_loss: 4.647  loss_ce: 0.09889  loss_mask: 0.1106  loss_dice: 0.1763  loss_ce_0: 0.05245  loss_mask_0: 0.1109  loss_dice_0: 0.1794  loss_ce_1: 0.09833  loss_mask_1: 0.1047  loss_dice_1: 0.1678  loss_ce_2: 0.09861  loss_mask_2: 0.1092  loss_dice_2: 0.1806  loss_ce_3: 0.09863  loss_mask_3: 0.1101  loss_dice_3: 0.1709  loss_ce_4: 0.09859  loss_mask_4: 0.1106  loss_dice_4: 0.1749  loss_ce_5: 0.09852  loss_mask_5: 0.1077  loss_dice_5: 0.1872  loss_ce_6: 0.0987  loss_mask_6: 0.109  loss_dice_6: 0.1718  loss_ce_7: 0.09867  loss_mask_7: 0.1062  loss_dice_7: 0.1805  loss_ce_8: 0.09882  loss_mask_8: 0.1168  loss_dice_8: 0.1824  time: 0.5767  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:14:38] d2.utils.events INFO:  eta: 1:10:12  iter: 22399  total_loss: 4.384  loss_ce: 0.09882  loss_mask: 0.1562  loss_dice: 0.1498  loss_ce_0: 0.05244  loss_mask_0: 0.1555  loss_dice_0: 0.1523  loss_ce_1: 0.09863  loss_mask_1: 0.155  loss_dice_1: 0.1486  loss_ce_2: 0.09852  loss_mask_2: 0.1631  loss_dice_2: 0.1498  loss_ce_3: 0.09866  loss_mask_3: 0.1514  loss_dice_3: 0.1421  loss_ce_4: 0.09859  loss_mask_4: 0.1507  loss_dice_4: 0.1508  loss_ce_5: 0.09844  loss_mask_5: 0.1546  loss_dice_5: 0.1528  loss_ce_6: 0.09859  loss_mask_6: 0.1592  loss_dice_6: 0.1484  loss_ce_7: 0.09863  loss_mask_7: 0.1575  loss_dice_7: 0.148  loss_ce_8: 0.09878  loss_mask_8: 0.157  loss_dice_8: 0.146  time: 0.5767  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:14:49] d2.utils.events INFO:  eta: 1:10:07  iter: 22419  total_loss: 4.333  loss_ce: 0.09874  loss_mask: 0.1745  loss_dice: 0.1355  loss_ce_0: 0.05243  loss_mask_0: 0.1727  loss_dice_0: 0.131  loss_ce_1: 0.09845  loss_mask_1: 0.1771  loss_dice_1: 0.1357  loss_ce_2: 0.09845  loss_mask_2: 0.1764  loss_dice_2: 0.1336  loss_ce_3: 0.09844  loss_mask_3: 0.1905  loss_dice_3: 0.1314  loss_ce_4: 0.09852  loss_mask_4: 0.1795  loss_dice_4: 0.1339  loss_ce_5: 0.09837  loss_mask_5: 0.1853  loss_dice_5: 0.1329  loss_ce_6: 0.09848  loss_mask_6: 0.184  loss_dice_6: 0.1292  loss_ce_7: 0.09852  loss_mask_7: 0.1782  loss_dice_7: 0.1345  loss_ce_8: 0.09874  loss_mask_8: 0.1866  loss_dice_8: 0.1359  time: 0.5767  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:15:02] d2.utils.events INFO:  eta: 1:10:13  iter: 22439  total_loss: 4.444  loss_ce: 0.1298  loss_mask: 0.1737  loss_dice: 0.1309  loss_ce_0: 0.06371  loss_mask_0: 0.1862  loss_dice_0: 0.1329  loss_ce_1: 0.13  loss_mask_1: 0.175  loss_dice_1: 0.1294  loss_ce_2: 0.13  loss_mask_2: 0.1774  loss_dice_2: 0.1359  loss_ce_3: 0.13  loss_mask_3: 0.1827  loss_dice_3: 0.1338  loss_ce_4: 0.13  loss_mask_4: 0.1769  loss_dice_4: 0.1406  loss_ce_5: 0.13  loss_mask_5: 0.1811  loss_dice_5: 0.1302  loss_ce_6: 0.13  loss_mask_6: 0.188  loss_dice_6: 0.1313  loss_ce_7: 0.1299  loss_mask_7: 0.1819  loss_dice_7: 0.1323  loss_ce_8: 0.1299  loss_mask_8: 0.1799  loss_dice_8: 0.1311  time: 0.5767  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:15:14] d2.utils.events INFO:  eta: 1:10:11  iter: 22459  total_loss: 4.684  loss_ce: 0.09859  loss_mask: 0.2168  loss_dice: 0.1479  loss_ce_0: 0.05238  loss_mask_0: 0.2168  loss_dice_0: 0.1552  loss_ce_1: 0.09845  loss_mask_1: 0.2148  loss_dice_1: 0.1528  loss_ce_2: 0.09831  loss_mask_2: 0.2291  loss_dice_2: 0.1613  loss_ce_3: 0.09826  loss_mask_3: 0.2131  loss_dice_3: 0.1524  loss_ce_4: 0.09833  loss_mask_4: 0.212  loss_dice_4: 0.1457  loss_ce_5: 0.09807  loss_mask_5: 0.2158  loss_dice_5: 0.1511  loss_ce_6: 0.09837  loss_mask_6: 0.221  loss_dice_6: 0.1589  loss_ce_7: 0.09837  loss_mask_7: 0.2243  loss_dice_7: 0.1549  loss_ce_8: 0.09859  loss_mask_8: 0.2236  loss_dice_8: 0.1506  time: 0.5768  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:15:27] d2.utils.events INFO:  eta: 1:10:14  iter: 22479  total_loss: 4.484  loss_ce: 0.1611  loss_mask: 0.1998  loss_dice: 0.1376  loss_ce_0: 0.07505  loss_mask_0: 0.1923  loss_dice_0: 0.138  loss_ce_1: 0.1612  loss_mask_1: 0.1992  loss_dice_1: 0.142  loss_ce_2: 0.1614  loss_mask_2: 0.1953  loss_dice_2: 0.1331  loss_ce_3: 0.1615  loss_mask_3: 0.2057  loss_dice_3: 0.1429  loss_ce_4: 0.1613  loss_mask_4: 0.2002  loss_dice_4: 0.1307  loss_ce_5: 0.1617  loss_mask_5: 0.1919  loss_dice_5: 0.1344  loss_ce_6: 0.1614  loss_mask_6: 0.198  loss_dice_6: 0.1351  loss_ce_7: 0.1614  loss_mask_7: 0.198  loss_dice_7: 0.138  loss_ce_8: 0.1611  loss_mask_8: 0.1929  loss_dice_8: 0.1408  time: 0.5768  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:15:40] d2.utils.events INFO:  eta: 1:10:14  iter: 22499  total_loss: 4.653  loss_ce: 0.09863  loss_mask: 0.174  loss_dice: 0.152  loss_ce_0: 0.05237  loss_mask_0: 0.1738  loss_dice_0: 0.1485  loss_ce_1: 0.09832  loss_mask_1: 0.1807  loss_dice_1: 0.1466  loss_ce_2: 0.09835  loss_mask_2: 0.17  loss_dice_2: 0.1493  loss_ce_3: 0.09833  loss_mask_3: 0.1776  loss_dice_3: 0.1511  loss_ce_4: 0.0984  loss_mask_4: 0.1748  loss_dice_4: 0.1524  loss_ce_5: 0.09822  loss_mask_5: 0.1837  loss_dice_5: 0.1546  loss_ce_6: 0.09837  loss_mask_6: 0.1751  loss_dice_6: 0.1472  loss_ce_7: 0.09837  loss_mask_7: 0.1764  loss_dice_7: 0.1512  loss_ce_8: 0.09856  loss_mask_8: 0.1777  loss_dice_8: 0.1556  time: 0.5769  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:15:52] d2.utils.events INFO:  eta: 1:10:19  iter: 22519  total_loss: 4.245  loss_ce: 0.1297  loss_mask: 0.1624  loss_dice: 0.1272  loss_ce_0: 0.06371  loss_mask_0: 0.1649  loss_dice_0: 0.1248  loss_ce_1: 0.1293  loss_mask_1: 0.1588  loss_dice_1: 0.1274  loss_ce_2: 0.1297  loss_mask_2: 0.1584  loss_dice_2: 0.1268  loss_ce_3: 0.1297  loss_mask_3: 0.1614  loss_dice_3: 0.1268  loss_ce_4: 0.1296  loss_mask_4: 0.1642  loss_dice_4: 0.1309  loss_ce_5: 0.1297  loss_mask_5: 0.1606  loss_dice_5: 0.136  loss_ce_6: 0.1296  loss_mask_6: 0.1644  loss_dice_6: 0.1292  loss_ce_7: 0.1297  loss_mask_7: 0.1616  loss_dice_7: 0.1272  loss_ce_8: 0.13  loss_mask_8: 0.1652  loss_dice_8: 0.1276  time: 0.5769  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:16:05] d2.utils.events INFO:  eta: 1:10:22  iter: 22539  total_loss: 4.366  loss_ce: 0.09848  loss_mask: 0.1739  loss_dice: 0.1408  loss_ce_0: 0.05236  loss_mask_0: 0.172  loss_dice_0: 0.1458  loss_ce_1: 0.09815  loss_mask_1: 0.1651  loss_dice_1: 0.1451  loss_ce_2: 0.09815  loss_mask_2: 0.1665  loss_dice_2: 0.1434  loss_ce_3: 0.09818  loss_mask_3: 0.1687  loss_dice_3: 0.1468  loss_ce_4: 0.09818  loss_mask_4: 0.1656  loss_dice_4: 0.146  loss_ce_5: 0.09807  loss_mask_5: 0.1696  loss_dice_5: 0.1463  loss_ce_6: 0.09822  loss_mask_6: 0.1571  loss_dice_6: 0.1452  loss_ce_7: 0.09826  loss_mask_7: 0.1709  loss_dice_7: 0.1422  loss_ce_8: 0.09845  loss_mask_8: 0.1717  loss_dice_8: 0.1447  time: 0.5770  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:16:18] d2.utils.events INFO:  eta: 1:10:28  iter: 22559  total_loss: 4.389  loss_ce: 0.09859  loss_mask: 0.1874  loss_dice: 0.1402  loss_ce_0: 0.05235  loss_mask_0: 0.184  loss_dice_0: 0.1349  loss_ce_1: 0.09872  loss_mask_1: 0.1846  loss_dice_1: 0.1413  loss_ce_2: 0.09846  loss_mask_2: 0.1848  loss_dice_2: 0.1439  loss_ce_3: 0.09829  loss_mask_3: 0.1857  loss_dice_3: 0.1415  loss_ce_4: 0.09848  loss_mask_4: 0.1854  loss_dice_4: 0.1399  loss_ce_5: 0.09822  loss_mask_5: 0.1883  loss_dice_5: 0.141  loss_ce_6: 0.09848  loss_mask_6: 0.1855  loss_dice_6: 0.1408  loss_ce_7: 0.09833  loss_mask_7: 0.1825  loss_dice_7: 0.1442  loss_ce_8: 0.09859  loss_mask_8: 0.1861  loss_dice_8: 0.1352  time: 0.5770  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:16:30] d2.utils.events INFO:  eta: 1:10:27  iter: 22579  total_loss: 4.767  loss_ce: 0.09841  loss_mask: 0.1865  loss_dice: 0.161  loss_ce_0: 0.05235  loss_mask_0: 0.1881  loss_dice_0: 0.1595  loss_ce_1: 0.0982  loss_mask_1: 0.1871  loss_dice_1: 0.1603  loss_ce_2: 0.09818  loss_mask_2: 0.1833  loss_dice_2: 0.1628  loss_ce_3: 0.09815  loss_mask_3: 0.1871  loss_dice_3: 0.1588  loss_ce_4: 0.0983  loss_mask_4: 0.1865  loss_dice_4: 0.1647  loss_ce_5: 0.09807  loss_mask_5: 0.178  loss_dice_5: 0.1611  loss_ce_6: 0.09822  loss_mask_6: 0.188  loss_dice_6: 0.1637  loss_ce_7: 0.0983  loss_mask_7: 0.184  loss_dice_7: 0.1536  loss_ce_8: 0.09841  loss_mask_8: 0.1842  loss_dice_8: 0.163  time: 0.5771  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:16:43] d2.utils.events INFO:  eta: 1:10:28  iter: 22599  total_loss: 4.739  loss_ce: 0.1613  loss_mask: 0.1961  loss_dice: 0.1185  loss_ce_0: 0.07509  loss_mask_0: 0.1939  loss_dice_0: 0.1226  loss_ce_1: 0.1616  loss_mask_1: 0.2028  loss_dice_1: 0.1211  loss_ce_2: 0.1616  loss_mask_2: 0.2035  loss_dice_2: 0.1178  loss_ce_3: 0.1618  loss_mask_3: 0.1972  loss_dice_3: 0.1214  loss_ce_4: 0.1617  loss_mask_4: 0.2007  loss_dice_4: 0.1203  loss_ce_5: 0.162  loss_mask_5: 0.1909  loss_dice_5: 0.1177  loss_ce_6: 0.1616  loss_mask_6: 0.1962  loss_dice_6: 0.1187  loss_ce_7: 0.1616  loss_mask_7: 0.1979  loss_dice_7: 0.1206  loss_ce_8: 0.1613  loss_mask_8: 0.2061  loss_dice_8: 0.1245  time: 0.5771  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:16:56] d2.utils.events INFO:  eta: 1:10:31  iter: 22619  total_loss: 4.512  loss_ce: 0.09848  loss_mask: 0.2023  loss_dice: 0.1283  loss_ce_0: 0.05235  loss_mask_0: 0.2132  loss_dice_0: 0.1307  loss_ce_1: 0.09811  loss_mask_1: 0.2102  loss_dice_1: 0.1285  loss_ce_2: 0.09824  loss_mask_2: 0.1972  loss_dice_2: 0.1304  loss_ce_3: 0.09814  loss_mask_3: 0.2042  loss_dice_3: 0.1289  loss_ce_4: 0.09826  loss_mask_4: 0.2045  loss_dice_4: 0.1265  loss_ce_5: 0.09807  loss_mask_5: 0.2004  loss_dice_5: 0.1315  loss_ce_6: 0.09826  loss_mask_6: 0.2011  loss_dice_6: 0.1282  loss_ce_7: 0.0983  loss_mask_7: 0.1939  loss_dice_7: 0.1291  loss_ce_8: 0.09848  loss_mask_8: 0.2074  loss_dice_8: 0.1294  time: 0.5772  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:17:08] d2.utils.events INFO:  eta: 1:10:35  iter: 22639  total_loss: 4.642  loss_ce: 0.09837  loss_mask: 0.2114  loss_dice: 0.1528  loss_ce_0: 0.05235  loss_mask_0: 0.2139  loss_dice_0: 0.1483  loss_ce_1: 0.09806  loss_mask_1: 0.2091  loss_dice_1: 0.1501  loss_ce_2: 0.09804  loss_mask_2: 0.2167  loss_dice_2: 0.1464  loss_ce_3: 0.098  loss_mask_3: 0.2074  loss_dice_3: 0.1481  loss_ce_4: 0.09807  loss_mask_4: 0.2134  loss_dice_4: 0.1491  loss_ce_5: 0.09793  loss_mask_5: 0.2094  loss_dice_5: 0.1519  loss_ce_6: 0.09807  loss_mask_6: 0.2059  loss_dice_6: 0.1558  loss_ce_7: 0.09818  loss_mask_7: 0.2128  loss_dice_7: 0.1511  loss_ce_8: 0.09837  loss_mask_8: 0.2101  loss_dice_8: 0.1499  time: 0.5772  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:17:21] d2.utils.events INFO:  eta: 1:10:35  iter: 22659  total_loss: 4.387  loss_ce: 0.09844  loss_mask: 0.1609  loss_dice: 0.1442  loss_ce_0: 0.05233  loss_mask_0: 0.1653  loss_dice_0: 0.1457  loss_ce_1: 0.09837  loss_mask_1: 0.1588  loss_dice_1: 0.1469  loss_ce_2: 0.0983  loss_mask_2: 0.1644  loss_dice_2: 0.1454  loss_ce_3: 0.09807  loss_mask_3: 0.1591  loss_dice_3: 0.1437  loss_ce_4: 0.0983  loss_mask_4: 0.1644  loss_dice_4: 0.1479  loss_ce_5: 0.09807  loss_mask_5: 0.1587  loss_dice_5: 0.1489  loss_ce_6: 0.0983  loss_mask_6: 0.1548  loss_dice_6: 0.1417  loss_ce_7: 0.09826  loss_mask_7: 0.1677  loss_dice_7: 0.1449  loss_ce_8: 0.09848  loss_mask_8: 0.1586  loss_dice_8: 0.152  time: 0.5773  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:17:34] d2.utils.events INFO:  eta: 1:10:34  iter: 22679  total_loss: 4.612  loss_ce: 0.09841  loss_mask: 0.201  loss_dice: 0.1332  loss_ce_0: 0.0523  loss_mask_0: 0.2014  loss_dice_0: 0.1343  loss_ce_1: 0.09843  loss_mask_1: 0.1929  loss_dice_1: 0.1351  loss_ce_2: 0.09831  loss_mask_2: 0.1915  loss_dice_2: 0.132  loss_ce_3: 0.09818  loss_mask_3: 0.2045  loss_dice_3: 0.1308  loss_ce_4: 0.09829  loss_mask_4: 0.1904  loss_dice_4: 0.1331  loss_ce_5: 0.09815  loss_mask_5: 0.1983  loss_dice_5: 0.1346  loss_ce_6: 0.09833  loss_mask_6: 0.2043  loss_dice_6: 0.131  loss_ce_7: 0.09815  loss_mask_7: 0.2033  loss_dice_7: 0.1379  loss_ce_8: 0.09845  loss_mask_8: 0.194  loss_dice_8: 0.1296  time: 0.5773  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:17:46] d2.utils.events INFO:  eta: 1:10:52  iter: 22699  total_loss: 4.522  loss_ce: 0.09833  loss_mask: 0.1754  loss_dice: 0.1434  loss_ce_0: 0.0523  loss_mask_0: 0.1662  loss_dice_0: 0.143  loss_ce_1: 0.09826  loss_mask_1: 0.1672  loss_dice_1: 0.1315  loss_ce_2: 0.09815  loss_mask_2: 0.1628  loss_dice_2: 0.1392  loss_ce_3: 0.09807  loss_mask_3: 0.1663  loss_dice_3: 0.1351  loss_ce_4: 0.09826  loss_mask_4: 0.1673  loss_dice_4: 0.1391  loss_ce_5: 0.098  loss_mask_5: 0.1636  loss_dice_5: 0.1448  loss_ce_6: 0.09818  loss_mask_6: 0.162  loss_dice_6: 0.1363  loss_ce_7: 0.09818  loss_mask_7: 0.1737  loss_dice_7: 0.1434  loss_ce_8: 0.0983  loss_mask_8: 0.1603  loss_dice_8: 0.1306  time: 0.5774  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:17:59] d2.utils.events INFO:  eta: 1:10:53  iter: 22719  total_loss: 4.404  loss_ce: 0.09833  loss_mask: 0.17  loss_dice: 0.1429  loss_ce_0: 0.0523  loss_mask_0: 0.1633  loss_dice_0: 0.1402  loss_ce_1: 0.0982  loss_mask_1: 0.1636  loss_dice_1: 0.1413  loss_ce_2: 0.09813  loss_mask_2: 0.1686  loss_dice_2: 0.147  loss_ce_3: 0.09803  loss_mask_3: 0.1657  loss_dice_3: 0.1337  loss_ce_4: 0.09815  loss_mask_4: 0.1562  loss_dice_4: 0.1349  loss_ce_5: 0.09792  loss_mask_5: 0.1705  loss_dice_5: 0.1367  loss_ce_6: 0.09818  loss_mask_6: 0.1748  loss_dice_6: 0.1346  loss_ce_7: 0.09815  loss_mask_7: 0.1709  loss_dice_7: 0.1342  loss_ce_8: 0.09833  loss_mask_8: 0.1724  loss_dice_8: 0.1403  time: 0.5774  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:18:12] d2.utils.events INFO:  eta: 1:10:56  iter: 22739  total_loss: 4.806  loss_ce: 0.09811  loss_mask: 0.1509  loss_dice: 0.1543  loss_ce_0: 0.05228  loss_mask_0: 0.1557  loss_dice_0: 0.1523  loss_ce_1: 0.0977  loss_mask_1: 0.1585  loss_dice_1: 0.1582  loss_ce_2: 0.0977  loss_mask_2: 0.1544  loss_dice_2: 0.161  loss_ce_3: 0.09777  loss_mask_3: 0.172  loss_dice_3: 0.1631  loss_ce_4: 0.09792  loss_mask_4: 0.1605  loss_dice_4: 0.1587  loss_ce_5: 0.09763  loss_mask_5: 0.1507  loss_dice_5: 0.1577  loss_ce_6: 0.09792  loss_mask_6: 0.1606  loss_dice_6: 0.1596  loss_ce_7: 0.09796  loss_mask_7: 0.1576  loss_dice_7: 0.1552  loss_ce_8: 0.09811  loss_mask_8: 0.1602  loss_dice_8: 0.1593  time: 0.5775  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:18:24] d2.utils.events INFO:  eta: 1:11:03  iter: 22759  total_loss: 4.886  loss_ce: 0.09807  loss_mask: 0.193  loss_dice: 0.145  loss_ce_0: 0.05225  loss_mask_0: 0.1995  loss_dice_0: 0.1437  loss_ce_1: 0.09806  loss_mask_1: 0.2017  loss_dice_1: 0.1423  loss_ce_2: 0.09798  loss_mask_2: 0.1946  loss_dice_2: 0.1411  loss_ce_3: 0.09789  loss_mask_3: 0.2019  loss_dice_3: 0.1416  loss_ce_4: 0.09796  loss_mask_4: 0.1994  loss_dice_4: 0.1397  loss_ce_5: 0.09785  loss_mask_5: 0.197  loss_dice_5: 0.1416  loss_ce_6: 0.09796  loss_mask_6: 0.1953  loss_dice_6: 0.1384  loss_ce_7: 0.09792  loss_mask_7: 0.1927  loss_dice_7: 0.1394  loss_ce_8: 0.09819  loss_mask_8: 0.1896  loss_dice_8: 0.1401  time: 0.5775  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:18:37] d2.utils.events INFO:  eta: 1:11:07  iter: 22779  total_loss: 4.441  loss_ce: 0.09822  loss_mask: 0.1989  loss_dice: 0.132  loss_ce_0: 0.05225  loss_mask_0: 0.1987  loss_dice_0: 0.1317  loss_ce_1: 0.09858  loss_mask_1: 0.2023  loss_dice_1: 0.1413  loss_ce_2: 0.09822  loss_mask_2: 0.2001  loss_dice_2: 0.1351  loss_ce_3: 0.09814  loss_mask_3: 0.1999  loss_dice_3: 0.1321  loss_ce_4: 0.09826  loss_mask_4: 0.1937  loss_dice_4: 0.1357  loss_ce_5: 0.098  loss_mask_5: 0.198  loss_dice_5: 0.1329  loss_ce_6: 0.09822  loss_mask_6: 0.1921  loss_dice_6: 0.1365  loss_ce_7: 0.09807  loss_mask_7: 0.1996  loss_dice_7: 0.1365  loss_ce_8: 0.09857  loss_mask_8: 0.1942  loss_dice_8: 0.1333  time: 0.5776  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:18:49] d2.utils.events INFO:  eta: 1:11:13  iter: 22799  total_loss: 4.429  loss_ce: 0.1299  loss_mask: 0.2002  loss_dice: 0.1379  loss_ce_0: 0.06373  loss_mask_0: 0.196  loss_dice_0: 0.1338  loss_ce_1: 0.13  loss_mask_1: 0.2012  loss_dice_1: 0.1345  loss_ce_2: 0.13  loss_mask_2: 0.194  loss_dice_2: 0.1373  loss_ce_3: 0.13  loss_mask_3: 0.1967  loss_dice_3: 0.1301  loss_ce_4: 0.13  loss_mask_4: 0.1995  loss_dice_4: 0.1331  loss_ce_5: 0.1301  loss_mask_5: 0.1954  loss_dice_5: 0.1348  loss_ce_6: 0.13  loss_mask_6: 0.2018  loss_dice_6: 0.139  loss_ce_7: 0.13  loss_mask_7: 0.1925  loss_dice_7: 0.1332  loss_ce_8: 0.1299  loss_mask_8: 0.1973  loss_dice_8: 0.1377  time: 0.5776  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:19:02] d2.utils.events INFO:  eta: 1:11:17  iter: 22819  total_loss: 4.76  loss_ce: 0.09818  loss_mask: 0.2123  loss_dice: 0.1403  loss_ce_0: 0.05226  loss_mask_0: 0.2089  loss_dice_0: 0.1421  loss_ce_1: 0.09822  loss_mask_1: 0.2217  loss_dice_1: 0.1394  loss_ce_2: 0.09798  loss_mask_2: 0.2183  loss_dice_2: 0.1483  loss_ce_3: 0.09792  loss_mask_3: 0.217  loss_dice_3: 0.14  loss_ce_4: 0.098  loss_mask_4: 0.2251  loss_dice_4: 0.1463  loss_ce_5: 0.09785  loss_mask_5: 0.2113  loss_dice_5: 0.1398  loss_ce_6: 0.09807  loss_mask_6: 0.2105  loss_dice_6: 0.1456  loss_ce_7: 0.098  loss_mask_7: 0.2134  loss_dice_7: 0.1391  loss_ce_8: 0.09818  loss_mask_8: 0.2097  loss_dice_8: 0.1386  time: 0.5777  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:19:15] d2.utils.events INFO:  eta: 1:11:14  iter: 22839  total_loss: 4.796  loss_ce: 0.09811  loss_mask: 0.2229  loss_dice: 0.1488  loss_ce_0: 0.05225  loss_mask_0: 0.2289  loss_dice_0: 0.151  loss_ce_1: 0.098  loss_mask_1: 0.2156  loss_dice_1: 0.1529  loss_ce_2: 0.09803  loss_mask_2: 0.2217  loss_dice_2: 0.1553  loss_ce_3: 0.09792  loss_mask_3: 0.241  loss_dice_3: 0.1557  loss_ce_4: 0.09803  loss_mask_4: 0.2209  loss_dice_4: 0.1458  loss_ce_5: 0.09778  loss_mask_5: 0.2352  loss_dice_5: 0.151  loss_ce_6: 0.098  loss_mask_6: 0.2252  loss_dice_6: 0.1524  loss_ce_7: 0.09792  loss_mask_7: 0.2198  loss_dice_7: 0.1533  loss_ce_8: 0.09815  loss_mask_8: 0.2319  loss_dice_8: 0.1543  time: 0.5777  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:19:27] d2.utils.events INFO:  eta: 1:11:15  iter: 22859  total_loss: 4.502  loss_ce: 0.09796  loss_mask: 0.2011  loss_dice: 0.1283  loss_ce_0: 0.05221  loss_mask_0: 0.199  loss_dice_0: 0.1338  loss_ce_1: 0.09796  loss_mask_1: 0.2041  loss_dice_1: 0.1379  loss_ce_2: 0.09778  loss_mask_2: 0.2072  loss_dice_2: 0.1378  loss_ce_3: 0.0977  loss_mask_3: 0.2107  loss_dice_3: 0.1298  loss_ce_4: 0.09781  loss_mask_4: 0.1948  loss_dice_4: 0.1336  loss_ce_5: 0.09763  loss_mask_5: 0.1949  loss_dice_5: 0.1352  loss_ce_6: 0.09778  loss_mask_6: 0.1951  loss_dice_6: 0.1361  loss_ce_7: 0.09781  loss_mask_7: 0.203  loss_dice_7: 0.1373  loss_ce_8: 0.098  loss_mask_8: 0.2017  loss_dice_8: 0.1327  time: 0.5778  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:19:40] d2.utils.events INFO:  eta: 1:11:14  iter: 22879  total_loss: 4.617  loss_ce: 0.13  loss_mask: 0.2062  loss_dice: 0.1397  loss_ce_0: 0.06374  loss_mask_0: 0.2061  loss_dice_0: 0.1347  loss_ce_1: 0.1298  loss_mask_1: 0.205  loss_dice_1: 0.1349  loss_ce_2: 0.1298  loss_mask_2: 0.2202  loss_dice_2: 0.1342  loss_ce_3: 0.1299  loss_mask_3: 0.2154  loss_dice_3: 0.1362  loss_ce_4: 0.1298  loss_mask_4: 0.2163  loss_dice_4: 0.134  loss_ce_5: 0.1299  loss_mask_5: 0.2093  loss_dice_5: 0.1322  loss_ce_6: 0.1298  loss_mask_6: 0.2083  loss_dice_6: 0.1367  loss_ce_7: 0.1299  loss_mask_7: 0.2123  loss_dice_7: 0.1308  loss_ce_8: 0.1299  loss_mask_8: 0.2126  loss_dice_8: 0.1348  time: 0.5778  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:19:53] d2.utils.events INFO:  eta: 1:11:27  iter: 22899  total_loss: 4.473  loss_ce: 0.1619  loss_mask: 0.1747  loss_dice: 0.1235  loss_ce_0: 0.0752  loss_mask_0: 0.1707  loss_dice_0: 0.1227  loss_ce_1: 0.1624  loss_mask_1: 0.1708  loss_dice_1: 0.1266  loss_ce_2: 0.1622  loss_mask_2: 0.176  loss_dice_2: 0.1273  loss_ce_3: 0.1622  loss_mask_3: 0.172  loss_dice_3: 0.1259  loss_ce_4: 0.1623  loss_mask_4: 0.1708  loss_dice_4: 0.1272  loss_ce_5: 0.1625  loss_mask_5: 0.1711  loss_dice_5: 0.1282  loss_ce_6: 0.1622  loss_mask_6: 0.1747  loss_dice_6: 0.1335  loss_ce_7: 0.1621  loss_mask_7: 0.172  loss_dice_7: 0.1293  loss_ce_8: 0.1621  loss_mask_8: 0.1769  loss_dice_8: 0.1302  time: 0.5778  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:20:05] d2.utils.events INFO:  eta: 1:11:19  iter: 22919  total_loss: 4.627  loss_ce: 0.09833  loss_mask: 0.2099  loss_dice: 0.1381  loss_ce_0: 0.05228  loss_mask_0: 0.2055  loss_dice_0: 0.1411  loss_ce_1: 0.09839  loss_mask_1: 0.2228  loss_dice_1: 0.1427  loss_ce_2: 0.09826  loss_mask_2: 0.2255  loss_dice_2: 0.1461  loss_ce_3: 0.09811  loss_mask_3: 0.2112  loss_dice_3: 0.1514  loss_ce_4: 0.09833  loss_mask_4: 0.2213  loss_dice_4: 0.1409  loss_ce_5: 0.09807  loss_mask_5: 0.2177  loss_dice_5: 0.1478  loss_ce_6: 0.09822  loss_mask_6: 0.222  loss_dice_6: 0.1471  loss_ce_7: 0.09815  loss_mask_7: 0.2216  loss_dice_7: 0.1419  loss_ce_8: 0.09837  loss_mask_8: 0.225  loss_dice_8: 0.1476  time: 0.5779  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:20:18] d2.utils.events INFO:  eta: 1:11:17  iter: 22939  total_loss: 4.556  loss_ce: 0.09833  loss_mask: 0.1889  loss_dice: 0.1262  loss_ce_0: 0.05229  loss_mask_0: 0.1853  loss_dice_0: 0.1236  loss_ce_1: 0.09854  loss_mask_1: 0.191  loss_dice_1: 0.1257  loss_ce_2: 0.09833  loss_mask_2: 0.1841  loss_dice_2: 0.1233  loss_ce_3: 0.09826  loss_mask_3: 0.1842  loss_dice_3: 0.1284  loss_ce_4: 0.09829  loss_mask_4: 0.188  loss_dice_4: 0.1281  loss_ce_5: 0.09815  loss_mask_5: 0.1926  loss_dice_5: 0.1285  loss_ce_6: 0.09837  loss_mask_6: 0.1944  loss_dice_6: 0.1299  loss_ce_7: 0.09822  loss_mask_7: 0.1896  loss_dice_7: 0.126  loss_ce_8: 0.09841  loss_mask_8: 0.1812  loss_dice_8: 0.1289  time: 0.5779  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:20:31] d2.utils.events INFO:  eta: 1:11:11  iter: 22959  total_loss: 4.512  loss_ce: 0.1616  loss_mask: 0.1942  loss_dice: 0.1298  loss_ce_0: 0.07516  loss_mask_0: 0.1937  loss_dice_0: 0.1345  loss_ce_1: 0.1616  loss_mask_1: 0.1905  loss_dice_1: 0.1303  loss_ce_2: 0.1617  loss_mask_2: 0.19  loss_dice_2: 0.1312  loss_ce_3: 0.1618  loss_mask_3: 0.1913  loss_dice_3: 0.1337  loss_ce_4: 0.1616  loss_mask_4: 0.1882  loss_dice_4: 0.1321  loss_ce_5: 0.1619  loss_mask_5: 0.2058  loss_dice_5: 0.1363  loss_ce_6: 0.1617  loss_mask_6: 0.1926  loss_dice_6: 0.1294  loss_ce_7: 0.1618  loss_mask_7: 0.1909  loss_dice_7: 0.1344  loss_ce_8: 0.1615  loss_mask_8: 0.18  loss_dice_8: 0.1351  time: 0.5780  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:20:43] d2.utils.events INFO:  eta: 1:11:06  iter: 22979  total_loss: 4.739  loss_ce: 0.09822  loss_mask: 0.1492  loss_dice: 0.1603  loss_ce_0: 0.0523  loss_mask_0: 0.1472  loss_dice_0: 0.1517  loss_ce_1: 0.09828  loss_mask_1: 0.1545  loss_dice_1: 0.1595  loss_ce_2: 0.09805  loss_mask_2: 0.1527  loss_dice_2: 0.1587  loss_ce_3: 0.09811  loss_mask_3: 0.1499  loss_dice_3: 0.1626  loss_ce_4: 0.09818  loss_mask_4: 0.1521  loss_dice_4: 0.1558  loss_ce_5: 0.098  loss_mask_5: 0.1502  loss_dice_5: 0.1607  loss_ce_6: 0.09822  loss_mask_6: 0.1504  loss_dice_6: 0.1669  loss_ce_7: 0.09826  loss_mask_7: 0.1583  loss_dice_7: 0.1636  loss_ce_8: 0.09876  loss_mask_8: 0.1624  loss_dice_8: 0.1582  time: 0.5780  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:20:56] d2.utils.events INFO:  eta: 1:10:59  iter: 22999  total_loss: 4.415  loss_ce: 0.09833  loss_mask: 0.1374  loss_dice: 0.148  loss_ce_0: 0.0523  loss_mask_0: 0.1421  loss_dice_0: 0.1473  loss_ce_1: 0.09833  loss_mask_1: 0.1401  loss_dice_1: 0.149  loss_ce_2: 0.09822  loss_mask_2: 0.1363  loss_dice_2: 0.1451  loss_ce_3: 0.09822  loss_mask_3: 0.1354  loss_dice_3: 0.1467  loss_ce_4: 0.09826  loss_mask_4: 0.1425  loss_dice_4: 0.1527  loss_ce_5: 0.09807  loss_mask_5: 0.1437  loss_dice_5: 0.1408  loss_ce_6: 0.09815  loss_mask_6: 0.1512  loss_dice_6: 0.1513  loss_ce_7: 0.09826  loss_mask_7: 0.1405  loss_dice_7: 0.1489  loss_ce_8: 0.09833  loss_mask_8: 0.1376  loss_dice_8: 0.1473  time: 0.5781  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:21:08] d2.utils.events INFO:  eta: 1:10:51  iter: 23019  total_loss: 4.446  loss_ce: 0.09822  loss_mask: 0.1654  loss_dice: 0.1323  loss_ce_0: 0.0523  loss_mask_0: 0.1699  loss_dice_0: 0.1354  loss_ce_1: 0.09826  loss_mask_1: 0.1657  loss_dice_1: 0.1346  loss_ce_2: 0.09822  loss_mask_2: 0.1646  loss_dice_2: 0.1309  loss_ce_3: 0.09815  loss_mask_3: 0.1664  loss_dice_3: 0.1401  loss_ce_4: 0.09815  loss_mask_4: 0.1699  loss_dice_4: 0.1298  loss_ce_5: 0.09807  loss_mask_5: 0.1634  loss_dice_5: 0.1354  loss_ce_6: 0.09815  loss_mask_6: 0.1698  loss_dice_6: 0.138  loss_ce_7: 0.09822  loss_mask_7: 0.1725  loss_dice_7: 0.1343  loss_ce_8: 0.09822  loss_mask_8: 0.1699  loss_dice_8: 0.1411  time: 0.5781  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:21:21] d2.utils.events INFO:  eta: 1:10:48  iter: 23039  total_loss: 4.933  loss_ce: 0.09804  loss_mask: 0.1873  loss_dice: 0.1775  loss_ce_0: 0.05224  loss_mask_0: 0.1818  loss_dice_0: 0.1767  loss_ce_1: 0.09781  loss_mask_1: 0.1854  loss_dice_1: 0.1782  loss_ce_2: 0.0979  loss_mask_2: 0.1923  loss_dice_2: 0.1845  loss_ce_3: 0.09792  loss_mask_3: 0.1925  loss_dice_3: 0.1785  loss_ce_4: 0.09799  loss_mask_4: 0.1878  loss_dice_4: 0.1735  loss_ce_5: 0.09785  loss_mask_5: 0.192  loss_dice_5: 0.1893  loss_ce_6: 0.09792  loss_mask_6: 0.1871  loss_dice_6: 0.1801  loss_ce_7: 0.098  loss_mask_7: 0.1956  loss_dice_7: 0.1729  loss_ce_8: 0.09793  loss_mask_8: 0.1833  loss_dice_8: 0.1831  time: 0.5782  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:21:34] d2.utils.events INFO:  eta: 1:10:38  iter: 23059  total_loss: 4.407  loss_ce: 0.09804  loss_mask: 0.1386  loss_dice: 0.1659  loss_ce_0: 0.05222  loss_mask_0: 0.1415  loss_dice_0: 0.159  loss_ce_1: 0.09794  loss_mask_1: 0.1499  loss_dice_1: 0.1656  loss_ce_2: 0.09805  loss_mask_2: 0.1386  loss_dice_2: 0.1618  loss_ce_3: 0.09792  loss_mask_3: 0.14  loss_dice_3: 0.1599  loss_ce_4: 0.098  loss_mask_4: 0.1417  loss_dice_4: 0.164  loss_ce_5: 0.09785  loss_mask_5: 0.1358  loss_dice_5: 0.1616  loss_ce_6: 0.09796  loss_mask_6: 0.1362  loss_dice_6: 0.1643  loss_ce_7: 0.09792  loss_mask_7: 0.1421  loss_dice_7: 0.1604  loss_ce_8: 0.09811  loss_mask_8: 0.1399  loss_dice_8: 0.1661  time: 0.5782  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:21:46] d2.utils.events INFO:  eta: 1:10:33  iter: 23079  total_loss: 4.487  loss_ce: 0.09785  loss_mask: 0.1664  loss_dice: 0.1507  loss_ce_0: 0.05221  loss_mask_0: 0.1652  loss_dice_0: 0.1575  loss_ce_1: 0.09761  loss_mask_1: 0.1696  loss_dice_1: 0.1543  loss_ce_2: 0.09765  loss_mask_2: 0.1655  loss_dice_2: 0.1527  loss_ce_3: 0.09766  loss_mask_3: 0.1599  loss_dice_3: 0.1469  loss_ce_4: 0.09763  loss_mask_4: 0.169  loss_dice_4: 0.1583  loss_ce_5: 0.09755  loss_mask_5: 0.1656  loss_dice_5: 0.1555  loss_ce_6: 0.09766  loss_mask_6: 0.1694  loss_dice_6: 0.1501  loss_ce_7: 0.09774  loss_mask_7: 0.1673  loss_dice_7: 0.1517  loss_ce_8: 0.09785  loss_mask_8: 0.164  loss_dice_8: 0.1518  time: 0.5783  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:21:59] d2.utils.events INFO:  eta: 1:10:26  iter: 23099  total_loss: 4.903  loss_ce: 0.09763  loss_mask: 0.2042  loss_dice: 0.1546  loss_ce_0: 0.05218  loss_mask_0: 0.1952  loss_dice_0: 0.1555  loss_ce_1: 0.09735  loss_mask_1: 0.2077  loss_dice_1: 0.1577  loss_ce_2: 0.09744  loss_mask_2: 0.1932  loss_dice_2: 0.1555  loss_ce_3: 0.0974  loss_mask_3: 0.1943  loss_dice_3: 0.1526  loss_ce_4: 0.09752  loss_mask_4: 0.1897  loss_dice_4: 0.1516  loss_ce_5: 0.09733  loss_mask_5: 0.2044  loss_dice_5: 0.1524  loss_ce_6: 0.09748  loss_mask_6: 0.1961  loss_dice_6: 0.1506  loss_ce_7: 0.09759  loss_mask_7: 0.1947  loss_dice_7: 0.1514  loss_ce_8: 0.09763  loss_mask_8: 0.1931  loss_dice_8: 0.1563  time: 0.5783  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:22:12] d2.utils.events INFO:  eta: 1:10:19  iter: 23119  total_loss: 4.531  loss_ce: 0.1624  loss_mask: 0.1849  loss_dice: 0.122  loss_ce_0: 0.07533  loss_mask_0: 0.1868  loss_dice_0: 0.1209  loss_ce_1: 0.1626  loss_mask_1: 0.1899  loss_dice_1: 0.1178  loss_ce_2: 0.1627  loss_mask_2: 0.1833  loss_dice_2: 0.117  loss_ce_3: 0.1626  loss_mask_3: 0.1862  loss_dice_3: 0.1184  loss_ce_4: 0.1627  loss_mask_4: 0.1861  loss_dice_4: 0.1229  loss_ce_5: 0.1629  loss_mask_5: 0.1932  loss_dice_5: 0.1231  loss_ce_6: 0.1626  loss_mask_6: 0.1807  loss_dice_6: 0.1188  loss_ce_7: 0.1627  loss_mask_7: 0.1815  loss_dice_7: 0.1221  loss_ce_8: 0.1624  loss_mask_8: 0.1826  loss_dice_8: 0.1222  time: 0.5784  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:22:24] d2.utils.events INFO:  eta: 1:10:17  iter: 23139  total_loss: 4.816  loss_ce: 0.1621  loss_mask: 0.199  loss_dice: 0.1263  loss_ce_0: 0.07524  loss_mask_0: 0.1971  loss_dice_0: 0.1276  loss_ce_1: 0.1618  loss_mask_1: 0.1977  loss_dice_1: 0.1292  loss_ce_2: 0.162  loss_mask_2: 0.1927  loss_dice_2: 0.1214  loss_ce_3: 0.1622  loss_mask_3: 0.2082  loss_dice_3: 0.1269  loss_ce_4: 0.1621  loss_mask_4: 0.1945  loss_dice_4: 0.1248  loss_ce_5: 0.1623  loss_mask_5: 0.2084  loss_dice_5: 0.1301  loss_ce_6: 0.1622  loss_mask_6: 0.1941  loss_dice_6: 0.1289  loss_ce_7: 0.1623  loss_mask_7: 0.2051  loss_dice_7: 0.1313  loss_ce_8: 0.1621  loss_mask_8: 0.1991  loss_dice_8: 0.1305  time: 0.5784  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:22:37] d2.utils.events INFO:  eta: 1:10:12  iter: 23159  total_loss: 4.427  loss_ce: 0.09818  loss_mask: 0.1701  loss_dice: 0.1436  loss_ce_0: 0.05225  loss_mask_0: 0.1653  loss_dice_0: 0.1379  loss_ce_1: 0.09831  loss_mask_1: 0.1688  loss_dice_1: 0.1447  loss_ce_2: 0.09824  loss_mask_2: 0.1713  loss_dice_2: 0.136  loss_ce_3: 0.09814  loss_mask_3: 0.175  loss_dice_3: 0.1371  loss_ce_4: 0.09829  loss_mask_4: 0.1734  loss_dice_4: 0.1385  loss_ce_5: 0.09809  loss_mask_5: 0.1673  loss_dice_5: 0.1396  loss_ce_6: 0.09818  loss_mask_6: 0.1759  loss_dice_6: 0.1452  loss_ce_7: 0.09804  loss_mask_7: 0.1707  loss_dice_7: 0.1425  loss_ce_8: 0.09865  loss_mask_8: 0.1713  loss_dice_8: 0.144  time: 0.5784  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:22:50] d2.utils.events INFO:  eta: 1:10:10  iter: 23179  total_loss: 4.511  loss_ce: 0.1615  loss_mask: 0.1664  loss_dice: 0.1376  loss_ce_0: 0.0752  loss_mask_0: 0.1726  loss_dice_0: 0.1391  loss_ce_1: 0.1609  loss_mask_1: 0.1666  loss_dice_1: 0.1439  loss_ce_2: 0.1614  loss_mask_2: 0.1702  loss_dice_2: 0.1363  loss_ce_3: 0.1617  loss_mask_3: 0.1678  loss_dice_3: 0.1409  loss_ce_4: 0.1613  loss_mask_4: 0.1681  loss_dice_4: 0.1359  loss_ce_5: 0.1619  loss_mask_5: 0.1703  loss_dice_5: 0.1435  loss_ce_6: 0.1614  loss_mask_6: 0.1707  loss_dice_6: 0.1421  loss_ce_7: 0.1617  loss_mask_7: 0.1721  loss_dice_7: 0.1417  loss_ce_8: 0.1615  loss_mask_8: 0.1626  loss_dice_8: 0.139  time: 0.5785  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:23:02] d2.utils.events INFO:  eta: 1:10:06  iter: 23199  total_loss: 4.941  loss_ce: 0.09815  loss_mask: 0.2168  loss_dice: 0.1582  loss_ce_0: 0.05228  loss_mask_0: 0.2175  loss_dice_0: 0.1533  loss_ce_1: 0.09819  loss_mask_1: 0.2259  loss_dice_1: 0.1597  loss_ce_2: 0.09809  loss_mask_2: 0.2141  loss_dice_2: 0.1541  loss_ce_3: 0.09803  loss_mask_3: 0.217  loss_dice_3: 0.1539  loss_ce_4: 0.09807  loss_mask_4: 0.2246  loss_dice_4: 0.1624  loss_ce_5: 0.09792  loss_mask_5: 0.2277  loss_dice_5: 0.1606  loss_ce_6: 0.09807  loss_mask_6: 0.209  loss_dice_6: 0.1545  loss_ce_7: 0.09811  loss_mask_7: 0.232  loss_dice_7: 0.1626  loss_ce_8: 0.09819  loss_mask_8: 0.2305  loss_dice_8: 0.1584  time: 0.5785  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:23:15] d2.utils.events INFO:  eta: 1:10:00  iter: 23219  total_loss: 4.784  loss_ce: 0.09826  loss_mask: 0.1804  loss_dice: 0.1576  loss_ce_0: 0.05228  loss_mask_0: 0.1847  loss_dice_0: 0.1585  loss_ce_1: 0.09845  loss_mask_1: 0.1891  loss_dice_1: 0.1558  loss_ce_2: 0.09826  loss_mask_2: 0.1755  loss_dice_2: 0.1524  loss_ce_3: 0.09815  loss_mask_3: 0.1805  loss_dice_3: 0.1509  loss_ce_4: 0.09826  loss_mask_4: 0.1845  loss_dice_4: 0.1636  loss_ce_5: 0.09807  loss_mask_5: 0.1816  loss_dice_5: 0.1525  loss_ce_6: 0.09822  loss_mask_6: 0.1821  loss_dice_6: 0.1568  loss_ce_7: 0.09815  loss_mask_7: 0.1854  loss_dice_7: 0.1552  loss_ce_8: 0.09826  loss_mask_8: 0.1792  loss_dice_8: 0.1529  time: 0.5786  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:23:28] d2.utils.events INFO:  eta: 1:09:51  iter: 23239  total_loss: 4.501  loss_ce: 0.1614  loss_mask: 0.1492  loss_dice: 0.1432  loss_ce_0: 0.07516  loss_mask_0: 0.1461  loss_dice_0: 0.1505  loss_ce_1: 0.1603  loss_mask_1: 0.152  loss_dice_1: 0.1455  loss_ce_2: 0.161  loss_mask_2: 0.1479  loss_dice_2: 0.1436  loss_ce_3: 0.1612  loss_mask_3: 0.1486  loss_dice_3: 0.1413  loss_ce_4: 0.1611  loss_mask_4: 0.1513  loss_dice_4: 0.1436  loss_ce_5: 0.1614  loss_mask_5: 0.1546  loss_dice_5: 0.1462  loss_ce_6: 0.1611  loss_mask_6: 0.1476  loss_dice_6: 0.1373  loss_ce_7: 0.1615  loss_mask_7: 0.1488  loss_dice_7: 0.1402  loss_ce_8: 0.1613  loss_mask_8: 0.1421  loss_dice_8: 0.1404  time: 0.5786  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:23:40] d2.utils.events INFO:  eta: 1:09:47  iter: 23259  total_loss: 4.482  loss_ce: 0.09844  loss_mask: 0.1965  loss_dice: 0.1391  loss_ce_0: 0.05232  loss_mask_0: 0.196  loss_dice_0: 0.1424  loss_ce_1: 0.09878  loss_mask_1: 0.1959  loss_dice_1: 0.1357  loss_ce_2: 0.09856  loss_mask_2: 0.1976  loss_dice_2: 0.1396  loss_ce_3: 0.09848  loss_mask_3: 0.1987  loss_dice_3: 0.1369  loss_ce_4: 0.09856  loss_mask_4: 0.196  loss_dice_4: 0.1368  loss_ce_5: 0.09837  loss_mask_5: 0.1959  loss_dice_5: 0.1418  loss_ce_6: 0.09844  loss_mask_6: 0.1969  loss_dice_6: 0.1422  loss_ce_7: 0.09837  loss_mask_7: 0.2008  loss_dice_7: 0.1372  loss_ce_8: 0.09852  loss_mask_8: 0.191  loss_dice_8: 0.1376  time: 0.5787  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:23:53] d2.utils.events INFO:  eta: 1:09:41  iter: 23279  total_loss: 4.415  loss_ce: 0.09856  loss_mask: 0.1806  loss_dice: 0.143  loss_ce_0: 0.05232  loss_mask_0: 0.1846  loss_dice_0: 0.1416  loss_ce_1: 0.09923  loss_mask_1: 0.1874  loss_dice_1: 0.1427  loss_ce_2: 0.09874  loss_mask_2: 0.1851  loss_dice_2: 0.1366  loss_ce_3: 0.09863  loss_mask_3: 0.1884  loss_dice_3: 0.1396  loss_ce_4: 0.09878  loss_mask_4: 0.1812  loss_dice_4: 0.145  loss_ce_5: 0.09852  loss_mask_5: 0.1786  loss_dice_5: 0.1437  loss_ce_6: 0.09867  loss_mask_6: 0.1873  loss_dice_6: 0.143  loss_ce_7: 0.09852  loss_mask_7: 0.1838  loss_dice_7: 0.137  loss_ce_8: 0.09863  loss_mask_8: 0.187  loss_dice_8: 0.1386  time: 0.5787  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:24:06] d2.utils.events INFO:  eta: 1:09:33  iter: 23299  total_loss: 4.611  loss_ce: 0.09837  loss_mask: 0.162  loss_dice: 0.1549  loss_ce_0: 0.05231  loss_mask_0: 0.1507  loss_dice_0: 0.1498  loss_ce_1: 0.09858  loss_mask_1: 0.161  loss_dice_1: 0.1601  loss_ce_2: 0.09841  loss_mask_2: 0.1577  loss_dice_2: 0.1569  loss_ce_3: 0.09837  loss_mask_3: 0.1574  loss_dice_3: 0.1533  loss_ce_4: 0.09848  loss_mask_4: 0.1546  loss_dice_4: 0.1493  loss_ce_5: 0.09829  loss_mask_5: 0.1583  loss_dice_5: 0.1535  loss_ce_6: 0.09837  loss_mask_6: 0.1525  loss_dice_6: 0.1498  loss_ce_7: 0.09833  loss_mask_7: 0.1556  loss_dice_7: 0.1523  loss_ce_8: 0.09845  loss_mask_8: 0.1526  loss_dice_8: 0.1504  time: 0.5788  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:24:18] d2.utils.events INFO:  eta: 1:09:25  iter: 23319  total_loss: 4.469  loss_ce: 0.09833  loss_mask: 0.1831  loss_dice: 0.1342  loss_ce_0: 0.05231  loss_mask_0: 0.1917  loss_dice_0: 0.1299  loss_ce_1: 0.09846  loss_mask_1: 0.1837  loss_dice_1: 0.1303  loss_ce_2: 0.09843  loss_mask_2: 0.1864  loss_dice_2: 0.1345  loss_ce_3: 0.09833  loss_mask_3: 0.1893  loss_dice_3: 0.1383  loss_ce_4: 0.09848  loss_mask_4: 0.192  loss_dice_4: 0.1293  loss_ce_5: 0.09837  loss_mask_5: 0.1898  loss_dice_5: 0.1293  loss_ce_6: 0.09841  loss_mask_6: 0.1802  loss_dice_6: 0.134  loss_ce_7: 0.09833  loss_mask_7: 0.1848  loss_dice_7: 0.1363  loss_ce_8: 0.09845  loss_mask_8: 0.1858  loss_dice_8: 0.1285  time: 0.5788  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:24:31] d2.utils.events INFO:  eta: 1:09:18  iter: 23339  total_loss: 5.21  loss_ce: 0.09833  loss_mask: 0.2167  loss_dice: 0.1585  loss_ce_0: 0.05229  loss_mask_0: 0.23  loss_dice_0: 0.1535  loss_ce_1: 0.09846  loss_mask_1: 0.2165  loss_dice_1: 0.1543  loss_ce_2: 0.09837  loss_mask_2: 0.2199  loss_dice_2: 0.1562  loss_ce_3: 0.09829  loss_mask_3: 0.209  loss_dice_3: 0.1617  loss_ce_4: 0.0983  loss_mask_4: 0.2208  loss_dice_4: 0.1538  loss_ce_5: 0.09829  loss_mask_5: 0.2209  loss_dice_5: 0.1595  loss_ce_6: 0.09837  loss_mask_6: 0.2166  loss_dice_6: 0.1533  loss_ce_7: 0.09822  loss_mask_7: 0.2301  loss_dice_7: 0.1562  loss_ce_8: 0.09837  loss_mask_8: 0.2237  loss_dice_8: 0.1554  time: 0.5789  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:24:44] d2.utils.events INFO:  eta: 1:09:09  iter: 23359  total_loss: 4.854  loss_ce: 0.09822  loss_mask: 0.2027  loss_dice: 0.1483  loss_ce_0: 0.05228  loss_mask_0: 0.191  loss_dice_0: 0.1454  loss_ce_1: 0.09841  loss_mask_1: 0.193  loss_dice_1: 0.1407  loss_ce_2: 0.09831  loss_mask_2: 0.2047  loss_dice_2: 0.1403  loss_ce_3: 0.09818  loss_mask_3: 0.2054  loss_dice_3: 0.1462  loss_ce_4: 0.09822  loss_mask_4: 0.2012  loss_dice_4: 0.1472  loss_ce_5: 0.09807  loss_mask_5: 0.2026  loss_dice_5: 0.1435  loss_ce_6: 0.09822  loss_mask_6: 0.1931  loss_dice_6: 0.1456  loss_ce_7: 0.09807  loss_mask_7: 0.1996  loss_dice_7: 0.1481  loss_ce_8: 0.09826  loss_mask_8: 0.2012  loss_dice_8: 0.1442  time: 0.5789  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:24:57] d2.utils.events INFO:  eta: 1:09:04  iter: 23379  total_loss: 4.856  loss_ce: 0.09811  loss_mask: 0.1831  loss_dice: 0.1682  loss_ce_0: 0.05227  loss_mask_0: 0.1943  loss_dice_0: 0.171  loss_ce_1: 0.09802  loss_mask_1: 0.2101  loss_dice_1: 0.1806  loss_ce_2: 0.09817  loss_mask_2: 0.195  loss_dice_2: 0.1767  loss_ce_3: 0.09811  loss_mask_3: 0.2087  loss_dice_3: 0.1748  loss_ce_4: 0.09815  loss_mask_4: 0.1938  loss_dice_4: 0.1735  loss_ce_5: 0.09807  loss_mask_5: 0.2055  loss_dice_5: 0.1722  loss_ce_6: 0.09815  loss_mask_6: 0.2045  loss_dice_6: 0.1752  loss_ce_7: 0.09807  loss_mask_7: 0.1969  loss_dice_7: 0.1708  loss_ce_8: 0.09815  loss_mask_8: 0.2079  loss_dice_8: 0.174  time: 0.5790  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:25:09] d2.utils.events INFO:  eta: 1:09:06  iter: 23399  total_loss: 4.637  loss_ce: 0.1299  loss_mask: 0.19  loss_dice: 0.1296  loss_ce_0: 0.06373  loss_mask_0: 0.197  loss_dice_0: 0.129  loss_ce_1: 0.1298  loss_mask_1: 0.2044  loss_dice_1: 0.1224  loss_ce_2: 0.1298  loss_mask_2: 0.2075  loss_dice_2: 0.1283  loss_ce_3: 0.1298  loss_mask_3: 0.2067  loss_dice_3: 0.1304  loss_ce_4: 0.1299  loss_mask_4: 0.1946  loss_dice_4: 0.13  loss_ce_5: 0.13  loss_mask_5: 0.2011  loss_dice_5: 0.129  loss_ce_6: 0.1298  loss_mask_6: 0.1979  loss_dice_6: 0.1249  loss_ce_7: 0.13  loss_mask_7: 0.1981  loss_dice_7: 0.1322  loss_ce_8: 0.1299  loss_mask_8: 0.1998  loss_dice_8: 0.1286  time: 0.5790  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:25:22] d2.utils.events INFO:  eta: 1:09:01  iter: 23419  total_loss: 4.338  loss_ce: 0.1617  loss_mask: 0.204  loss_dice: 0.1174  loss_ce_0: 0.07518  loss_mask_0: 0.2007  loss_dice_0: 0.12  loss_ce_1: 0.1613  loss_mask_1: 0.2024  loss_dice_1: 0.1176  loss_ce_2: 0.1615  loss_mask_2: 0.2018  loss_dice_2: 0.1202  loss_ce_3: 0.1616  loss_mask_3: 0.2038  loss_dice_3: 0.1191  loss_ce_4: 0.1614  loss_mask_4: 0.1958  loss_dice_4: 0.1173  loss_ce_5: 0.1618  loss_mask_5: 0.1984  loss_dice_5: 0.1177  loss_ce_6: 0.1616  loss_mask_6: 0.1993  loss_dice_6: 0.1161  loss_ce_7: 0.1618  loss_mask_7: 0.2046  loss_dice_7: 0.1193  loss_ce_8: 0.1617  loss_mask_8: 0.2033  loss_dice_8: 0.1214  time: 0.5791  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:25:34] d2.utils.events INFO:  eta: 1:08:48  iter: 23439  total_loss: 4.303  loss_ce: 0.1298  loss_mask: 0.1418  loss_dice: 0.1455  loss_ce_0: 0.06372  loss_mask_0: 0.1372  loss_dice_0: 0.1509  loss_ce_1: 0.1295  loss_mask_1: 0.1439  loss_dice_1: 0.1489  loss_ce_2: 0.1297  loss_mask_2: 0.1405  loss_dice_2: 0.148  loss_ce_3: 0.1298  loss_mask_3: 0.1443  loss_dice_3: 0.1494  loss_ce_4: 0.1298  loss_mask_4: 0.1388  loss_dice_4: 0.1528  loss_ce_5: 0.1299  loss_mask_5: 0.1376  loss_dice_5: 0.1471  loss_ce_6: 0.1297  loss_mask_6: 0.1441  loss_dice_6: 0.1457  loss_ce_7: 0.1298  loss_mask_7: 0.135  loss_dice_7: 0.1511  loss_ce_8: 0.1299  loss_mask_8: 0.1424  loss_dice_8: 0.1416  time: 0.5791  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:25:47] d2.utils.events INFO:  eta: 1:08:35  iter: 23459  total_loss: 4.948  loss_ce: 0.1615  loss_mask: 0.2048  loss_dice: 0.1351  loss_ce_0: 0.07511  loss_mask_0: 0.2075  loss_dice_0: 0.1396  loss_ce_1: 0.1613  loss_mask_1: 0.2072  loss_dice_1: 0.1383  loss_ce_2: 0.1615  loss_mask_2: 0.2048  loss_dice_2: 0.1337  loss_ce_3: 0.1614  loss_mask_3: 0.2059  loss_dice_3: 0.136  loss_ce_4: 0.1615  loss_mask_4: 0.2012  loss_dice_4: 0.1354  loss_ce_5: 0.1617  loss_mask_5: 0.2026  loss_dice_5: 0.134  loss_ce_6: 0.1614  loss_mask_6: 0.205  loss_dice_6: 0.1387  loss_ce_7: 0.1615  loss_mask_7: 0.2052  loss_dice_7: 0.1367  loss_ce_8: 0.1614  loss_mask_8: 0.2137  loss_dice_8: 0.1358  time: 0.5791  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:26:00] d2.utils.events INFO:  eta: 1:08:23  iter: 23479  total_loss: 4.475  loss_ce: 0.1611  loss_mask: 0.1734  loss_dice: 0.1407  loss_ce_0: 0.07507  loss_mask_0: 0.1675  loss_dice_0: 0.1393  loss_ce_1: 0.1606  loss_mask_1: 0.1571  loss_dice_1: 0.1381  loss_ce_2: 0.1609  loss_mask_2: 0.1668  loss_dice_2: 0.1399  loss_ce_3: 0.1609  loss_mask_3: 0.1715  loss_dice_3: 0.1375  loss_ce_4: 0.1609  loss_mask_4: 0.1734  loss_dice_4: 0.141  loss_ce_5: 0.1609  loss_mask_5: 0.1646  loss_dice_5: 0.1356  loss_ce_6: 0.1609  loss_mask_6: 0.1683  loss_dice_6: 0.1416  loss_ce_7: 0.1611  loss_mask_7: 0.1817  loss_dice_7: 0.1417  loss_ce_8: 0.1609  loss_mask_8: 0.1637  loss_dice_8: 0.1361  time: 0.5792  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:26:12] d2.utils.events INFO:  eta: 1:08:10  iter: 23499  total_loss: 4.879  loss_ce: 0.161  loss_mask: 0.1862  loss_dice: 0.1591  loss_ce_0: 0.07504  loss_mask_0: 0.1624  loss_dice_0: 0.1538  loss_ce_1: 0.161  loss_mask_1: 0.1721  loss_dice_1: 0.1528  loss_ce_2: 0.1609  loss_mask_2: 0.1717  loss_dice_2: 0.1515  loss_ce_3: 0.1609  loss_mask_3: 0.1654  loss_dice_3: 0.1525  loss_ce_4: 0.1609  loss_mask_4: 0.1718  loss_dice_4: 0.1493  loss_ce_5: 0.161  loss_mask_5: 0.1641  loss_dice_5: 0.1542  loss_ce_6: 0.161  loss_mask_6: 0.1678  loss_dice_6: 0.1558  loss_ce_7: 0.161  loss_mask_7: 0.1717  loss_dice_7: 0.1505  loss_ce_8: 0.161  loss_mask_8: 0.1672  loss_dice_8: 0.1466  time: 0.5792  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:26:25] d2.utils.events INFO:  eta: 1:07:58  iter: 23519  total_loss: 4.43  loss_ce: 0.099  loss_mask: 0.1832  loss_dice: 0.1418  loss_ce_0: 0.05242  loss_mask_0: 0.1889  loss_dice_0: 0.1441  loss_ce_1: 0.09925  loss_mask_1: 0.1831  loss_dice_1: 0.1378  loss_ce_2: 0.09912  loss_mask_2: 0.1836  loss_dice_2: 0.1397  loss_ce_3: 0.09915  loss_mask_3: 0.1826  loss_dice_3: 0.1416  loss_ce_4: 0.09915  loss_mask_4: 0.1891  loss_dice_4: 0.1388  loss_ce_5: 0.09904  loss_mask_5: 0.1847  loss_dice_5: 0.134  loss_ce_6: 0.09904  loss_mask_6: 0.1772  loss_dice_6: 0.1409  loss_ce_7: 0.09897  loss_mask_7: 0.1864  loss_dice_7: 0.1362  loss_ce_8: 0.09908  loss_mask_8: 0.1802  loss_dice_8: 0.1352  time: 0.5793  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:26:38] d2.utils.events INFO:  eta: 1:07:43  iter: 23539  total_loss: 4.305  loss_ce: 0.09915  loss_mask: 0.1915  loss_dice: 0.1371  loss_ce_0: 0.05242  loss_mask_0: 0.1903  loss_dice_0: 0.1401  loss_ce_1: 0.09966  loss_mask_1: 0.1865  loss_dice_1: 0.1408  loss_ce_2: 0.09945  loss_mask_2: 0.1953  loss_dice_2: 0.1377  loss_ce_3: 0.09937  loss_mask_3: 0.1953  loss_dice_3: 0.1348  loss_ce_4: 0.09945  loss_mask_4: 0.1838  loss_dice_4: 0.1338  loss_ce_5: 0.09934  loss_mask_5: 0.184  loss_dice_5: 0.135  loss_ce_6: 0.09938  loss_mask_6: 0.184  loss_dice_6: 0.1312  loss_ce_7: 0.09915  loss_mask_7: 0.1838  loss_dice_7: 0.1381  loss_ce_8: 0.09927  loss_mask_8: 0.1929  loss_dice_8: 0.1385  time: 0.5793  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:26:50] d2.utils.events INFO:  eta: 1:07:32  iter: 23559  total_loss: 4.332  loss_ce: 0.099  loss_mask: 0.1809  loss_dice: 0.1338  loss_ce_0: 0.05242  loss_mask_0: 0.1768  loss_dice_0: 0.1376  loss_ce_1: 0.09917  loss_mask_1: 0.1749  loss_dice_1: 0.1415  loss_ce_2: 0.09912  loss_mask_2: 0.1656  loss_dice_2: 0.1334  loss_ce_3: 0.09908  loss_mask_3: 0.1799  loss_dice_3: 0.1393  loss_ce_4: 0.09912  loss_mask_4: 0.168  loss_dice_4: 0.1332  loss_ce_5: 0.09904  loss_mask_5: 0.1755  loss_dice_5: 0.1355  loss_ce_6: 0.09904  loss_mask_6: 0.1791  loss_dice_6: 0.1369  loss_ce_7: 0.09897  loss_mask_7: 0.1777  loss_dice_7: 0.1371  loss_ce_8: 0.099  loss_mask_8: 0.1759  loss_dice_8: 0.1343  time: 0.5794  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:27:03] d2.utils.events INFO:  eta: 1:07:20  iter: 23579  total_loss: 4.715  loss_ce: 0.09886  loss_mask: 0.196  loss_dice: 0.1451  loss_ce_0: 0.05239  loss_mask_0: 0.1961  loss_dice_0: 0.1518  loss_ce_1: 0.09919  loss_mask_1: 0.1991  loss_dice_1: 0.1457  loss_ce_2: 0.09906  loss_mask_2: 0.2007  loss_dice_2: 0.1516  loss_ce_3: 0.09908  loss_mask_3: 0.1907  loss_dice_3: 0.146  loss_ce_4: 0.09912  loss_mask_4: 0.2138  loss_dice_4: 0.1484  loss_ce_5: 0.09897  loss_mask_5: 0.2019  loss_dice_5: 0.1492  loss_ce_6: 0.099  loss_mask_6: 0.2005  loss_dice_6: 0.1522  loss_ce_7: 0.09889  loss_mask_7: 0.1966  loss_dice_7: 0.1486  loss_ce_8: 0.09897  loss_mask_8: 0.2023  loss_dice_8: 0.1475  time: 0.5794  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:27:16] d2.utils.events INFO:  eta: 1:07:09  iter: 23599  total_loss: 4.48  loss_ce: 0.1609  loss_mask: 0.1531  loss_dice: 0.1391  loss_ce_0: 0.07501  loss_mask_0: 0.1365  loss_dice_0: 0.143  loss_ce_1: 0.1605  loss_mask_1: 0.1411  loss_dice_1: 0.1404  loss_ce_2: 0.1606  loss_mask_2: 0.1433  loss_dice_2: 0.1401  loss_ce_3: 0.1607  loss_mask_3: 0.1409  loss_dice_3: 0.1419  loss_ce_4: 0.1607  loss_mask_4: 0.1403  loss_dice_4: 0.1398  loss_ce_5: 0.1606  loss_mask_5: 0.1415  loss_dice_5: 0.1441  loss_ce_6: 0.1607  loss_mask_6: 0.142  loss_dice_6: 0.14  loss_ce_7: 0.1608  loss_mask_7: 0.145  loss_dice_7: 0.1402  loss_ce_8: 0.1608  loss_mask_8: 0.1394  loss_dice_8: 0.1445  time: 0.5795  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:27:28] d2.utils.events INFO:  eta: 1:06:55  iter: 23619  total_loss: 4.359  loss_ce: 0.09897  loss_mask: 0.176  loss_dice: 0.1468  loss_ce_0: 0.0524  loss_mask_0: 0.1775  loss_dice_0: 0.1438  loss_ce_1: 0.0994  loss_mask_1: 0.1664  loss_dice_1: 0.1427  loss_ce_2: 0.09919  loss_mask_2: 0.1697  loss_dice_2: 0.1388  loss_ce_3: 0.09908  loss_mask_3: 0.181  loss_dice_3: 0.1366  loss_ce_4: 0.09919  loss_mask_4: 0.18  loss_dice_4: 0.1451  loss_ce_5: 0.09896  loss_mask_5: 0.1789  loss_dice_5: 0.1419  loss_ce_6: 0.09908  loss_mask_6: 0.1743  loss_dice_6: 0.14  loss_ce_7: 0.09889  loss_mask_7: 0.1783  loss_dice_7: 0.1466  loss_ce_8: 0.09904  loss_mask_8: 0.1622  loss_dice_8: 0.1365  time: 0.5795  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:27:41] d2.utils.events INFO:  eta: 1:06:42  iter: 23639  total_loss: 4.481  loss_ce: 0.09908  loss_mask: 0.1866  loss_dice: 0.1426  loss_ce_0: 0.05244  loss_mask_0: 0.1803  loss_dice_0: 0.1438  loss_ce_1: 0.0996  loss_mask_1: 0.1756  loss_dice_1: 0.142  loss_ce_2: 0.0993  loss_mask_2: 0.1758  loss_dice_2: 0.1402  loss_ce_3: 0.0993  loss_mask_3: 0.1772  loss_dice_3: 0.138  loss_ce_4: 0.09934  loss_mask_4: 0.1748  loss_dice_4: 0.1369  loss_ce_5: 0.09919  loss_mask_5: 0.1765  loss_dice_5: 0.1449  loss_ce_6: 0.0993  loss_mask_6: 0.1704  loss_dice_6: 0.1357  loss_ce_7: 0.09904  loss_mask_7: 0.1712  loss_dice_7: 0.1412  loss_ce_8: 0.09912  loss_mask_8: 0.1787  loss_dice_8: 0.1395  time: 0.5795  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:27:54] d2.utils.events INFO:  eta: 1:06:29  iter: 23659  total_loss: 4.404  loss_ce: 0.09897  loss_mask: 0.2043  loss_dice: 0.1239  loss_ce_0: 0.05244  loss_mask_0: 0.1986  loss_dice_0: 0.1218  loss_ce_1: 0.09927  loss_mask_1: 0.2043  loss_dice_1: 0.1264  loss_ce_2: 0.09921  loss_mask_2: 0.205  loss_dice_2: 0.1248  loss_ce_3: 0.09919  loss_mask_3: 0.2055  loss_dice_3: 0.1261  loss_ce_4: 0.09919  loss_mask_4: 0.2042  loss_dice_4: 0.1259  loss_ce_5: 0.09904  loss_mask_5: 0.2051  loss_dice_5: 0.1241  loss_ce_6: 0.09911  loss_mask_6: 0.2048  loss_dice_6: 0.1256  loss_ce_7: 0.09897  loss_mask_7: 0.2095  loss_dice_7: 0.1248  loss_ce_8: 0.09904  loss_mask_8: 0.2009  loss_dice_8: 0.1263  time: 0.5796  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:28:06] d2.utils.events INFO:  eta: 1:06:17  iter: 23679  total_loss: 4.502  loss_ce: 0.1298  loss_mask: 0.1795  loss_dice: 0.1361  loss_ce_0: 0.06369  loss_mask_0: 0.169  loss_dice_0: 0.1448  loss_ce_1: 0.1297  loss_mask_1: 0.1709  loss_dice_1: 0.1398  loss_ce_2: 0.1296  loss_mask_2: 0.1725  loss_dice_2: 0.1515  loss_ce_3: 0.1297  loss_mask_3: 0.1787  loss_dice_3: 0.1442  loss_ce_4: 0.1298  loss_mask_4: 0.1702  loss_dice_4: 0.1471  loss_ce_5: 0.1297  loss_mask_5: 0.1688  loss_dice_5: 0.144  loss_ce_6: 0.1298  loss_mask_6: 0.1718  loss_dice_6: 0.1425  loss_ce_7: 0.1298  loss_mask_7: 0.1839  loss_dice_7: 0.1403  loss_ce_8: 0.1297  loss_mask_8: 0.1786  loss_dice_8: 0.135  time: 0.5796  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:28:19] d2.utils.events INFO:  eta: 1:06:06  iter: 23699  total_loss: 4.613  loss_ce: 0.09897  loss_mask: 0.1856  loss_dice: 0.1502  loss_ce_0: 0.05243  loss_mask_0: 0.1893  loss_dice_0: 0.1484  loss_ce_1: 0.09945  loss_mask_1: 0.19  loss_dice_1: 0.141  loss_ce_2: 0.09925  loss_mask_2: 0.1892  loss_dice_2: 0.1473  loss_ce_3: 0.09919  loss_mask_3: 0.1866  loss_dice_3: 0.1424  loss_ce_4: 0.09923  loss_mask_4: 0.1894  loss_dice_4: 0.1489  loss_ce_5: 0.09911  loss_mask_5: 0.1904  loss_dice_5: 0.1466  loss_ce_6: 0.09919  loss_mask_6: 0.1879  loss_dice_6: 0.1494  loss_ce_7: 0.09897  loss_mask_7: 0.1864  loss_dice_7: 0.1496  loss_ce_8: 0.09908  loss_mask_8: 0.19  loss_dice_8: 0.151  time: 0.5797  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:28:32] d2.utils.events INFO:  eta: 1:05:54  iter: 23719  total_loss: 4.973  loss_ce: 0.1298  loss_mask: 0.2187  loss_dice: 0.1394  loss_ce_0: 0.06369  loss_mask_0: 0.2231  loss_dice_0: 0.1401  loss_ce_1: 0.1298  loss_mask_1: 0.2209  loss_dice_1: 0.142  loss_ce_2: 0.1299  loss_mask_2: 0.2302  loss_dice_2: 0.1405  loss_ce_3: 0.1298  loss_mask_3: 0.2246  loss_dice_3: 0.1458  loss_ce_4: 0.1299  loss_mask_4: 0.2194  loss_dice_4: 0.1406  loss_ce_5: 0.1298  loss_mask_5: 0.2181  loss_dice_5: 0.1414  loss_ce_6: 0.1299  loss_mask_6: 0.2149  loss_dice_6: 0.1397  loss_ce_7: 0.1299  loss_mask_7: 0.2153  loss_dice_7: 0.1404  loss_ce_8: 0.13  loss_mask_8: 0.2169  loss_dice_8: 0.1424  time: 0.5797  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:28:44] d2.utils.events INFO:  eta: 1:05:41  iter: 23739  total_loss: 4.989  loss_ce: 0.09919  loss_mask: 0.2238  loss_dice: 0.1394  loss_ce_0: 0.05244  loss_mask_0: 0.2348  loss_dice_0: 0.1389  loss_ce_1: 0.0999  loss_mask_1: 0.234  loss_dice_1: 0.1456  loss_ce_2: 0.09951  loss_mask_2: 0.2363  loss_dice_2: 0.1358  loss_ce_3: 0.09937  loss_mask_3: 0.2273  loss_dice_3: 0.1376  loss_ce_4: 0.09945  loss_mask_4: 0.2294  loss_dice_4: 0.1407  loss_ce_5: 0.09926  loss_mask_5: 0.2293  loss_dice_5: 0.1441  loss_ce_6: 0.09945  loss_mask_6: 0.2285  loss_dice_6: 0.1406  loss_ce_7: 0.09915  loss_mask_7: 0.232  loss_dice_7: 0.14  loss_ce_8: 0.09927  loss_mask_8: 0.2312  loss_dice_8: 0.1389  time: 0.5798  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:28:57] d2.utils.events INFO:  eta: 1:05:28  iter: 23759  total_loss: 4.785  loss_ce: 0.1603  loss_mask: 0.2155  loss_dice: 0.149  loss_ce_0: 0.07493  loss_mask_0: 0.2071  loss_dice_0: 0.1498  loss_ce_1: 0.1595  loss_mask_1: 0.1996  loss_dice_1: 0.1478  loss_ce_2: 0.1599  loss_mask_2: 0.1943  loss_dice_2: 0.1426  loss_ce_3: 0.1601  loss_mask_3: 0.201  loss_dice_3: 0.1432  loss_ce_4: 0.16  loss_mask_4: 0.1966  loss_dice_4: 0.1452  loss_ce_5: 0.1602  loss_mask_5: 0.2054  loss_dice_5: 0.1434  loss_ce_6: 0.1601  loss_mask_6: 0.1998  loss_dice_6: 0.1484  loss_ce_7: 0.1604  loss_mask_7: 0.197  loss_dice_7: 0.1505  loss_ce_8: 0.1602  loss_mask_8: 0.1924  loss_dice_8: 0.151  time: 0.5798  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:29:10] d2.utils.events INFO:  eta: 1:05:17  iter: 23779  total_loss: 4.373  loss_ce: 0.09915  loss_mask: 0.1726  loss_dice: 0.1396  loss_ce_0: 0.05246  loss_mask_0: 0.187  loss_dice_0: 0.1397  loss_ce_1: 0.09949  loss_mask_1: 0.1868  loss_dice_1: 0.1451  loss_ce_2: 0.09932  loss_mask_2: 0.1765  loss_dice_2: 0.1429  loss_ce_3: 0.0993  loss_mask_3: 0.1819  loss_dice_3: 0.1391  loss_ce_4: 0.0993  loss_mask_4: 0.1847  loss_dice_4: 0.142  loss_ce_5: 0.09919  loss_mask_5: 0.1709  loss_dice_5: 0.138  loss_ce_6: 0.09919  loss_mask_6: 0.1763  loss_dice_6: 0.1344  loss_ce_7: 0.09911  loss_mask_7: 0.1759  loss_dice_7: 0.1376  loss_ce_8: 0.09915  loss_mask_8: 0.1809  loss_dice_8: 0.1385  time: 0.5798  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:29:22] d2.utils.events INFO:  eta: 1:05:05  iter: 23799  total_loss: 4.476  loss_ce: 0.09912  loss_mask: 0.1653  loss_dice: 0.1331  loss_ce_0: 0.05245  loss_mask_0: 0.1497  loss_dice_0: 0.1338  loss_ce_1: 0.09947  loss_mask_1: 0.1563  loss_dice_1: 0.1406  loss_ce_2: 0.09936  loss_mask_2: 0.1516  loss_dice_2: 0.1351  loss_ce_3: 0.09923  loss_mask_3: 0.1597  loss_dice_3: 0.1341  loss_ce_4: 0.0993  loss_mask_4: 0.1528  loss_dice_4: 0.133  loss_ce_5: 0.09927  loss_mask_5: 0.1615  loss_dice_5: 0.1385  loss_ce_6: 0.09926  loss_mask_6: 0.1614  loss_dice_6: 0.1397  loss_ce_7: 0.09915  loss_mask_7: 0.1589  loss_dice_7: 0.1398  loss_ce_8: 0.09962  loss_mask_8: 0.1496  loss_dice_8: 0.1348  time: 0.5799  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:29:35] d2.utils.events INFO:  eta: 1:04:50  iter: 23819  total_loss: 4.342  loss_ce: 0.099  loss_mask: 0.1521  loss_dice: 0.1367  loss_ce_0: 0.05244  loss_mask_0: 0.1634  loss_dice_0: 0.1407  loss_ce_1: 0.09951  loss_mask_1: 0.1463  loss_dice_1: 0.1387  loss_ce_2: 0.09928  loss_mask_2: 0.1546  loss_dice_2: 0.1409  loss_ce_3: 0.09922  loss_mask_3: 0.1542  loss_dice_3: 0.132  loss_ce_4: 0.09926  loss_mask_4: 0.1524  loss_dice_4: 0.1388  loss_ce_5: 0.09911  loss_mask_5: 0.1503  loss_dice_5: 0.1373  loss_ce_6: 0.09911  loss_mask_6: 0.1587  loss_dice_6: 0.1462  loss_ce_7: 0.09911  loss_mask_7: 0.1524  loss_dice_7: 0.135  loss_ce_8: 0.09915  loss_mask_8: 0.1521  loss_dice_8: 0.1347  time: 0.5799  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:29:47] d2.utils.events INFO:  eta: 1:04:38  iter: 23839  total_loss: 4.383  loss_ce: 0.1607  loss_mask: 0.1937  loss_dice: 0.1398  loss_ce_0: 0.07494  loss_mask_0: 0.1757  loss_dice_0: 0.1358  loss_ce_1: 0.1603  loss_mask_1: 0.1885  loss_dice_1: 0.14  loss_ce_2: 0.1603  loss_mask_2: 0.188  loss_dice_2: 0.1387  loss_ce_3: 0.1603  loss_mask_3: 0.188  loss_dice_3: 0.1361  loss_ce_4: 0.1603  loss_mask_4: 0.189  loss_dice_4: 0.1363  loss_ce_5: 0.1604  loss_mask_5: 0.1861  loss_dice_5: 0.1328  loss_ce_6: 0.1604  loss_mask_6: 0.187  loss_dice_6: 0.1352  loss_ce_7: 0.1605  loss_mask_7: 0.195  loss_dice_7: 0.1372  loss_ce_8: 0.1605  loss_mask_8: 0.2  loss_dice_8: 0.1348  time: 0.5800  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:30:00] d2.utils.events INFO:  eta: 1:04:25  iter: 23859  total_loss: 4.754  loss_ce: 0.1603  loss_mask: 0.2135  loss_dice: 0.1321  loss_ce_0: 0.07491  loss_mask_0: 0.2181  loss_dice_0: 0.1348  loss_ce_1: 0.1596  loss_mask_1: 0.2132  loss_dice_1: 0.1343  loss_ce_2: 0.16  loss_mask_2: 0.2144  loss_dice_2: 0.1368  loss_ce_3: 0.1601  loss_mask_3: 0.2208  loss_dice_3: 0.1327  loss_ce_4: 0.16  loss_mask_4: 0.2047  loss_dice_4: 0.1335  loss_ce_5: 0.1602  loss_mask_5: 0.2282  loss_dice_5: 0.1331  loss_ce_6: 0.1599  loss_mask_6: 0.2151  loss_dice_6: 0.1334  loss_ce_7: 0.1604  loss_mask_7: 0.2054  loss_dice_7: 0.1346  loss_ce_8: 0.1602  loss_mask_8: 0.2219  loss_dice_8: 0.1385  time: 0.5800  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:30:13] d2.utils.events INFO:  eta: 1:04:12  iter: 23879  total_loss: 4.518  loss_ce: 0.09927  loss_mask: 0.2047  loss_dice: 0.1519  loss_ce_0: 0.05246  loss_mask_0: 0.2072  loss_dice_0: 0.1501  loss_ce_1: 0.09964  loss_mask_1: 0.2093  loss_dice_1: 0.1554  loss_ce_2: 0.09947  loss_mask_2: 0.2006  loss_dice_2: 0.1529  loss_ce_3: 0.09945  loss_mask_3: 0.1913  loss_dice_3: 0.1529  loss_ce_4: 0.09945  loss_mask_4: 0.197  loss_dice_4: 0.152  loss_ce_5: 0.09941  loss_mask_5: 0.2045  loss_dice_5: 0.1481  loss_ce_6: 0.09941  loss_mask_6: 0.1978  loss_dice_6: 0.1495  loss_ce_7: 0.0993  loss_mask_7: 0.1906  loss_dice_7: 0.1515  loss_ce_8: 0.0993  loss_mask_8: 0.1941  loss_dice_8: 0.1484  time: 0.5801  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:30:25] d2.utils.events INFO:  eta: 1:04:00  iter: 23899  total_loss: 4.724  loss_ce: 0.09915  loss_mask: 0.2013  loss_dice: 0.1445  loss_ce_0: 0.05246  loss_mask_0: 0.209  loss_dice_0: 0.1395  loss_ce_1: 0.09943  loss_mask_1: 0.1976  loss_dice_1: 0.1433  loss_ce_2: 0.09932  loss_mask_2: 0.204  loss_dice_2: 0.1424  loss_ce_3: 0.0993  loss_mask_3: 0.2149  loss_dice_3: 0.1448  loss_ce_4: 0.09926  loss_mask_4: 0.2072  loss_dice_4: 0.1481  loss_ce_5: 0.09926  loss_mask_5: 0.1977  loss_dice_5: 0.1417  loss_ce_6: 0.09926  loss_mask_6: 0.195  loss_dice_6: 0.1395  loss_ce_7: 0.09919  loss_mask_7: 0.2126  loss_dice_7: 0.1389  loss_ce_8: 0.09923  loss_mask_8: 0.2017  loss_dice_8: 0.144  time: 0.5801  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:30:38] d2.utils.events INFO:  eta: 1:03:48  iter: 23919  total_loss: 4.439  loss_ce: 0.1297  loss_mask: 0.1579  loss_dice: 0.1373  loss_ce_0: 0.06368  loss_mask_0: 0.1611  loss_dice_0: 0.1388  loss_ce_1: 0.1293  loss_mask_1: 0.1644  loss_dice_1: 0.1405  loss_ce_2: 0.1295  loss_mask_2: 0.1619  loss_dice_2: 0.1466  loss_ce_3: 0.1296  loss_mask_3: 0.1601  loss_dice_3: 0.1435  loss_ce_4: 0.1295  loss_mask_4: 0.1479  loss_dice_4: 0.133  loss_ce_5: 0.1297  loss_mask_5: 0.1647  loss_dice_5: 0.1421  loss_ce_6: 0.1295  loss_mask_6: 0.1611  loss_dice_6: 0.1371  loss_ce_7: 0.1297  loss_mask_7: 0.1609  loss_dice_7: 0.1411  loss_ce_8: 0.1296  loss_mask_8: 0.1613  loss_dice_8: 0.1402  time: 0.5801  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:30:51] d2.utils.events INFO:  eta: 1:03:38  iter: 23939  total_loss: 4.597  loss_ce: 0.09919  loss_mask: 0.1963  loss_dice: 0.1431  loss_ce_0: 0.05246  loss_mask_0: 0.1992  loss_dice_0: 0.1385  loss_ce_1: 0.09955  loss_mask_1: 0.2026  loss_dice_1: 0.14  loss_ce_2: 0.09934  loss_mask_2: 0.1929  loss_dice_2: 0.1415  loss_ce_3: 0.09937  loss_mask_3: 0.199  loss_dice_3: 0.1443  loss_ce_4: 0.09934  loss_mask_4: 0.1944  loss_dice_4: 0.1407  loss_ce_5: 0.09927  loss_mask_5: 0.196  loss_dice_5: 0.1381  loss_ce_6: 0.09926  loss_mask_6: 0.191  loss_dice_6: 0.1373  loss_ce_7: 0.09923  loss_mask_7: 0.1982  loss_dice_7: 0.1452  loss_ce_8: 0.09927  loss_mask_8: 0.1872  loss_dice_8: 0.1348  time: 0.5802  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:31:03] d2.utils.events INFO:  eta: 1:03:26  iter: 23959  total_loss: 4.301  loss_ce: 0.1297  loss_mask: 0.1545  loss_dice: 0.1427  loss_ce_0: 0.06368  loss_mask_0: 0.1609  loss_dice_0: 0.1482  loss_ce_1: 0.1294  loss_mask_1: 0.1587  loss_dice_1: 0.1508  loss_ce_2: 0.1295  loss_mask_2: 0.1597  loss_dice_2: 0.1532  loss_ce_3: 0.1295  loss_mask_3: 0.1622  loss_dice_3: 0.1465  loss_ce_4: 0.1295  loss_mask_4: 0.1579  loss_dice_4: 0.1462  loss_ce_5: 0.1295  loss_mask_5: 0.1554  loss_dice_5: 0.1411  loss_ce_6: 0.1296  loss_mask_6: 0.1503  loss_dice_6: 0.1437  loss_ce_7: 0.1296  loss_mask_7: 0.1632  loss_dice_7: 0.146  loss_ce_8: 0.1296  loss_mask_8: 0.1642  loss_dice_8: 0.1491  time: 0.5802  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:31:16] d2.utils.events INFO:  eta: 1:03:15  iter: 23979  total_loss: 4.571  loss_ce: 0.0993  loss_mask: 0.2106  loss_dice: 0.1383  loss_ce_0: 0.05246  loss_mask_0: 0.2074  loss_dice_0: 0.1363  loss_ce_1: 0.0999  loss_mask_1: 0.2064  loss_dice_1: 0.1338  loss_ce_2: 0.0997  loss_mask_2: 0.208  loss_dice_2: 0.1337  loss_ce_3: 0.09953  loss_mask_3: 0.2158  loss_dice_3: 0.1378  loss_ce_4: 0.09964  loss_mask_4: 0.2101  loss_dice_4: 0.1336  loss_ce_5: 0.09956  loss_mask_5: 0.2137  loss_dice_5: 0.1316  loss_ce_6: 0.09956  loss_mask_6: 0.21  loss_dice_6: 0.1318  loss_ce_7: 0.09934  loss_mask_7: 0.2147  loss_dice_7: 0.1359  loss_ce_8: 0.09949  loss_mask_8: 0.2072  loss_dice_8: 0.1368  time: 0.5803  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:31:29] d2.utils.events INFO:  eta: 1:03:01  iter: 23999  total_loss: 4.501  loss_ce: 0.09904  loss_mask: 0.1901  loss_dice: 0.1432  loss_ce_0: 0.05242  loss_mask_0: 0.1966  loss_dice_0: 0.153  loss_ce_1: 0.09934  loss_mask_1: 0.1936  loss_dice_1: 0.1509  loss_ce_2: 0.09913  loss_mask_2: 0.1907  loss_dice_2: 0.1475  loss_ce_3: 0.09911  loss_mask_3: 0.1955  loss_dice_3: 0.1541  loss_ce_4: 0.09923  loss_mask_4: 0.1903  loss_dice_4: 0.1555  loss_ce_5: 0.09912  loss_mask_5: 0.1922  loss_dice_5: 0.1455  loss_ce_6: 0.09908  loss_mask_6: 0.1958  loss_dice_6: 0.1506  loss_ce_7: 0.09915  loss_mask_7: 0.1911  loss_dice_7: 0.1377  loss_ce_8: 0.09915  loss_mask_8: 0.1934  loss_dice_8: 0.1452  time: 0.5803  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:31:41] d2.utils.events INFO:  eta: 1:02:47  iter: 24019  total_loss: 4.723  loss_ce: 0.1297  loss_mask: 0.1855  loss_dice: 0.147  loss_ce_0: 0.06369  loss_mask_0: 0.1835  loss_dice_0: 0.1405  loss_ce_1: 0.1294  loss_mask_1: 0.1986  loss_dice_1: 0.1383  loss_ce_2: 0.1295  loss_mask_2: 0.1896  loss_dice_2: 0.1429  loss_ce_3: 0.1296  loss_mask_3: 0.1871  loss_dice_3: 0.1397  loss_ce_4: 0.1295  loss_mask_4: 0.1943  loss_dice_4: 0.146  loss_ce_5: 0.1296  loss_mask_5: 0.1805  loss_dice_5: 0.1422  loss_ce_6: 0.1295  loss_mask_6: 0.1924  loss_dice_6: 0.1465  loss_ce_7: 0.1297  loss_mask_7: 0.183  loss_dice_7: 0.147  loss_ce_8: 0.1297  loss_mask_8: 0.174  loss_dice_8: 0.1417  time: 0.5804  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:31:54] d2.utils.events INFO:  eta: 1:02:34  iter: 24039  total_loss: 4.702  loss_ce: 0.09908  loss_mask: 0.2027  loss_dice: 0.1415  loss_ce_0: 0.05242  loss_mask_0: 0.1971  loss_dice_0: 0.1477  loss_ce_1: 0.0993  loss_mask_1: 0.2053  loss_dice_1: 0.1404  loss_ce_2: 0.09921  loss_mask_2: 0.1975  loss_dice_2: 0.1445  loss_ce_3: 0.09919  loss_mask_3: 0.2023  loss_dice_3: 0.1467  loss_ce_4: 0.09926  loss_mask_4: 0.2049  loss_dice_4: 0.1456  loss_ce_5: 0.09911  loss_mask_5: 0.1997  loss_dice_5: 0.1422  loss_ce_6: 0.09915  loss_mask_6: 0.2062  loss_dice_6: 0.1445  loss_ce_7: 0.09904  loss_mask_7: 0.2028  loss_dice_7: 0.1453  loss_ce_8: 0.09915  loss_mask_8: 0.1939  loss_dice_8: 0.1414  time: 0.5804  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:32:07] d2.utils.events INFO:  eta: 1:02:21  iter: 24059  total_loss: 4.434  loss_ce: 0.1297  loss_mask: 0.1799  loss_dice: 0.1309  loss_ce_0: 0.06369  loss_mask_0: 0.175  loss_dice_0: 0.1284  loss_ce_1: 0.1294  loss_mask_1: 0.1813  loss_dice_1: 0.1314  loss_ce_2: 0.1296  loss_mask_2: 0.165  loss_dice_2: 0.1282  loss_ce_3: 0.1296  loss_mask_3: 0.1793  loss_dice_3: 0.136  loss_ce_4: 0.1296  loss_mask_4: 0.1696  loss_dice_4: 0.1293  loss_ce_5: 0.1297  loss_mask_5: 0.1756  loss_dice_5: 0.1309  loss_ce_6: 0.1296  loss_mask_6: 0.1677  loss_dice_6: 0.1315  loss_ce_7: 0.1297  loss_mask_7: 0.1686  loss_dice_7: 0.1331  loss_ce_8: 0.1297  loss_mask_8: 0.1816  loss_dice_8: 0.1344  time: 0.5804  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:32:19] d2.utils.events INFO:  eta: 1:02:09  iter: 24079  total_loss: 4.371  loss_ce: 0.1297  loss_mask: 0.1782  loss_dice: 0.1302  loss_ce_0: 0.06369  loss_mask_0: 0.1821  loss_dice_0: 0.1302  loss_ce_1: 0.1293  loss_mask_1: 0.1816  loss_dice_1: 0.1303  loss_ce_2: 0.1295  loss_mask_2: 0.1932  loss_dice_2: 0.1267  loss_ce_3: 0.1296  loss_mask_3: 0.1845  loss_dice_3: 0.1261  loss_ce_4: 0.1294  loss_mask_4: 0.1867  loss_dice_4: 0.1213  loss_ce_5: 0.1296  loss_mask_5: 0.1894  loss_dice_5: 0.1304  loss_ce_6: 0.1296  loss_mask_6: 0.177  loss_dice_6: 0.126  loss_ce_7: 0.1296  loss_mask_7: 0.1787  loss_dice_7: 0.1305  loss_ce_8: 0.1296  loss_mask_8: 0.1765  loss_dice_8: 0.1305  time: 0.5805  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:32:32] d2.utils.events INFO:  eta: 1:01:58  iter: 24099  total_loss: 4.678  loss_ce: 0.1296  loss_mask: 0.1173  loss_dice: 0.2083  loss_ce_0: 0.06369  loss_mask_0: 0.1065  loss_dice_0: 0.2006  loss_ce_1: 0.1295  loss_mask_1: 0.1161  loss_dice_1: 0.2101  loss_ce_2: 0.1294  loss_mask_2: 0.1146  loss_dice_2: 0.1907  loss_ce_3: 0.1295  loss_mask_3: 0.1141  loss_dice_3: 0.1989  loss_ce_4: 0.1295  loss_mask_4: 0.1182  loss_dice_4: 0.2094  loss_ce_5: 0.1297  loss_mask_5: 0.1115  loss_dice_5: 0.2082  loss_ce_6: 0.1295  loss_mask_6: 0.1142  loss_dice_6: 0.2092  loss_ce_7: 0.1297  loss_mask_7: 0.1104  loss_dice_7: 0.2008  loss_ce_8: 0.1298  loss_mask_8: 0.1112  loss_dice_8: 0.1866  time: 0.5805  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:32:45] d2.utils.events INFO:  eta: 1:01:47  iter: 24119  total_loss: 4.626  loss_ce: 0.16  loss_mask: 0.2126  loss_dice: 0.1466  loss_ce_0: 0.07488  loss_mask_0: 0.2119  loss_dice_0: 0.1474  loss_ce_1: 0.1595  loss_mask_1: 0.2137  loss_dice_1: 0.1457  loss_ce_2: 0.1597  loss_mask_2: 0.2096  loss_dice_2: 0.1442  loss_ce_3: 0.1598  loss_mask_3: 0.2085  loss_dice_3: 0.1443  loss_ce_4: 0.1597  loss_mask_4: 0.2136  loss_dice_4: 0.1455  loss_ce_5: 0.1598  loss_mask_5: 0.2093  loss_dice_5: 0.1466  loss_ce_6: 0.1598  loss_mask_6: 0.2081  loss_dice_6: 0.1467  loss_ce_7: 0.16  loss_mask_7: 0.1986  loss_dice_7: 0.1463  loss_ce_8: 0.1599  loss_mask_8: 0.2096  loss_dice_8: 0.147  time: 0.5806  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:32:57] d2.utils.events INFO:  eta: 1:01:33  iter: 24139  total_loss: 4.357  loss_ce: 0.09923  loss_mask: 0.1313  loss_dice: 0.1669  loss_ce_0: 0.05248  loss_mask_0: 0.1291  loss_dice_0: 0.1596  loss_ce_1: 0.09958  loss_mask_1: 0.13  loss_dice_1: 0.1662  loss_ce_2: 0.09951  loss_mask_2: 0.133  loss_dice_2: 0.1634  loss_ce_3: 0.09949  loss_mask_3: 0.1362  loss_dice_3: 0.1659  loss_ce_4: 0.09949  loss_mask_4: 0.1289  loss_dice_4: 0.1607  loss_ce_5: 0.09941  loss_mask_5: 0.1326  loss_dice_5: 0.1606  loss_ce_6: 0.09945  loss_mask_6: 0.1333  loss_dice_6: 0.1604  loss_ce_7: 0.09934  loss_mask_7: 0.1313  loss_dice_7: 0.1613  loss_ce_8: 0.0993  loss_mask_8: 0.1305  loss_dice_8: 0.1678  time: 0.5806  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:33:10] d2.utils.events INFO:  eta: 1:01:21  iter: 24159  total_loss: 4.491  loss_ce: 0.1601  loss_mask: 0.1736  loss_dice: 0.1329  loss_ce_0: 0.07487  loss_mask_0: 0.1714  loss_dice_0: 0.1243  loss_ce_1: 0.1595  loss_mask_1: 0.1705  loss_dice_1: 0.1258  loss_ce_2: 0.1597  loss_mask_2: 0.1704  loss_dice_2: 0.1285  loss_ce_3: 0.16  loss_mask_3: 0.1746  loss_dice_3: 0.1338  loss_ce_4: 0.1598  loss_mask_4: 0.1635  loss_dice_4: 0.1248  loss_ce_5: 0.16  loss_mask_5: 0.1654  loss_dice_5: 0.1233  loss_ce_6: 0.1598  loss_mask_6: 0.1739  loss_dice_6: 0.1222  loss_ce_7: 0.16  loss_mask_7: 0.1656  loss_dice_7: 0.1281  loss_ce_8: 0.1598  loss_mask_8: 0.1701  loss_dice_8: 0.1251  time: 0.5807  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:33:23] d2.utils.events INFO:  eta: 1:01:10  iter: 24179  total_loss: 4.424  loss_ce: 0.09949  loss_mask: 0.1887  loss_dice: 0.1288  loss_ce_0: 0.0525  loss_mask_0: 0.1954  loss_dice_0: 0.1336  loss_ce_1: 0.09987  loss_mask_1: 0.1948  loss_dice_1: 0.1319  loss_ce_2: 0.09977  loss_mask_2: 0.1937  loss_dice_2: 0.1318  loss_ce_3: 0.09968  loss_mask_3: 0.2011  loss_dice_3: 0.1306  loss_ce_4: 0.09968  loss_mask_4: 0.1932  loss_dice_4: 0.1272  loss_ce_5: 0.09971  loss_mask_5: 0.1913  loss_dice_5: 0.1284  loss_ce_6: 0.09968  loss_mask_6: 0.1874  loss_dice_6: 0.1286  loss_ce_7: 0.09956  loss_mask_7: 0.1964  loss_dice_7: 0.1304  loss_ce_8: 0.0996  loss_mask_8: 0.1935  loss_dice_8: 0.1265  time: 0.5807  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:33:35] d2.utils.events INFO:  eta: 1:00:58  iter: 24199  total_loss: 4.363  loss_ce: 0.09949  loss_mask: 0.1827  loss_dice: 0.127  loss_ce_0: 0.05251  loss_mask_0: 0.1833  loss_dice_0: 0.1301  loss_ce_1: 0.09987  loss_mask_1: 0.1817  loss_dice_1: 0.1324  loss_ce_2: 0.09973  loss_mask_2: 0.1789  loss_dice_2: 0.1283  loss_ce_3: 0.09967  loss_mask_3: 0.1803  loss_dice_3: 0.1223  loss_ce_4: 0.09964  loss_mask_4: 0.1751  loss_dice_4: 0.1287  loss_ce_5: 0.09956  loss_mask_5: 0.1721  loss_dice_5: 0.1272  loss_ce_6: 0.09968  loss_mask_6: 0.1798  loss_dice_6: 0.1248  loss_ce_7: 0.09949  loss_mask_7: 0.1746  loss_dice_7: 0.1316  loss_ce_8: 0.0996  loss_mask_8: 0.175  loss_dice_8: 0.1354  time: 0.5807  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:33:48] d2.utils.events INFO:  eta: 1:00:45  iter: 24219  total_loss: 4.779  loss_ce: 0.1296  loss_mask: 0.1826  loss_dice: 0.1412  loss_ce_0: 0.06367  loss_mask_0: 0.1792  loss_dice_0: 0.1299  loss_ce_1: 0.1293  loss_mask_1: 0.1918  loss_dice_1: 0.1385  loss_ce_2: 0.1295  loss_mask_2: 0.1762  loss_dice_2: 0.1369  loss_ce_3: 0.1295  loss_mask_3: 0.1836  loss_dice_3: 0.1388  loss_ce_4: 0.1294  loss_mask_4: 0.1806  loss_dice_4: 0.1399  loss_ce_5: 0.1296  loss_mask_5: 0.1758  loss_dice_5: 0.1364  loss_ce_6: 0.1295  loss_mask_6: 0.1788  loss_dice_6: 0.136  loss_ce_7: 0.1296  loss_mask_7: 0.1821  loss_dice_7: 0.1368  loss_ce_8: 0.1296  loss_mask_8: 0.1747  loss_dice_8: 0.1399  time: 0.5808  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:34:01] d2.utils.events INFO:  eta: 1:00:33  iter: 24239  total_loss: 4.604  loss_ce: 0.16  loss_mask: 0.1943  loss_dice: 0.123  loss_ce_0: 0.07486  loss_mask_0: 0.1987  loss_dice_0: 0.1259  loss_ce_1: 0.1595  loss_mask_1: 0.2026  loss_dice_1: 0.1264  loss_ce_2: 0.1597  loss_mask_2: 0.2003  loss_dice_2: 0.1264  loss_ce_3: 0.1597  loss_mask_3: 0.2034  loss_dice_3: 0.1264  loss_ce_4: 0.1598  loss_mask_4: 0.1984  loss_dice_4: 0.1266  loss_ce_5: 0.1599  loss_mask_5: 0.1971  loss_dice_5: 0.125  loss_ce_6: 0.1599  loss_mask_6: 0.1913  loss_dice_6: 0.1259  loss_ce_7: 0.1599  loss_mask_7: 0.1964  loss_dice_7: 0.1254  loss_ce_8: 0.1597  loss_mask_8: 0.2007  loss_dice_8: 0.1253  time: 0.5808  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:34:13] d2.utils.events INFO:  eta: 1:00:21  iter: 24259  total_loss: 4.634  loss_ce: 0.1296  loss_mask: 0.1651  loss_dice: 0.1499  loss_ce_0: 0.06367  loss_mask_0: 0.1612  loss_dice_0: 0.147  loss_ce_1: 0.1292  loss_mask_1: 0.1724  loss_dice_1: 0.1571  loss_ce_2: 0.1294  loss_mask_2: 0.1664  loss_dice_2: 0.1549  loss_ce_3: 0.1295  loss_mask_3: 0.173  loss_dice_3: 0.1456  loss_ce_4: 0.1294  loss_mask_4: 0.1646  loss_dice_4: 0.1468  loss_ce_5: 0.1294  loss_mask_5: 0.1696  loss_dice_5: 0.1551  loss_ce_6: 0.1294  loss_mask_6: 0.1601  loss_dice_6: 0.1518  loss_ce_7: 0.1296  loss_mask_7: 0.1677  loss_dice_7: 0.1548  loss_ce_8: 0.1299  loss_mask_8: 0.1591  loss_dice_8: 0.151  time: 0.5809  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:34:26] d2.utils.events INFO:  eta: 1:00:09  iter: 24279  total_loss: 4.564  loss_ce: 0.1598  loss_mask: 0.1735  loss_dice: 0.1481  loss_ce_0: 0.0748  loss_mask_0: 0.1738  loss_dice_0: 0.1498  loss_ce_1: 0.1591  loss_mask_1: 0.1697  loss_dice_1: 0.1405  loss_ce_2: 0.1596  loss_mask_2: 0.1725  loss_dice_2: 0.1435  loss_ce_3: 0.1594  loss_mask_3: 0.1651  loss_dice_3: 0.1445  loss_ce_4: 0.1595  loss_mask_4: 0.174  loss_dice_4: 0.1472  loss_ce_5: 0.1596  loss_mask_5: 0.1755  loss_dice_5: 0.151  loss_ce_6: 0.1593  loss_mask_6: 0.176  loss_dice_6: 0.1472  loss_ce_7: 0.1595  loss_mask_7: 0.1659  loss_dice_7: 0.1416  loss_ce_8: 0.1596  loss_mask_8: 0.1811  loss_dice_8: 0.1517  time: 0.5809  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:34:39] d2.utils.events INFO:  eta: 0:59:57  iter: 24299  total_loss: 4.318  loss_ce: 0.09975  loss_mask: 0.1412  loss_dice: 0.1438  loss_ce_0: 0.05254  loss_mask_0: 0.1401  loss_dice_0: 0.1467  loss_ce_1: 0.1001  loss_mask_1: 0.1356  loss_dice_1: 0.1498  loss_ce_2: 0.09998  loss_mask_2: 0.1381  loss_dice_2: 0.1493  loss_ce_3: 0.1001  loss_mask_3: 0.1357  loss_dice_3: 0.1485  loss_ce_4: 0.09998  loss_mask_4: 0.139  loss_dice_4: 0.1538  loss_ce_5: 0.1  loss_mask_5: 0.1356  loss_dice_5: 0.1524  loss_ce_6: 0.09994  loss_mask_6: 0.1421  loss_dice_6: 0.1543  loss_ce_7: 0.09983  loss_mask_7: 0.1422  loss_dice_7: 0.1486  loss_ce_8: 0.09983  loss_mask_8: 0.1406  loss_dice_8: 0.1565  time: 0.5810  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:34:51] d2.utils.events INFO:  eta: 0:59:43  iter: 24319  total_loss: 4.681  loss_ce: 0.1002  loss_mask: 0.21  loss_dice: 0.1291  loss_ce_0: 0.05254  loss_mask_0: 0.2176  loss_dice_0: 0.1233  loss_ce_1: 0.1014  loss_mask_1: 0.2108  loss_dice_1: 0.1226  loss_ce_2: 0.1008  loss_mask_2: 0.2127  loss_dice_2: 0.1278  loss_ce_3: 0.1007  loss_mask_3: 0.2091  loss_dice_3: 0.1279  loss_ce_4: 0.1007  loss_mask_4: 0.2192  loss_dice_4: 0.1265  loss_ce_5: 0.1005  loss_mask_5: 0.2107  loss_dice_5: 0.1269  loss_ce_6: 0.1007  loss_mask_6: 0.2051  loss_dice_6: 0.124  loss_ce_7: 0.1002  loss_mask_7: 0.209  loss_dice_7: 0.1278  loss_ce_8: 0.1003  loss_mask_8: 0.2102  loss_dice_8: 0.1257  time: 0.5810  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:35:04] d2.utils.events INFO:  eta: 0:59:29  iter: 24339  total_loss: 4.441  loss_ce: 0.09983  loss_mask: 0.1859  loss_dice: 0.1508  loss_ce_0: 0.05254  loss_mask_0: 0.1803  loss_dice_0: 0.1485  loss_ce_1: 0.1003  loss_mask_1: 0.1839  loss_dice_1: 0.1503  loss_ce_2: 0.1002  loss_mask_2: 0.1735  loss_dice_2: 0.1512  loss_ce_3: 0.1001  loss_mask_3: 0.1818  loss_dice_3: 0.1462  loss_ce_4: 0.1001  loss_mask_4: 0.1864  loss_dice_4: 0.1438  loss_ce_5: 0.1  loss_mask_5: 0.1741  loss_dice_5: 0.1479  loss_ce_6: 0.1001  loss_mask_6: 0.1831  loss_dice_6: 0.1471  loss_ce_7: 0.0999  loss_mask_7: 0.1868  loss_dice_7: 0.1494  loss_ce_8: 0.09994  loss_mask_8: 0.1748  loss_dice_8: 0.1402  time: 0.5810  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:35:16] d2.utils.events INFO:  eta: 0:59:17  iter: 24359  total_loss: 4.585  loss_ce: 0.09972  loss_mask: 0.1781  loss_dice: 0.1523  loss_ce_0: 0.05254  loss_mask_0: 0.1778  loss_dice_0: 0.1523  loss_ce_1: 0.1001  loss_mask_1: 0.1835  loss_dice_1: 0.1557  loss_ce_2: 0.1  loss_mask_2: 0.1911  loss_dice_2: 0.1527  loss_ce_3: 0.09998  loss_mask_3: 0.1903  loss_dice_3: 0.1499  loss_ce_4: 0.1001  loss_mask_4: 0.1798  loss_dice_4: 0.1511  loss_ce_5: 0.09994  loss_mask_5: 0.184  loss_dice_5: 0.1529  loss_ce_6: 0.09994  loss_mask_6: 0.173  loss_dice_6: 0.1547  loss_ce_7: 0.09979  loss_mask_7: 0.1798  loss_dice_7: 0.1582  loss_ce_8: 0.09979  loss_mask_8: 0.1835  loss_dice_8: 0.1503  time: 0.5811  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:35:29] d2.utils.events INFO:  eta: 0:59:04  iter: 24379  total_loss: 4.702  loss_ce: 0.1593  loss_mask: 0.1894  loss_dice: 0.1279  loss_ce_0: 0.07473  loss_mask_0: 0.1941  loss_dice_0: 0.1333  loss_ce_1: 0.1583  loss_mask_1: 0.1831  loss_dice_1: 0.1281  loss_ce_2: 0.1586  loss_mask_2: 0.1973  loss_dice_2: 0.1271  loss_ce_3: 0.1588  loss_mask_3: 0.2066  loss_dice_3: 0.1272  loss_ce_4: 0.1587  loss_mask_4: 0.1988  loss_dice_4: 0.1293  loss_ce_5: 0.1587  loss_mask_5: 0.1984  loss_dice_5: 0.1312  loss_ce_6: 0.1587  loss_mask_6: 0.1935  loss_dice_6: 0.1293  loss_ce_7: 0.1591  loss_mask_7: 0.1989  loss_dice_7: 0.1279  loss_ce_8: 0.159  loss_mask_8: 0.1994  loss_dice_8: 0.1292  time: 0.5811  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:35:42] d2.utils.events INFO:  eta: 0:58:51  iter: 24399  total_loss: 4.733  loss_ce: 0.09994  loss_mask: 0.2147  loss_dice: 0.1379  loss_ce_0: 0.0526  loss_mask_0: 0.2194  loss_dice_0: 0.1369  loss_ce_1: 0.1005  loss_mask_1: 0.2162  loss_dice_1: 0.1343  loss_ce_2: 0.1003  loss_mask_2: 0.2211  loss_dice_2: 0.1351  loss_ce_3: 0.1002  loss_mask_3: 0.2205  loss_dice_3: 0.1353  loss_ce_4: 0.1003  loss_mask_4: 0.2167  loss_dice_4: 0.137  loss_ce_5: 0.1003  loss_mask_5: 0.2141  loss_dice_5: 0.1302  loss_ce_6: 0.1002  loss_mask_6: 0.2168  loss_dice_6: 0.1371  loss_ce_7: 0.1001  loss_mask_7: 0.2167  loss_dice_7: 0.1385  loss_ce_8: 0.1001  loss_mask_8: 0.2207  loss_dice_8: 0.1425  time: 0.5812  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:35:55] d2.utils.events INFO:  eta: 0:58:38  iter: 24419  total_loss: 4.571  loss_ce: 0.09964  loss_mask: 0.1555  loss_dice: 0.1584  loss_ce_0: 0.05257  loss_mask_0: 0.1486  loss_dice_0: 0.1492  loss_ce_1: 0.09983  loss_mask_1: 0.1531  loss_dice_1: 0.1523  loss_ce_2: 0.09985  loss_mask_2: 0.1562  loss_dice_2: 0.1544  loss_ce_3: 0.0999  loss_mask_3: 0.1579  loss_dice_3: 0.1538  loss_ce_4: 0.09984  loss_mask_4: 0.1499  loss_dice_4: 0.1573  loss_ce_5: 0.09971  loss_mask_5: 0.1579  loss_dice_5: 0.1516  loss_ce_6: 0.09986  loss_mask_6: 0.1597  loss_dice_6: 0.1522  loss_ce_7: 0.09971  loss_mask_7: 0.147  loss_dice_7: 0.1524  loss_ce_8: 0.09972  loss_mask_8: 0.149  loss_dice_8: 0.1536  time: 0.5812  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:36:07] d2.utils.events INFO:  eta: 0:58:27  iter: 24439  total_loss: 4.369  loss_ce: 0.1596  loss_mask: 0.1685  loss_dice: 0.1318  loss_ce_0: 0.07475  loss_mask_0: 0.1759  loss_dice_0: 0.1316  loss_ce_1: 0.1589  loss_mask_1: 0.1769  loss_dice_1: 0.1269  loss_ce_2: 0.1591  loss_mask_2: 0.1892  loss_dice_2: 0.1302  loss_ce_3: 0.1592  loss_mask_3: 0.1906  loss_dice_3: 0.1279  loss_ce_4: 0.159  loss_mask_4: 0.1861  loss_dice_4: 0.1274  loss_ce_5: 0.1592  loss_mask_5: 0.1803  loss_dice_5: 0.1324  loss_ce_6: 0.1592  loss_mask_6: 0.1782  loss_dice_6: 0.1296  loss_ce_7: 0.1594  loss_mask_7: 0.1787  loss_dice_7: 0.1262  loss_ce_8: 0.1594  loss_mask_8: 0.1721  loss_dice_8: 0.1367  time: 0.5813  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:36:20] d2.utils.events INFO:  eta: 0:58:15  iter: 24459  total_loss: 4.497  loss_ce: 0.09994  loss_mask: 0.1612  loss_dice: 0.1529  loss_ce_0: 0.05259  loss_mask_0: 0.1656  loss_dice_0: 0.1494  loss_ce_1: 0.1005  loss_mask_1: 0.1666  loss_dice_1: 0.1466  loss_ce_2: 0.1003  loss_mask_2: 0.1639  loss_dice_2: 0.147  loss_ce_3: 0.1002  loss_mask_3: 0.1664  loss_dice_3: 0.1509  loss_ce_4: 0.1003  loss_mask_4: 0.17  loss_dice_4: 0.1514  loss_ce_5: 0.1002  loss_mask_5: 0.1691  loss_dice_5: 0.1483  loss_ce_6: 0.1002  loss_mask_6: 0.1606  loss_dice_6: 0.1519  loss_ce_7: 0.1  loss_mask_7: 0.1743  loss_dice_7: 0.1469  loss_ce_8: 0.1001  loss_mask_8: 0.1699  loss_dice_8: 0.1517  time: 0.5813  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:36:32] d2.utils.events INFO:  eta: 0:58:01  iter: 24479  total_loss: 4.281  loss_ce: 0.1296  loss_mask: 0.1689  loss_dice: 0.1414  loss_ce_0: 0.06366  loss_mask_0: 0.1616  loss_dice_0: 0.148  loss_ce_1: 0.1294  loss_mask_1: 0.1585  loss_dice_1: 0.1439  loss_ce_2: 0.1294  loss_mask_2: 0.1601  loss_dice_2: 0.1475  loss_ce_3: 0.1294  loss_mask_3: 0.1604  loss_dice_3: 0.1424  loss_ce_4: 0.1294  loss_mask_4: 0.1577  loss_dice_4: 0.1453  loss_ce_5: 0.1295  loss_mask_5: 0.1564  loss_dice_5: 0.1451  loss_ce_6: 0.1294  loss_mask_6: 0.1604  loss_dice_6: 0.1441  loss_ce_7: 0.1295  loss_mask_7: 0.1616  loss_dice_7: 0.1414  loss_ce_8: 0.1295  loss_mask_8: 0.1586  loss_dice_8: 0.1413  time: 0.5813  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:36:45] d2.utils.events INFO:  eta: 0:57:49  iter: 24499  total_loss: 4.489  loss_ce: 0.1  loss_mask: 0.1741  loss_dice: 0.1546  loss_ce_0: 0.0526  loss_mask_0: 0.182  loss_dice_0: 0.1503  loss_ce_1: 0.1006  loss_mask_1: 0.1725  loss_dice_1: 0.1531  loss_ce_2: 0.1003  loss_mask_2: 0.1743  loss_dice_2: 0.1659  loss_ce_3: 0.1002  loss_mask_3: 0.1728  loss_dice_3: 0.1609  loss_ce_4: 0.1004  loss_mask_4: 0.1718  loss_dice_4: 0.1583  loss_ce_5: 0.1002  loss_mask_5: 0.1764  loss_dice_5: 0.1565  loss_ce_6: 0.1002  loss_mask_6: 0.171  loss_dice_6: 0.1535  loss_ce_7: 0.1001  loss_mask_7: 0.1753  loss_dice_7: 0.1582  loss_ce_8: 0.1002  loss_mask_8: 0.1686  loss_dice_8: 0.1621  time: 0.5814  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:36:58] d2.utils.events INFO:  eta: 0:57:36  iter: 24519  total_loss: 4.375  loss_ce: 0.159  loss_mask: 0.2046  loss_dice: 0.1242  loss_ce_0: 0.07468  loss_mask_0: 0.2062  loss_dice_0: 0.1253  loss_ce_1: 0.1582  loss_mask_1: 0.2032  loss_dice_1: 0.1238  loss_ce_2: 0.1584  loss_mask_2: 0.2054  loss_dice_2: 0.1242  loss_ce_3: 0.1586  loss_mask_3: 0.213  loss_dice_3: 0.1229  loss_ce_4: 0.1586  loss_mask_4: 0.2074  loss_dice_4: 0.1219  loss_ce_5: 0.1586  loss_mask_5: 0.2073  loss_dice_5: 0.124  loss_ce_6: 0.1586  loss_mask_6: 0.2148  loss_dice_6: 0.1226  loss_ce_7: 0.159  loss_mask_7: 0.2055  loss_dice_7: 0.1221  loss_ce_8: 0.1588  loss_mask_8: 0.2019  loss_dice_8: 0.1251  time: 0.5814  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:37:11] d2.utils.events INFO:  eta: 0:57:25  iter: 24539  total_loss: 4.654  loss_ce: 0.1002  loss_mask: 0.2  loss_dice: 0.1421  loss_ce_0: 0.05264  loss_mask_0: 0.2058  loss_dice_0: 0.1448  loss_ce_1: 0.1009  loss_mask_1: 0.2041  loss_dice_1: 0.1487  loss_ce_2: 0.1007  loss_mask_2: 0.2083  loss_dice_2: 0.1407  loss_ce_3: 0.1005  loss_mask_3: 0.2019  loss_dice_3: 0.1463  loss_ce_4: 0.1006  loss_mask_4: 0.2023  loss_dice_4: 0.1494  loss_ce_5: 0.1005  loss_mask_5: 0.2076  loss_dice_5: 0.1444  loss_ce_6: 0.1005  loss_mask_6: 0.2126  loss_dice_6: 0.144  loss_ce_7: 0.1003  loss_mask_7: 0.2135  loss_dice_7: 0.147  loss_ce_8: 0.1004  loss_mask_8: 0.2038  loss_dice_8: 0.1469  time: 0.5815  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:37:23] d2.utils.events INFO:  eta: 0:57:12  iter: 24559  total_loss: 4.384  loss_ce: 0.09994  loss_mask: 0.1719  loss_dice: 0.1486  loss_ce_0: 0.05261  loss_mask_0: 0.1757  loss_dice_0: 0.1462  loss_ce_1: 0.1001  loss_mask_1: 0.1798  loss_dice_1: 0.1506  loss_ce_2: 0.1001  loss_mask_2: 0.1959  loss_dice_2: 0.1499  loss_ce_3: 0.1001  loss_mask_3: 0.1845  loss_dice_3: 0.1511  loss_ce_4: 0.1001  loss_mask_4: 0.1773  loss_dice_4: 0.1495  loss_ce_5: 0.1002  loss_mask_5: 0.187  loss_dice_5: 0.1488  loss_ce_6: 0.1  loss_mask_6: 0.1845  loss_dice_6: 0.1525  loss_ce_7: 0.1  loss_mask_7: 0.1819  loss_dice_7: 0.1502  loss_ce_8: 0.1  loss_mask_8: 0.1812  loss_dice_8: 0.1486  time: 0.5815  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:37:36] d2.utils.events INFO:  eta: 0:56:59  iter: 24579  total_loss: 4.614  loss_ce: 0.09998  loss_mask: 0.1177  loss_dice: 0.1839  loss_ce_0: 0.05259  loss_mask_0: 0.1187  loss_dice_0: 0.1976  loss_ce_1: 0.1004  loss_mask_1: 0.1174  loss_dice_1: 0.1924  loss_ce_2: 0.1002  loss_mask_2: 0.1196  loss_dice_2: 0.1913  loss_ce_3: 0.1003  loss_mask_3: 0.1155  loss_dice_3: 0.196  loss_ce_4: 0.1004  loss_mask_4: 0.1105  loss_dice_4: 0.1836  loss_ce_5: 0.1002  loss_mask_5: 0.1202  loss_dice_5: 0.1873  loss_ce_6: 0.1002  loss_mask_6: 0.1206  loss_dice_6: 0.2048  loss_ce_7: 0.1001  loss_mask_7: 0.1248  loss_dice_7: 0.2038  loss_ce_8: 0.1003  loss_mask_8: 0.1176  loss_dice_8: 0.1856  time: 0.5815  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:37:48] d2.utils.events INFO:  eta: 0:56:46  iter: 24599  total_loss: 4.641  loss_ce: 0.1297  loss_mask: 0.1931  loss_dice: 0.1295  loss_ce_0: 0.06365  loss_mask_0: 0.1909  loss_dice_0: 0.1351  loss_ce_1: 0.1297  loss_mask_1: 0.185  loss_dice_1: 0.1279  loss_ce_2: 0.1297  loss_mask_2: 0.1976  loss_dice_2: 0.1311  loss_ce_3: 0.1296  loss_mask_3: 0.201  loss_dice_3: 0.1323  loss_ce_4: 0.1296  loss_mask_4: 0.1846  loss_dice_4: 0.1285  loss_ce_5: 0.1296  loss_mask_5: 0.1903  loss_dice_5: 0.1293  loss_ce_6: 0.1296  loss_mask_6: 0.1906  loss_dice_6: 0.1305  loss_ce_7: 0.1296  loss_mask_7: 0.1913  loss_dice_7: 0.1279  loss_ce_8: 0.1299  loss_mask_8: 0.191  loss_dice_8: 0.1336  time: 0.5816  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:38:01] d2.utils.events INFO:  eta: 0:56:34  iter: 24619  total_loss: 4.407  loss_ce: 0.09998  loss_mask: 0.1673  loss_dice: 0.139  loss_ce_0: 0.05259  loss_mask_0: 0.181  loss_dice_0: 0.1411  loss_ce_1: 0.1008  loss_mask_1: 0.1776  loss_dice_1: 0.1453  loss_ce_2: 0.1004  loss_mask_2: 0.1664  loss_dice_2: 0.1385  loss_ce_3: 0.1002  loss_mask_3: 0.1685  loss_dice_3: 0.1435  loss_ce_4: 0.1004  loss_mask_4: 0.1652  loss_dice_4: 0.1367  loss_ce_5: 0.1002  loss_mask_5: 0.1704  loss_dice_5: 0.1374  loss_ce_6: 0.1003  loss_mask_6: 0.1708  loss_dice_6: 0.1368  loss_ce_7: 0.1001  loss_mask_7: 0.1666  loss_dice_7: 0.1396  loss_ce_8: 0.1001  loss_mask_8: 0.1706  loss_dice_8: 0.1425  time: 0.5816  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:38:14] d2.utils.events INFO:  eta: 0:56:22  iter: 24639  total_loss: 4.416  loss_ce: 0.09998  loss_mask: 0.1432  loss_dice: 0.145  loss_ce_0: 0.05258  loss_mask_0: 0.1485  loss_dice_0: 0.1424  loss_ce_1: 0.1007  loss_mask_1: 0.1441  loss_dice_1: 0.1423  loss_ce_2: 0.1004  loss_mask_2: 0.1516  loss_dice_2: 0.1409  loss_ce_3: 0.1004  loss_mask_3: 0.1455  loss_dice_3: 0.144  loss_ce_4: 0.1004  loss_mask_4: 0.1396  loss_dice_4: 0.1461  loss_ce_5: 0.1003  loss_mask_5: 0.1436  loss_dice_5: 0.1434  loss_ce_6: 0.1004  loss_mask_6: 0.1508  loss_dice_6: 0.1445  loss_ce_7: 0.1001  loss_mask_7: 0.1526  loss_dice_7: 0.1396  loss_ce_8: 0.1002  loss_mask_8: 0.1501  loss_dice_8: 0.1475  time: 0.5817  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:38:26] d2.utils.events INFO:  eta: 0:56:10  iter: 24659  total_loss: 4.645  loss_ce: 0.1295  loss_mask: 0.1749  loss_dice: 0.1361  loss_ce_0: 0.06366  loss_mask_0: 0.1898  loss_dice_0: 0.1293  loss_ce_1: 0.129  loss_mask_1: 0.1833  loss_dice_1: 0.1376  loss_ce_2: 0.1293  loss_mask_2: 0.1773  loss_dice_2: 0.1392  loss_ce_3: 0.1293  loss_mask_3: 0.1834  loss_dice_3: 0.1314  loss_ce_4: 0.1293  loss_mask_4: 0.1823  loss_dice_4: 0.133  loss_ce_5: 0.1294  loss_mask_5: 0.1822  loss_dice_5: 0.14  loss_ce_6: 0.1292  loss_mask_6: 0.1672  loss_dice_6: 0.1403  loss_ce_7: 0.1295  loss_mask_7: 0.1796  loss_dice_7: 0.1405  loss_ce_8: 0.1293  loss_mask_8: 0.1748  loss_dice_8: 0.1455  time: 0.5817  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:38:39] d2.utils.events INFO:  eta: 0:55:59  iter: 24679  total_loss: 4.572  loss_ce: 0.09975  loss_mask: 0.1909  loss_dice: 0.1426  loss_ce_0: 0.05257  loss_mask_0: 0.1956  loss_dice_0: 0.1433  loss_ce_1: 0.1001  loss_mask_1: 0.197  loss_dice_1: 0.1481  loss_ce_2: 0.09988  loss_mask_2: 0.1945  loss_dice_2: 0.1445  loss_ce_3: 0.09986  loss_mask_3: 0.1939  loss_dice_3: 0.1513  loss_ce_4: 0.09994  loss_mask_4: 0.1958  loss_dice_4: 0.1449  loss_ce_5: 0.09987  loss_mask_5: 0.2001  loss_dice_5: 0.1395  loss_ce_6: 0.09986  loss_mask_6: 0.1917  loss_dice_6: 0.1508  loss_ce_7: 0.09979  loss_mask_7: 0.1921  loss_dice_7: 0.1463  loss_ce_8: 0.09975  loss_mask_8: 0.1953  loss_dice_8: 0.1438  time: 0.5817  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:38:52] d2.utils.events INFO:  eta: 0:55:47  iter: 24699  total_loss: 4.576  loss_ce: 0.09971  loss_mask: 0.1734  loss_dice: 0.1441  loss_ce_0: 0.05256  loss_mask_0: 0.1704  loss_dice_0: 0.1457  loss_ce_1: 0.1001  loss_mask_1: 0.174  loss_dice_1: 0.1461  loss_ce_2: 0.09968  loss_mask_2: 0.1725  loss_dice_2: 0.1472  loss_ce_3: 0.09982  loss_mask_3: 0.1693  loss_dice_3: 0.1444  loss_ce_4: 0.09986  loss_mask_4: 0.1716  loss_dice_4: 0.148  loss_ce_5: 0.09979  loss_mask_5: 0.1721  loss_dice_5: 0.149  loss_ce_6: 0.0999  loss_mask_6: 0.1747  loss_dice_6: 0.148  loss_ce_7: 0.09983  loss_mask_7: 0.1729  loss_dice_7: 0.1438  loss_ce_8: 0.1001  loss_mask_8: 0.1716  loss_dice_8: 0.1437  time: 0.5818  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:39:04] d2.utils.events INFO:  eta: 0:55:33  iter: 24719  total_loss: 4.532  loss_ce: 0.09975  loss_mask: 0.1607  loss_dice: 0.1501  loss_ce_0: 0.05253  loss_mask_0: 0.1651  loss_dice_0: 0.1474  loss_ce_1: 0.1  loss_mask_1: 0.1603  loss_dice_1: 0.1495  loss_ce_2: 0.1  loss_mask_2: 0.1735  loss_dice_2: 0.1492  loss_ce_3: 0.09997  loss_mask_3: 0.1681  loss_dice_3: 0.1482  loss_ce_4: 0.09994  loss_mask_4: 0.1688  loss_dice_4: 0.1484  loss_ce_5: 0.1  loss_mask_5: 0.1646  loss_dice_5: 0.1457  loss_ce_6: 0.1  loss_mask_6: 0.1646  loss_dice_6: 0.1472  loss_ce_7: 0.09979  loss_mask_7: 0.1613  loss_dice_7: 0.1419  loss_ce_8: 0.09987  loss_mask_8: 0.16  loss_dice_8: 0.1482  time: 0.5818  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:39:17] d2.utils.events INFO:  eta: 0:55:22  iter: 24739  total_loss: 4.719  loss_ce: 0.09964  loss_mask: 0.1795  loss_dice: 0.161  loss_ce_0: 0.05252  loss_mask_0: 0.1744  loss_dice_0: 0.1695  loss_ce_1: 0.09972  loss_mask_1: 0.1812  loss_dice_1: 0.1621  loss_ce_2: 0.09951  loss_mask_2: 0.1841  loss_dice_2: 0.1656  loss_ce_3: 0.09967  loss_mask_3: 0.1752  loss_dice_3: 0.1569  loss_ce_4: 0.09971  loss_mask_4: 0.1807  loss_dice_4: 0.1635  loss_ce_5: 0.09956  loss_mask_5: 0.1771  loss_dice_5: 0.1623  loss_ce_6: 0.09979  loss_mask_6: 0.1714  loss_dice_6: 0.1611  loss_ce_7: 0.09956  loss_mask_7: 0.179  loss_dice_7: 0.1677  loss_ce_8: 0.09987  loss_mask_8: 0.1799  loss_dice_8: 0.1656  time: 0.5819  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:39:29] d2.utils.events INFO:  eta: 0:55:10  iter: 24759  total_loss: 4.665  loss_ce: 0.09953  loss_mask: 0.1428  loss_dice: 0.1666  loss_ce_0: 0.05251  loss_mask_0: 0.1395  loss_dice_0: 0.1599  loss_ce_1: 0.09985  loss_mask_1: 0.1297  loss_dice_1: 0.1554  loss_ce_2: 0.0996  loss_mask_2: 0.1377  loss_dice_2: 0.1651  loss_ce_3: 0.09964  loss_mask_3: 0.1407  loss_dice_3: 0.1602  loss_ce_4: 0.09967  loss_mask_4: 0.1387  loss_dice_4: 0.1691  loss_ce_5: 0.09956  loss_mask_5: 0.1426  loss_dice_5: 0.1613  loss_ce_6: 0.09971  loss_mask_6: 0.1343  loss_dice_6: 0.1615  loss_ce_7: 0.09956  loss_mask_7: 0.1382  loss_dice_7: 0.1563  loss_ce_8: 0.09957  loss_mask_8: 0.1342  loss_dice_8: 0.1634  time: 0.5819  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:39:42] d2.utils.events INFO:  eta: 0:54:58  iter: 24779  total_loss: 4.539  loss_ce: 0.09949  loss_mask: 0.1917  loss_dice: 0.1467  loss_ce_0: 0.05249  loss_mask_0: 0.1975  loss_dice_0: 0.1462  loss_ce_1: 0.09966  loss_mask_1: 0.1999  loss_dice_1: 0.1407  loss_ce_2: 0.0996  loss_mask_2: 0.201  loss_dice_2: 0.1433  loss_ce_3: 0.09964  loss_mask_3: 0.191  loss_dice_3: 0.1418  loss_ce_4: 0.09953  loss_mask_4: 0.1988  loss_dice_4: 0.1447  loss_ce_5: 0.09949  loss_mask_5: 0.2042  loss_dice_5: 0.1449  loss_ce_6: 0.09968  loss_mask_6: 0.2019  loss_dice_6: 0.1449  loss_ce_7: 0.09941  loss_mask_7: 0.1963  loss_dice_7: 0.1398  loss_ce_8: 0.09957  loss_mask_8: 0.1991  loss_dice_8: 0.1407  time: 0.5819  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:39:55] d2.utils.events INFO:  eta: 0:54:45  iter: 24799  total_loss: 4.396  loss_ce: 0.0993  loss_mask: 0.1906  loss_dice: 0.1458  loss_ce_0: 0.05249  loss_mask_0: 0.1895  loss_dice_0: 0.1475  loss_ce_1: 0.09917  loss_mask_1: 0.1895  loss_dice_1: 0.1434  loss_ce_2: 0.09919  loss_mask_2: 0.1898  loss_dice_2: 0.1474  loss_ce_3: 0.09919  loss_mask_3: 0.1928  loss_dice_3: 0.142  loss_ce_4: 0.09919  loss_mask_4: 0.1847  loss_dice_4: 0.1472  loss_ce_5: 0.09911  loss_mask_5: 0.1849  loss_dice_5: 0.147  loss_ce_6: 0.09919  loss_mask_6: 0.1917  loss_dice_6: 0.1387  loss_ce_7: 0.09919  loss_mask_7: 0.1915  loss_dice_7: 0.1456  loss_ce_8: 0.09923  loss_mask_8: 0.19  loss_dice_8: 0.1463  time: 0.5820  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:40:08] d2.utils.events INFO:  eta: 0:54:34  iter: 24819  total_loss: 4.742  loss_ce: 0.16  loss_mask: 0.2186  loss_dice: 0.1319  loss_ce_0: 0.07492  loss_mask_0: 0.2136  loss_dice_0: 0.13  loss_ce_1: 0.1591  loss_mask_1: 0.217  loss_dice_1: 0.1289  loss_ce_2: 0.1597  loss_mask_2: 0.2178  loss_dice_2: 0.1276  loss_ce_3: 0.1598  loss_mask_3: 0.2249  loss_dice_3: 0.1306  loss_ce_4: 0.1597  loss_mask_4: 0.2152  loss_dice_4: 0.1257  loss_ce_5: 0.1599  loss_mask_5: 0.2226  loss_dice_5: 0.1255  loss_ce_6: 0.1596  loss_mask_6: 0.2251  loss_dice_6: 0.1335  loss_ce_7: 0.1601  loss_mask_7: 0.2155  loss_dice_7: 0.1256  loss_ce_8: 0.1596  loss_mask_8: 0.2165  loss_dice_8: 0.1274  time: 0.5820  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:40:20] d2.utils.events INFO:  eta: 0:54:23  iter: 24839  total_loss: 4.926  loss_ce: 0.09911  loss_mask: 0.1908  loss_dice: 0.1613  loss_ce_0: 0.05245  loss_mask_0: 0.1901  loss_dice_0: 0.1677  loss_ce_1: 0.09889  loss_mask_1: 0.1824  loss_dice_1: 0.1669  loss_ce_2: 0.09909  loss_mask_2: 0.1831  loss_dice_2: 0.1628  loss_ce_3: 0.09911  loss_mask_3: 0.1863  loss_dice_3: 0.1616  loss_ce_4: 0.09902  loss_mask_4: 0.1937  loss_dice_4: 0.1628  loss_ce_5: 0.09911  loss_mask_5: 0.1852  loss_dice_5: 0.1633  loss_ce_6: 0.09911  loss_mask_6: 0.1848  loss_dice_6: 0.169  loss_ce_7: 0.09904  loss_mask_7: 0.182  loss_dice_7: 0.1601  loss_ce_8: 0.09908  loss_mask_8: 0.1901  loss_dice_8: 0.1667  time: 0.5821  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:40:33] d2.utils.events INFO:  eta: 0:54:14  iter: 24859  total_loss: 4.637  loss_ce: 0.16  loss_mask: 0.195  loss_dice: 0.132  loss_ce_0: 0.0749  loss_mask_0: 0.1994  loss_dice_0: 0.1342  loss_ce_1: 0.159  loss_mask_1: 0.1954  loss_dice_1: 0.1341  loss_ce_2: 0.1598  loss_mask_2: 0.2051  loss_dice_2: 0.1345  loss_ce_3: 0.1599  loss_mask_3: 0.1939  loss_dice_3: 0.1291  loss_ce_4: 0.1597  loss_mask_4: 0.1951  loss_dice_4: 0.136  loss_ce_5: 0.1598  loss_mask_5: 0.2044  loss_dice_5: 0.1344  loss_ce_6: 0.1597  loss_mask_6: 0.1974  loss_dice_6: 0.1338  loss_ce_7: 0.16  loss_mask_7: 0.1993  loss_dice_7: 0.1284  loss_ce_8: 0.1599  loss_mask_8: 0.1896  loss_dice_8: 0.1286  time: 0.5821  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:40:46] d2.utils.events INFO:  eta: 0:54:03  iter: 24879  total_loss: 4.436  loss_ce: 0.1602  loss_mask: 0.1635  loss_dice: 0.1385  loss_ce_0: 0.07487  loss_mask_0: 0.163  loss_dice_0: 0.1314  loss_ce_1: 0.1599  loss_mask_1: 0.1513  loss_dice_1: 0.1304  loss_ce_2: 0.1601  loss_mask_2: 0.1654  loss_dice_2: 0.1363  loss_ce_3: 0.1601  loss_mask_3: 0.1628  loss_dice_3: 0.1397  loss_ce_4: 0.16  loss_mask_4: 0.1552  loss_dice_4: 0.1319  loss_ce_5: 0.16  loss_mask_5: 0.1583  loss_dice_5: 0.1358  loss_ce_6: 0.1601  loss_mask_6: 0.1581  loss_dice_6: 0.1335  loss_ce_7: 0.1602  loss_mask_7: 0.1572  loss_dice_7: 0.1393  loss_ce_8: 0.1601  loss_mask_8: 0.1542  loss_dice_8: 0.1294  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:40:58] d2.utils.events INFO:  eta: 0:53:49  iter: 24899  total_loss: 4.579  loss_ce: 0.1598  loss_mask: 0.1874  loss_dice: 0.1447  loss_ce_0: 0.07483  loss_mask_0: 0.1787  loss_dice_0: 0.1458  loss_ce_1: 0.1591  loss_mask_1: 0.1822  loss_dice_1: 0.1415  loss_ce_2: 0.1595  loss_mask_2: 0.1781  loss_dice_2: 0.1433  loss_ce_3: 0.1596  loss_mask_3: 0.1793  loss_dice_3: 0.1438  loss_ce_4: 0.1595  loss_mask_4: 0.1816  loss_dice_4: 0.1385  loss_ce_5: 0.1596  loss_mask_5: 0.1788  loss_dice_5: 0.1433  loss_ce_6: 0.1594  loss_mask_6: 0.175  loss_dice_6: 0.1394  loss_ce_7: 0.1598  loss_mask_7: 0.1783  loss_dice_7: 0.1409  loss_ce_8: 0.1598  loss_mask_8: 0.1655  loss_dice_8: 0.1399  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:41:11] d2.utils.events INFO:  eta: 0:53:36  iter: 24919  total_loss: 4.894  loss_ce: 0.0996  loss_mask: 0.2406  loss_dice: 0.1452  loss_ce_0: 0.0525  loss_mask_0: 0.2271  loss_dice_0: 0.1487  loss_ce_1: 0.09998  loss_mask_1: 0.237  loss_dice_1: 0.1503  loss_ce_2: 0.09979  loss_mask_2: 0.2323  loss_dice_2: 0.1501  loss_ce_3: 0.09971  loss_mask_3: 0.2353  loss_dice_3: 0.1501  loss_ce_4: 0.09983  loss_mask_4: 0.2381  loss_dice_4: 0.1502  loss_ce_5: 0.09971  loss_mask_5: 0.2301  loss_dice_5: 0.148  loss_ce_6: 0.09971  loss_mask_6: 0.2369  loss_dice_6: 0.1469  loss_ce_7: 0.09964  loss_mask_7: 0.2444  loss_dice_7: 0.1486  loss_ce_8: 0.09975  loss_mask_8: 0.2353  loss_dice_8: 0.1447  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:41:23] d2.utils.events INFO:  eta: 0:53:23  iter: 24939  total_loss: 4.489  loss_ce: 0.09953  loss_mask: 0.1655  loss_dice: 0.1476  loss_ce_0: 0.05253  loss_mask_0: 0.1664  loss_dice_0: 0.1445  loss_ce_1: 0.09996  loss_mask_1: 0.1781  loss_dice_1: 0.1426  loss_ce_2: 0.09964  loss_mask_2: 0.1608  loss_dice_2: 0.1426  loss_ce_3: 0.09956  loss_mask_3: 0.1665  loss_dice_3: 0.1431  loss_ce_4: 0.09964  loss_mask_4: 0.1733  loss_dice_4: 0.1467  loss_ce_5: 0.09979  loss_mask_5: 0.1743  loss_dice_5: 0.1377  loss_ce_6: 0.09964  loss_mask_6: 0.1586  loss_dice_6: 0.1405  loss_ce_7: 0.09964  loss_mask_7: 0.1705  loss_dice_7: 0.1384  loss_ce_8: 0.09975  loss_mask_8: 0.1738  loss_dice_8: 0.1461  time: 0.5823  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:41:35] d2.utils.events INFO:  eta: 0:53:03  iter: 24959  total_loss: 4.922  loss_ce: 0.09934  loss_mask: 0.203  loss_dice: 0.1618  loss_ce_0: 0.05249  loss_mask_0: 0.2111  loss_dice_0: 0.1619  loss_ce_1: 0.09938  loss_mask_1: 0.218  loss_dice_1: 0.1593  loss_ce_2: 0.0994  loss_mask_2: 0.2099  loss_dice_2: 0.1596  loss_ce_3: 0.09934  loss_mask_3: 0.2142  loss_dice_3: 0.1559  loss_ce_4: 0.09945  loss_mask_4: 0.2092  loss_dice_4: 0.1585  loss_ce_5: 0.09941  loss_mask_5: 0.2154  loss_dice_5: 0.1555  loss_ce_6: 0.0993  loss_mask_6: 0.2122  loss_dice_6: 0.1624  loss_ce_7: 0.09938  loss_mask_7: 0.2097  loss_dice_7: 0.1599  loss_ce_8: 0.09945  loss_mask_8: 0.2131  loss_dice_8: 0.1596  time: 0.5823  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:41:46] d2.utils.events INFO:  eta: 0:52:47  iter: 24979  total_loss: 4.558  loss_ce: 0.09945  loss_mask: 0.1965  loss_dice: 0.1462  loss_ce_0: 0.05246  loss_mask_0: 0.2001  loss_dice_0: 0.148  loss_ce_1: 0.09987  loss_mask_1: 0.1994  loss_dice_1: 0.152  loss_ce_2: 0.09966  loss_mask_2: 0.1875  loss_dice_2: 0.1478  loss_ce_3: 0.09952  loss_mask_3: 0.1896  loss_dice_3: 0.1481  loss_ce_4: 0.09968  loss_mask_4: 0.2027  loss_dice_4: 0.1493  loss_ce_5: 0.09956  loss_mask_5: 0.199  loss_dice_5: 0.1507  loss_ce_6: 0.0996  loss_mask_6: 0.2023  loss_dice_6: 0.1481  loss_ce_7: 0.09945  loss_mask_7: 0.1979  loss_dice_7: 0.1468  loss_ce_8: 0.09957  loss_mask_8: 0.2022  loss_dice_8: 0.1466  time: 0.5823  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:41:58] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256to256_instance/model_0024999.pth
[07/11 15:41:58] d2.data.datasets.coco INFO: Loaded 754 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_val2017.json
[07/11 15:41:58] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/11 15:41:58] d2.data.common INFO: Serializing 754 elements to byte tensors and concatenating them all ...
[07/11 15:41:58] d2.data.common INFO: Serialized dataset takes 2.49 MiB
[07/11 15:41:58] d2.evaluation.evaluator INFO: Start inference on 754 batches
[07/11 15:42:03] d2.evaluation.evaluator INFO: Inference done 11/754. Dataloading: 0.0005 s/iter. Inference: 0.1324 s/iter. Eval: 0.2539 s/iter. Total: 0.3868 s/iter. ETA=0:04:47
[07/11 15:42:08] d2.evaluation.evaluator INFO: Inference done 24/754. Dataloading: 0.0007 s/iter. Inference: 0.1377 s/iter. Eval: 0.2620 s/iter. Total: 0.4004 s/iter. ETA=0:04:52
[07/11 15:42:13] d2.evaluation.evaluator INFO: Inference done 37/754. Dataloading: 0.0007 s/iter. Inference: 0.1356 s/iter. Eval: 0.2615 s/iter. Total: 0.3979 s/iter. ETA=0:04:45
[07/11 15:42:18] d2.evaluation.evaluator INFO: Inference done 50/754. Dataloading: 0.0007 s/iter. Inference: 0.1357 s/iter. Eval: 0.2602 s/iter. Total: 0.3967 s/iter. ETA=0:04:39
[07/11 15:42:24] d2.evaluation.evaluator INFO: Inference done 63/754. Dataloading: 0.0007 s/iter. Inference: 0.1376 s/iter. Eval: 0.2605 s/iter. Total: 0.3989 s/iter. ETA=0:04:35
[07/11 15:42:29] d2.evaluation.evaluator INFO: Inference done 76/754. Dataloading: 0.0007 s/iter. Inference: 0.1392 s/iter. Eval: 0.2613 s/iter. Total: 0.4013 s/iter. ETA=0:04:32
[07/11 15:42:34] d2.evaluation.evaluator INFO: Inference done 89/754. Dataloading: 0.0007 s/iter. Inference: 0.1391 s/iter. Eval: 0.2617 s/iter. Total: 0.4016 s/iter. ETA=0:04:27
[07/11 15:42:40] d2.evaluation.evaluator INFO: Inference done 102/754. Dataloading: 0.0007 s/iter. Inference: 0.1393 s/iter. Eval: 0.2634 s/iter. Total: 0.4035 s/iter. ETA=0:04:23
[07/11 15:42:45] d2.evaluation.evaluator INFO: Inference done 115/754. Dataloading: 0.0007 s/iter. Inference: 0.1391 s/iter. Eval: 0.2638 s/iter. Total: 0.4037 s/iter. ETA=0:04:17
[07/11 15:42:50] d2.evaluation.evaluator INFO: Inference done 128/754. Dataloading: 0.0007 s/iter. Inference: 0.1393 s/iter. Eval: 0.2644 s/iter. Total: 0.4045 s/iter. ETA=0:04:13
[07/11 15:42:56] d2.evaluation.evaluator INFO: Inference done 141/754. Dataloading: 0.0007 s/iter. Inference: 0.1395 s/iter. Eval: 0.2642 s/iter. Total: 0.4045 s/iter. ETA=0:04:07
[07/11 15:43:01] d2.evaluation.evaluator INFO: Inference done 154/754. Dataloading: 0.0007 s/iter. Inference: 0.1399 s/iter. Eval: 0.2642 s/iter. Total: 0.4049 s/iter. ETA=0:04:02
[07/11 15:43:06] d2.evaluation.evaluator INFO: Inference done 167/754. Dataloading: 0.0007 s/iter. Inference: 0.1405 s/iter. Eval: 0.2641 s/iter. Total: 0.4054 s/iter. ETA=0:03:57
[07/11 15:43:12] d2.evaluation.evaluator INFO: Inference done 180/754. Dataloading: 0.0007 s/iter. Inference: 0.1402 s/iter. Eval: 0.2644 s/iter. Total: 0.4054 s/iter. ETA=0:03:52
[07/11 15:43:17] d2.evaluation.evaluator INFO: Inference done 192/754. Dataloading: 0.0007 s/iter. Inference: 0.1403 s/iter. Eval: 0.2652 s/iter. Total: 0.4063 s/iter. ETA=0:03:48
[07/11 15:43:22] d2.evaluation.evaluator INFO: Inference done 205/754. Dataloading: 0.0007 s/iter. Inference: 0.1410 s/iter. Eval: 0.2648 s/iter. Total: 0.4066 s/iter. ETA=0:03:43
[07/11 15:43:27] d2.evaluation.evaluator INFO: Inference done 217/754. Dataloading: 0.0007 s/iter. Inference: 0.1412 s/iter. Eval: 0.2653 s/iter. Total: 0.4073 s/iter. ETA=0:03:38
[07/11 15:43:32] d2.evaluation.evaluator INFO: Inference done 230/754. Dataloading: 0.0007 s/iter. Inference: 0.1409 s/iter. Eval: 0.2654 s/iter. Total: 0.4071 s/iter. ETA=0:03:33
[07/11 15:43:37] d2.evaluation.evaluator INFO: Inference done 243/754. Dataloading: 0.0007 s/iter. Inference: 0.1405 s/iter. Eval: 0.2650 s/iter. Total: 0.4063 s/iter. ETA=0:03:27
[07/11 15:43:42] d2.evaluation.evaluator INFO: Inference done 256/754. Dataloading: 0.0007 s/iter. Inference: 0.1402 s/iter. Eval: 0.2648 s/iter. Total: 0.4058 s/iter. ETA=0:03:22
[07/11 15:43:48] d2.evaluation.evaluator INFO: Inference done 269/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2650 s/iter. Total: 0.4059 s/iter. ETA=0:03:16
[07/11 15:43:53] d2.evaluation.evaluator INFO: Inference done 282/754. Dataloading: 0.0007 s/iter. Inference: 0.1401 s/iter. Eval: 0.2650 s/iter. Total: 0.4059 s/iter. ETA=0:03:11
[07/11 15:43:58] d2.evaluation.evaluator INFO: Inference done 295/754. Dataloading: 0.0007 s/iter. Inference: 0.1402 s/iter. Eval: 0.2648 s/iter. Total: 0.4058 s/iter. ETA=0:03:06
[07/11 15:44:04] d2.evaluation.evaluator INFO: Inference done 308/754. Dataloading: 0.0007 s/iter. Inference: 0.1404 s/iter. Eval: 0.2647 s/iter. Total: 0.4059 s/iter. ETA=0:03:01
[07/11 15:44:09] d2.evaluation.evaluator INFO: Inference done 320/754. Dataloading: 0.0007 s/iter. Inference: 0.1405 s/iter. Eval: 0.2650 s/iter. Total: 0.4063 s/iter. ETA=0:02:56
[07/11 15:44:14] d2.evaluation.evaluator INFO: Inference done 333/754. Dataloading: 0.0007 s/iter. Inference: 0.1408 s/iter. Eval: 0.2652 s/iter. Total: 0.4067 s/iter. ETA=0:02:51
[07/11 15:44:19] d2.evaluation.evaluator INFO: Inference done 346/754. Dataloading: 0.0007 s/iter. Inference: 0.1406 s/iter. Eval: 0.2650 s/iter. Total: 0.4064 s/iter. ETA=0:02:45
[07/11 15:44:24] d2.evaluation.evaluator INFO: Inference done 359/754. Dataloading: 0.0007 s/iter. Inference: 0.1404 s/iter. Eval: 0.2651 s/iter. Total: 0.4063 s/iter. ETA=0:02:40
[07/11 15:44:30] d2.evaluation.evaluator INFO: Inference done 372/754. Dataloading: 0.0007 s/iter. Inference: 0.1401 s/iter. Eval: 0.2652 s/iter. Total: 0.4061 s/iter. ETA=0:02:35
[07/11 15:44:35] d2.evaluation.evaluator INFO: Inference done 385/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2648 s/iter. Total: 0.4056 s/iter. ETA=0:02:29
[07/11 15:44:40] d2.evaluation.evaluator INFO: Inference done 398/754. Dataloading: 0.0007 s/iter. Inference: 0.1401 s/iter. Eval: 0.2649 s/iter. Total: 0.4057 s/iter. ETA=0:02:24
[07/11 15:44:45] d2.evaluation.evaluator INFO: Inference done 411/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2649 s/iter. Total: 0.4057 s/iter. ETA=0:02:19
[07/11 15:44:51] d2.evaluation.evaluator INFO: Inference done 424/754. Dataloading: 0.0007 s/iter. Inference: 0.1399 s/iter. Eval: 0.2651 s/iter. Total: 0.4059 s/iter. ETA=0:02:13
[07/11 15:44:56] d2.evaluation.evaluator INFO: Inference done 437/754. Dataloading: 0.0007 s/iter. Inference: 0.1396 s/iter. Eval: 0.2651 s/iter. Total: 0.4056 s/iter. ETA=0:02:08
[07/11 15:45:01] d2.evaluation.evaluator INFO: Inference done 450/754. Dataloading: 0.0007 s/iter. Inference: 0.1397 s/iter. Eval: 0.2652 s/iter. Total: 0.4057 s/iter. ETA=0:02:03
[07/11 15:45:06] d2.evaluation.evaluator INFO: Inference done 463/754. Dataloading: 0.0007 s/iter. Inference: 0.1395 s/iter. Eval: 0.2655 s/iter. Total: 0.4058 s/iter. ETA=0:01:58
[07/11 15:45:12] d2.evaluation.evaluator INFO: Inference done 476/754. Dataloading: 0.0007 s/iter. Inference: 0.1395 s/iter. Eval: 0.2655 s/iter. Total: 0.4058 s/iter. ETA=0:01:52
[07/11 15:45:17] d2.evaluation.evaluator INFO: Inference done 489/754. Dataloading: 0.0007 s/iter. Inference: 0.1399 s/iter. Eval: 0.2652 s/iter. Total: 0.4059 s/iter. ETA=0:01:47
[07/11 15:45:22] d2.evaluation.evaluator INFO: Inference done 501/754. Dataloading: 0.0007 s/iter. Inference: 0.1399 s/iter. Eval: 0.2655 s/iter. Total: 0.4062 s/iter. ETA=0:01:42
[07/11 15:45:28] d2.evaluation.evaluator INFO: Inference done 514/754. Dataloading: 0.0007 s/iter. Inference: 0.1401 s/iter. Eval: 0.2656 s/iter. Total: 0.4065 s/iter. ETA=0:01:37
[07/11 15:45:33] d2.evaluation.evaluator INFO: Inference done 527/754. Dataloading: 0.0007 s/iter. Inference: 0.1401 s/iter. Eval: 0.2655 s/iter. Total: 0.4064 s/iter. ETA=0:01:32
[07/11 15:45:38] d2.evaluation.evaluator INFO: Inference done 540/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2657 s/iter. Total: 0.4065 s/iter. ETA=0:01:26
[07/11 15:45:43] d2.evaluation.evaluator INFO: Inference done 553/754. Dataloading: 0.0007 s/iter. Inference: 0.1401 s/iter. Eval: 0.2658 s/iter. Total: 0.4067 s/iter. ETA=0:01:21
[07/11 15:45:49] d2.evaluation.evaluator INFO: Inference done 566/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2659 s/iter. Total: 0.4067 s/iter. ETA=0:01:16
[07/11 15:45:54] d2.evaluation.evaluator INFO: Inference done 579/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2659 s/iter. Total: 0.4067 s/iter. ETA=0:01:11
[07/11 15:45:59] d2.evaluation.evaluator INFO: Inference done 592/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2660 s/iter. Total: 0.4068 s/iter. ETA=0:01:05
[07/11 15:46:05] d2.evaluation.evaluator INFO: Inference done 605/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2661 s/iter. Total: 0.4068 s/iter. ETA=0:01:00
[07/11 15:46:10] d2.evaluation.evaluator INFO: Inference done 618/754. Dataloading: 0.0007 s/iter. Inference: 0.1400 s/iter. Eval: 0.2660 s/iter. Total: 0.4068 s/iter. ETA=0:00:55
[07/11 15:46:15] d2.evaluation.evaluator INFO: Inference done 631/754. Dataloading: 0.0007 s/iter. Inference: 0.1399 s/iter. Eval: 0.2659 s/iter. Total: 0.4067 s/iter. ETA=0:00:50
[07/11 15:46:20] d2.evaluation.evaluator INFO: Inference done 644/754. Dataloading: 0.0007 s/iter. Inference: 0.1398 s/iter. Eval: 0.2658 s/iter. Total: 0.4064 s/iter. ETA=0:00:44
[07/11 15:46:26] d2.evaluation.evaluator INFO: Inference done 657/754. Dataloading: 0.0007 s/iter. Inference: 0.1398 s/iter. Eval: 0.2658 s/iter. Total: 0.4065 s/iter. ETA=0:00:39
[07/11 15:46:31] d2.evaluation.evaluator INFO: Inference done 670/754. Dataloading: 0.0007 s/iter. Inference: 0.1397 s/iter. Eval: 0.2659 s/iter. Total: 0.4064 s/iter. ETA=0:00:34
[07/11 15:46:36] d2.evaluation.evaluator INFO: Inference done 683/754. Dataloading: 0.0007 s/iter. Inference: 0.1397 s/iter. Eval: 0.2659 s/iter. Total: 0.4064 s/iter. ETA=0:00:28
[07/11 15:46:41] d2.evaluation.evaluator INFO: Inference done 696/754. Dataloading: 0.0007 s/iter. Inference: 0.1396 s/iter. Eval: 0.2660 s/iter. Total: 0.4064 s/iter. ETA=0:00:23
[07/11 15:46:47] d2.evaluation.evaluator INFO: Inference done 709/754. Dataloading: 0.0007 s/iter. Inference: 0.1395 s/iter. Eval: 0.2660 s/iter. Total: 0.4063 s/iter. ETA=0:00:18
[07/11 15:46:52] d2.evaluation.evaluator INFO: Inference done 722/754. Dataloading: 0.0007 s/iter. Inference: 0.1394 s/iter. Eval: 0.2660 s/iter. Total: 0.4062 s/iter. ETA=0:00:12
[07/11 15:46:57] d2.evaluation.evaluator INFO: Inference done 734/754. Dataloading: 0.0007 s/iter. Inference: 0.1394 s/iter. Eval: 0.2662 s/iter. Total: 0.4064 s/iter. ETA=0:00:08
[07/11 15:47:02] d2.evaluation.evaluator INFO: Inference done 747/754. Dataloading: 0.0007 s/iter. Inference: 0.1394 s/iter. Eval: 0.2663 s/iter. Total: 0.4065 s/iter. ETA=0:00:02
[07/11 15:47:05] d2.evaluation.evaluator INFO: Total inference time: 0:05:04.688674 (0.406794 s / iter per device, on 1 devices)
[07/11 15:47:05] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:44 (0.139408 s / iter per device, on 1 devices)
[07/11 15:47:06] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/11 15:47:06] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/11 15:47:07] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/11 15:47:09] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/11 15:47:09] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 15:47:09] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/11 15:47:17] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 66.151 | 66.151 | 66.151 |  nan  |  nan  | 66.151 |
[07/11 15:47:17] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 15:47:17] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 79.385 | defect     | 52.918 |
[07/11 15:47:17] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_new in csv format:
[07/11 15:47:17] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/11 15:47:17] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 15:47:17] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/11 15:47:17] d2.evaluation.testing INFO: copypaste: Task: segm
[07/11 15:47:17] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 15:47:17] d2.evaluation.testing INFO: copypaste: 66.1514,66.1514,66.1514,nan,nan,66.1514
[07/11 15:47:17] d2.utils.events INFO:  eta: 0:52:30  iter: 24999  total_loss: 4.499  loss_ce: 0.09949  loss_mask: 0.195  loss_dice: 0.1332  loss_ce_0: 0.05247  loss_mask_0: 0.1947  loss_dice_0: 0.1296  loss_ce_1: 0.1  loss_mask_1: 0.1975  loss_dice_1: 0.1316  loss_ce_2: 0.09968  loss_mask_2: 0.2076  loss_dice_2: 0.135  loss_ce_3: 0.09952  loss_mask_3: 0.1979  loss_dice_3: 0.1321  loss_ce_4: 0.09971  loss_mask_4: 0.201  loss_dice_4: 0.1384  loss_ce_5: 0.09956  loss_mask_5: 0.2018  loss_dice_5: 0.1325  loss_ce_6: 0.09968  loss_mask_6: 0.2035  loss_dice_6: 0.1301  loss_ce_7: 0.09938  loss_mask_7: 0.2047  loss_dice_7: 0.1321  loss_ce_8: 0.09984  loss_mask_8: 0.1993  loss_dice_8: 0.1327  time: 0.5823  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:47:17] d2.engine.hooks INFO: Not saving as latest eval score for total_loss is 4.54347, not better than best score 4.45740 @ iteration 4999.
[07/11 15:47:29] d2.utils.events INFO:  eta: 0:52:12  iter: 25019  total_loss: 4.558  loss_ce: 0.1296  loss_mask: 0.1717  loss_dice: 0.1388  loss_ce_0: 0.06367  loss_mask_0: 0.166  loss_dice_0: 0.1399  loss_ce_1: 0.1296  loss_mask_1: 0.1671  loss_dice_1: 0.134  loss_ce_2: 0.1295  loss_mask_2: 0.175  loss_dice_2: 0.137  loss_ce_3: 0.1296  loss_mask_3: 0.1678  loss_dice_3: 0.1388  loss_ce_4: 0.1295  loss_mask_4: 0.171  loss_dice_4: 0.1375  loss_ce_5: 0.1295  loss_mask_5: 0.1629  loss_dice_5: 0.142  loss_ce_6: 0.1295  loss_mask_6: 0.1736  loss_dice_6: 0.1339  loss_ce_7: 0.1296  loss_mask_7: 0.1735  loss_dice_7: 0.1357  loss_ce_8: 0.1295  loss_mask_8: 0.1703  loss_dice_8: 0.1381  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:47:41] d2.utils.events INFO:  eta: 0:51:55  iter: 25039  total_loss: 4.642  loss_ce: 0.09957  loss_mask: 0.1753  loss_dice: 0.1755  loss_ce_0: 0.05253  loss_mask_0: 0.1776  loss_dice_0: 0.1624  loss_ce_1: 0.1001  loss_mask_1: 0.1781  loss_dice_1: 0.1669  loss_ce_2: 0.09984  loss_mask_2: 0.1749  loss_dice_2: 0.1615  loss_ce_3: 0.09971  loss_mask_3: 0.1662  loss_dice_3: 0.1752  loss_ce_4: 0.09994  loss_mask_4: 0.1839  loss_dice_4: 0.1605  loss_ce_5: 0.09971  loss_mask_5: 0.1799  loss_dice_5: 0.1668  loss_ce_6: 0.09982  loss_mask_6: 0.1695  loss_dice_6: 0.1706  loss_ce_7: 0.0996  loss_mask_7: 0.1768  loss_dice_7: 0.1783  loss_ce_8: 0.09968  loss_mask_8: 0.1859  loss_dice_8: 0.1599  time: 0.5823  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:47:52] d2.utils.events INFO:  eta: 0:51:39  iter: 25059  total_loss: 4.68  loss_ce: 0.09953  loss_mask: 0.1847  loss_dice: 0.1486  loss_ce_0: 0.0525  loss_mask_0: 0.1801  loss_dice_0: 0.1542  loss_ce_1: 0.09985  loss_mask_1: 0.1838  loss_dice_1: 0.1496  loss_ce_2: 0.09968  loss_mask_2: 0.18  loss_dice_2: 0.1519  loss_ce_3: 0.09964  loss_mask_3: 0.1802  loss_dice_3: 0.1527  loss_ce_4: 0.09968  loss_mask_4: 0.1807  loss_dice_4: 0.1524  loss_ce_5: 0.09964  loss_mask_5: 0.1729  loss_dice_5: 0.1464  loss_ce_6: 0.09964  loss_mask_6: 0.1887  loss_dice_6: 0.1491  loss_ce_7: 0.09953  loss_mask_7: 0.1856  loss_dice_7: 0.146  loss_ce_8: 0.09957  loss_mask_8: 0.1908  loss_dice_8: 0.1488  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:48:04] d2.utils.events INFO:  eta: 0:51:21  iter: 25079  total_loss: 4.47  loss_ce: 0.1598  loss_mask: 0.1951  loss_dice: 0.1374  loss_ce_0: 0.07484  loss_mask_0: 0.1914  loss_dice_0: 0.1381  loss_ce_1: 0.1588  loss_mask_1: 0.1918  loss_dice_1: 0.1402  loss_ce_2: 0.1592  loss_mask_2: 0.2002  loss_dice_2: 0.1383  loss_ce_3: 0.1595  loss_mask_3: 0.1918  loss_dice_3: 0.1357  loss_ce_4: 0.1594  loss_mask_4: 0.1887  loss_dice_4: 0.1369  loss_ce_5: 0.1596  loss_mask_5: 0.1767  loss_dice_5: 0.1378  loss_ce_6: 0.1593  loss_mask_6: 0.1936  loss_dice_6: 0.1357  loss_ce_7: 0.1598  loss_mask_7: 0.1863  loss_dice_7: 0.1386  loss_ce_8: 0.1596  loss_mask_8: 0.1864  loss_dice_8: 0.1327  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:48:15] d2.utils.events INFO:  eta: 0:51:00  iter: 25099  total_loss: 4.459  loss_ce: 0.1295  loss_mask: 0.1691  loss_dice: 0.1364  loss_ce_0: 0.06366  loss_mask_0: 0.1775  loss_dice_0: 0.1415  loss_ce_1: 0.1288  loss_mask_1: 0.1713  loss_dice_1: 0.1349  loss_ce_2: 0.1293  loss_mask_2: 0.1671  loss_dice_2: 0.14  loss_ce_3: 0.1294  loss_mask_3: 0.181  loss_dice_3: 0.1384  loss_ce_4: 0.1292  loss_mask_4: 0.1708  loss_dice_4: 0.1394  loss_ce_5: 0.1295  loss_mask_5: 0.174  loss_dice_5: 0.1381  loss_ce_6: 0.1293  loss_mask_6: 0.1755  loss_dice_6: 0.1382  loss_ce_7: 0.1295  loss_mask_7: 0.1753  loss_dice_7: 0.1413  loss_ce_8: 0.1294  loss_mask_8: 0.174  loss_dice_8: 0.136  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:48:27] d2.utils.events INFO:  eta: 0:50:39  iter: 25119  total_loss: 4.55  loss_ce: 0.09953  loss_mask: 0.1717  loss_dice: 0.1473  loss_ce_0: 0.0525  loss_mask_0: 0.1655  loss_dice_0: 0.1482  loss_ce_1: 0.0999  loss_mask_1: 0.1637  loss_dice_1: 0.1428  loss_ce_2: 0.09971  loss_mask_2: 0.1664  loss_dice_2: 0.1391  loss_ce_3: 0.09956  loss_mask_3: 0.1687  loss_dice_3: 0.1479  loss_ce_4: 0.09975  loss_mask_4: 0.1666  loss_dice_4: 0.1402  loss_ce_5: 0.09964  loss_mask_5: 0.1673  loss_dice_5: 0.1398  loss_ce_6: 0.09956  loss_mask_6: 0.1683  loss_dice_6: 0.1415  loss_ce_7: 0.09949  loss_mask_7: 0.1622  loss_dice_7: 0.1485  loss_ce_8: 0.09964  loss_mask_8: 0.1606  loss_dice_8: 0.1463  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:48:38] d2.utils.events INFO:  eta: 0:50:21  iter: 25139  total_loss: 4.579  loss_ce: 0.1295  loss_mask: 0.1519  loss_dice: 0.1508  loss_ce_0: 0.06366  loss_mask_0: 0.1504  loss_dice_0: 0.1456  loss_ce_1: 0.1293  loss_mask_1: 0.1552  loss_dice_1: 0.1511  loss_ce_2: 0.1294  loss_mask_2: 0.1514  loss_dice_2: 0.1516  loss_ce_3: 0.1294  loss_mask_3: 0.163  loss_dice_3: 0.1493  loss_ce_4: 0.1295  loss_mask_4: 0.1567  loss_dice_4: 0.1498  loss_ce_5: 0.1295  loss_mask_5: 0.1578  loss_dice_5: 0.1551  loss_ce_6: 0.1294  loss_mask_6: 0.1489  loss_dice_6: 0.1572  loss_ce_7: 0.1296  loss_mask_7: 0.1605  loss_dice_7: 0.1512  loss_ce_8: 0.1295  loss_mask_8: 0.1671  loss_dice_8: 0.1499  time: 0.5822  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:48:50] d2.utils.events INFO:  eta: 0:50:04  iter: 25159  total_loss: 4.778  loss_ce: 0.09949  loss_mask: 0.2198  loss_dice: 0.1587  loss_ce_0: 0.0525  loss_mask_0: 0.2063  loss_dice_0: 0.1473  loss_ce_1: 0.0997  loss_mask_1: 0.2252  loss_dice_1: 0.15  loss_ce_2: 0.09955  loss_mask_2: 0.2176  loss_dice_2: 0.1485  loss_ce_3: 0.09956  loss_mask_3: 0.2235  loss_dice_3: 0.1519  loss_ce_4: 0.09964  loss_mask_4: 0.2174  loss_dice_4: 0.1509  loss_ce_5: 0.09956  loss_mask_5: 0.2284  loss_dice_5: 0.1525  loss_ce_6: 0.09953  loss_mask_6: 0.2231  loss_dice_6: 0.1514  loss_ce_7: 0.09949  loss_mask_7: 0.2159  loss_dice_7: 0.1529  loss_ce_8: 0.09964  loss_mask_8: 0.216  loss_dice_8: 0.1496  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:49:02] d2.utils.events INFO:  eta: 0:49:48  iter: 25179  total_loss: 4.618  loss_ce: 0.1297  loss_mask: 0.1951  loss_dice: 0.1451  loss_ce_0: 0.06367  loss_mask_0: 0.2004  loss_dice_0: 0.1487  loss_ce_1: 0.1297  loss_mask_1: 0.2002  loss_dice_1: 0.146  loss_ce_2: 0.1297  loss_mask_2: 0.1895  loss_dice_2: 0.144  loss_ce_3: 0.1296  loss_mask_3: 0.1943  loss_dice_3: 0.1445  loss_ce_4: 0.1296  loss_mask_4: 0.1977  loss_dice_4: 0.1531  loss_ce_5: 0.1296  loss_mask_5: 0.2013  loss_dice_5: 0.1523  loss_ce_6: 0.1297  loss_mask_6: 0.1929  loss_dice_6: 0.1513  loss_ce_7: 0.1296  loss_mask_7: 0.2049  loss_dice_7: 0.1499  loss_ce_8: 0.1297  loss_mask_8: 0.2028  loss_dice_8: 0.1501  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:49:13] d2.utils.events INFO:  eta: 0:49:31  iter: 25199  total_loss: 4.35  loss_ce: 0.09927  loss_mask: 0.1723  loss_dice: 0.1539  loss_ce_0: 0.05248  loss_mask_0: 0.1703  loss_dice_0: 0.1523  loss_ce_1: 0.09908  loss_mask_1: 0.1822  loss_dice_1: 0.1589  loss_ce_2: 0.09917  loss_mask_2: 0.1646  loss_dice_2: 0.148  loss_ce_3: 0.09926  loss_mask_3: 0.1683  loss_dice_3: 0.1577  loss_ce_4: 0.09926  loss_mask_4: 0.1705  loss_dice_4: 0.1542  loss_ce_5: 0.09926  loss_mask_5: 0.1693  loss_dice_5: 0.1571  loss_ce_6: 0.0993  loss_mask_6: 0.1676  loss_dice_6: 0.1532  loss_ce_7: 0.09923  loss_mask_7: 0.1745  loss_dice_7: 0.1492  loss_ce_8: 0.09927  loss_mask_8: 0.1687  loss_dice_8: 0.1504  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:49:25] d2.utils.events INFO:  eta: 0:49:13  iter: 25219  total_loss: 4.474  loss_ce: 0.09923  loss_mask: 0.1706  loss_dice: 0.146  loss_ce_0: 0.05246  loss_mask_0: 0.1694  loss_dice_0: 0.1484  loss_ce_1: 0.09953  loss_mask_1: 0.173  loss_dice_1: 0.147  loss_ce_2: 0.09939  loss_mask_2: 0.1659  loss_dice_2: 0.1517  loss_ce_3: 0.0993  loss_mask_3: 0.1805  loss_dice_3: 0.1504  loss_ce_4: 0.09934  loss_mask_4: 0.1649  loss_dice_4: 0.1517  loss_ce_5: 0.09919  loss_mask_5: 0.1729  loss_dice_5: 0.1459  loss_ce_6: 0.09934  loss_mask_6: 0.1627  loss_dice_6: 0.1492  loss_ce_7: 0.09926  loss_mask_7: 0.1683  loss_dice_7: 0.1482  loss_ce_8: 0.09934  loss_mask_8: 0.1694  loss_dice_8: 0.1427  time: 0.5822  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:49:36] d2.utils.events INFO:  eta: 0:48:57  iter: 25239  total_loss: 4.916  loss_ce: 0.09923  loss_mask: 0.2032  loss_dice: 0.1569  loss_ce_0: 0.05244  loss_mask_0: 0.2037  loss_dice_0: 0.1543  loss_ce_1: 0.09953  loss_mask_1: 0.2011  loss_dice_1: 0.1631  loss_ce_2: 0.09936  loss_mask_2: 0.2019  loss_dice_2: 0.1568  loss_ce_3: 0.09934  loss_mask_3: 0.211  loss_dice_3: 0.1524  loss_ce_4: 0.09938  loss_mask_4: 0.2059  loss_dice_4: 0.1603  loss_ce_5: 0.09926  loss_mask_5: 0.2121  loss_dice_5: 0.1546  loss_ce_6: 0.0993  loss_mask_6: 0.2113  loss_dice_6: 0.1572  loss_ce_7: 0.09927  loss_mask_7: 0.2015  loss_dice_7: 0.1566  loss_ce_8: 0.09941  loss_mask_8: 0.211  loss_dice_8: 0.1582  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:49:48] d2.utils.events INFO:  eta: 0:48:38  iter: 25259  total_loss: 4.602  loss_ce: 0.09945  loss_mask: 0.2027  loss_dice: 0.1301  loss_ce_0: 0.05246  loss_mask_0: 0.2087  loss_dice_0: 0.1349  loss_ce_1: 0.09998  loss_mask_1: 0.2097  loss_dice_1: 0.1351  loss_ce_2: 0.09971  loss_mask_2: 0.2006  loss_dice_2: 0.1276  loss_ce_3: 0.09952  loss_mask_3: 0.2097  loss_dice_3: 0.1339  loss_ce_4: 0.09968  loss_mask_4: 0.2014  loss_dice_4: 0.1339  loss_ce_5: 0.09964  loss_mask_5: 0.2035  loss_dice_5: 0.1357  loss_ce_6: 0.09964  loss_mask_6: 0.1982  loss_dice_6: 0.1326  loss_ce_7: 0.09945  loss_mask_7: 0.203  loss_dice_7: 0.1348  loss_ce_8: 0.09964  loss_mask_8: 0.2036  loss_dice_8: 0.1362  time: 0.5822  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:50:00] d2.utils.events INFO:  eta: 0:48:19  iter: 25279  total_loss: 4.735  loss_ce: 0.1601  loss_mask: 0.1697  loss_dice: 0.1459  loss_ce_0: 0.07488  loss_mask_0: 0.1735  loss_dice_0: 0.1383  loss_ce_1: 0.1593  loss_mask_1: 0.1708  loss_dice_1: 0.1462  loss_ce_2: 0.1599  loss_mask_2: 0.1703  loss_dice_2: 0.1448  loss_ce_3: 0.16  loss_mask_3: 0.1677  loss_dice_3: 0.147  loss_ce_4: 0.1597  loss_mask_4: 0.1673  loss_dice_4: 0.1462  loss_ce_5: 0.1599  loss_mask_5: 0.1677  loss_dice_5: 0.1426  loss_ce_6: 0.1598  loss_mask_6: 0.1708  loss_dice_6: 0.14  loss_ce_7: 0.1599  loss_mask_7: 0.1825  loss_dice_7: 0.1446  loss_ce_8: 0.1596  loss_mask_8: 0.1734  loss_dice_8: 0.1404  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:50:11] d2.utils.events INFO:  eta: 0:47:57  iter: 25299  total_loss: 4.476  loss_ce: 0.09956  loss_mask: 0.1819  loss_dice: 0.1374  loss_ce_0: 0.05246  loss_mask_0: 0.1861  loss_dice_0: 0.1333  loss_ce_1: 0.1003  loss_mask_1: 0.182  loss_dice_1: 0.1348  loss_ce_2: 0.09977  loss_mask_2: 0.1871  loss_dice_2: 0.14  loss_ce_3: 0.0996  loss_mask_3: 0.1864  loss_dice_3: 0.139  loss_ce_4: 0.09983  loss_mask_4: 0.18  loss_dice_4: 0.1371  loss_ce_5: 0.09956  loss_mask_5: 0.1777  loss_dice_5: 0.1362  loss_ce_6: 0.09979  loss_mask_6: 0.1877  loss_dice_6: 0.1367  loss_ce_7: 0.09949  loss_mask_7: 0.1823  loss_dice_7: 0.1365  loss_ce_8: 0.09971  loss_mask_8: 0.1751  loss_dice_8: 0.1359  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:50:23] d2.utils.events INFO:  eta: 0:47:42  iter: 25319  total_loss: 4.589  loss_ce: 0.09912  loss_mask: 0.1956  loss_dice: 0.1516  loss_ce_0: 0.05244  loss_mask_0: 0.1919  loss_dice_0: 0.148  loss_ce_1: 0.09936  loss_mask_1: 0.1918  loss_dice_1: 0.1514  loss_ce_2: 0.09908  loss_mask_2: 0.1897  loss_dice_2: 0.1525  loss_ce_3: 0.09911  loss_mask_3: 0.1965  loss_dice_3: 0.1511  loss_ce_4: 0.09919  loss_mask_4: 0.1966  loss_dice_4: 0.1496  loss_ce_5: 0.09912  loss_mask_5: 0.1891  loss_dice_5: 0.1495  loss_ce_6: 0.09911  loss_mask_6: 0.1983  loss_dice_6: 0.1475  loss_ce_7: 0.09919  loss_mask_7: 0.1969  loss_dice_7: 0.1443  loss_ce_8: 0.09923  loss_mask_8: 0.1895  loss_dice_8: 0.1489  time: 0.5822  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:50:34] d2.utils.events INFO:  eta: 0:47:25  iter: 25339  total_loss: 4.652  loss_ce: 0.1297  loss_mask: 0.1895  loss_dice: 0.1487  loss_ce_0: 0.06368  loss_mask_0: 0.1956  loss_dice_0: 0.1425  loss_ce_1: 0.1295  loss_mask_1: 0.1905  loss_dice_1: 0.1425  loss_ce_2: 0.1297  loss_mask_2: 0.1953  loss_dice_2: 0.1399  loss_ce_3: 0.1297  loss_mask_3: 0.1983  loss_dice_3: 0.1458  loss_ce_4: 0.1298  loss_mask_4: 0.1914  loss_dice_4: 0.145  loss_ce_5: 0.1297  loss_mask_5: 0.1939  loss_dice_5: 0.148  loss_ce_6: 0.1297  loss_mask_6: 0.2002  loss_dice_6: 0.1449  loss_ce_7: 0.1298  loss_mask_7: 0.187  loss_dice_7: 0.1413  loss_ce_8: 0.1295  loss_mask_8: 0.1971  loss_dice_8: 0.1483  time: 0.5822  data_time: 0.0030  lr: 1e-06  max_mem: 2811M
[07/11 15:50:46] d2.utils.events INFO:  eta: 0:47:09  iter: 25359  total_loss: 4.458  loss_ce: 0.1297  loss_mask: 0.1613  loss_dice: 0.129  loss_ce_0: 0.06368  loss_mask_0: 0.1643  loss_dice_0: 0.1298  loss_ce_1: 0.1294  loss_mask_1: 0.1724  loss_dice_1: 0.1377  loss_ce_2: 0.1296  loss_mask_2: 0.1663  loss_dice_2: 0.13  loss_ce_3: 0.1297  loss_mask_3: 0.1728  loss_dice_3: 0.1301  loss_ce_4: 0.1296  loss_mask_4: 0.1724  loss_dice_4: 0.1328  loss_ce_5: 0.1295  loss_mask_5: 0.1763  loss_dice_5: 0.1324  loss_ce_6: 0.1297  loss_mask_6: 0.1689  loss_dice_6: 0.1316  loss_ce_7: 0.1296  loss_mask_7: 0.1775  loss_dice_7: 0.1301  loss_ce_8: 0.1296  loss_mask_8: 0.1527  loss_dice_8: 0.126  time: 0.5822  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 15:50:58] d2.utils.events INFO:  eta: 0:46:52  iter: 25379  total_loss: 4.272  loss_ce: 0.09893  loss_mask: 0.1778  loss_dice: 0.1462  loss_ce_0: 0.05241  loss_mask_0: 0.1745  loss_dice_0: 0.1461  loss_ce_1: 0.09876  loss_mask_1: 0.1853  loss_dice_1: 0.1492  loss_ce_2: 0.09885  loss_mask_2: 0.1816  loss_dice_2: 0.1487  loss_ce_3: 0.09885  loss_mask_3: 0.1866  loss_dice_3: 0.1479  loss_ce_4: 0.09889  loss_mask_4: 0.1796  loss_dice_4: 0.1497  loss_ce_5: 0.09882  loss_mask_5: 0.1739  loss_dice_5: 0.1538  loss_ce_6: 0.09885  loss_mask_6: 0.1762  loss_dice_6: 0.1502  loss_ce_7: 0.09893  loss_mask_7: 0.1815  loss_dice_7: 0.1484  loss_ce_8: 0.09889  loss_mask_8: 0.1917  loss_dice_8: 0.149  time: 0.5822  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:51:09] d2.utils.events INFO:  eta: 0:46:30  iter: 25399  total_loss: 4.472  loss_ce: 0.1298  loss_mask: 0.1921  loss_dice: 0.1314  loss_ce_0: 0.06368  loss_mask_0: 0.1985  loss_dice_0: 0.1303  loss_ce_1: 0.1298  loss_mask_1: 0.1986  loss_dice_1: 0.1282  loss_ce_2: 0.1298  loss_mask_2: 0.1828  loss_dice_2: 0.1306  loss_ce_3: 0.1297  loss_mask_3: 0.1891  loss_dice_3: 0.1299  loss_ce_4: 0.1297  loss_mask_4: 0.1917  loss_dice_4: 0.1301  loss_ce_5: 0.1297  loss_mask_5: 0.2061  loss_dice_5: 0.1338  loss_ce_6: 0.1298  loss_mask_6: 0.182  loss_dice_6: 0.1363  loss_ce_7: 0.1298  loss_mask_7: 0.1934  loss_dice_7: 0.1351  loss_ce_8: 0.1298  loss_mask_8: 0.1935  loss_dice_8: 0.1344  time: 0.5822  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 15:51:21] d2.utils.events INFO:  eta: 0:46:12  iter: 25419  total_loss: 4.687  loss_ce: 0.1603  loss_mask: 0.1816  loss_dice: 0.1563  loss_ce_0: 0.07494  loss_mask_0: 0.186  loss_dice_0: 0.1602  loss_ce_1: 0.1595  loss_mask_1: 0.1875  loss_dice_1: 0.1554  loss_ce_2: 0.16  loss_mask_2: 0.1843  loss_dice_2: 0.161  loss_ce_3: 0.1603  loss_mask_3: 0.1857  loss_dice_3: 0.1556  loss_ce_4: 0.16  loss_mask_4: 0.1962  loss_dice_4: 0.1593  loss_ce_5: 0.1603  loss_mask_5: 0.1791  loss_dice_5: 0.1525  loss_ce_6: 0.16  loss_mask_6: 0.1915  loss_dice_6: 0.1591  loss_ce_7: 0.1604  loss_mask_7: 0.1881  loss_dice_7: 0.1617  loss_ce_8: 0.1601  loss_mask_8: 0.1875  loss_dice_8: 0.1546  time: 0.5822  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 15:51:32] d2.utils.events INFO:  eta: 0:45:57  iter: 25439  total_loss: 4.443  loss_ce: 0.09904  loss_mask: 0.1723  loss_dice: 0.1567  loss_ce_0: 0.05241  loss_mask_0: 0.172  loss_dice_0: 0.154  loss_ce_1: 0.09897  loss_mask_1: 0.1671  loss_dice_1: 0.1601  loss_ce_2: 0.09902  loss_mask_2: 0.1709  loss_dice_2: 0.1593  loss_ce_3: 0.09904  loss_mask_3: 0.177  loss_dice_3: 0.155  loss_ce_4: 0.09904  loss_mask_4: 0.1771  loss_dice_4: 0.1572  loss_ce_5: 0.09896  loss_mask_5: 0.1718  loss_dice_5: 0.1596  loss_ce_6: 0.09904  loss_mask_6: 0.174  loss_dice_6: 0.154  loss_ce_7: 0.099  loss_mask_7: 0.1756  loss_dice_7: 0.1559  loss_ce_8: 0.09912  loss_mask_8: 0.1746  loss_dice_8: 0.1569  time: 0.5822  data_time: 0.0032  lr: 1e-06  max_mem: 2811M
[07/11 15:51:44] d2.utils.events INFO:  eta: 0:45:41  iter: 25459  total_loss: 4.56  loss_ce: 0.09911  loss_mask: 0.1868  loss_dice: 0.1293  loss_ce_0: 0.05237  loss_mask_0: 0.1856  loss_dice_0: 0.13  loss_ce_1: 0.09977  loss_mask_1: 0.1898  loss_dice_1: 0.1267  loss_ce_2: 0.09939  loss_mask_2: 0.1873  loss_dice_2: 0.1256  loss_ce_3: 0.09922  loss_mask_3: 0.1937  loss_dice_3: 0.132  loss_ce_4: 0.09949  loss_mask_4: 0.196  loss_dice_4: 0.1296  loss_ce_5: 0.09919  loss_mask_5: 0.1873  loss_dice_5: 0.1308  loss_ce_6: 0.09945  loss_mask_6: 0.1953  loss_dice_6: 0.1269  loss_ce_7: 0.09919  loss_mask_7: 0.1964  loss_dice_7: 0.1297  loss_ce_8: 0.09938  loss_mask_8: 0.1876  loss_dice_8: 0.1351  time: 0.5822  data_time: 0.0025  lr: 1e-06  max_mem: 2811M
[07/11 15:51:56] d2.utils.events INFO:  eta: 0:45:22  iter: 25479  total_loss: 4.373  loss_ce: 0.09889  loss_mask: 0.1977  loss_dice: 0.1369  loss_ce_0: 0.05237  loss_mask_0: 0.1888  loss_dice_0: 0.1328  loss_ce_1: 0.09904  loss_mask_1: 0.1868  loss_dice_1: 0.1378  loss_ce_2: 0.09882  loss_mask_2: 0.1925  loss_dice_2: 0.1325  loss_ce_3: 0.09881  loss_mask_3: 0.1828  loss_dice_3: 0.1367  loss_ce_4: 0.09896  loss_mask_4: 0.1862  loss_dice_4: 0.1364  loss_ce_5: 0.09874  loss_mask_5: 0.1878  loss_dice_5: 0.1285  loss_ce_6: 0.09889  loss_mask_6: 0.1931  loss_dice_6: 0.1372  loss_ce_7: 0.09882  loss_mask_7: 0.1919  loss_dice_7: 0.1325  loss_ce_8: 0.09897  loss_mask_8: 0.1906  loss_dice_8: 0.1311  time: 0.5822  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:52:07] d2.utils.events INFO:  eta: 0:45:05  iter: 25499  total_loss: 4.891  loss_ce: 0.099  loss_mask: 0.2136  loss_dice: 0.1364  loss_ce_0: 0.05235  loss_mask_0: 0.2115  loss_dice_0: 0.141  loss_ce_1: 0.09977  loss_mask_1: 0.2125  loss_dice_1: 0.1395  loss_ce_2: 0.09932  loss_mask_2: 0.2123  loss_dice_2: 0.1385  loss_ce_3: 0.09922  loss_mask_3: 0.2187  loss_dice_3: 0.1377  loss_ce_4: 0.09941  loss_mask_4: 0.2069  loss_dice_4: 0.136  loss_ce_5: 0.09911  loss_mask_5: 0.2185  loss_dice_5: 0.1375  loss_ce_6: 0.09934  loss_mask_6: 0.2075  loss_dice_6: 0.1368  loss_ce_7: 0.099  loss_mask_7: 0.2098  loss_dice_7: 0.14  loss_ce_8: 0.09919  loss_mask_8: 0.2051  loss_dice_8: 0.1386  time: 0.5822  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 15:52:19] d2.utils.events INFO:  eta: 0:44:47  iter: 25519  total_loss: 4.721  loss_ce: 0.1296  loss_mask: 0.2022  loss_dice: 0.1534  loss_ce_0: 0.06369  loss_mask_0: 0.2127  loss_dice_0: 0.1472  loss_ce_1: 0.1291  loss_mask_1: 0.2058  loss_dice_1: 0.1492  loss_ce_2: 0.1294  loss_mask_2: 0.204  loss_dice_2: 0.1531  loss_ce_3: 0.1294  loss_mask_3: 0.2074  loss_dice_3: 0.146  loss_ce_4: 0.1295  loss_mask_4: 0.1998  loss_dice_4: 0.1402  loss_ce_5: 0.1295  loss_mask_5: 0.2076  loss_dice_5: 0.1494  loss_ce_6: 0.1295  loss_mask_6: 0.2108  loss_dice_6: 0.1451  loss_ce_7: 0.1296  loss_mask_7: 0.2038  loss_dice_7: 0.1511  loss_ce_8: 0.1296  loss_mask_8: 0.2147  loss_dice_8: 0.1418  time: 0.5822  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:52:31] d2.utils.events INFO:  eta: 0:44:29  iter: 25539  total_loss: 4.761  loss_ce: 0.09878  loss_mask: 0.1538  loss_dice: 0.1639  loss_ce_0: 0.05235  loss_mask_0: 0.1502  loss_dice_0: 0.1588  loss_ce_1: 0.09889  loss_mask_1: 0.1518  loss_dice_1: 0.162  loss_ce_2: 0.0988  loss_mask_2: 0.1475  loss_dice_2: 0.1594  loss_ce_3: 0.09874  loss_mask_3: 0.1456  loss_dice_3: 0.1591  loss_ce_4: 0.09885  loss_mask_4: 0.1537  loss_dice_4: 0.1609  loss_ce_5: 0.09882  loss_mask_5: 0.1505  loss_dice_5: 0.1664  loss_ce_6: 0.09878  loss_mask_6: 0.1459  loss_dice_6: 0.1611  loss_ce_7: 0.09878  loss_mask_7: 0.1471  loss_dice_7: 0.1606  loss_ce_8: 0.09885  loss_mask_8: 0.1617  loss_dice_8: 0.1662  time: 0.5822  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:52:42] d2.utils.events INFO:  eta: 0:44:13  iter: 25559  total_loss: 4.524  loss_ce: 0.09878  loss_mask: 0.1476  loss_dice: 0.148  loss_ce_0: 0.05239  loss_mask_0: 0.1439  loss_dice_0: 0.1418  loss_ce_1: 0.09904  loss_mask_1: 0.1528  loss_dice_1: 0.1391  loss_ce_2: 0.09876  loss_mask_2: 0.1396  loss_dice_2: 0.141  loss_ce_3: 0.09881  loss_mask_3: 0.139  loss_dice_3: 0.1471  loss_ce_4: 0.09893  loss_mask_4: 0.1476  loss_dice_4: 0.1459  loss_ce_5: 0.09874  loss_mask_5: 0.1512  loss_dice_5: 0.1451  loss_ce_6: 0.09885  loss_mask_6: 0.1488  loss_dice_6: 0.1364  loss_ce_7: 0.09882  loss_mask_7: 0.1422  loss_dice_7: 0.1467  loss_ce_8: 0.09889  loss_mask_8: 0.1397  loss_dice_8: 0.143  time: 0.5822  data_time: 0.0023  lr: 1e-06  max_mem: 2811M
[07/11 15:52:54] d2.utils.events INFO:  eta: 0:43:56  iter: 25579  total_loss: 4.657  loss_ce: 0.09882  loss_mask: 0.2184  loss_dice: 0.1353  loss_ce_0: 0.05237  loss_mask_0: 0.2223  loss_dice_0: 0.134  loss_ce_1: 0.09908  loss_mask_1: 0.228  loss_dice_1: 0.1377  loss_ce_2: 0.09887  loss_mask_2: 0.2249  loss_dice_2: 0.1388  loss_ce_3: 0.09878  loss_mask_3: 0.2249  loss_dice_3: 0.1364  loss_ce_4: 0.09896  loss_mask_4: 0.23  loss_dice_4: 0.1335  loss_ce_5: 0.09889  loss_mask_5: 0.2237  loss_dice_5: 0.1387  loss_ce_6: 0.09889  loss_mask_6: 0.2114  loss_dice_6: 0.1336  loss_ce_7: 0.09882  loss_mask_7: 0.2286  loss_dice_7: 0.1374  loss_ce_8: 0.09893  loss_mask_8: 0.2223  loss_dice_8: 0.137  time: 0.5822  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 15:53:06] d2.utils.events INFO:  eta: 0:43:38  iter: 25599  total_loss: 4.586  loss_ce: 0.09867  loss_mask: 0.2046  loss_dice: 0.1326  loss_ce_0: 0.05233  loss_mask_0: 0.2098  loss_dice_0: 0.1382  loss_ce_1: 0.09865  loss_mask_1: 0.2011  loss_dice_1: 0.1358  loss_ce_2: 0.09857  loss_mask_2: 0.2097  loss_dice_2: 0.1397  loss_ce_3: 0.09855  loss_mask_3: 0.2012  loss_dice_3: 0.1344  loss_ce_4: 0.09859  loss_mask_4: 0.2095  loss_dice_4: 0.1374  loss_ce_5: 0.09852  loss_mask_5: 0.2057  loss_dice_5: 0.1391  loss_ce_6: 0.09859  loss_mask_6: 0.2144  loss_dice_6: 0.1383  loss_ce_7: 0.09859  loss_mask_7: 0.2141  loss_dice_7: 0.1398  loss_ce_8: 0.09867  loss_mask_8: 0.2024  loss_dice_8: 0.1341  time: 0.5822  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 15:53:17] d2.utils.events INFO:  eta: 0:43:23  iter: 25619  total_loss: 4.66  loss_ce: 0.09889  loss_mask: 0.2106  loss_dice: 0.133  loss_ce_0: 0.05233  loss_mask_0: 0.2124  loss_dice_0: 0.1371  loss_ce_1: 0.09949  loss_mask_1: 0.2115  loss_dice_1: 0.1334  loss_ce_2: 0.09906  loss_mask_2: 0.2112  loss_dice_2: 0.1342  loss_ce_3: 0.099  loss_mask_3: 0.2143  loss_dice_3: 0.1336  loss_ce_4: 0.09915  loss_mask_4: 0.2051  loss_dice_4: 0.1321  loss_ce_5: 0.09904  loss_mask_5: 0.2071  loss_dice_5: 0.1349  loss_ce_6: 0.09919  loss_mask_6: 0.2241  loss_dice_6: 0.1364  loss_ce_7: 0.09882  loss_mask_7: 0.2088  loss_dice_7: 0.1315  loss_ce_8: 0.099  loss_mask_8: 0.2027  loss_dice_8: 0.1311  time: 0.5822  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 15:53:29] d2.utils.events INFO:  eta: 0:43:01  iter: 25639  total_loss: 4.725  loss_ce: 0.1609  loss_mask: 0.1855  loss_dice: 0.1438  loss_ce_0: 0.07503  loss_mask_0: 0.1949  loss_dice_0: 0.1548  loss_ce_1: 0.1608  loss_mask_1: 0.1936  loss_dice_1: 0.1507  loss_ce_2: 0.1609  loss_mask_2: 0.1957  loss_dice_2: 0.1501  loss_ce_3: 0.1609  loss_mask_3: 0.1957  loss_dice_3: 0.1538  loss_ce_4: 0.1609  loss_mask_4: 0.1924  loss_dice_4: 0.1499  loss_ce_5: 0.1609  loss_mask_5: 0.1921  loss_dice_5: 0.1505  loss_ce_6: 0.1609  loss_mask_6: 0.1928  loss_dice_6: 0.1474  loss_ce_7: 0.161  loss_mask_7: 0.1909  loss_dice_7: 0.1593  loss_ce_8: 0.1609  loss_mask_8: 0.1769  loss_dice_8: 0.1475  time: 0.5822  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:53:39] d2.utils.events INFO:  eta: 0:42:38  iter: 25659  total_loss: 4.266  loss_ce: 0.09885  loss_mask: 0.1735  loss_dice: 0.1301  loss_ce_0: 0.05239  loss_mask_0: 0.1794  loss_dice_0: 0.1298  loss_ce_1: 0.0991  loss_mask_1: 0.176  loss_dice_1: 0.1301  loss_ce_2: 0.09876  loss_mask_2: 0.178  loss_dice_2: 0.1275  loss_ce_3: 0.09874  loss_mask_3: 0.1856  loss_dice_3: 0.1236  loss_ce_4: 0.09885  loss_mask_4: 0.1763  loss_dice_4: 0.1294  loss_ce_5: 0.09874  loss_mask_5: 0.1738  loss_dice_5: 0.1329  loss_ce_6: 0.09885  loss_mask_6: 0.1735  loss_dice_6: 0.1276  loss_ce_7: 0.09874  loss_mask_7: 0.1741  loss_dice_7: 0.1254  loss_ce_8: 0.09893  loss_mask_8: 0.1744  loss_dice_8: 0.1236  time: 0.5821  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:53:52] d2.utils.events INFO:  eta: 0:42:26  iter: 25679  total_loss: 4.51  loss_ce: 0.1606  loss_mask: 0.1767  loss_dice: 0.1295  loss_ce_0: 0.075  loss_mask_0: 0.1851  loss_dice_0: 0.13  loss_ce_1: 0.1601  loss_mask_1: 0.1788  loss_dice_1: 0.1322  loss_ce_2: 0.1604  loss_mask_2: 0.18  loss_dice_2: 0.1352  loss_ce_3: 0.1606  loss_mask_3: 0.182  loss_dice_3: 0.1335  loss_ce_4: 0.1604  loss_mask_4: 0.1884  loss_dice_4: 0.1356  loss_ce_5: 0.1607  loss_mask_5: 0.1701  loss_dice_5: 0.1322  loss_ce_6: 0.1606  loss_mask_6: 0.1826  loss_dice_6: 0.1302  loss_ce_7: 0.1607  loss_mask_7: 0.1772  loss_dice_7: 0.1284  loss_ce_8: 0.1604  loss_mask_8: 0.1766  loss_dice_8: 0.131  time: 0.5822  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:54:05] d2.utils.events INFO:  eta: 0:42:13  iter: 25699  total_loss: 4.773  loss_ce: 0.09908  loss_mask: 0.175  loss_dice: 0.1519  loss_ce_0: 0.0524  loss_mask_0: 0.1706  loss_dice_0: 0.1506  loss_ce_1: 0.0993  loss_mask_1: 0.1671  loss_dice_1: 0.1424  loss_ce_2: 0.09921  loss_mask_2: 0.1712  loss_dice_2: 0.1426  loss_ce_3: 0.09915  loss_mask_3: 0.1721  loss_dice_3: 0.1509  loss_ce_4: 0.09923  loss_mask_4: 0.1697  loss_dice_4: 0.1491  loss_ce_5: 0.09904  loss_mask_5: 0.178  loss_dice_5: 0.1516  loss_ce_6: 0.09919  loss_mask_6: 0.1742  loss_dice_6: 0.1509  loss_ce_7: 0.09908  loss_mask_7: 0.1727  loss_dice_7: 0.1496  loss_ce_8: 0.09915  loss_mask_8: 0.1735  loss_dice_8: 0.1516  time: 0.5822  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 15:54:17] d2.utils.events INFO:  eta: 0:42:02  iter: 25719  total_loss: 4.82  loss_ce: 0.09904  loss_mask: 0.2295  loss_dice: 0.1369  loss_ce_0: 0.05239  loss_mask_0: 0.2264  loss_dice_0: 0.1368  loss_ce_1: 0.09938  loss_mask_1: 0.2355  loss_dice_1: 0.1333  loss_ce_2: 0.09921  loss_mask_2: 0.2332  loss_dice_2: 0.1379  loss_ce_3: 0.09915  loss_mask_3: 0.2259  loss_dice_3: 0.137  loss_ce_4: 0.09919  loss_mask_4: 0.2348  loss_dice_4: 0.1371  loss_ce_5: 0.09904  loss_mask_5: 0.2407  loss_dice_5: 0.1356  loss_ce_6: 0.09919  loss_mask_6: 0.2219  loss_dice_6: 0.1384  loss_ce_7: 0.099  loss_mask_7: 0.2314  loss_dice_7: 0.1344  loss_ce_8: 0.09912  loss_mask_8: 0.2262  loss_dice_8: 0.1335  time: 0.5823  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 15:54:30] d2.utils.events INFO:  eta: 0:41:52  iter: 25739  total_loss: 4.55  loss_ce: 0.1297  loss_mask: 0.1886  loss_dice: 0.1362  loss_ce_0: 0.06367  loss_mask_0: 0.1932  loss_dice_0: 0.1355  loss_ce_1: 0.1294  loss_mask_1: 0.1916  loss_dice_1: 0.1319  loss_ce_2: 0.1296  loss_mask_2: 0.2012  loss_dice_2: 0.1354  loss_ce_3: 0.1296  loss_mask_3: 0.183  loss_dice_3: 0.1259  loss_ce_4: 0.1297  loss_mask_4: 0.1942  loss_dice_4: 0.137  loss_ce_5: 0.1297  loss_mask_5: 0.1976  loss_dice_5: 0.1302  loss_ce_6: 0.1296  loss_mask_6: 0.1888  loss_dice_6: 0.1279  loss_ce_7: 0.1297  loss_mask_7: 0.2027  loss_dice_7: 0.1318  loss_ce_8: 0.1297  loss_mask_8: 0.19  loss_dice_8: 0.1283  time: 0.5823  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 15:54:43] d2.utils.events INFO:  eta: 0:41:41  iter: 25759  total_loss: 4.899  loss_ce: 0.09912  loss_mask: 0.2352  loss_dice: 0.1434  loss_ce_0: 0.05242  loss_mask_0: 0.2368  loss_dice_0: 0.1505  loss_ce_1: 0.09934  loss_mask_1: 0.2404  loss_dice_1: 0.1494  loss_ce_2: 0.09926  loss_mask_2: 0.2367  loss_dice_2: 0.1481  loss_ce_3: 0.09923  loss_mask_3: 0.2409  loss_dice_3: 0.1446  loss_ce_4: 0.09926  loss_mask_4: 0.2375  loss_dice_4: 0.1481  loss_ce_5: 0.09926  loss_mask_5: 0.2412  loss_dice_5: 0.1463  loss_ce_6: 0.0993  loss_mask_6: 0.2365  loss_dice_6: 0.152  loss_ce_7: 0.09915  loss_mask_7: 0.239  loss_dice_7: 0.1477  loss_ce_8: 0.09923  loss_mask_8: 0.2381  loss_dice_8: 0.1479  time: 0.5823  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 15:54:55] d2.utils.events INFO:  eta: 0:41:30  iter: 25779  total_loss: 4.569  loss_ce: 0.1297  loss_mask: 0.2083  loss_dice: 0.1356  loss_ce_0: 0.06367  loss_mask_0: 0.2111  loss_dice_0: 0.1362  loss_ce_1: 0.1293  loss_mask_1: 0.2003  loss_dice_1: 0.132  loss_ce_2: 0.1295  loss_mask_2: 0.2073  loss_dice_2: 0.1383  loss_ce_3: 0.1295  loss_mask_3: 0.2086  loss_dice_3: 0.1324  loss_ce_4: 0.1294  loss_mask_4: 0.2071  loss_dice_4: 0.1311  loss_ce_5: 0.1297  loss_mask_5: 0.2115  loss_dice_5: 0.1381  loss_ce_6: 0.1295  loss_mask_6: 0.2006  loss_dice_6: 0.1335  loss_ce_7: 0.1296  loss_mask_7: 0.2103  loss_dice_7: 0.1353  loss_ce_8: 0.1296  loss_mask_8: 0.2072  loss_dice_8: 0.1333  time: 0.5824  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 15:55:08] d2.utils.events INFO:  eta: 0:41:18  iter: 25799  total_loss: 4.502  loss_ce: 0.09897  loss_mask: 0.1958  loss_dice: 0.1362  loss_ce_0: 0.0524  loss_mask_0: 0.2097  loss_dice_0: 0.1402  loss_ce_1: 0.09919  loss_mask_1: 0.1931  loss_dice_1: 0.1401  loss_ce_2: 0.09908  loss_mask_2: 0.1952  loss_dice_2: 0.1383  loss_ce_3: 0.09908  loss_mask_3: 0.1986  loss_dice_3: 0.1407  loss_ce_4: 0.09911  loss_mask_4: 0.1963  loss_dice_4: 0.1435  loss_ce_5: 0.09896  loss_mask_5: 0.1958  loss_dice_5: 0.1366  loss_ce_6: 0.09908  loss_mask_6: 0.1908  loss_dice_6: 0.1384  loss_ce_7: 0.09897  loss_mask_7: 0.2068  loss_dice_7: 0.1413  loss_ce_8: 0.09908  loss_mask_8: 0.1932  loss_dice_8: 0.1364  time: 0.5824  data_time: 0.0025  lr: 1e-06  max_mem: 2811M
[07/11 15:55:21] d2.utils.events INFO:  eta: 0:41:08  iter: 25819  total_loss: 4.905  loss_ce: 0.09889  loss_mask: 0.1994  loss_dice: 0.1669  loss_ce_0: 0.05238  loss_mask_0: 0.1795  loss_dice_0: 0.1622  loss_ce_1: 0.0988  loss_mask_1: 0.1928  loss_dice_1: 0.1663  loss_ce_2: 0.09885  loss_mask_2: 0.1913  loss_dice_2: 0.162  loss_ce_3: 0.09893  loss_mask_3: 0.1842  loss_dice_3: 0.1617  loss_ce_4: 0.09883  loss_mask_4: 0.1904  loss_dice_4: 0.1572  loss_ce_5: 0.09882  loss_mask_5: 0.1809  loss_dice_5: 0.1623  loss_ce_6: 0.09893  loss_mask_6: 0.1876  loss_dice_6: 0.1716  loss_ce_7: 0.09882  loss_mask_7: 0.1949  loss_dice_7: 0.1672  loss_ce_8: 0.09893  loss_mask_8: 0.1966  loss_dice_8: 0.1718  time: 0.5825  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 15:55:33] d2.utils.events INFO:  eta: 0:40:57  iter: 25839  total_loss: 4.546  loss_ce: 0.099  loss_mask: 0.2134  loss_dice: 0.133  loss_ce_0: 0.05234  loss_mask_0: 0.2089  loss_dice_0: 0.1307  loss_ce_1: 0.0999  loss_mask_1: 0.2144  loss_dice_1: 0.1341  loss_ce_2: 0.09939  loss_mask_2: 0.2016  loss_dice_2: 0.1313  loss_ce_3: 0.09926  loss_mask_3: 0.2148  loss_dice_3: 0.1305  loss_ce_4: 0.09926  loss_mask_4: 0.2046  loss_dice_4: 0.134  loss_ce_5: 0.09911  loss_mask_5: 0.2127  loss_dice_5: 0.1357  loss_ce_6: 0.09926  loss_mask_6: 0.2054  loss_dice_6: 0.1347  loss_ce_7: 0.099  loss_mask_7: 0.2063  loss_dice_7: 0.134  loss_ce_8: 0.09919  loss_mask_8: 0.215  loss_dice_8: 0.1329  time: 0.5825  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 15:55:46] d2.utils.events INFO:  eta: 0:40:46  iter: 25859  total_loss: 4.403  loss_ce: 0.1607  loss_mask: 0.1932  loss_dice: 0.1266  loss_ce_0: 0.07503  loss_mask_0: 0.1919  loss_dice_0: 0.1216  loss_ce_1: 0.1601  loss_mask_1: 0.1914  loss_dice_1: 0.1336  loss_ce_2: 0.1605  loss_mask_2: 0.1869  loss_dice_2: 0.1227  loss_ce_3: 0.1606  loss_mask_3: 0.1882  loss_dice_3: 0.1286  loss_ce_4: 0.1605  loss_mask_4: 0.1886  loss_dice_4: 0.1249  loss_ce_5: 0.1606  loss_mask_5: 0.1983  loss_dice_5: 0.1277  loss_ce_6: 0.1606  loss_mask_6: 0.195  loss_dice_6: 0.1328  loss_ce_7: 0.1608  loss_mask_7: 0.1895  loss_dice_7: 0.1256  loss_ce_8: 0.1604  loss_mask_8: 0.1862  loss_dice_8: 0.124  time: 0.5825  data_time: 0.0024  lr: 1e-06  max_mem: 2811M
[07/11 15:55:59] d2.utils.events INFO:  eta: 0:40:38  iter: 25879  total_loss: 4.437  loss_ce: 0.1603  loss_mask: 0.1741  loss_dice: 0.137  loss_ce_0: 0.07497  loss_mask_0: 0.1784  loss_dice_0: 0.1454  loss_ce_1: 0.1593  loss_mask_1: 0.1786  loss_dice_1: 0.1438  loss_ce_2: 0.16  loss_mask_2: 0.1754  loss_dice_2: 0.1405  loss_ce_3: 0.1602  loss_mask_3: 0.1753  loss_dice_3: 0.1439  loss_ce_4: 0.16  loss_mask_4: 0.1738  loss_dice_4: 0.1459  loss_ce_5: 0.1603  loss_mask_5: 0.1782  loss_dice_5: 0.1493  loss_ce_6: 0.16  loss_mask_6: 0.1729  loss_dice_6: 0.1391  loss_ce_7: 0.1604  loss_mask_7: 0.1804  loss_dice_7: 0.1391  loss_ce_8: 0.1602  loss_mask_8: 0.1782  loss_dice_8: 0.1484  time: 0.5826  data_time: 0.0023  lr: 1e-06  max_mem: 2811M
[07/11 15:56:12] d2.utils.events INFO:  eta: 0:40:26  iter: 25899  total_loss: 4.735  loss_ce: 0.1603  loss_mask: 0.1806  loss_dice: 0.1467  loss_ce_0: 0.07493  loss_mask_0: 0.1765  loss_dice_0: 0.1472  loss_ce_1: 0.1598  loss_mask_1: 0.1822  loss_dice_1: 0.1446  loss_ce_2: 0.1601  loss_mask_2: 0.1805  loss_dice_2: 0.1484  loss_ce_3: 0.1602  loss_mask_3: 0.1755  loss_dice_3: 0.1497  loss_ce_4: 0.1601  loss_mask_4: 0.1817  loss_dice_4: 0.1474  loss_ce_5: 0.1602  loss_mask_5: 0.1794  loss_dice_5: 0.1443  loss_ce_6: 0.1601  loss_mask_6: 0.175  loss_dice_6: 0.1411  loss_ce_7: 0.1603  loss_mask_7: 0.1823  loss_dice_7: 0.149  loss_ce_8: 0.1602  loss_mask_8: 0.1785  loss_dice_8: 0.1485  time: 0.5826  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 15:56:24] d2.utils.events INFO:  eta: 0:40:13  iter: 25919  total_loss: 4.476  loss_ce: 0.09927  loss_mask: 0.2051  loss_dice: 0.1337  loss_ce_0: 0.05243  loss_mask_0: 0.2046  loss_dice_0: 0.1337  loss_ce_1: 0.09953  loss_mask_1: 0.2025  loss_dice_1: 0.1351  loss_ce_2: 0.09934  loss_mask_2: 0.2093  loss_dice_2: 0.131  loss_ce_3: 0.09937  loss_mask_3: 0.2093  loss_dice_3: 0.1356  loss_ce_4: 0.09938  loss_mask_4: 0.2072  loss_dice_4: 0.1307  loss_ce_5: 0.09941  loss_mask_5: 0.2099  loss_dice_5: 0.1322  loss_ce_6: 0.0993  loss_mask_6: 0.2078  loss_dice_6: 0.1368  loss_ce_7: 0.0993  loss_mask_7: 0.2018  loss_dice_7: 0.1325  loss_ce_8: 0.09934  loss_mask_8: 0.2028  loss_dice_8: 0.1264  time: 0.5827  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 15:56:37] d2.utils.events INFO:  eta: 0:40:02  iter: 25939  total_loss: 4.597  loss_ce: 0.09934  loss_mask: 0.2053  loss_dice: 0.1266  loss_ce_0: 0.05243  loss_mask_0: 0.2154  loss_dice_0: 0.1281  loss_ce_1: 0.09985  loss_mask_1: 0.2049  loss_dice_1: 0.1232  loss_ce_2: 0.09958  loss_mask_2: 0.2028  loss_dice_2: 0.1261  loss_ce_3: 0.09952  loss_mask_3: 0.2087  loss_dice_3: 0.1278  loss_ce_4: 0.09964  loss_mask_4: 0.2011  loss_dice_4: 0.1205  loss_ce_5: 0.09941  loss_mask_5: 0.2155  loss_dice_5: 0.1305  loss_ce_6: 0.0996  loss_mask_6: 0.2146  loss_dice_6: 0.1331  loss_ce_7: 0.09934  loss_mask_7: 0.2083  loss_dice_7: 0.1251  loss_ce_8: 0.09949  loss_mask_8: 0.207  loss_dice_8: 0.1237  time: 0.5827  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 15:56:49] d2.utils.events INFO:  eta: 0:39:56  iter: 25959  total_loss: 4.341  loss_ce: 0.1298  loss_mask: 0.1683  loss_dice: 0.1241  loss_ce_0: 0.06367  loss_mask_0: 0.1715  loss_dice_0: 0.1245  loss_ce_1: 0.1298  loss_mask_1: 0.1703  loss_dice_1: 0.126  loss_ce_2: 0.1298  loss_mask_2: 0.1813  loss_dice_2: 0.1304  loss_ce_3: 0.1298  loss_mask_3: 0.1691  loss_dice_3: 0.1268  loss_ce_4: 0.1298  loss_mask_4: 0.1677  loss_dice_4: 0.1309  loss_ce_5: 0.1298  loss_mask_5: 0.1681  loss_dice_5: 0.1274  loss_ce_6: 0.1298  loss_mask_6: 0.1671  loss_dice_6: 0.1244  loss_ce_7: 0.1298  loss_mask_7: 0.1662  loss_dice_7: 0.1254  loss_ce_8: 0.1298  loss_mask_8: 0.1697  loss_dice_8: 0.1263  time: 0.5827  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:57:02] d2.utils.events INFO:  eta: 0:39:50  iter: 25979  total_loss: 4.595  loss_ce: 0.0993  loss_mask: 0.1883  loss_dice: 0.1466  loss_ce_0: 0.05243  loss_mask_0: 0.1886  loss_dice_0: 0.1473  loss_ce_1: 0.09973  loss_mask_1: 0.1889  loss_dice_1: 0.1566  loss_ce_2: 0.09953  loss_mask_2: 0.1892  loss_dice_2: 0.155  loss_ce_3: 0.09952  loss_mask_3: 0.1801  loss_dice_3: 0.1524  loss_ce_4: 0.09956  loss_mask_4: 0.1958  loss_dice_4: 0.1493  loss_ce_5: 0.09941  loss_mask_5: 0.1876  loss_dice_5: 0.1516  loss_ce_6: 0.0996  loss_mask_6: 0.1887  loss_dice_6: 0.1528  loss_ce_7: 0.09941  loss_mask_7: 0.1889  loss_dice_7: 0.1547  loss_ce_8: 0.09945  loss_mask_8: 0.1925  loss_dice_8: 0.1519  time: 0.5828  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:57:15] d2.utils.events INFO:  eta: 0:39:41  iter: 25999  total_loss: 4.714  loss_ce: 0.09919  loss_mask: 0.1635  loss_dice: 0.1518  loss_ce_0: 0.05243  loss_mask_0: 0.1584  loss_dice_0: 0.1464  loss_ce_1: 0.09949  loss_mask_1: 0.1642  loss_dice_1: 0.1446  loss_ce_2: 0.09928  loss_mask_2: 0.1686  loss_dice_2: 0.1485  loss_ce_3: 0.09934  loss_mask_3: 0.1698  loss_dice_3: 0.1504  loss_ce_4: 0.09934  loss_mask_4: 0.1691  loss_dice_4: 0.1472  loss_ce_5: 0.09926  loss_mask_5: 0.171  loss_dice_5: 0.1513  loss_ce_6: 0.09934  loss_mask_6: 0.1678  loss_dice_6: 0.1497  loss_ce_7: 0.09923  loss_mask_7: 0.1629  loss_dice_7: 0.1474  loss_ce_8: 0.09926  loss_mask_8: 0.1661  loss_dice_8: 0.1432  time: 0.5828  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 15:57:27] d2.utils.events INFO:  eta: 0:39:34  iter: 26019  total_loss: 4.523  loss_ce: 0.1296  loss_mask: 0.204  loss_dice: 0.1203  loss_ce_0: 0.06367  loss_mask_0: 0.2008  loss_dice_0: 0.1174  loss_ce_1: 0.1296  loss_mask_1: 0.2015  loss_dice_1: 0.1223  loss_ce_2: 0.1296  loss_mask_2: 0.2007  loss_dice_2: 0.1189  loss_ce_3: 0.1295  loss_mask_3: 0.2045  loss_dice_3: 0.1225  loss_ce_4: 0.1296  loss_mask_4: 0.2001  loss_dice_4: 0.1228  loss_ce_5: 0.1295  loss_mask_5: 0.2018  loss_dice_5: 0.1192  loss_ce_6: 0.1296  loss_mask_6: 0.2043  loss_dice_6: 0.1167  loss_ce_7: 0.1297  loss_mask_7: 0.2006  loss_dice_7: 0.1193  loss_ce_8: 0.1297  loss_mask_8: 0.199  loss_dice_8: 0.1185  time: 0.5828  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:57:40] d2.utils.events INFO:  eta: 0:39:29  iter: 26039  total_loss: 4.552  loss_ce: 0.09904  loss_mask: 0.186  loss_dice: 0.1699  loss_ce_0: 0.05242  loss_mask_0: 0.1836  loss_dice_0: 0.1702  loss_ce_1: 0.09904  loss_mask_1: 0.1762  loss_dice_1: 0.1661  loss_ce_2: 0.09906  loss_mask_2: 0.18  loss_dice_2: 0.1708  loss_ce_3: 0.09911  loss_mask_3: 0.1902  loss_dice_3: 0.172  loss_ce_4: 0.09915  loss_mask_4: 0.1875  loss_dice_4: 0.1646  loss_ce_5: 0.09912  loss_mask_5: 0.176  loss_dice_5: 0.1717  loss_ce_6: 0.09908  loss_mask_6: 0.1794  loss_dice_6: 0.1669  loss_ce_7: 0.09911  loss_mask_7: 0.1894  loss_dice_7: 0.1706  loss_ce_8: 0.09912  loss_mask_8: 0.1859  loss_dice_8: 0.1651  time: 0.5829  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:57:53] d2.utils.events INFO:  eta: 0:39:23  iter: 26059  total_loss: 4.426  loss_ce: 0.09911  loss_mask: 0.172  loss_dice: 0.1271  loss_ce_0: 0.0524  loss_mask_0: 0.1674  loss_dice_0: 0.1256  loss_ce_1: 0.09964  loss_mask_1: 0.1619  loss_dice_1: 0.1218  loss_ce_2: 0.09936  loss_mask_2: 0.164  loss_dice_2: 0.1245  loss_ce_3: 0.09941  loss_mask_3: 0.1622  loss_dice_3: 0.1258  loss_ce_4: 0.09941  loss_mask_4: 0.1626  loss_dice_4: 0.1257  loss_ce_5: 0.09919  loss_mask_5: 0.1681  loss_dice_5: 0.1286  loss_ce_6: 0.09945  loss_mask_6: 0.1593  loss_dice_6: 0.1263  loss_ce_7: 0.09919  loss_mask_7: 0.1661  loss_dice_7: 0.1241  loss_ce_8: 0.0993  loss_mask_8: 0.1702  loss_dice_8: 0.1251  time: 0.5829  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:58:05] d2.utils.events INFO:  eta: 0:39:19  iter: 26079  total_loss: 4.529  loss_ce: 0.09915  loss_mask: 0.2042  loss_dice: 0.1337  loss_ce_0: 0.05243  loss_mask_0: 0.198  loss_dice_0: 0.1329  loss_ce_1: 0.09971  loss_mask_1: 0.2022  loss_dice_1: 0.1371  loss_ce_2: 0.09939  loss_mask_2: 0.1925  loss_dice_2: 0.1325  loss_ce_3: 0.0993  loss_mask_3: 0.197  loss_dice_3: 0.1385  loss_ce_4: 0.09945  loss_mask_4: 0.2002  loss_dice_4: 0.1252  loss_ce_5: 0.09926  loss_mask_5: 0.1983  loss_dice_5: 0.1307  loss_ce_6: 0.09934  loss_mask_6: 0.1947  loss_dice_6: 0.1333  loss_ce_7: 0.09911  loss_mask_7: 0.1986  loss_dice_7: 0.1303  loss_ce_8: 0.0993  loss_mask_8: 0.1973  loss_dice_8: 0.1303  time: 0.5829  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:58:18] d2.utils.events INFO:  eta: 0:39:16  iter: 26099  total_loss: 4.782  loss_ce: 0.09904  loss_mask: 0.2106  loss_dice: 0.1444  loss_ce_0: 0.05243  loss_mask_0: 0.2058  loss_dice_0: 0.1446  loss_ce_1: 0.09932  loss_mask_1: 0.2018  loss_dice_1: 0.1396  loss_ce_2: 0.09924  loss_mask_2: 0.2055  loss_dice_2: 0.1449  loss_ce_3: 0.09922  loss_mask_3: 0.2031  loss_dice_3: 0.1409  loss_ce_4: 0.09923  loss_mask_4: 0.2068  loss_dice_4: 0.1402  loss_ce_5: 0.09911  loss_mask_5: 0.2024  loss_dice_5: 0.1447  loss_ce_6: 0.09926  loss_mask_6: 0.2107  loss_dice_6: 0.1429  loss_ce_7: 0.09904  loss_mask_7: 0.199  loss_dice_7: 0.1393  loss_ce_8: 0.09912  loss_mask_8: 0.2136  loss_dice_8: 0.1446  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:58:30] d2.utils.events INFO:  eta: 0:39:11  iter: 26119  total_loss: 4.542  loss_ce: 0.09904  loss_mask: 0.2192  loss_dice: 0.1489  loss_ce_0: 0.05241  loss_mask_0: 0.2125  loss_dice_0: 0.1514  loss_ce_1: 0.09919  loss_mask_1: 0.214  loss_dice_1: 0.1494  loss_ce_2: 0.0991  loss_mask_2: 0.2231  loss_dice_2: 0.1518  loss_ce_3: 0.09904  loss_mask_3: 0.2122  loss_dice_3: 0.1527  loss_ce_4: 0.09923  loss_mask_4: 0.2143  loss_dice_4: 0.1461  loss_ce_5: 0.09904  loss_mask_5: 0.2143  loss_dice_5: 0.1536  loss_ce_6: 0.09911  loss_mask_6: 0.2158  loss_dice_6: 0.1504  loss_ce_7: 0.09904  loss_mask_7: 0.2156  loss_dice_7: 0.1524  loss_ce_8: 0.09915  loss_mask_8: 0.219  loss_dice_8: 0.153  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:58:43] d2.utils.events INFO:  eta: 0:39:04  iter: 26139  total_loss: 4.534  loss_ce: 0.09893  loss_mask: 0.1847  loss_dice: 0.134  loss_ce_0: 0.05241  loss_mask_0: 0.1783  loss_dice_0: 0.13  loss_ce_1: 0.09915  loss_mask_1: 0.1797  loss_dice_1: 0.1338  loss_ce_2: 0.09908  loss_mask_2: 0.182  loss_dice_2: 0.132  loss_ce_3: 0.099  loss_mask_3: 0.1802  loss_dice_3: 0.1338  loss_ce_4: 0.09915  loss_mask_4: 0.1806  loss_dice_4: 0.1346  loss_ce_5: 0.09904  loss_mask_5: 0.1794  loss_dice_5: 0.1354  loss_ce_6: 0.09904  loss_mask_6: 0.1901  loss_dice_6: 0.136  loss_ce_7: 0.099  loss_mask_7: 0.1849  loss_dice_7: 0.1328  loss_ce_8: 0.09912  loss_mask_8: 0.1888  loss_dice_8: 0.1374  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:58:56] d2.utils.events INFO:  eta: 0:38:57  iter: 26159  total_loss: 4.412  loss_ce: 0.09897  loss_mask: 0.195  loss_dice: 0.1279  loss_ce_0: 0.05239  loss_mask_0: 0.1901  loss_dice_0: 0.1265  loss_ce_1: 0.09934  loss_mask_1: 0.1893  loss_dice_1: 0.1347  loss_ce_2: 0.09908  loss_mask_2: 0.1882  loss_dice_2: 0.1279  loss_ce_3: 0.099  loss_mask_3: 0.1894  loss_dice_3: 0.1343  loss_ce_4: 0.09911  loss_mask_4: 0.1872  loss_dice_4: 0.1286  loss_ce_5: 0.09897  loss_mask_5: 0.19  loss_dice_5: 0.1311  loss_ce_6: 0.09908  loss_mask_6: 0.1925  loss_dice_6: 0.1308  loss_ce_7: 0.09896  loss_mask_7: 0.1919  loss_dice_7: 0.1294  loss_ce_8: 0.09912  loss_mask_8: 0.1916  loss_dice_8: 0.1236  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:59:08] d2.utils.events INFO:  eta: 0:38:49  iter: 26179  total_loss: 4.655  loss_ce: 0.09908  loss_mask: 0.2053  loss_dice: 0.1337  loss_ce_0: 0.05239  loss_mask_0: 0.2165  loss_dice_0: 0.1263  loss_ce_1: 0.09942  loss_mask_1: 0.201  loss_dice_1: 0.13  loss_ce_2: 0.09917  loss_mask_2: 0.207  loss_dice_2: 0.1341  loss_ce_3: 0.09911  loss_mask_3: 0.2055  loss_dice_3: 0.1313  loss_ce_4: 0.0993  loss_mask_4: 0.2064  loss_dice_4: 0.134  loss_ce_5: 0.09919  loss_mask_5: 0.2127  loss_dice_5: 0.1325  loss_ce_6: 0.09915  loss_mask_6: 0.2023  loss_dice_6: 0.1298  loss_ce_7: 0.09908  loss_mask_7: 0.2082  loss_dice_7: 0.1328  loss_ce_8: 0.09915  loss_mask_8: 0.2085  loss_dice_8: 0.1316  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:59:21] d2.utils.events INFO:  eta: 0:38:40  iter: 26199  total_loss: 4.954  loss_ce: 0.099  loss_mask: 0.1529  loss_dice: 0.1811  loss_ce_0: 0.05237  loss_mask_0: 0.1612  loss_dice_0: 0.1846  loss_ce_1: 0.09936  loss_mask_1: 0.1669  loss_dice_1: 0.1886  loss_ce_2: 0.09911  loss_mask_2: 0.1565  loss_dice_2: 0.1779  loss_ce_3: 0.09904  loss_mask_3: 0.1502  loss_dice_3: 0.1744  loss_ce_4: 0.09911  loss_mask_4: 0.1515  loss_dice_4: 0.1742  loss_ce_5: 0.09911  loss_mask_5: 0.1635  loss_dice_5: 0.1859  loss_ce_6: 0.09911  loss_mask_6: 0.1557  loss_dice_6: 0.1842  loss_ce_7: 0.099  loss_mask_7: 0.1608  loss_dice_7: 0.1773  loss_ce_8: 0.09915  loss_mask_8: 0.1656  loss_dice_8: 0.1932  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:59:32] d2.utils.events INFO:  eta: 0:38:28  iter: 26219  total_loss: 4.595  loss_ce: 0.09871  loss_mask: 0.2029  loss_dice: 0.1308  loss_ce_0: 0.05234  loss_mask_0: 0.1995  loss_dice_0: 0.1266  loss_ce_1: 0.09887  loss_mask_1: 0.2004  loss_dice_1: 0.1294  loss_ce_2: 0.09878  loss_mask_2: 0.2056  loss_dice_2: 0.1276  loss_ce_3: 0.09881  loss_mask_3: 0.2054  loss_dice_3: 0.1327  loss_ce_4: 0.09881  loss_mask_4: 0.203  loss_dice_4: 0.1311  loss_ce_5: 0.09874  loss_mask_5: 0.1987  loss_dice_5: 0.1325  loss_ce_6: 0.09881  loss_mask_6: 0.212  loss_dice_6: 0.1299  loss_ce_7: 0.0987  loss_mask_7: 0.2099  loss_dice_7: 0.1343  loss_ce_8: 0.09882  loss_mask_8: 0.2028  loss_dice_8: 0.1322  time: 0.5831  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 15:59:44] d2.utils.events INFO:  eta: 0:38:16  iter: 26239  total_loss: 4.444  loss_ce: 0.09859  loss_mask: 0.193  loss_dice: 0.1346  loss_ce_0: 0.05233  loss_mask_0: 0.1941  loss_dice_0: 0.1318  loss_ce_1: 0.09867  loss_mask_1: 0.1865  loss_dice_1: 0.1381  loss_ce_2: 0.09863  loss_mask_2: 0.1886  loss_dice_2: 0.1376  loss_ce_3: 0.09866  loss_mask_3: 0.191  loss_dice_3: 0.13  loss_ce_4: 0.09867  loss_mask_4: 0.1948  loss_dice_4: 0.1308  loss_ce_5: 0.09859  loss_mask_5: 0.1915  loss_dice_5: 0.1325  loss_ce_6: 0.0987  loss_mask_6: 0.1864  loss_dice_6: 0.1337  loss_ce_7: 0.09859  loss_mask_7: 0.1866  loss_dice_7: 0.1296  loss_ce_8: 0.09867  loss_mask_8: 0.1919  loss_dice_8: 0.1379  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 15:59:55] d2.utils.events INFO:  eta: 0:38:04  iter: 26259  total_loss: 4.279  loss_ce: 0.09859  loss_mask: 0.1812  loss_dice: 0.1453  loss_ce_0: 0.05231  loss_mask_0: 0.1755  loss_dice_0: 0.1422  loss_ce_1: 0.09882  loss_mask_1: 0.179  loss_dice_1: 0.1337  loss_ce_2: 0.09869  loss_mask_2: 0.176  loss_dice_2: 0.1363  loss_ce_3: 0.09859  loss_mask_3: 0.1811  loss_dice_3: 0.1384  loss_ce_4: 0.09866  loss_mask_4: 0.1679  loss_dice_4: 0.1412  loss_ce_5: 0.09867  loss_mask_5: 0.1832  loss_dice_5: 0.1422  loss_ce_6: 0.09863  loss_mask_6: 0.1753  loss_dice_6: 0.1378  loss_ce_7: 0.09859  loss_mask_7: 0.1738  loss_dice_7: 0.1412  loss_ce_8: 0.09871  loss_mask_8: 0.1826  loss_dice_8: 0.1359  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:00:07] d2.utils.events INFO:  eta: 0:37:51  iter: 26279  total_loss: 4.509  loss_ce: 0.1299  loss_mask: 0.1853  loss_dice: 0.1454  loss_ce_0: 0.06369  loss_mask_0: 0.1859  loss_dice_0: 0.1483  loss_ce_1: 0.1299  loss_mask_1: 0.1912  loss_dice_1: 0.15  loss_ce_2: 0.1299  loss_mask_2: 0.1797  loss_dice_2: 0.1457  loss_ce_3: 0.1299  loss_mask_3: 0.1809  loss_dice_3: 0.1419  loss_ce_4: 0.1299  loss_mask_4: 0.1893  loss_dice_4: 0.1502  loss_ce_5: 0.1298  loss_mask_5: 0.1878  loss_dice_5: 0.1467  loss_ce_6: 0.13  loss_mask_6: 0.1809  loss_dice_6: 0.1471  loss_ce_7: 0.1299  loss_mask_7: 0.1958  loss_dice_7: 0.1517  loss_ce_8: 0.1299  loss_mask_8: 0.1845  loss_dice_8: 0.1439  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:00:18] d2.utils.events INFO:  eta: 0:37:39  iter: 26299  total_loss: 4.461  loss_ce: 0.09841  loss_mask: 0.174  loss_dice: 0.1497  loss_ce_0: 0.05225  loss_mask_0: 0.1729  loss_dice_0: 0.1504  loss_ce_1: 0.09835  loss_mask_1: 0.1693  loss_dice_1: 0.1465  loss_ce_2: 0.09833  loss_mask_2: 0.1745  loss_dice_2: 0.1507  loss_ce_3: 0.09848  loss_mask_3: 0.1869  loss_dice_3: 0.1488  loss_ce_4: 0.09848  loss_mask_4: 0.1748  loss_dice_4: 0.1496  loss_ce_5: 0.09829  loss_mask_5: 0.1708  loss_dice_5: 0.1504  loss_ce_6: 0.09837  loss_mask_6: 0.1798  loss_dice_6: 0.1444  loss_ce_7: 0.09837  loss_mask_7: 0.1707  loss_dice_7: 0.153  loss_ce_8: 0.09837  loss_mask_8: 0.1696  loss_dice_8: 0.149  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:00:30] d2.utils.events INFO:  eta: 0:37:27  iter: 26319  total_loss: 4.408  loss_ce: 0.1614  loss_mask: 0.18  loss_dice: 0.1311  loss_ce_0: 0.07516  loss_mask_0: 0.177  loss_dice_0: 0.1338  loss_ce_1: 0.1609  loss_mask_1: 0.1749  loss_dice_1: 0.1333  loss_ce_2: 0.161  loss_mask_2: 0.1747  loss_dice_2: 0.1274  loss_ce_3: 0.1613  loss_mask_3: 0.1775  loss_dice_3: 0.1321  loss_ce_4: 0.1611  loss_mask_4: 0.1686  loss_dice_4: 0.1331  loss_ce_5: 0.1613  loss_mask_5: 0.1771  loss_dice_5: 0.1302  loss_ce_6: 0.1612  loss_mask_6: 0.1752  loss_dice_6: 0.1335  loss_ce_7: 0.1614  loss_mask_7: 0.1666  loss_dice_7: 0.1314  loss_ce_8: 0.1612  loss_mask_8: 0.1794  loss_dice_8: 0.1329  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:00:41] d2.utils.events INFO:  eta: 0:37:14  iter: 26339  total_loss: 4.516  loss_ce: 0.09837  loss_mask: 0.2198  loss_dice: 0.1313  loss_ce_0: 0.05223  loss_mask_0: 0.2212  loss_dice_0: 0.128  loss_ce_1: 0.09852  loss_mask_1: 0.2214  loss_dice_1: 0.128  loss_ce_2: 0.09842  loss_mask_2: 0.2122  loss_dice_2: 0.1286  loss_ce_3: 0.09829  loss_mask_3: 0.2224  loss_dice_3: 0.1349  loss_ce_4: 0.09844  loss_mask_4: 0.2212  loss_dice_4: 0.1339  loss_ce_5: 0.09822  loss_mask_5: 0.2136  loss_dice_5: 0.1287  loss_ce_6: 0.09837  loss_mask_6: 0.2203  loss_dice_6: 0.1274  loss_ce_7: 0.09833  loss_mask_7: 0.2217  loss_dice_7: 0.1305  loss_ce_8: 0.09844  loss_mask_8: 0.2238  loss_dice_8: 0.1293  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:00:53] d2.utils.events INFO:  eta: 0:37:02  iter: 26359  total_loss: 4.548  loss_ce: 0.09815  loss_mask: 0.2043  loss_dice: 0.1504  loss_ce_0: 0.05221  loss_mask_0: 0.2032  loss_dice_0: 0.1521  loss_ce_1: 0.09805  loss_mask_1: 0.1937  loss_dice_1: 0.1489  loss_ce_2: 0.098  loss_mask_2: 0.2039  loss_dice_2: 0.1531  loss_ce_3: 0.09803  loss_mask_3: 0.1982  loss_dice_3: 0.1509  loss_ce_4: 0.09807  loss_mask_4: 0.2046  loss_dice_4: 0.1499  loss_ce_5: 0.098  loss_mask_5: 0.1938  loss_dice_5: 0.1429  loss_ce_6: 0.098  loss_mask_6: 0.2029  loss_dice_6: 0.1508  loss_ce_7: 0.09811  loss_mask_7: 0.2095  loss_dice_7: 0.1469  loss_ce_8: 0.09807  loss_mask_8: 0.1948  loss_dice_8: 0.1473  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:01:04] d2.utils.events INFO:  eta: 0:36:50  iter: 26379  total_loss: 4.674  loss_ce: 0.09807  loss_mask: 0.197  loss_dice: 0.1478  loss_ce_0: 0.05218  loss_mask_0: 0.1996  loss_dice_0: 0.1472  loss_ce_1: 0.09807  loss_mask_1: 0.2061  loss_dice_1: 0.1384  loss_ce_2: 0.09792  loss_mask_2: 0.1963  loss_dice_2: 0.1439  loss_ce_3: 0.09792  loss_mask_3: 0.1966  loss_dice_3: 0.1454  loss_ce_4: 0.09803  loss_mask_4: 0.1974  loss_dice_4: 0.1435  loss_ce_5: 0.098  loss_mask_5: 0.1973  loss_dice_5: 0.1439  loss_ce_6: 0.098  loss_mask_6: 0.2015  loss_dice_6: 0.1436  loss_ce_7: 0.098  loss_mask_7: 0.1975  loss_dice_7: 0.1422  loss_ce_8: 0.09811  loss_mask_8: 0.2025  loss_dice_8: 0.143  time: 0.5831  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:01:15] d2.utils.events INFO:  eta: 0:36:37  iter: 26399  total_loss: 4.569  loss_ce: 0.09785  loss_mask: 0.1693  loss_dice: 0.1606  loss_ce_0: 0.05215  loss_mask_0: 0.1638  loss_dice_0: 0.1578  loss_ce_1: 0.09746  loss_mask_1: 0.1719  loss_dice_1: 0.1537  loss_ce_2: 0.09779  loss_mask_2: 0.1664  loss_dice_2: 0.1581  loss_ce_3: 0.09785  loss_mask_3: 0.1576  loss_dice_3: 0.1467  loss_ce_4: 0.09773  loss_mask_4: 0.1706  loss_dice_4: 0.1587  loss_ce_5: 0.09777  loss_mask_5: 0.1671  loss_dice_5: 0.157  loss_ce_6: 0.09781  loss_mask_6: 0.1662  loss_dice_6: 0.1595  loss_ce_7: 0.09775  loss_mask_7: 0.1728  loss_dice_7: 0.1622  loss_ce_8: 0.09792  loss_mask_8: 0.171  loss_dice_8: 0.1554  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:01:27] d2.utils.events INFO:  eta: 0:36:25  iter: 26419  total_loss: 4.403  loss_ce: 0.1299  loss_mask: 0.1986  loss_dice: 0.1466  loss_ce_0: 0.06372  loss_mask_0: 0.1924  loss_dice_0: 0.1501  loss_ce_1: 0.1299  loss_mask_1: 0.1936  loss_dice_1: 0.1494  loss_ce_2: 0.13  loss_mask_2: 0.1914  loss_dice_2: 0.1475  loss_ce_3: 0.13  loss_mask_3: 0.193  loss_dice_3: 0.1515  loss_ce_4: 0.1299  loss_mask_4: 0.1975  loss_dice_4: 0.1484  loss_ce_5: 0.13  loss_mask_5: 0.1907  loss_dice_5: 0.1441  loss_ce_6: 0.13  loss_mask_6: 0.1957  loss_dice_6: 0.1469  loss_ce_7: 0.13  loss_mask_7: 0.1907  loss_dice_7: 0.1469  loss_ce_8: 0.1299  loss_mask_8: 0.1908  loss_dice_8: 0.1474  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:01:38] d2.utils.events INFO:  eta: 0:36:11  iter: 26439  total_loss: 4.797  loss_ce: 0.09781  loss_mask: 0.1465  loss_dice: 0.1775  loss_ce_0: 0.05212  loss_mask_0: 0.151  loss_dice_0: 0.1812  loss_ce_1: 0.09765  loss_mask_1: 0.1423  loss_dice_1: 0.1812  loss_ce_2: 0.09759  loss_mask_2: 0.1393  loss_dice_2: 0.1718  loss_ce_3: 0.0977  loss_mask_3: 0.1434  loss_dice_3: 0.1788  loss_ce_4: 0.09777  loss_mask_4: 0.1397  loss_dice_4: 0.1727  loss_ce_5: 0.09755  loss_mask_5: 0.1525  loss_dice_5: 0.1811  loss_ce_6: 0.0977  loss_mask_6: 0.1488  loss_dice_6: 0.1739  loss_ce_7: 0.0977  loss_mask_7: 0.1558  loss_dice_7: 0.182  loss_ce_8: 0.09785  loss_mask_8: 0.1418  loss_dice_8: 0.1753  time: 0.5831  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:01:50] d2.utils.events INFO:  eta: 0:35:57  iter: 26459  total_loss: 4.773  loss_ce: 0.09807  loss_mask: 0.1821  loss_dice: 0.1422  loss_ce_0: 0.05212  loss_mask_0: 0.1863  loss_dice_0: 0.1461  loss_ce_1: 0.09857  loss_mask_1: 0.2063  loss_dice_1: 0.1522  loss_ce_2: 0.09814  loss_mask_2: 0.196  loss_dice_2: 0.1501  loss_ce_3: 0.09807  loss_mask_3: 0.1934  loss_dice_3: 0.1467  loss_ce_4: 0.09826  loss_mask_4: 0.1813  loss_dice_4: 0.141  loss_ce_5: 0.09807  loss_mask_5: 0.1911  loss_dice_5: 0.1502  loss_ce_6: 0.09814  loss_mask_6: 0.1901  loss_dice_6: 0.1383  loss_ce_7: 0.098  loss_mask_7: 0.1958  loss_dice_7: 0.1474  loss_ce_8: 0.09811  loss_mask_8: 0.1863  loss_dice_8: 0.1443  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:02:01] d2.utils.events INFO:  eta: 0:35:44  iter: 26479  total_loss: 4.521  loss_ce: 0.13  loss_mask: 0.1656  loss_dice: 0.15  loss_ce_0: 0.06372  loss_mask_0: 0.1637  loss_dice_0: 0.1527  loss_ce_1: 0.1301  loss_mask_1: 0.1626  loss_dice_1: 0.1506  loss_ce_2: 0.1301  loss_mask_2: 0.1668  loss_dice_2: 0.1484  loss_ce_3: 0.13  loss_mask_3: 0.1661  loss_dice_3: 0.1501  loss_ce_4: 0.1301  loss_mask_4: 0.1669  loss_dice_4: 0.1529  loss_ce_5: 0.1301  loss_mask_5: 0.1609  loss_dice_5: 0.1482  loss_ce_6: 0.1301  loss_mask_6: 0.1647  loss_dice_6: 0.1522  loss_ce_7: 0.13  loss_mask_7: 0.1644  loss_dice_7: 0.1537  loss_ce_8: 0.13  loss_mask_8: 0.154  loss_dice_8: 0.1477  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:02:13] d2.utils.events INFO:  eta: 0:35:33  iter: 26499  total_loss: 4.455  loss_ce: 0.09785  loss_mask: 0.1727  loss_dice: 0.1313  loss_ce_0: 0.05215  loss_mask_0: 0.1814  loss_dice_0: 0.1414  loss_ce_1: 0.09766  loss_mask_1: 0.1818  loss_dice_1: 0.1311  loss_ce_2: 0.09779  loss_mask_2: 0.1774  loss_dice_2: 0.1394  loss_ce_3: 0.09785  loss_mask_3: 0.1838  loss_dice_3: 0.1355  loss_ce_4: 0.09779  loss_mask_4: 0.1767  loss_dice_4: 0.1388  loss_ce_5: 0.09777  loss_mask_5: 0.1772  loss_dice_5: 0.1406  loss_ce_6: 0.09792  loss_mask_6: 0.1793  loss_dice_6: 0.1388  loss_ce_7: 0.09775  loss_mask_7: 0.1814  loss_dice_7: 0.1316  loss_ce_8: 0.09789  loss_mask_8: 0.1803  loss_dice_8: 0.1381  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:02:24] d2.utils.events INFO:  eta: 0:35:20  iter: 26519  total_loss: 4.314  loss_ce: 0.098  loss_mask: 0.1879  loss_dice: 0.124  loss_ce_0: 0.05213  loss_mask_0: 0.1881  loss_dice_0: 0.1193  loss_ce_1: 0.09809  loss_mask_1: 0.1846  loss_dice_1: 0.1207  loss_ce_2: 0.098  loss_mask_2: 0.1824  loss_dice_2: 0.1227  loss_ce_3: 0.09788  loss_mask_3: 0.1779  loss_dice_3: 0.1177  loss_ce_4: 0.098  loss_mask_4: 0.189  loss_dice_4: 0.1183  loss_ce_5: 0.09785  loss_mask_5: 0.1905  loss_dice_5: 0.1203  loss_ce_6: 0.09803  loss_mask_6: 0.1843  loss_dice_6: 0.1156  loss_ce_7: 0.09789  loss_mask_7: 0.1839  loss_dice_7: 0.1216  loss_ce_8: 0.09815  loss_mask_8: 0.1852  loss_dice_8: 0.1205  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:02:36] d2.utils.events INFO:  eta: 0:35:07  iter: 26539  total_loss: 4.77  loss_ce: 0.13  loss_mask: 0.1837  loss_dice: 0.1409  loss_ce_0: 0.06372  loss_mask_0: 0.197  loss_dice_0: 0.1412  loss_ce_1: 0.1295  loss_mask_1: 0.19  loss_dice_1: 0.1404  loss_ce_2: 0.1298  loss_mask_2: 0.1924  loss_dice_2: 0.1415  loss_ce_3: 0.1298  loss_mask_3: 0.1894  loss_dice_3: 0.1466  loss_ce_4: 0.1298  loss_mask_4: 0.1935  loss_dice_4: 0.145  loss_ce_5: 0.1298  loss_mask_5: 0.192  loss_dice_5: 0.1428  loss_ce_6: 0.1298  loss_mask_6: 0.184  loss_dice_6: 0.1466  loss_ce_7: 0.13  loss_mask_7: 0.1884  loss_dice_7: 0.1438  loss_ce_8: 0.1295  loss_mask_8: 0.189  loss_dice_8: 0.147  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:02:47] d2.utils.events INFO:  eta: 0:34:54  iter: 26559  total_loss: 4.71  loss_ce: 0.0977  loss_mask: 0.1768  loss_dice: 0.1487  loss_ce_0: 0.05211  loss_mask_0: 0.1661  loss_dice_0: 0.1446  loss_ce_1: 0.09766  loss_mask_1: 0.1818  loss_dice_1: 0.1469  loss_ce_2: 0.09759  loss_mask_2: 0.1821  loss_dice_2: 0.1528  loss_ce_3: 0.09755  loss_mask_3: 0.1643  loss_dice_3: 0.1471  loss_ce_4: 0.09774  loss_mask_4: 0.1853  loss_dice_4: 0.1507  loss_ce_5: 0.09748  loss_mask_5: 0.1681  loss_dice_5: 0.1507  loss_ce_6: 0.09766  loss_mask_6: 0.174  loss_dice_6: 0.1487  loss_ce_7: 0.0977  loss_mask_7: 0.1609  loss_dice_7: 0.1442  loss_ce_8: 0.09774  loss_mask_8: 0.1769  loss_dice_8: 0.1471  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:02:59] d2.utils.events INFO:  eta: 0:34:42  iter: 26579  total_loss: 4.308  loss_ce: 0.09789  loss_mask: 0.201  loss_dice: 0.1326  loss_ce_0: 0.05211  loss_mask_0: 0.2036  loss_dice_0: 0.139  loss_ce_1: 0.09843  loss_mask_1: 0.2139  loss_dice_1: 0.1372  loss_ce_2: 0.09802  loss_mask_2: 0.2004  loss_dice_2: 0.1372  loss_ce_3: 0.09788  loss_mask_3: 0.2066  loss_dice_3: 0.13  loss_ce_4: 0.09811  loss_mask_4: 0.2002  loss_dice_4: 0.1376  loss_ce_5: 0.09785  loss_mask_5: 0.2063  loss_dice_5: 0.1334  loss_ce_6: 0.098  loss_mask_6: 0.2095  loss_dice_6: 0.134  loss_ce_7: 0.09781  loss_mask_7: 0.2004  loss_dice_7: 0.1369  loss_ce_8: 0.098  loss_mask_8: 0.2016  loss_dice_8: 0.1354  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:03:10] d2.utils.events INFO:  eta: 0:34:28  iter: 26599  total_loss: 4.554  loss_ce: 0.09767  loss_mask: 0.1747  loss_dice: 0.1497  loss_ce_0: 0.05211  loss_mask_0: 0.1801  loss_dice_0: 0.1492  loss_ce_1: 0.09739  loss_mask_1: 0.1877  loss_dice_1: 0.1492  loss_ce_2: 0.0975  loss_mask_2: 0.1794  loss_dice_2: 0.1479  loss_ce_3: 0.09762  loss_mask_3: 0.1833  loss_dice_3: 0.1529  loss_ce_4: 0.09753  loss_mask_4: 0.1785  loss_dice_4: 0.1502  loss_ce_5: 0.09755  loss_mask_5: 0.1809  loss_dice_5: 0.1487  loss_ce_6: 0.09774  loss_mask_6: 0.175  loss_dice_6: 0.1491  loss_ce_7: 0.09763  loss_mask_7: 0.1821  loss_dice_7: 0.1449  loss_ce_8: 0.0977  loss_mask_8: 0.1848  loss_dice_8: 0.1489  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:03:22] d2.utils.events INFO:  eta: 0:34:15  iter: 26619  total_loss: 4.38  loss_ce: 0.09778  loss_mask: 0.1705  loss_dice: 0.1299  loss_ce_0: 0.05211  loss_mask_0: 0.1566  loss_dice_0: 0.1321  loss_ce_1: 0.09831  loss_mask_1: 0.1596  loss_dice_1: 0.1319  loss_ce_2: 0.09794  loss_mask_2: 0.1543  loss_dice_2: 0.1302  loss_ce_3: 0.09777  loss_mask_3: 0.1632  loss_dice_3: 0.1303  loss_ce_4: 0.09792  loss_mask_4: 0.1551  loss_dice_4: 0.1339  loss_ce_5: 0.0977  loss_mask_5: 0.1544  loss_dice_5: 0.1293  loss_ce_6: 0.09792  loss_mask_6: 0.1632  loss_dice_6: 0.1329  loss_ce_7: 0.09792  loss_mask_7: 0.1613  loss_dice_7: 0.1295  loss_ce_8: 0.09789  loss_mask_8: 0.1609  loss_dice_8: 0.1374  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:03:33] d2.utils.events INFO:  eta: 0:34:04  iter: 26639  total_loss: 4.489  loss_ce: 0.09789  loss_mask: 0.1846  loss_dice: 0.1426  loss_ce_0: 0.05212  loss_mask_0: 0.1983  loss_dice_0: 0.1461  loss_ce_1: 0.09815  loss_mask_1: 0.1695  loss_dice_1: 0.1441  loss_ce_2: 0.09785  loss_mask_2: 0.1858  loss_dice_2: 0.137  loss_ce_3: 0.09777  loss_mask_3: 0.1964  loss_dice_3: 0.1451  loss_ce_4: 0.09789  loss_mask_4: 0.1819  loss_dice_4: 0.1464  loss_ce_5: 0.0977  loss_mask_5: 0.1802  loss_dice_5: 0.1421  loss_ce_6: 0.09785  loss_mask_6: 0.1927  loss_dice_6: 0.1501  loss_ce_7: 0.09777  loss_mask_7: 0.1879  loss_dice_7: 0.1435  loss_ce_8: 0.09792  loss_mask_8: 0.1801  loss_dice_8: 0.1457  time: 0.5830  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:03:45] d2.utils.events INFO:  eta: 0:33:53  iter: 26659  total_loss: 4.675  loss_ce: 0.1299  loss_mask: 0.187  loss_dice: 0.1529  loss_ce_0: 0.06372  loss_mask_0: 0.1848  loss_dice_0: 0.1547  loss_ce_1: 0.1296  loss_mask_1: 0.1819  loss_dice_1: 0.1486  loss_ce_2: 0.1298  loss_mask_2: 0.1796  loss_dice_2: 0.1501  loss_ce_3: 0.1298  loss_mask_3: 0.1846  loss_dice_3: 0.1559  loss_ce_4: 0.1298  loss_mask_4: 0.1774  loss_dice_4: 0.1492  loss_ce_5: 0.1299  loss_mask_5: 0.1756  loss_dice_5: 0.1498  loss_ce_6: 0.1298  loss_mask_6: 0.1812  loss_dice_6: 0.1482  loss_ce_7: 0.13  loss_mask_7: 0.1762  loss_dice_7: 0.154  loss_ce_8: 0.1299  loss_mask_8: 0.1784  loss_dice_8: 0.1586  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:03:56] d2.utils.events INFO:  eta: 0:33:37  iter: 26679  total_loss: 4.566  loss_ce: 0.0977  loss_mask: 0.1943  loss_dice: 0.1518  loss_ce_0: 0.05209  loss_mask_0: 0.1924  loss_dice_0: 0.1548  loss_ce_1: 0.09774  loss_mask_1: 0.1882  loss_dice_1: 0.1481  loss_ce_2: 0.09765  loss_mask_2: 0.1945  loss_dice_2: 0.1572  loss_ce_3: 0.09759  loss_mask_3: 0.1973  loss_dice_3: 0.1573  loss_ce_4: 0.0977  loss_mask_4: 0.1894  loss_dice_4: 0.1531  loss_ce_5: 0.0977  loss_mask_5: 0.1919  loss_dice_5: 0.149  loss_ce_6: 0.09766  loss_mask_6: 0.1851  loss_dice_6: 0.1534  loss_ce_7: 0.0977  loss_mask_7: 0.1968  loss_dice_7: 0.1508  loss_ce_8: 0.09785  loss_mask_8: 0.1926  loss_dice_8: 0.146  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:04:08] d2.utils.events INFO:  eta: 0:33:20  iter: 26699  total_loss: 4.516  loss_ce: 0.1622  loss_mask: 0.1654  loss_dice: 0.1492  loss_ce_0: 0.0754  loss_mask_0: 0.1586  loss_dice_0: 0.1448  loss_ce_1: 0.1615  loss_mask_1: 0.1597  loss_dice_1: 0.1515  loss_ce_2: 0.1619  loss_mask_2: 0.1613  loss_dice_2: 0.1478  loss_ce_3: 0.1621  loss_mask_3: 0.1658  loss_dice_3: 0.1509  loss_ce_4: 0.162  loss_mask_4: 0.1576  loss_dice_4: 0.1502  loss_ce_5: 0.1624  loss_mask_5: 0.1587  loss_dice_5: 0.1552  loss_ce_6: 0.1619  loss_mask_6: 0.163  loss_dice_6: 0.1516  loss_ce_7: 0.1622  loss_mask_7: 0.1562  loss_dice_7: 0.1555  loss_ce_8: 0.1616  loss_mask_8: 0.1554  loss_dice_8: 0.1495  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:04:19] d2.utils.events INFO:  eta: 0:33:04  iter: 26719  total_loss: 4.609  loss_ce: 0.1623  loss_mask: 0.1391  loss_dice: 0.1475  loss_ce_0: 0.07531  loss_mask_0: 0.1384  loss_dice_0: 0.1541  loss_ce_1: 0.1619  loss_mask_1: 0.1334  loss_dice_1: 0.152  loss_ce_2: 0.1624  loss_mask_2: 0.1401  loss_dice_2: 0.1522  loss_ce_3: 0.1624  loss_mask_3: 0.1315  loss_dice_3: 0.15  loss_ce_4: 0.1621  loss_mask_4: 0.1354  loss_dice_4: 0.1482  loss_ce_5: 0.1625  loss_mask_5: 0.1365  loss_dice_5: 0.1602  loss_ce_6: 0.1622  loss_mask_6: 0.1371  loss_dice_6: 0.1534  loss_ce_7: 0.1622  loss_mask_7: 0.1352  loss_dice_7: 0.1592  loss_ce_8: 0.1623  loss_mask_8: 0.1444  loss_dice_8: 0.1582  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:04:31] d2.utils.events INFO:  eta: 0:32:47  iter: 26739  total_loss: 4.438  loss_ce: 0.1299  loss_mask: 0.1658  loss_dice: 0.1207  loss_ce_0: 0.06372  loss_mask_0: 0.1626  loss_dice_0: 0.1202  loss_ce_1: 0.1298  loss_mask_1: 0.1695  loss_dice_1: 0.1209  loss_ce_2: 0.1298  loss_mask_2: 0.1748  loss_dice_2: 0.1241  loss_ce_3: 0.1298  loss_mask_3: 0.1672  loss_dice_3: 0.1209  loss_ce_4: 0.1298  loss_mask_4: 0.1624  loss_dice_4: 0.1202  loss_ce_5: 0.1298  loss_mask_5: 0.1668  loss_dice_5: 0.1241  loss_ce_6: 0.1298  loss_mask_6: 0.1618  loss_dice_6: 0.1225  loss_ce_7: 0.1299  loss_mask_7: 0.1729  loss_dice_7: 0.1302  loss_ce_8: 0.1298  loss_mask_8: 0.1728  loss_dice_8: 0.1288  time: 0.5830  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:04:42] d2.utils.events INFO:  eta: 0:32:30  iter: 26759  total_loss: 4.534  loss_ce: 0.09818  loss_mask: 0.194  loss_dice: 0.1269  loss_ce_0: 0.05217  loss_mask_0: 0.2061  loss_dice_0: 0.1267  loss_ce_1: 0.09843  loss_mask_1: 0.2014  loss_dice_1: 0.1315  loss_ce_2: 0.09833  loss_mask_2: 0.206  loss_dice_2: 0.131  loss_ce_3: 0.09822  loss_mask_3: 0.2019  loss_dice_3: 0.1377  loss_ce_4: 0.09826  loss_mask_4: 0.2026  loss_dice_4: 0.1312  loss_ce_5: 0.09807  loss_mask_5: 0.202  loss_dice_5: 0.1315  loss_ce_6: 0.09833  loss_mask_6: 0.2051  loss_dice_6: 0.1255  loss_ce_7: 0.09815  loss_mask_7: 0.205  loss_dice_7: 0.1322  loss_ce_8: 0.09833  loss_mask_8: 0.2047  loss_dice_8: 0.1345  time: 0.5829  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:04:54] d2.utils.events INFO:  eta: 0:32:09  iter: 26779  total_loss: 4.142  loss_ce: 0.09796  loss_mask: 0.1607  loss_dice: 0.1494  loss_ce_0: 0.05214  loss_mask_0: 0.1572  loss_dice_0: 0.1482  loss_ce_1: 0.09787  loss_mask_1: 0.1472  loss_dice_1: 0.1443  loss_ce_2: 0.09794  loss_mask_2: 0.1519  loss_dice_2: 0.1495  loss_ce_3: 0.098  loss_mask_3: 0.1488  loss_dice_3: 0.1474  loss_ce_4: 0.09803  loss_mask_4: 0.16  loss_dice_4: 0.1506  loss_ce_5: 0.09792  loss_mask_5: 0.1489  loss_dice_5: 0.1433  loss_ce_6: 0.09803  loss_mask_6: 0.1484  loss_dice_6: 0.1432  loss_ce_7: 0.09792  loss_mask_7: 0.1594  loss_dice_7: 0.1514  loss_ce_8: 0.09796  loss_mask_8: 0.1581  loss_dice_8: 0.1439  time: 0.5829  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 16:05:05] d2.utils.events INFO:  eta: 0:31:51  iter: 26799  total_loss: 4.836  loss_ce: 0.1618  loss_mask: 0.1656  loss_dice: 0.147  loss_ce_0: 0.07532  loss_mask_0: 0.1649  loss_dice_0: 0.1511  loss_ce_1: 0.1608  loss_mask_1: 0.1634  loss_dice_1: 0.1476  loss_ce_2: 0.1613  loss_mask_2: 0.161  loss_dice_2: 0.1476  loss_ce_3: 0.1617  loss_mask_3: 0.1706  loss_dice_3: 0.1425  loss_ce_4: 0.1613  loss_mask_4: 0.1673  loss_dice_4: 0.1466  loss_ce_5: 0.1618  loss_mask_5: 0.1619  loss_dice_5: 0.1406  loss_ce_6: 0.1615  loss_mask_6: 0.1726  loss_dice_6: 0.1456  loss_ce_7: 0.1617  loss_mask_7: 0.1731  loss_dice_7: 0.1464  loss_ce_8: 0.1615  loss_mask_8: 0.1719  loss_dice_8: 0.1437  time: 0.5829  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:05:16] d2.utils.events INFO:  eta: 0:31:33  iter: 26819  total_loss: 4.597  loss_ce: 0.09807  loss_mask: 0.1822  loss_dice: 0.1381  loss_ce_0: 0.05212  loss_mask_0: 0.1883  loss_dice_0: 0.1355  loss_ce_1: 0.09824  loss_mask_1: 0.1774  loss_dice_1: 0.1373  loss_ce_2: 0.09807  loss_mask_2: 0.1808  loss_dice_2: 0.1364  loss_ce_3: 0.09811  loss_mask_3: 0.1838  loss_dice_3: 0.136  loss_ce_4: 0.09803  loss_mask_4: 0.1861  loss_dice_4: 0.1409  loss_ce_5: 0.09792  loss_mask_5: 0.1813  loss_dice_5: 0.1389  loss_ce_6: 0.09811  loss_mask_6: 0.1865  loss_dice_6: 0.1465  loss_ce_7: 0.09803  loss_mask_7: 0.1789  loss_dice_7: 0.1323  loss_ce_8: 0.09815  loss_mask_8: 0.182  loss_dice_8: 0.1322  time: 0.5829  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:05:27] d2.utils.events INFO:  eta: 0:31:14  iter: 26839  total_loss: 4.515  loss_ce: 0.09815  loss_mask: 0.1362  loss_dice: 0.1393  loss_ce_0: 0.05211  loss_mask_0: 0.1389  loss_dice_0: 0.1476  loss_ce_1: 0.09872  loss_mask_1: 0.147  loss_dice_1: 0.1511  loss_ce_2: 0.09839  loss_mask_2: 0.14  loss_dice_2: 0.1455  loss_ce_3: 0.09825  loss_mask_3: 0.136  loss_dice_3: 0.1543  loss_ce_4: 0.09833  loss_mask_4: 0.1415  loss_dice_4: 0.1516  loss_ce_5: 0.09822  loss_mask_5: 0.1381  loss_dice_5: 0.146  loss_ce_6: 0.09826  loss_mask_6: 0.1372  loss_dice_6: 0.1515  loss_ce_7: 0.09811  loss_mask_7: 0.1418  loss_dice_7: 0.1537  loss_ce_8: 0.09826  loss_mask_8: 0.1397  loss_dice_8: 0.1515  time: 0.5829  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:05:37] d2.utils.events INFO:  eta: 0:30:58  iter: 26859  total_loss: 4.648  loss_ce: 0.09807  loss_mask: 0.214  loss_dice: 0.132  loss_ce_0: 0.05212  loss_mask_0: 0.2103  loss_dice_0: 0.1379  loss_ce_1: 0.09835  loss_mask_1: 0.2059  loss_dice_1: 0.131  loss_ce_2: 0.09818  loss_mask_2: 0.2078  loss_dice_2: 0.1324  loss_ce_3: 0.09807  loss_mask_3: 0.2042  loss_dice_3: 0.1322  loss_ce_4: 0.09818  loss_mask_4: 0.2046  loss_dice_4: 0.1357  loss_ce_5: 0.09792  loss_mask_5: 0.2017  loss_dice_5: 0.1317  loss_ce_6: 0.09811  loss_mask_6: 0.2129  loss_dice_6: 0.1342  loss_ce_7: 0.098  loss_mask_7: 0.209  loss_dice_7: 0.1308  loss_ce_8: 0.09811  loss_mask_8: 0.2115  loss_dice_8: 0.1301  time: 0.5828  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:05:48] d2.utils.events INFO:  eta: 0:30:38  iter: 26879  total_loss: 4.396  loss_ce: 0.1617  loss_mask: 0.138  loss_dice: 0.1298  loss_ce_0: 0.07527  loss_mask_0: 0.142  loss_dice_0: 0.1278  loss_ce_1: 0.1608  loss_mask_1: 0.1388  loss_dice_1: 0.1328  loss_ce_2: 0.1613  loss_mask_2: 0.1345  loss_dice_2: 0.1317  loss_ce_3: 0.1615  loss_mask_3: 0.1378  loss_dice_3: 0.1341  loss_ce_4: 0.1615  loss_mask_4: 0.1385  loss_dice_4: 0.1288  loss_ce_5: 0.1615  loss_mask_5: 0.1388  loss_dice_5: 0.1256  loss_ce_6: 0.1614  loss_mask_6: 0.135  loss_dice_6: 0.1309  loss_ce_7: 0.1618  loss_mask_7: 0.1395  loss_dice_7: 0.1282  loss_ce_8: 0.1614  loss_mask_8: 0.1351  loss_dice_8: 0.1284  time: 0.5828  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:05:58] d2.utils.events INFO:  eta: 0:30:22  iter: 26899  total_loss: 4.452  loss_ce: 0.09844  loss_mask: 0.2023  loss_dice: 0.1444  loss_ce_0: 0.05219  loss_mask_0: 0.1977  loss_dice_0: 0.1412  loss_ce_1: 0.09917  loss_mask_1: 0.2008  loss_dice_1: 0.1416  loss_ce_2: 0.0987  loss_mask_2: 0.1991  loss_dice_2: 0.1419  loss_ce_3: 0.09855  loss_mask_3: 0.197  loss_dice_3: 0.1361  loss_ce_4: 0.0987  loss_mask_4: 0.1967  loss_dice_4: 0.1433  loss_ce_5: 0.09852  loss_mask_5: 0.1994  loss_dice_5: 0.1436  loss_ce_6: 0.09867  loss_mask_6: 0.1934  loss_dice_6: 0.1357  loss_ce_7: 0.09844  loss_mask_7: 0.2016  loss_dice_7: 0.142  loss_ce_8: 0.09856  loss_mask_8: 0.2054  loss_dice_8: 0.1451  time: 0.5827  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 16:06:08] d2.utils.events INFO:  eta: 0:30:04  iter: 26919  total_loss: 4.746  loss_ce: 0.1613  loss_mask: 0.2035  loss_dice: 0.1432  loss_ce_0: 0.07521  loss_mask_0: 0.2054  loss_dice_0: 0.1443  loss_ce_1: 0.1606  loss_mask_1: 0.2015  loss_dice_1: 0.1504  loss_ce_2: 0.1611  loss_mask_2: 0.2228  loss_dice_2: 0.1442  loss_ce_3: 0.1613  loss_mask_3: 0.2226  loss_dice_3: 0.1411  loss_ce_4: 0.161  loss_mask_4: 0.2058  loss_dice_4: 0.1397  loss_ce_5: 0.1613  loss_mask_5: 0.2071  loss_dice_5: 0.1441  loss_ce_6: 0.1611  loss_mask_6: 0.2087  loss_dice_6: 0.1443  loss_ce_7: 0.1614  loss_mask_7: 0.2077  loss_dice_7: 0.143  loss_ce_8: 0.1612  loss_mask_8: 0.22  loss_dice_8: 0.1492  time: 0.5827  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:06:19] d2.utils.events INFO:  eta: 0:29:48  iter: 26939  total_loss: 4.609  loss_ce: 0.09829  loss_mask: 0.214  loss_dice: 0.1286  loss_ce_0: 0.0522  loss_mask_0: 0.2182  loss_dice_0: 0.1305  loss_ce_1: 0.09831  loss_mask_1: 0.215  loss_dice_1: 0.134  loss_ce_2: 0.09828  loss_mask_2: 0.2128  loss_dice_2: 0.1343  loss_ce_3: 0.09837  loss_mask_3: 0.2191  loss_dice_3: 0.1299  loss_ce_4: 0.09833  loss_mask_4: 0.2104  loss_dice_4: 0.1318  loss_ce_5: 0.09822  loss_mask_5: 0.2153  loss_dice_5: 0.1359  loss_ce_6: 0.09833  loss_mask_6: 0.216  loss_dice_6: 0.1351  loss_ce_7: 0.09826  loss_mask_7: 0.2173  loss_dice_7: 0.1242  loss_ce_8: 0.09829  loss_mask_8: 0.2086  loss_dice_8: 0.1323  time: 0.5826  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:06:29] d2.utils.events INFO:  eta: 0:29:31  iter: 26959  total_loss: 4.678  loss_ce: 0.09844  loss_mask: 0.2077  loss_dice: 0.1326  loss_ce_0: 0.05218  loss_mask_0: 0.1982  loss_dice_0: 0.1306  loss_ce_1: 0.0991  loss_mask_1: 0.2146  loss_dice_1: 0.1299  loss_ce_2: 0.09872  loss_mask_2: 0.1972  loss_dice_2: 0.1292  loss_ce_3: 0.09859  loss_mask_3: 0.2009  loss_dice_3: 0.134  loss_ce_4: 0.0987  loss_mask_4: 0.1952  loss_dice_4: 0.1295  loss_ce_5: 0.09844  loss_mask_5: 0.2085  loss_dice_5: 0.1292  loss_ce_6: 0.09874  loss_mask_6: 0.1978  loss_dice_6: 0.1314  loss_ce_7: 0.09844  loss_mask_7: 0.1987  loss_dice_7: 0.1274  loss_ce_8: 0.09859  loss_mask_8: 0.2024  loss_dice_8: 0.1259  time: 0.5826  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:06:39] d2.utils.events INFO:  eta: 0:29:16  iter: 26979  total_loss: 4.663  loss_ce: 0.09829  loss_mask: 0.1741  loss_dice: 0.1424  loss_ce_0: 0.05216  loss_mask_0: 0.1837  loss_dice_0: 0.1417  loss_ce_1: 0.09876  loss_mask_1: 0.1803  loss_dice_1: 0.1433  loss_ce_2: 0.09852  loss_mask_2: 0.1822  loss_dice_2: 0.1422  loss_ce_3: 0.0984  loss_mask_3: 0.1807  loss_dice_3: 0.1424  loss_ce_4: 0.09855  loss_mask_4: 0.1767  loss_dice_4: 0.1414  loss_ce_5: 0.09829  loss_mask_5: 0.1797  loss_dice_5: 0.1477  loss_ce_6: 0.09855  loss_mask_6: 0.1755  loss_dice_6: 0.1428  loss_ce_7: 0.0983  loss_mask_7: 0.1917  loss_dice_7: 0.1434  loss_ce_8: 0.09841  loss_mask_8: 0.1744  loss_dice_8: 0.1354  time: 0.5825  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:06:49] d2.utils.events INFO:  eta: 0:28:59  iter: 26999  total_loss: 4.529  loss_ce: 0.09829  loss_mask: 0.1585  loss_dice: 0.1525  loss_ce_0: 0.05215  loss_mask_0: 0.1378  loss_dice_0: 0.1561  loss_ce_1: 0.099  loss_mask_1: 0.1417  loss_dice_1: 0.1558  loss_ce_2: 0.09848  loss_mask_2: 0.1468  loss_dice_2: 0.1541  loss_ce_3: 0.09837  loss_mask_3: 0.1447  loss_dice_3: 0.1519  loss_ce_4: 0.09848  loss_mask_4: 0.139  loss_dice_4: 0.1507  loss_ce_5: 0.09822  loss_mask_5: 0.1455  loss_dice_5: 0.1528  loss_ce_6: 0.09852  loss_mask_6: 0.1527  loss_dice_6: 0.1551  loss_ce_7: 0.09826  loss_mask_7: 0.1483  loss_dice_7: 0.1474  loss_ce_8: 0.09837  loss_mask_8: 0.1566  loss_dice_8: 0.1484  time: 0.5825  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:07:00] d2.utils.events INFO:  eta: 0:28:40  iter: 27019  total_loss: 4.666  loss_ce: 0.1617  loss_mask: 0.1875  loss_dice: 0.1609  loss_ce_0: 0.07526  loss_mask_0: 0.1837  loss_dice_0: 0.159  loss_ce_1: 0.1613  loss_mask_1: 0.189  loss_dice_1: 0.152  loss_ce_2: 0.1616  loss_mask_2: 0.1918  loss_dice_2: 0.1658  loss_ce_3: 0.1616  loss_mask_3: 0.1907  loss_dice_3: 0.1639  loss_ce_4: 0.1615  loss_mask_4: 0.188  loss_dice_4: 0.1603  loss_ce_5: 0.1619  loss_mask_5: 0.1823  loss_dice_5: 0.156  loss_ce_6: 0.1616  loss_mask_6: 0.1906  loss_dice_6: 0.1597  loss_ce_7: 0.1617  loss_mask_7: 0.1893  loss_dice_7: 0.1603  loss_ce_8: 0.1616  loss_mask_8: 0.1822  loss_dice_8: 0.154  time: 0.5824  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:07:10] d2.utils.events INFO:  eta: 0:28:22  iter: 27039  total_loss: 4.69  loss_ce: 0.09837  loss_mask: 0.2002  loss_dice: 0.1455  loss_ce_0: 0.05217  loss_mask_0: 0.1928  loss_dice_0: 0.1474  loss_ce_1: 0.09884  loss_mask_1: 0.1939  loss_dice_1: 0.1437  loss_ce_2: 0.09852  loss_mask_2: 0.1889  loss_dice_2: 0.1474  loss_ce_3: 0.09848  loss_mask_3: 0.1899  loss_dice_3: 0.1422  loss_ce_4: 0.09852  loss_mask_4: 0.1944  loss_dice_4: 0.145  loss_ce_5: 0.09844  loss_mask_5: 0.1958  loss_dice_5: 0.1389  loss_ce_6: 0.09855  loss_mask_6: 0.2022  loss_dice_6: 0.142  loss_ce_7: 0.09833  loss_mask_7: 0.2029  loss_dice_7: 0.1445  loss_ce_8: 0.09844  loss_mask_8: 0.1941  loss_dice_8: 0.1477  time: 0.5824  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:07:20] d2.utils.events INFO:  eta: 0:27:59  iter: 27059  total_loss: 4.601  loss_ce: 0.09822  loss_mask: 0.1783  loss_dice: 0.1572  loss_ce_0: 0.05217  loss_mask_0: 0.1829  loss_dice_0: 0.153  loss_ce_1: 0.09818  loss_mask_1: 0.1862  loss_dice_1: 0.1545  loss_ce_2: 0.09822  loss_mask_2: 0.176  loss_dice_2: 0.1552  loss_ce_3: 0.09837  loss_mask_3: 0.1742  loss_dice_3: 0.1554  loss_ce_4: 0.09822  loss_mask_4: 0.1668  loss_dice_4: 0.1552  loss_ce_5: 0.09829  loss_mask_5: 0.1714  loss_dice_5: 0.1574  loss_ce_6: 0.09829  loss_mask_6: 0.1838  loss_dice_6: 0.1603  loss_ce_7: 0.09826  loss_mask_7: 0.1737  loss_dice_7: 0.1543  loss_ce_8: 0.09822  loss_mask_8: 0.1779  loss_dice_8: 0.1604  time: 0.5823  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:07:30] d2.utils.events INFO:  eta: 0:27:43  iter: 27079  total_loss: 4.565  loss_ce: 0.0983  loss_mask: 0.2179  loss_dice: 0.122  loss_ce_0: 0.05217  loss_mask_0: 0.2079  loss_dice_0: 0.1232  loss_ce_1: 0.09885  loss_mask_1: 0.2127  loss_dice_1: 0.1262  loss_ce_2: 0.09874  loss_mask_2: 0.2179  loss_dice_2: 0.1224  loss_ce_3: 0.09848  loss_mask_3: 0.2187  loss_dice_3: 0.1232  loss_ce_4: 0.09859  loss_mask_4: 0.209  loss_dice_4: 0.1268  loss_ce_5: 0.09844  loss_mask_5: 0.2129  loss_dice_5: 0.1275  loss_ce_6: 0.09859  loss_mask_6: 0.2128  loss_dice_6: 0.126  loss_ce_7: 0.09837  loss_mask_7: 0.2108  loss_dice_7: 0.1255  loss_ce_8: 0.09856  loss_mask_8: 0.2171  loss_dice_8: 0.1265  time: 0.5822  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:07:41] d2.utils.events INFO:  eta: 0:27:25  iter: 27099  total_loss: 4.649  loss_ce: 0.1299  loss_mask: 0.1842  loss_dice: 0.1319  loss_ce_0: 0.0637  loss_mask_0: 0.1862  loss_dice_0: 0.1335  loss_ce_1: 0.13  loss_mask_1: 0.2024  loss_dice_1: 0.1364  loss_ce_2: 0.1299  loss_mask_2: 0.1819  loss_dice_2: 0.1315  loss_ce_3: 0.13  loss_mask_3: 0.1955  loss_dice_3: 0.1335  loss_ce_4: 0.1301  loss_mask_4: 0.1912  loss_dice_4: 0.1323  loss_ce_5: 0.13  loss_mask_5: 0.1969  loss_dice_5: 0.1351  loss_ce_6: 0.1299  loss_mask_6: 0.1903  loss_dice_6: 0.1327  loss_ce_7: 0.13  loss_mask_7: 0.1896  loss_dice_7: 0.1356  loss_ce_8: 0.1299  loss_mask_8: 0.1854  loss_dice_8: 0.1327  time: 0.5822  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 16:07:51] d2.utils.events INFO:  eta: 0:27:06  iter: 27119  total_loss: 4.458  loss_ce: 0.09848  loss_mask: 0.1932  loss_dice: 0.13  loss_ce_0: 0.05218  loss_mask_0: 0.2002  loss_dice_0: 0.1315  loss_ce_1: 0.099  loss_mask_1: 0.1998  loss_dice_1: 0.1309  loss_ce_2: 0.09865  loss_mask_2: 0.2082  loss_dice_2: 0.1333  loss_ce_3: 0.09848  loss_mask_3: 0.1993  loss_dice_3: 0.1285  loss_ce_4: 0.09866  loss_mask_4: 0.1971  loss_dice_4: 0.1304  loss_ce_5: 0.09844  loss_mask_5: 0.2008  loss_dice_5: 0.1227  loss_ce_6: 0.09859  loss_mask_6: 0.2037  loss_dice_6: 0.131  loss_ce_7: 0.09841  loss_mask_7: 0.1983  loss_dice_7: 0.1318  loss_ce_8: 0.09859  loss_mask_8: 0.1999  loss_dice_8: 0.1342  time: 0.5821  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:08:02] d2.utils.events INFO:  eta: 0:26:50  iter: 27139  total_loss: 4.417  loss_ce: 0.09844  loss_mask: 0.1769  loss_dice: 0.1498  loss_ce_0: 0.05218  loss_mask_0: 0.173  loss_dice_0: 0.1488  loss_ce_1: 0.09884  loss_mask_1: 0.1739  loss_dice_1: 0.1481  loss_ce_2: 0.09865  loss_mask_2: 0.1723  loss_dice_2: 0.1472  loss_ce_3: 0.09848  loss_mask_3: 0.1766  loss_dice_3: 0.151  loss_ce_4: 0.09867  loss_mask_4: 0.1819  loss_dice_4: 0.159  loss_ce_5: 0.09844  loss_mask_5: 0.1801  loss_dice_5: 0.145  loss_ce_6: 0.09863  loss_mask_6: 0.1754  loss_dice_6: 0.1528  loss_ce_7: 0.09852  loss_mask_7: 0.1689  loss_dice_7: 0.1546  loss_ce_8: 0.09894  loss_mask_8: 0.1739  loss_dice_8: 0.1492  time: 0.5821  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:08:13] d2.utils.events INFO:  eta: 0:26:34  iter: 27159  total_loss: 4.652  loss_ce: 0.1612  loss_mask: 0.1964  loss_dice: 0.1238  loss_ce_0: 0.07522  loss_mask_0: 0.189  loss_dice_0: 0.1253  loss_ce_1: 0.1601  loss_mask_1: 0.1833  loss_dice_1: 0.1269  loss_ce_2: 0.1608  loss_mask_2: 0.1835  loss_dice_2: 0.1297  loss_ce_3: 0.161  loss_mask_3: 0.1919  loss_dice_3: 0.1315  loss_ce_4: 0.1608  loss_mask_4: 0.1848  loss_dice_4: 0.1291  loss_ce_5: 0.161  loss_mask_5: 0.1927  loss_dice_5: 0.1256  loss_ce_6: 0.161  loss_mask_6: 0.187  loss_dice_6: 0.1302  loss_ce_7: 0.1612  loss_mask_7: 0.197  loss_dice_7: 0.1279  loss_ce_8: 0.161  loss_mask_8: 0.1838  loss_dice_8: 0.1284  time: 0.5821  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:08:24] d2.utils.events INFO:  eta: 0:26:19  iter: 27179  total_loss: 4.687  loss_ce: 0.1299  loss_mask: 0.1963  loss_dice: 0.1389  loss_ce_0: 0.0637  loss_mask_0: 0.2064  loss_dice_0: 0.1448  loss_ce_1: 0.1298  loss_mask_1: 0.2012  loss_dice_1: 0.1453  loss_ce_2: 0.1299  loss_mask_2: 0.2049  loss_dice_2: 0.1423  loss_ce_3: 0.1298  loss_mask_3: 0.1971  loss_dice_3: 0.1427  loss_ce_4: 0.1299  loss_mask_4: 0.2058  loss_dice_4: 0.1423  loss_ce_5: 0.1298  loss_mask_5: 0.1968  loss_dice_5: 0.1435  loss_ce_6: 0.1298  loss_mask_6: 0.2004  loss_dice_6: 0.1401  loss_ce_7: 0.1299  loss_mask_7: 0.2025  loss_dice_7: 0.1402  loss_ce_8: 0.1298  loss_mask_8: 0.2074  loss_dice_8: 0.1415  time: 0.5821  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:08:36] d2.utils.events INFO:  eta: 0:26:07  iter: 27199  total_loss: 4.755  loss_ce: 0.161  loss_mask: 0.1647  loss_dice: 0.1366  loss_ce_0: 0.07516  loss_mask_0: 0.1718  loss_dice_0: 0.141  loss_ce_1: 0.16  loss_mask_1: 0.1673  loss_dice_1: 0.1336  loss_ce_2: 0.1604  loss_mask_2: 0.1689  loss_dice_2: 0.1398  loss_ce_3: 0.1606  loss_mask_3: 0.1689  loss_dice_3: 0.1395  loss_ce_4: 0.1604  loss_mask_4: 0.169  loss_dice_4: 0.1458  loss_ce_5: 0.1607  loss_mask_5: 0.1774  loss_dice_5: 0.1368  loss_ce_6: 0.1605  loss_mask_6: 0.1644  loss_dice_6: 0.146  loss_ce_7: 0.1608  loss_mask_7: 0.1715  loss_dice_7: 0.1395  loss_ce_8: 0.1607  loss_mask_8: 0.1621  loss_dice_8: 0.1394  time: 0.5821  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 16:08:47] d2.utils.events INFO:  eta: 0:25:56  iter: 27219  total_loss: 4.648  loss_ce: 0.09878  loss_mask: 0.1977  loss_dice: 0.1481  loss_ce_0: 0.0523  loss_mask_0: 0.2016  loss_dice_0: 0.1466  loss_ce_1: 0.09928  loss_mask_1: 0.1972  loss_dice_1: 0.1474  loss_ce_2: 0.09893  loss_mask_2: 0.1945  loss_dice_2: 0.1469  loss_ce_3: 0.09892  loss_mask_3: 0.2085  loss_dice_3: 0.1518  loss_ce_4: 0.099  loss_mask_4: 0.2092  loss_dice_4: 0.1376  loss_ce_5: 0.09889  loss_mask_5: 0.2039  loss_dice_5: 0.147  loss_ce_6: 0.09904  loss_mask_6: 0.2072  loss_dice_6: 0.1473  loss_ce_7: 0.09889  loss_mask_7: 0.2102  loss_dice_7: 0.1544  loss_ce_8: 0.09924  loss_mask_8: 0.197  loss_dice_8: 0.1418  time: 0.5821  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 16:08:59] d2.utils.events INFO:  eta: 0:25:42  iter: 27239  total_loss: 4.62  loss_ce: 0.09882  loss_mask: 0.2052  loss_dice: 0.1617  loss_ce_0: 0.0523  loss_mask_0: 0.2003  loss_dice_0: 0.157  loss_ce_1: 0.09897  loss_mask_1: 0.2098  loss_dice_1: 0.1558  loss_ce_2: 0.09882  loss_mask_2: 0.211  loss_dice_2: 0.1611  loss_ce_3: 0.09881  loss_mask_3: 0.2084  loss_dice_3: 0.1563  loss_ce_4: 0.09893  loss_mask_4: 0.2096  loss_dice_4: 0.1573  loss_ce_5: 0.09882  loss_mask_5: 0.2041  loss_dice_5: 0.1477  loss_ce_6: 0.09881  loss_mask_6: 0.2137  loss_dice_6: 0.1632  loss_ce_7: 0.09878  loss_mask_7: 0.2121  loss_dice_7: 0.16  loss_ce_8: 0.09889  loss_mask_8: 0.2029  loss_dice_8: 0.1632  time: 0.5821  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:09:10] d2.utils.events INFO:  eta: 0:25:29  iter: 27259  total_loss: 4.801  loss_ce: 0.1605  loss_mask: 0.2255  loss_dice: 0.1293  loss_ce_0: 0.07505  loss_mask_0: 0.2247  loss_dice_0: 0.1336  loss_ce_1: 0.1595  loss_mask_1: 0.2228  loss_dice_1: 0.1346  loss_ce_2: 0.16  loss_mask_2: 0.2284  loss_dice_2: 0.1305  loss_ce_3: 0.1601  loss_mask_3: 0.2166  loss_dice_3: 0.134  loss_ce_4: 0.1601  loss_mask_4: 0.2233  loss_dice_4: 0.1331  loss_ce_5: 0.1604  loss_mask_5: 0.2255  loss_dice_5: 0.1347  loss_ce_6: 0.16  loss_mask_6: 0.2286  loss_dice_6: 0.1311  loss_ce_7: 0.1606  loss_mask_7: 0.2265  loss_dice_7: 0.1383  loss_ce_8: 0.1604  loss_mask_8: 0.227  loss_dice_8: 0.137  time: 0.5821  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:09:22] d2.utils.events INFO:  eta: 0:25:17  iter: 27279  total_loss: 4.529  loss_ce: 0.1298  loss_mask: 0.1895  loss_dice: 0.1416  loss_ce_0: 0.06368  loss_mask_0: 0.1856  loss_dice_0: 0.1372  loss_ce_1: 0.1295  loss_mask_1: 0.1885  loss_dice_1: 0.1383  loss_ce_2: 0.1297  loss_mask_2: 0.1859  loss_dice_2: 0.1424  loss_ce_3: 0.1297  loss_mask_3: 0.1857  loss_dice_3: 0.141  loss_ce_4: 0.1297  loss_mask_4: 0.1844  loss_dice_4: 0.149  loss_ce_5: 0.1297  loss_mask_5: 0.1929  loss_dice_5: 0.1383  loss_ce_6: 0.1296  loss_mask_6: 0.187  loss_dice_6: 0.1422  loss_ce_7: 0.1298  loss_mask_7: 0.1963  loss_dice_7: 0.1432  loss_ce_8: 0.1296  loss_mask_8: 0.1908  loss_dice_8: 0.1358  time: 0.5821  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:09:33] d2.utils.events INFO:  eta: 0:25:05  iter: 27299  total_loss: 4.54  loss_ce: 0.09893  loss_mask: 0.2085  loss_dice: 0.1202  loss_ce_0: 0.05231  loss_mask_0: 0.2018  loss_dice_0: 0.1228  loss_ce_1: 0.09934  loss_mask_1: 0.2176  loss_dice_1: 0.1238  loss_ce_2: 0.09915  loss_mask_2: 0.2099  loss_dice_2: 0.1237  loss_ce_3: 0.09904  loss_mask_3: 0.2094  loss_dice_3: 0.1251  loss_ce_4: 0.09915  loss_mask_4: 0.2012  loss_dice_4: 0.1225  loss_ce_5: 0.09911  loss_mask_5: 0.2146  loss_dice_5: 0.1222  loss_ce_6: 0.09915  loss_mask_6: 0.206  loss_dice_6: 0.1233  loss_ce_7: 0.09893  loss_mask_7: 0.2033  loss_dice_7: 0.1162  loss_ce_8: 0.09904  loss_mask_8: 0.2063  loss_dice_8: 0.1231  time: 0.5820  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:09:45] d2.utils.events INFO:  eta: 0:24:55  iter: 27319  total_loss: 4.658  loss_ce: 0.09893  loss_mask: 0.1966  loss_dice: 0.1375  loss_ce_0: 0.05231  loss_mask_0: 0.2014  loss_dice_0: 0.1336  loss_ce_1: 0.09925  loss_mask_1: 0.2015  loss_dice_1: 0.1396  loss_ce_2: 0.09909  loss_mask_2: 0.2024  loss_dice_2: 0.14  loss_ce_3: 0.09904  loss_mask_3: 0.1988  loss_dice_3: 0.1361  loss_ce_4: 0.09915  loss_mask_4: 0.1917  loss_dice_4: 0.1339  loss_ce_5: 0.09904  loss_mask_5: 0.1984  loss_dice_5: 0.1327  loss_ce_6: 0.09908  loss_mask_6: 0.1968  loss_dice_6: 0.1406  loss_ce_7: 0.09893  loss_mask_7: 0.2026  loss_dice_7: 0.1411  loss_ce_8: 0.09897  loss_mask_8: 0.1924  loss_dice_8: 0.1414  time: 0.5820  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:09:56] d2.utils.events INFO:  eta: 0:24:44  iter: 27339  total_loss: 4.427  loss_ce: 0.09889  loss_mask: 0.1438  loss_dice: 0.1389  loss_ce_0: 0.05231  loss_mask_0: 0.1553  loss_dice_0: 0.1411  loss_ce_1: 0.09919  loss_mask_1: 0.1453  loss_dice_1: 0.1412  loss_ce_2: 0.09897  loss_mask_2: 0.1491  loss_dice_2: 0.1423  loss_ce_3: 0.099  loss_mask_3: 0.14  loss_dice_3: 0.1404  loss_ce_4: 0.09908  loss_mask_4: 0.1407  loss_dice_4: 0.1415  loss_ce_5: 0.09904  loss_mask_5: 0.144  loss_dice_5: 0.1415  loss_ce_6: 0.09896  loss_mask_6: 0.1369  loss_dice_6: 0.144  loss_ce_7: 0.09893  loss_mask_7: 0.1426  loss_dice_7: 0.1402  loss_ce_8: 0.099  loss_mask_8: 0.1397  loss_dice_8: 0.1336  time: 0.5820  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 16:10:08] d2.utils.events INFO:  eta: 0:24:32  iter: 27359  total_loss: 4.636  loss_ce: 0.1608  loss_mask: 0.1817  loss_dice: 0.1435  loss_ce_0: 0.07505  loss_mask_0: 0.1863  loss_dice_0: 0.1506  loss_ce_1: 0.1605  loss_mask_1: 0.1781  loss_dice_1: 0.1446  loss_ce_2: 0.1607  loss_mask_2: 0.1823  loss_dice_2: 0.1495  loss_ce_3: 0.1607  loss_mask_3: 0.1791  loss_dice_3: 0.1466  loss_ce_4: 0.1604  loss_mask_4: 0.1905  loss_dice_4: 0.1459  loss_ce_5: 0.1606  loss_mask_5: 0.1855  loss_dice_5: 0.1467  loss_ce_6: 0.1605  loss_mask_6: 0.182  loss_dice_6: 0.1465  loss_ce_7: 0.1607  loss_mask_7: 0.1771  loss_dice_7: 0.1466  loss_ce_8: 0.1608  loss_mask_8: 0.189  loss_dice_8: 0.1451  time: 0.5820  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:10:19] d2.utils.events INFO:  eta: 0:24:21  iter: 27379  total_loss: 4.923  loss_ce: 0.1298  loss_mask: 0.2102  loss_dice: 0.1556  loss_ce_0: 0.06368  loss_mask_0: 0.2042  loss_dice_0: 0.1452  loss_ce_1: 0.1296  loss_mask_1: 0.2168  loss_dice_1: 0.1466  loss_ce_2: 0.1297  loss_mask_2: 0.2109  loss_dice_2: 0.1475  loss_ce_3: 0.1297  loss_mask_3: 0.1975  loss_dice_3: 0.1491  loss_ce_4: 0.1297  loss_mask_4: 0.2073  loss_dice_4: 0.1532  loss_ce_5: 0.1298  loss_mask_5: 0.2048  loss_dice_5: 0.1529  loss_ce_6: 0.1297  loss_mask_6: 0.2187  loss_dice_6: 0.1566  loss_ce_7: 0.1297  loss_mask_7: 0.2124  loss_dice_7: 0.1537  loss_ce_8: 0.1298  loss_mask_8: 0.2211  loss_dice_8: 0.1532  time: 0.5820  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:10:30] d2.utils.events INFO:  eta: 0:24:09  iter: 27399  total_loss: 4.465  loss_ce: 0.099  loss_mask: 0.1941  loss_dice: 0.1309  loss_ce_0: 0.05231  loss_mask_0: 0.193  loss_dice_0: 0.13  loss_ce_1: 0.09921  loss_mask_1: 0.1975  loss_dice_1: 0.129  loss_ce_2: 0.09911  loss_mask_2: 0.1955  loss_dice_2: 0.129  loss_ce_3: 0.09911  loss_mask_3: 0.1953  loss_dice_3: 0.1329  loss_ce_4: 0.09915  loss_mask_4: 0.1975  loss_dice_4: 0.1318  loss_ce_5: 0.09911  loss_mask_5: 0.1977  loss_dice_5: 0.1301  loss_ce_6: 0.09908  loss_mask_6: 0.2068  loss_dice_6: 0.1338  loss_ce_7: 0.09904  loss_mask_7: 0.195  loss_dice_7: 0.1299  loss_ce_8: 0.09912  loss_mask_8: 0.1999  loss_dice_8: 0.1349  time: 0.5820  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 16:10:42] d2.utils.events INFO:  eta: 0:24:00  iter: 27419  total_loss: 4.342  loss_ce: 0.1606  loss_mask: 0.1768  loss_dice: 0.1368  loss_ce_0: 0.07503  loss_mask_0: 0.1788  loss_dice_0: 0.1337  loss_ce_1: 0.1602  loss_mask_1: 0.1871  loss_dice_1: 0.1339  loss_ce_2: 0.1604  loss_mask_2: 0.1811  loss_dice_2: 0.1404  loss_ce_3: 0.1603  loss_mask_3: 0.1825  loss_dice_3: 0.1381  loss_ce_4: 0.1603  loss_mask_4: 0.1743  loss_dice_4: 0.1331  loss_ce_5: 0.1605  loss_mask_5: 0.1784  loss_dice_5: 0.1363  loss_ce_6: 0.1604  loss_mask_6: 0.1819  loss_dice_6: 0.1398  loss_ce_7: 0.1606  loss_mask_7: 0.1751  loss_dice_7: 0.137  loss_ce_8: 0.1604  loss_mask_8: 0.1856  loss_dice_8: 0.1343  time: 0.5820  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:10:53] d2.utils.events INFO:  eta: 0:23:47  iter: 27439  total_loss: 4.824  loss_ce: 0.09911  loss_mask: 0.1483  loss_dice: 0.1638  loss_ce_0: 0.05235  loss_mask_0: 0.1498  loss_dice_0: 0.16  loss_ce_1: 0.0994  loss_mask_1: 0.1473  loss_dice_1: 0.1599  loss_ce_2: 0.09923  loss_mask_2: 0.1562  loss_dice_2: 0.164  loss_ce_3: 0.09926  loss_mask_3: 0.1574  loss_dice_3: 0.1613  loss_ce_4: 0.09926  loss_mask_4: 0.1521  loss_dice_4: 0.1638  loss_ce_5: 0.09926  loss_mask_5: 0.1558  loss_dice_5: 0.1543  loss_ce_6: 0.09923  loss_mask_6: 0.1548  loss_dice_6: 0.1628  loss_ce_7: 0.09911  loss_mask_7: 0.147  loss_dice_7: 0.1598  loss_ce_8: 0.09923  loss_mask_8: 0.1599  loss_dice_8: 0.1631  time: 0.5820  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:11:05] d2.utils.events INFO:  eta: 0:23:36  iter: 27459  total_loss: 4.591  loss_ce: 0.1602  loss_mask: 0.1794  loss_dice: 0.1454  loss_ce_0: 0.07496  loss_mask_0: 0.1779  loss_dice_0: 0.1432  loss_ce_1: 0.1593  loss_mask_1: 0.177  loss_dice_1: 0.1393  loss_ce_2: 0.1597  loss_mask_2: 0.1701  loss_dice_2: 0.1373  loss_ce_3: 0.16  loss_mask_3: 0.1724  loss_dice_3: 0.1385  loss_ce_4: 0.1598  loss_mask_4: 0.1759  loss_dice_4: 0.1392  loss_ce_5: 0.16  loss_mask_5: 0.1795  loss_dice_5: 0.1386  loss_ce_6: 0.1599  loss_mask_6: 0.1797  loss_dice_6: 0.1371  loss_ce_7: 0.1601  loss_mask_7: 0.1689  loss_dice_7: 0.139  loss_ce_8: 0.16  loss_mask_8: 0.175  loss_dice_8: 0.1348  time: 0.5820  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:11:16] d2.utils.events INFO:  eta: 0:23:25  iter: 27479  total_loss: 4.581  loss_ce: 0.09945  loss_mask: 0.1547  loss_dice: 0.1414  loss_ce_0: 0.05237  loss_mask_0: 0.1594  loss_dice_0: 0.1443  loss_ce_1: 0.1004  loss_mask_1: 0.1609  loss_dice_1: 0.1448  loss_ce_2: 0.09992  loss_mask_2: 0.1588  loss_dice_2: 0.1524  loss_ce_3: 0.09986  loss_mask_3: 0.1527  loss_dice_3: 0.1425  loss_ce_4: 0.0999  loss_mask_4: 0.152  loss_dice_4: 0.1417  loss_ce_5: 0.09986  loss_mask_5: 0.1619  loss_dice_5: 0.1458  loss_ce_6: 0.09994  loss_mask_6: 0.1581  loss_dice_6: 0.1429  loss_ce_7: 0.0996  loss_mask_7: 0.152  loss_dice_7: 0.1427  loss_ce_8: 0.09964  loss_mask_8: 0.1639  loss_dice_8: 0.1533  time: 0.5820  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:11:28] d2.utils.events INFO:  eta: 0:23:14  iter: 27499  total_loss: 4.703  loss_ce: 0.09938  loss_mask: 0.2077  loss_dice: 0.1418  loss_ce_0: 0.05237  loss_mask_0: 0.2022  loss_dice_0: 0.1422  loss_ce_1: 0.09992  loss_mask_1: 0.2078  loss_dice_1: 0.1417  loss_ce_2: 0.09973  loss_mask_2: 0.2037  loss_dice_2: 0.137  loss_ce_3: 0.09967  loss_mask_3: 0.2131  loss_dice_3: 0.1284  loss_ce_4: 0.09967  loss_mask_4: 0.2024  loss_dice_4: 0.1435  loss_ce_5: 0.09971  loss_mask_5: 0.2088  loss_dice_5: 0.1434  loss_ce_6: 0.09968  loss_mask_6: 0.2067  loss_dice_6: 0.1444  loss_ce_7: 0.09949  loss_mask_7: 0.2128  loss_dice_7: 0.1388  loss_ce_8: 0.09964  loss_mask_8: 0.209  loss_dice_8: 0.1468  time: 0.5820  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:11:39] d2.utils.events INFO:  eta: 0:23:02  iter: 27519  total_loss: 4.787  loss_ce: 0.09949  loss_mask: 0.1734  loss_dice: 0.1472  loss_ce_0: 0.05238  loss_mask_0: 0.1766  loss_dice_0: 0.1526  loss_ce_1: 0.1003  loss_mask_1: 0.1775  loss_dice_1: 0.154  loss_ce_2: 0.09992  loss_mask_2: 0.1785  loss_dice_2: 0.1494  loss_ce_3: 0.09986  loss_mask_3: 0.1714  loss_dice_3: 0.1443  loss_ce_4: 0.09998  loss_mask_4: 0.1762  loss_dice_4: 0.1608  loss_ce_5: 0.09986  loss_mask_5: 0.1775  loss_dice_5: 0.1497  loss_ce_6: 0.09994  loss_mask_6: 0.1746  loss_dice_6: 0.1484  loss_ce_7: 0.09956  loss_mask_7: 0.1757  loss_dice_7: 0.1499  loss_ce_8: 0.09964  loss_mask_8: 0.1869  loss_dice_8: 0.1519  time: 0.5820  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:11:50] d2.utils.events INFO:  eta: 0:22:51  iter: 27539  total_loss: 4.449  loss_ce: 0.09938  loss_mask: 0.1807  loss_dice: 0.1412  loss_ce_0: 0.05237  loss_mask_0: 0.1908  loss_dice_0: 0.1438  loss_ce_1: 0.09994  loss_mask_1: 0.1941  loss_dice_1: 0.1452  loss_ce_2: 0.09966  loss_mask_2: 0.1915  loss_dice_2: 0.1467  loss_ce_3: 0.0996  loss_mask_3: 0.1923  loss_dice_3: 0.1434  loss_ce_4: 0.09975  loss_mask_4: 0.1936  loss_dice_4: 0.1453  loss_ce_5: 0.09956  loss_mask_5: 0.1871  loss_dice_5: 0.1405  loss_ce_6: 0.09964  loss_mask_6: 0.1802  loss_dice_6: 0.1468  loss_ce_7: 0.09945  loss_mask_7: 0.19  loss_dice_7: 0.1422  loss_ce_8: 0.09953  loss_mask_8: 0.1825  loss_dice_8: 0.1371  time: 0.5820  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:12:02] d2.utils.events INFO:  eta: 0:22:40  iter: 27559  total_loss: 4.527  loss_ce: 0.0993  loss_mask: 0.1647  loss_dice: 0.1389  loss_ce_0: 0.05237  loss_mask_0: 0.1671  loss_dice_0: 0.1434  loss_ce_1: 0.09988  loss_mask_1: 0.1617  loss_dice_1: 0.1433  loss_ce_2: 0.09947  loss_mask_2: 0.1642  loss_dice_2: 0.1388  loss_ce_3: 0.09952  loss_mask_3: 0.1667  loss_dice_3: 0.1394  loss_ce_4: 0.09956  loss_mask_4: 0.1621  loss_dice_4: 0.1354  loss_ce_5: 0.09941  loss_mask_5: 0.1609  loss_dice_5: 0.1387  loss_ce_6: 0.09953  loss_mask_6: 0.1646  loss_dice_6: 0.1391  loss_ce_7: 0.09941  loss_mask_7: 0.1708  loss_dice_7: 0.1368  loss_ce_8: 0.09956  loss_mask_8: 0.1661  loss_dice_8: 0.1399  time: 0.5820  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:12:13] d2.utils.events INFO:  eta: 0:22:29  iter: 27579  total_loss: 4.599  loss_ce: 0.1296  loss_mask: 0.1793  loss_dice: 0.1434  loss_ce_0: 0.06366  loss_mask_0: 0.1893  loss_dice_0: 0.1476  loss_ce_1: 0.1295  loss_mask_1: 0.1781  loss_dice_1: 0.141  loss_ce_2: 0.1294  loss_mask_2: 0.1835  loss_dice_2: 0.1466  loss_ce_3: 0.1295  loss_mask_3: 0.1837  loss_dice_3: 0.1446  loss_ce_4: 0.1296  loss_mask_4: 0.1865  loss_dice_4: 0.1404  loss_ce_5: 0.1297  loss_mask_5: 0.1813  loss_dice_5: 0.1419  loss_ce_6: 0.1294  loss_mask_6: 0.1852  loss_dice_6: 0.1459  loss_ce_7: 0.1297  loss_mask_7: 0.1909  loss_dice_7: 0.1373  loss_ce_8: 0.1298  loss_mask_8: 0.186  loss_dice_8: 0.1481  time: 0.5819  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 16:12:25] d2.utils.events INFO:  eta: 0:22:17  iter: 27599  total_loss: 4.782  loss_ce: 0.1298  loss_mask: 0.2225  loss_dice: 0.1458  loss_ce_0: 0.06364  loss_mask_0: 0.2096  loss_dice_0: 0.1458  loss_ce_1: 0.1299  loss_mask_1: 0.2071  loss_dice_1: 0.1416  loss_ce_2: 0.1299  loss_mask_2: 0.2141  loss_dice_2: 0.1397  loss_ce_3: 0.1298  loss_mask_3: 0.2048  loss_dice_3: 0.138  loss_ce_4: 0.1298  loss_mask_4: 0.2193  loss_dice_4: 0.1461  loss_ce_5: 0.1298  loss_mask_5: 0.2119  loss_dice_5: 0.1396  loss_ce_6: 0.1299  loss_mask_6: 0.2064  loss_dice_6: 0.1432  loss_ce_7: 0.1298  loss_mask_7: 0.2105  loss_dice_7: 0.1408  loss_ce_8: 0.13  loss_mask_8: 0.2082  loss_dice_8: 0.1442  time: 0.5819  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:12:36] d2.utils.events INFO:  eta: 0:22:06  iter: 27619  total_loss: 4.794  loss_ce: 0.09934  loss_mask: 0.2087  loss_dice: 0.1469  loss_ce_0: 0.05241  loss_mask_0: 0.2141  loss_dice_0: 0.1491  loss_ce_1: 0.09975  loss_mask_1: 0.2222  loss_dice_1: 0.1424  loss_ce_2: 0.09954  loss_mask_2: 0.22  loss_dice_2: 0.1426  loss_ce_3: 0.09964  loss_mask_3: 0.2181  loss_dice_3: 0.1464  loss_ce_4: 0.09968  loss_mask_4: 0.2085  loss_dice_4: 0.1469  loss_ce_5: 0.09956  loss_mask_5: 0.2131  loss_dice_5: 0.1452  loss_ce_6: 0.09956  loss_mask_6: 0.2182  loss_dice_6: 0.1454  loss_ce_7: 0.09941  loss_mask_7: 0.2115  loss_dice_7: 0.1434  loss_ce_8: 0.09941  loss_mask_8: 0.2095  loss_dice_8: 0.1438  time: 0.5819  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:12:48] d2.utils.events INFO:  eta: 0:21:54  iter: 27639  total_loss: 4.541  loss_ce: 0.16  loss_mask: 0.1827  loss_dice: 0.1331  loss_ce_0: 0.0749  loss_mask_0: 0.1854  loss_dice_0: 0.1403  loss_ce_1: 0.1593  loss_mask_1: 0.1744  loss_dice_1: 0.1372  loss_ce_2: 0.1597  loss_mask_2: 0.1859  loss_dice_2: 0.1356  loss_ce_3: 0.1597  loss_mask_3: 0.1777  loss_dice_3: 0.1354  loss_ce_4: 0.1594  loss_mask_4: 0.1847  loss_dice_4: 0.1297  loss_ce_5: 0.1598  loss_mask_5: 0.1813  loss_dice_5: 0.1378  loss_ce_6: 0.1596  loss_mask_6: 0.1757  loss_dice_6: 0.1388  loss_ce_7: 0.1599  loss_mask_7: 0.1725  loss_dice_7: 0.1338  loss_ce_8: 0.1598  loss_mask_8: 0.1828  loss_dice_8: 0.1375  time: 0.5819  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:12:59] d2.utils.events INFO:  eta: 0:21:43  iter: 27659  total_loss: 4.252  loss_ce: 0.09941  loss_mask: 0.1774  loss_dice: 0.1335  loss_ce_0: 0.05239  loss_mask_0: 0.1786  loss_dice_0: 0.1434  loss_ce_1: 0.1  loss_mask_1: 0.1858  loss_dice_1: 0.1414  loss_ce_2: 0.09969  loss_mask_2: 0.1855  loss_dice_2: 0.1374  loss_ce_3: 0.09963  loss_mask_3: 0.186  loss_dice_3: 0.1401  loss_ce_4: 0.09979  loss_mask_4: 0.1786  loss_dice_4: 0.1354  loss_ce_5: 0.09964  loss_mask_5: 0.1774  loss_dice_5: 0.1369  loss_ce_6: 0.09971  loss_mask_6: 0.1763  loss_dice_6: 0.141  loss_ce_7: 0.09952  loss_mask_7: 0.1807  loss_dice_7: 0.1423  loss_ce_8: 0.09968  loss_mask_8: 0.1789  loss_dice_8: 0.1345  time: 0.5819  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:13:10] d2.utils.events INFO:  eta: 0:21:31  iter: 27679  total_loss: 4.7  loss_ce: 0.09919  loss_mask: 0.2221  loss_dice: 0.1309  loss_ce_0: 0.05234  loss_mask_0: 0.2116  loss_dice_0: 0.1254  loss_ce_1: 0.09949  loss_mask_1: 0.22  loss_dice_1: 0.1325  loss_ce_2: 0.09939  loss_mask_2: 0.215  loss_dice_2: 0.1361  loss_ce_3: 0.09933  loss_mask_3: 0.2092  loss_dice_3: 0.1359  loss_ce_4: 0.09941  loss_mask_4: 0.2109  loss_dice_4: 0.1331  loss_ce_5: 0.09934  loss_mask_5: 0.2175  loss_dice_5: 0.1281  loss_ce_6: 0.09937  loss_mask_6: 0.2086  loss_dice_6: 0.1287  loss_ce_7: 0.09919  loss_mask_7: 0.2082  loss_dice_7: 0.1289  loss_ce_8: 0.0993  loss_mask_8: 0.2026  loss_dice_8: 0.1281  time: 0.5819  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:13:22] d2.utils.events INFO:  eta: 0:21:20  iter: 27699  total_loss: 4.167  loss_ce: 0.09897  loss_mask: 0.1619  loss_dice: 0.1444  loss_ce_0: 0.0523  loss_mask_0: 0.1596  loss_dice_0: 0.1444  loss_ce_1: 0.09913  loss_mask_1: 0.15  loss_dice_1: 0.1406  loss_ce_2: 0.09909  loss_mask_2: 0.1551  loss_dice_2: 0.1408  loss_ce_3: 0.09911  loss_mask_3: 0.1618  loss_dice_3: 0.146  loss_ce_4: 0.09911  loss_mask_4: 0.1524  loss_dice_4: 0.1376  loss_ce_5: 0.09896  loss_mask_5: 0.1652  loss_dice_5: 0.1419  loss_ce_6: 0.09919  loss_mask_6: 0.1599  loss_dice_6: 0.1416  loss_ce_7: 0.099  loss_mask_7: 0.1605  loss_dice_7: 0.1511  loss_ce_8: 0.09915  loss_mask_8: 0.1705  loss_dice_8: 0.1406  time: 0.5819  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:13:33] d2.utils.events INFO:  eta: 0:21:08  iter: 27719  total_loss: 4.623  loss_ce: 0.09896  loss_mask: 0.1192  loss_dice: 0.1646  loss_ce_0: 0.05226  loss_mask_0: 0.1244  loss_dice_0: 0.1691  loss_ce_1: 0.0994  loss_mask_1: 0.1187  loss_dice_1: 0.1657  loss_ce_2: 0.09911  loss_mask_2: 0.1252  loss_dice_2: 0.154  loss_ce_3: 0.09907  loss_mask_3: 0.1222  loss_dice_3: 0.1663  loss_ce_4: 0.09911  loss_mask_4: 0.1163  loss_dice_4: 0.1594  loss_ce_5: 0.09904  loss_mask_5: 0.124  loss_dice_5: 0.1674  loss_ce_6: 0.09911  loss_mask_6: 0.1261  loss_dice_6: 0.1669  loss_ce_7: 0.09893  loss_mask_7: 0.1155  loss_dice_7: 0.1612  loss_ce_8: 0.09897  loss_mask_8: 0.1151  loss_dice_8: 0.1619  time: 0.5819  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 16:13:45] d2.utils.events INFO:  eta: 0:20:57  iter: 27739  total_loss: 4.493  loss_ce: 0.1298  loss_mask: 0.1618  loss_dice: 0.1345  loss_ce_0: 0.06368  loss_mask_0: 0.1648  loss_dice_0: 0.1363  loss_ce_1: 0.1295  loss_mask_1: 0.1633  loss_dice_1: 0.1327  loss_ce_2: 0.1297  loss_mask_2: 0.1706  loss_dice_2: 0.1376  loss_ce_3: 0.1297  loss_mask_3: 0.1604  loss_dice_3: 0.1364  loss_ce_4: 0.1296  loss_mask_4: 0.1747  loss_dice_4: 0.143  loss_ce_5: 0.1296  loss_mask_5: 0.1627  loss_dice_5: 0.1337  loss_ce_6: 0.1297  loss_mask_6: 0.1738  loss_dice_6: 0.1444  loss_ce_7: 0.1297  loss_mask_7: 0.17  loss_dice_7: 0.1418  loss_ce_8: 0.1298  loss_mask_8: 0.1587  loss_dice_8: 0.134  time: 0.5819  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:13:56] d2.utils.events INFO:  eta: 0:20:48  iter: 27759  total_loss: 4.722  loss_ce: 0.1299  loss_mask: 0.2071  loss_dice: 0.1336  loss_ce_0: 0.06368  loss_mask_0: 0.2157  loss_dice_0: 0.1403  loss_ce_1: 0.1299  loss_mask_1: 0.2232  loss_dice_1: 0.1393  loss_ce_2: 0.1298  loss_mask_2: 0.2136  loss_dice_2: 0.1428  loss_ce_3: 0.1299  loss_mask_3: 0.208  loss_dice_3: 0.14  loss_ce_4: 0.1298  loss_mask_4: 0.2084  loss_dice_4: 0.1401  loss_ce_5: 0.1299  loss_mask_5: 0.2112  loss_dice_5: 0.1375  loss_ce_6: 0.1299  loss_mask_6: 0.2025  loss_dice_6: 0.14  loss_ce_7: 0.1299  loss_mask_7: 0.2064  loss_dice_7: 0.1382  loss_ce_8: 0.1298  loss_mask_8: 0.2083  loss_dice_8: 0.1396  time: 0.5819  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:14:07] d2.utils.events INFO:  eta: 0:20:36  iter: 27779  total_loss: 4.494  loss_ce: 0.09908  loss_mask: 0.19  loss_dice: 0.1465  loss_ce_0: 0.05229  loss_mask_0: 0.1972  loss_dice_0: 0.142  loss_ce_1: 0.0999  loss_mask_1: 0.193  loss_dice_1: 0.1448  loss_ce_2: 0.09943  loss_mask_2: 0.1927  loss_dice_2: 0.1435  loss_ce_3: 0.09934  loss_mask_3: 0.1865  loss_dice_3: 0.1489  loss_ce_4: 0.09949  loss_mask_4: 0.1915  loss_dice_4: 0.1473  loss_ce_5: 0.09919  loss_mask_5: 0.1889  loss_dice_5: 0.1463  loss_ce_6: 0.09952  loss_mask_6: 0.1891  loss_dice_6: 0.1435  loss_ce_7: 0.09911  loss_mask_7: 0.1864  loss_dice_7: 0.1405  loss_ce_8: 0.09923  loss_mask_8: 0.1831  loss_dice_8: 0.1441  time: 0.5818  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:14:19] d2.utils.events INFO:  eta: 0:20:23  iter: 27799  total_loss: 4.677  loss_ce: 0.09904  loss_mask: 0.1998  loss_dice: 0.137  loss_ce_0: 0.05229  loss_mask_0: 0.2127  loss_dice_0: 0.1309  loss_ce_1: 0.09953  loss_mask_1: 0.2051  loss_dice_1: 0.1323  loss_ce_2: 0.09921  loss_mask_2: 0.2107  loss_dice_2: 0.1354  loss_ce_3: 0.09907  loss_mask_3: 0.2085  loss_dice_3: 0.1375  loss_ce_4: 0.09926  loss_mask_4: 0.2109  loss_dice_4: 0.1393  loss_ce_5: 0.09919  loss_mask_5: 0.1997  loss_dice_5: 0.1389  loss_ce_6: 0.09923  loss_mask_6: 0.2056  loss_dice_6: 0.1396  loss_ce_7: 0.099  loss_mask_7: 0.1916  loss_dice_7: 0.1422  loss_ce_8: 0.09915  loss_mask_8: 0.1975  loss_dice_8: 0.1355  time: 0.5818  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:14:30] d2.utils.events INFO:  eta: 0:20:13  iter: 27819  total_loss: 4.792  loss_ce: 0.09908  loss_mask: 0.1925  loss_dice: 0.1404  loss_ce_0: 0.05229  loss_mask_0: 0.1909  loss_dice_0: 0.1337  loss_ce_1: 0.09958  loss_mask_1: 0.192  loss_dice_1: 0.1423  loss_ce_2: 0.09936  loss_mask_2: 0.186  loss_dice_2: 0.1363  loss_ce_3: 0.0993  loss_mask_3: 0.1958  loss_dice_3: 0.1335  loss_ce_4: 0.0993  loss_mask_4: 0.1965  loss_dice_4: 0.1318  loss_ce_5: 0.09926  loss_mask_5: 0.2044  loss_dice_5: 0.1388  loss_ce_6: 0.09945  loss_mask_6: 0.1867  loss_dice_6: 0.1358  loss_ce_7: 0.09908  loss_mask_7: 0.1954  loss_dice_7: 0.1341  loss_ce_8: 0.09923  loss_mask_8: 0.1888  loss_dice_8: 0.1309  time: 0.5818  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:14:42] d2.utils.events INFO:  eta: 0:20:05  iter: 27839  total_loss: 4.656  loss_ce: 0.1298  loss_mask: 0.1807  loss_dice: 0.1557  loss_ce_0: 0.06367  loss_mask_0: 0.186  loss_dice_0: 0.1536  loss_ce_1: 0.1294  loss_mask_1: 0.1942  loss_dice_1: 0.1562  loss_ce_2: 0.1295  loss_mask_2: 0.1906  loss_dice_2: 0.1573  loss_ce_3: 0.1296  loss_mask_3: 0.1966  loss_dice_3: 0.1578  loss_ce_4: 0.1296  loss_mask_4: 0.189  loss_dice_4: 0.1587  loss_ce_5: 0.1296  loss_mask_5: 0.1875  loss_dice_5: 0.1553  loss_ce_6: 0.1295  loss_mask_6: 0.1895  loss_dice_6: 0.1585  loss_ce_7: 0.1296  loss_mask_7: 0.1946  loss_dice_7: 0.158  loss_ce_8: 0.1296  loss_mask_8: 0.1906  loss_dice_8: 0.1555  time: 0.5818  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:14:53] d2.utils.events INFO:  eta: 0:19:56  iter: 27859  total_loss: 4.631  loss_ce: 0.13  loss_mask: 0.1871  loss_dice: 0.1332  loss_ce_0: 0.06367  loss_mask_0: 0.1856  loss_dice_0: 0.1355  loss_ce_1: 0.1303  loss_mask_1: 0.1922  loss_dice_1: 0.1306  loss_ce_2: 0.13  loss_mask_2: 0.197  loss_dice_2: 0.133  loss_ce_3: 0.1301  loss_mask_3: 0.1915  loss_dice_3: 0.1266  loss_ce_4: 0.13  loss_mask_4: 0.1818  loss_dice_4: 0.1337  loss_ce_5: 0.1298  loss_mask_5: 0.1865  loss_dice_5: 0.1335  loss_ce_6: 0.1301  loss_mask_6: 0.1877  loss_dice_6: 0.1278  loss_ce_7: 0.13  loss_mask_7: 0.1886  loss_dice_7: 0.1377  loss_ce_8: 0.1299  loss_mask_8: 0.1904  loss_dice_8: 0.1299  time: 0.5818  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:15:05] d2.utils.events INFO:  eta: 0:19:46  iter: 27879  total_loss: 4.411  loss_ce: 0.099  loss_mask: 0.1579  loss_dice: 0.136  loss_ce_0: 0.05232  loss_mask_0: 0.1618  loss_dice_0: 0.1354  loss_ce_1: 0.0991  loss_mask_1: 0.1609  loss_dice_1: 0.1367  loss_ce_2: 0.09917  loss_mask_2: 0.16  loss_dice_2: 0.1377  loss_ce_3: 0.09911  loss_mask_3: 0.1639  loss_dice_3: 0.1344  loss_ce_4: 0.09908  loss_mask_4: 0.1668  loss_dice_4: 0.1424  loss_ce_5: 0.09904  loss_mask_5: 0.1628  loss_dice_5: 0.1368  loss_ce_6: 0.09915  loss_mask_6: 0.1581  loss_dice_6: 0.1367  loss_ce_7: 0.09904  loss_mask_7: 0.1585  loss_dice_7: 0.1349  loss_ce_8: 0.09911  loss_mask_8: 0.1619  loss_dice_8: 0.1367  time: 0.5818  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:15:16] d2.utils.events INFO:  eta: 0:19:36  iter: 27899  total_loss: 4.465  loss_ce: 0.1605  loss_mask: 0.1811  loss_dice: 0.158  loss_ce_0: 0.07502  loss_mask_0: 0.1838  loss_dice_0: 0.1608  loss_ce_1: 0.1601  loss_mask_1: 0.1889  loss_dice_1: 0.1633  loss_ce_2: 0.1603  loss_mask_2: 0.1826  loss_dice_2: 0.1626  loss_ce_3: 0.1603  loss_mask_3: 0.1764  loss_dice_3: 0.1597  loss_ce_4: 0.1602  loss_mask_4: 0.183  loss_dice_4: 0.1598  loss_ce_5: 0.1603  loss_mask_5: 0.1859  loss_dice_5: 0.1575  loss_ce_6: 0.1601  loss_mask_6: 0.1781  loss_dice_6: 0.1561  loss_ce_7: 0.1605  loss_mask_7: 0.1744  loss_dice_7: 0.1628  loss_ce_8: 0.1599  loss_mask_8: 0.1818  loss_dice_8: 0.1598  time: 0.5818  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:15:28] d2.utils.events INFO:  eta: 0:19:26  iter: 27919  total_loss: 5.159  loss_ce: 0.1604  loss_mask: 0.1853  loss_dice: 0.1389  loss_ce_0: 0.07499  loss_mask_0: 0.2005  loss_dice_0: 0.1409  loss_ce_1: 0.16  loss_mask_1: 0.1906  loss_dice_1: 0.1418  loss_ce_2: 0.1602  loss_mask_2: 0.2048  loss_dice_2: 0.1402  loss_ce_3: 0.1601  loss_mask_3: 0.1913  loss_dice_3: 0.1399  loss_ce_4: 0.16  loss_mask_4: 0.199  loss_dice_4: 0.1388  loss_ce_5: 0.1602  loss_mask_5: 0.1986  loss_dice_5: 0.1418  loss_ce_6: 0.16  loss_mask_6: 0.2076  loss_dice_6: 0.14  loss_ce_7: 0.1603  loss_mask_7: 0.1913  loss_dice_7: 0.14  loss_ce_8: 0.1603  loss_mask_8: 0.1949  loss_dice_8: 0.1419  time: 0.5818  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:15:39] d2.utils.events INFO:  eta: 0:19:17  iter: 27939  total_loss: 4.25  loss_ce: 0.09938  loss_mask: 0.1688  loss_dice: 0.1394  loss_ce_0: 0.05241  loss_mask_0: 0.1673  loss_dice_0: 0.1456  loss_ce_1: 0.09971  loss_mask_1: 0.1668  loss_dice_1: 0.1437  loss_ce_2: 0.09956  loss_mask_2: 0.1671  loss_dice_2: 0.1418  loss_ce_3: 0.0996  loss_mask_3: 0.1678  loss_dice_3: 0.1438  loss_ce_4: 0.09964  loss_mask_4: 0.1664  loss_dice_4: 0.1403  loss_ce_5: 0.09956  loss_mask_5: 0.1626  loss_dice_5: 0.1368  loss_ce_6: 0.0996  loss_mask_6: 0.1665  loss_dice_6: 0.1379  loss_ce_7: 0.09949  loss_mask_7: 0.1656  loss_dice_7: 0.1427  loss_ce_8: 0.09945  loss_mask_8: 0.1687  loss_dice_8: 0.1417  time: 0.5818  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:15:50] d2.utils.events INFO:  eta: 0:19:06  iter: 27959  total_loss: 4.304  loss_ce: 0.09938  loss_mask: 0.1619  loss_dice: 0.141  loss_ce_0: 0.05241  loss_mask_0: 0.1578  loss_dice_0: 0.1442  loss_ce_1: 0.1  loss_mask_1: 0.155  loss_dice_1: 0.1457  loss_ce_2: 0.09975  loss_mask_2: 0.1629  loss_dice_2: 0.1461  loss_ce_3: 0.09967  loss_mask_3: 0.1605  loss_dice_3: 0.1423  loss_ce_4: 0.09979  loss_mask_4: 0.161  loss_dice_4: 0.1419  loss_ce_5: 0.09964  loss_mask_5: 0.1554  loss_dice_5: 0.1435  loss_ce_6: 0.09975  loss_mask_6: 0.1707  loss_dice_6: 0.1497  loss_ce_7: 0.09949  loss_mask_7: 0.1599  loss_dice_7: 0.1464  loss_ce_8: 0.09956  loss_mask_8: 0.1702  loss_dice_8: 0.1402  time: 0.5818  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:16:02] d2.utils.events INFO:  eta: 0:18:58  iter: 27979  total_loss: 4.889  loss_ce: 0.09915  loss_mask: 0.1817  loss_dice: 0.1771  loss_ce_0: 0.05234  loss_mask_0: 0.1899  loss_dice_0: 0.1715  loss_ce_1: 0.09949  loss_mask_1: 0.1748  loss_dice_1: 0.1697  loss_ce_2: 0.09928  loss_mask_2: 0.1858  loss_dice_2: 0.1667  loss_ce_3: 0.09933  loss_mask_3: 0.1886  loss_dice_3: 0.1699  loss_ce_4: 0.09941  loss_mask_4: 0.1831  loss_dice_4: 0.1766  loss_ce_5: 0.09926  loss_mask_5: 0.1896  loss_dice_5: 0.182  loss_ce_6: 0.0993  loss_mask_6: 0.1909  loss_dice_6: 0.1651  loss_ce_7: 0.0993  loss_mask_7: 0.1743  loss_dice_7: 0.1783  loss_ce_8: 0.09927  loss_mask_8: 0.1906  loss_dice_8: 0.1711  time: 0.5818  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:16:13] d2.utils.events INFO:  eta: 0:18:48  iter: 27999  total_loss: 4.436  loss_ce: 0.1297  loss_mask: 0.1748  loss_dice: 0.1465  loss_ce_0: 0.06366  loss_mask_0: 0.1604  loss_dice_0: 0.1362  loss_ce_1: 0.1292  loss_mask_1: 0.1675  loss_dice_1: 0.1338  loss_ce_2: 0.1296  loss_mask_2: 0.1704  loss_dice_2: 0.1353  loss_ce_3: 0.1296  loss_mask_3: 0.1712  loss_dice_3: 0.1404  loss_ce_4: 0.1296  loss_mask_4: 0.1656  loss_dice_4: 0.1423  loss_ce_5: 0.1295  loss_mask_5: 0.171  loss_dice_5: 0.1394  loss_ce_6: 0.1295  loss_mask_6: 0.155  loss_dice_6: 0.1385  loss_ce_7: 0.1296  loss_mask_7: 0.1598  loss_dice_7: 0.142  loss_ce_8: 0.1293  loss_mask_8: 0.173  loss_dice_8: 0.146  time: 0.5818  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:16:25] d2.utils.events INFO:  eta: 0:18:38  iter: 28019  total_loss: 4.425  loss_ce: 0.09915  loss_mask: 0.2119  loss_dice: 0.1336  loss_ce_0: 0.05234  loss_mask_0: 0.2089  loss_dice_0: 0.1339  loss_ce_1: 0.09928  loss_mask_1: 0.2055  loss_dice_1: 0.1338  loss_ce_2: 0.09928  loss_mask_2: 0.2281  loss_dice_2: 0.142  loss_ce_3: 0.09937  loss_mask_3: 0.2089  loss_dice_3: 0.1348  loss_ce_4: 0.09941  loss_mask_4: 0.2161  loss_dice_4: 0.1392  loss_ce_5: 0.09926  loss_mask_5: 0.2143  loss_dice_5: 0.1348  loss_ce_6: 0.0993  loss_mask_6: 0.1995  loss_dice_6: 0.1396  loss_ce_7: 0.09922  loss_mask_7: 0.2027  loss_dice_7: 0.1369  loss_ce_8: 0.09919  loss_mask_8: 0.2164  loss_dice_8: 0.1391  time: 0.5818  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:16:36] d2.utils.events INFO:  eta: 0:18:28  iter: 28039  total_loss: 4.567  loss_ce: 0.1298  loss_mask: 0.168  loss_dice: 0.1428  loss_ce_0: 0.06368  loss_mask_0: 0.1706  loss_dice_0: 0.1458  loss_ce_1: 0.1297  loss_mask_1: 0.1726  loss_dice_1: 0.1489  loss_ce_2: 0.1298  loss_mask_2: 0.1741  loss_dice_2: 0.1448  loss_ce_3: 0.1299  loss_mask_3: 0.1595  loss_dice_3: 0.1427  loss_ce_4: 0.13  loss_mask_4: 0.1752  loss_dice_4: 0.1503  loss_ce_5: 0.1298  loss_mask_5: 0.1666  loss_dice_5: 0.1408  loss_ce_6: 0.1299  loss_mask_6: 0.1674  loss_dice_6: 0.139  loss_ce_7: 0.1298  loss_mask_7: 0.1651  loss_dice_7: 0.1472  loss_ce_8: 0.1298  loss_mask_8: 0.1816  loss_dice_8: 0.1454  time: 0.5817  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:16:48] d2.utils.events INFO:  eta: 0:18:18  iter: 28059  total_loss: 4.879  loss_ce: 0.1298  loss_mask: 0.2004  loss_dice: 0.1321  loss_ce_0: 0.06367  loss_mask_0: 0.2032  loss_dice_0: 0.1298  loss_ce_1: 0.1297  loss_mask_1: 0.2028  loss_dice_1: 0.13  loss_ce_2: 0.1299  loss_mask_2: 0.2077  loss_dice_2: 0.13  loss_ce_3: 0.1297  loss_mask_3: 0.2066  loss_dice_3: 0.1284  loss_ce_4: 0.1298  loss_mask_4: 0.1978  loss_dice_4: 0.1257  loss_ce_5: 0.1296  loss_mask_5: 0.2029  loss_dice_5: 0.1292  loss_ce_6: 0.1298  loss_mask_6: 0.2047  loss_dice_6: 0.1307  loss_ce_7: 0.1298  loss_mask_7: 0.2118  loss_dice_7: 0.1283  loss_ce_8: 0.1295  loss_mask_8: 0.1979  loss_dice_8: 0.1285  time: 0.5817  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 16:16:59] d2.utils.events INFO:  eta: 0:18:08  iter: 28079  total_loss: 4.138  loss_ce: 0.09919  loss_mask: 0.1507  loss_dice: 0.1314  loss_ce_0: 0.0523  loss_mask_0: 0.1516  loss_dice_0: 0.1314  loss_ce_1: 0.1  loss_mask_1: 0.1542  loss_dice_1: 0.1301  loss_ce_2: 0.09956  loss_mask_2: 0.1616  loss_dice_2: 0.1296  loss_ce_3: 0.09941  loss_mask_3: 0.1569  loss_dice_3: 0.1301  loss_ce_4: 0.0996  loss_mask_4: 0.1607  loss_dice_4: 0.1318  loss_ce_5: 0.09941  loss_mask_5: 0.1589  loss_dice_5: 0.127  loss_ce_6: 0.09952  loss_mask_6: 0.1615  loss_dice_6: 0.1301  loss_ce_7: 0.09923  loss_mask_7: 0.1552  loss_dice_7: 0.1279  loss_ce_8: 0.09938  loss_mask_8: 0.16  loss_dice_8: 0.1286  time: 0.5817  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:17:11] d2.utils.events INFO:  eta: 0:17:58  iter: 28099  total_loss: 4.675  loss_ce: 0.09893  loss_mask: 0.1659  loss_dice: 0.1629  loss_ce_0: 0.0523  loss_mask_0: 0.1682  loss_dice_0: 0.165  loss_ce_1: 0.09908  loss_mask_1: 0.1586  loss_dice_1: 0.1666  loss_ce_2: 0.09893  loss_mask_2: 0.1776  loss_dice_2: 0.1691  loss_ce_3: 0.09904  loss_mask_3: 0.166  loss_dice_3: 0.172  loss_ce_4: 0.09896  loss_mask_4: 0.1674  loss_dice_4: 0.1629  loss_ce_5: 0.09889  loss_mask_5: 0.1615  loss_dice_5: 0.1676  loss_ce_6: 0.099  loss_mask_6: 0.1715  loss_dice_6: 0.1687  loss_ce_7: 0.09889  loss_mask_7: 0.1688  loss_dice_7: 0.1719  loss_ce_8: 0.09897  loss_mask_8: 0.1557  loss_dice_8: 0.1704  time: 0.5817  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:17:22] d2.utils.events INFO:  eta: 0:17:49  iter: 28119  total_loss: 4.805  loss_ce: 0.09885  loss_mask: 0.19  loss_dice: 0.1473  loss_ce_0: 0.05228  loss_mask_0: 0.1826  loss_dice_0: 0.147  loss_ce_1: 0.09917  loss_mask_1: 0.183  loss_dice_1: 0.148  loss_ce_2: 0.09902  loss_mask_2: 0.1791  loss_dice_2: 0.1559  loss_ce_3: 0.09907  loss_mask_3: 0.1874  loss_dice_3: 0.1519  loss_ce_4: 0.09904  loss_mask_4: 0.1877  loss_dice_4: 0.1476  loss_ce_5: 0.09904  loss_mask_5: 0.1922  loss_dice_5: 0.1501  loss_ce_6: 0.09911  loss_mask_6: 0.1858  loss_dice_6: 0.1445  loss_ce_7: 0.09889  loss_mask_7: 0.1798  loss_dice_7: 0.1421  loss_ce_8: 0.099  loss_mask_8: 0.1817  loss_dice_8: 0.1463  time: 0.5817  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:17:34] d2.utils.events INFO:  eta: 0:17:40  iter: 28139  total_loss: 4.286  loss_ce: 0.1297  loss_mask: 0.1632  loss_dice: 0.1459  loss_ce_0: 0.06367  loss_mask_0: 0.1631  loss_dice_0: 0.1483  loss_ce_1: 0.1295  loss_mask_1: 0.1562  loss_dice_1: 0.1428  loss_ce_2: 0.1296  loss_mask_2: 0.1544  loss_dice_2: 0.1454  loss_ce_3: 0.1297  loss_mask_3: 0.1561  loss_dice_3: 0.143  loss_ce_4: 0.1297  loss_mask_4: 0.1654  loss_dice_4: 0.1419  loss_ce_5: 0.1298  loss_mask_5: 0.1653  loss_dice_5: 0.1472  loss_ce_6: 0.1296  loss_mask_6: 0.1617  loss_dice_6: 0.145  loss_ce_7: 0.1298  loss_mask_7: 0.1604  loss_dice_7: 0.1446  loss_ce_8: 0.1294  loss_mask_8: 0.1588  loss_dice_8: 0.1399  time: 0.5817  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:17:45] d2.utils.events INFO:  eta: 0:17:29  iter: 28159  total_loss: 4.619  loss_ce: 0.1297  loss_mask: 0.186  loss_dice: 0.1515  loss_ce_0: 0.06367  loss_mask_0: 0.1834  loss_dice_0: 0.1512  loss_ce_1: 0.1294  loss_mask_1: 0.1808  loss_dice_1: 0.1414  loss_ce_2: 0.1296  loss_mask_2: 0.1913  loss_dice_2: 0.1484  loss_ce_3: 0.1297  loss_mask_3: 0.2014  loss_dice_3: 0.1513  loss_ce_4: 0.1296  loss_mask_4: 0.1824  loss_dice_4: 0.1505  loss_ce_5: 0.1295  loss_mask_5: 0.185  loss_dice_5: 0.1544  loss_ce_6: 0.1297  loss_mask_6: 0.1807  loss_dice_6: 0.1435  loss_ce_7: 0.1297  loss_mask_7: 0.1896  loss_dice_7: 0.1526  loss_ce_8: 0.1296  loss_mask_8: 0.177  loss_dice_8: 0.1505  time: 0.5817  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:17:57] d2.utils.events INFO:  eta: 0:17:17  iter: 28179  total_loss: 4.633  loss_ce: 0.1604  loss_mask: 0.1351  loss_dice: 0.1514  loss_ce_0: 0.07503  loss_mask_0: 0.1416  loss_dice_0: 0.1527  loss_ce_1: 0.1596  loss_mask_1: 0.1346  loss_dice_1: 0.1432  loss_ce_2: 0.1602  loss_mask_2: 0.1312  loss_dice_2: 0.1446  loss_ce_3: 0.1602  loss_mask_3: 0.1302  loss_dice_3: 0.1481  loss_ce_4: 0.16  loss_mask_4: 0.1411  loss_dice_4: 0.155  loss_ce_5: 0.1603  loss_mask_5: 0.1443  loss_dice_5: 0.1585  loss_ce_6: 0.1601  loss_mask_6: 0.1395  loss_dice_6: 0.1532  loss_ce_7: 0.1604  loss_mask_7: 0.1375  loss_dice_7: 0.1433  loss_ce_8: 0.1604  loss_mask_8: 0.1369  loss_dice_8: 0.1421  time: 0.5817  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:18:08] d2.utils.events INFO:  eta: 0:17:05  iter: 28199  total_loss: 4.6  loss_ce: 0.09904  loss_mask: 0.2217  loss_dice: 0.1558  loss_ce_0: 0.05231  loss_mask_0: 0.2159  loss_dice_0: 0.1587  loss_ce_1: 0.09928  loss_mask_1: 0.2126  loss_dice_1: 0.1527  loss_ce_2: 0.09917  loss_mask_2: 0.21  loss_dice_2: 0.1596  loss_ce_3: 0.09911  loss_mask_3: 0.223  loss_dice_3: 0.1645  loss_ce_4: 0.09926  loss_mask_4: 0.2143  loss_dice_4: 0.1554  loss_ce_5: 0.09911  loss_mask_5: 0.2177  loss_dice_5: 0.1627  loss_ce_6: 0.09915  loss_mask_6: 0.2112  loss_dice_6: 0.1635  loss_ce_7: 0.09904  loss_mask_7: 0.2221  loss_dice_7: 0.1598  loss_ce_8: 0.09912  loss_mask_8: 0.221  loss_dice_8: 0.1541  time: 0.5817  data_time: 0.0022  lr: 1e-06  max_mem: 2811M
[07/11 16:18:19] d2.utils.events INFO:  eta: 0:16:54  iter: 28219  total_loss: 4.716  loss_ce: 0.1297  loss_mask: 0.184  loss_dice: 0.1346  loss_ce_0: 0.06367  loss_mask_0: 0.1958  loss_dice_0: 0.1356  loss_ce_1: 0.1292  loss_mask_1: 0.1792  loss_dice_1: 0.1368  loss_ce_2: 0.1294  loss_mask_2: 0.1939  loss_dice_2: 0.1379  loss_ce_3: 0.1295  loss_mask_3: 0.1868  loss_dice_3: 0.1327  loss_ce_4: 0.1295  loss_mask_4: 0.1855  loss_dice_4: 0.1385  loss_ce_5: 0.1295  loss_mask_5: 0.1858  loss_dice_5: 0.1356  loss_ce_6: 0.1294  loss_mask_6: 0.195  loss_dice_6: 0.1286  loss_ce_7: 0.1296  loss_mask_7: 0.1816  loss_dice_7: 0.13  loss_ce_8: 0.1296  loss_mask_8: 0.1884  loss_dice_8: 0.1346  time: 0.5817  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:18:31] d2.utils.events INFO:  eta: 0:16:43  iter: 28239  total_loss: 4.684  loss_ce: 0.1297  loss_mask: 0.1461  loss_dice: 0.1528  loss_ce_0: 0.06367  loss_mask_0: 0.1577  loss_dice_0: 0.1529  loss_ce_1: 0.129  loss_mask_1: 0.1593  loss_dice_1: 0.151  loss_ce_2: 0.1294  loss_mask_2: 0.1619  loss_dice_2: 0.1568  loss_ce_3: 0.1295  loss_mask_3: 0.157  loss_dice_3: 0.1608  loss_ce_4: 0.1294  loss_mask_4: 0.163  loss_dice_4: 0.1542  loss_ce_5: 0.1296  loss_mask_5: 0.1565  loss_dice_5: 0.1578  loss_ce_6: 0.1294  loss_mask_6: 0.1524  loss_dice_6: 0.1513  loss_ce_7: 0.1296  loss_mask_7: 0.1518  loss_dice_7: 0.1642  loss_ce_8: 0.1296  loss_mask_8: 0.1589  loss_dice_8: 0.1545  time: 0.5817  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:18:42] d2.utils.events INFO:  eta: 0:16:30  iter: 28259  total_loss: 4.393  loss_ce: 0.09896  loss_mask: 0.165  loss_dice: 0.1409  loss_ce_0: 0.05231  loss_mask_0: 0.16  loss_dice_0: 0.1425  loss_ce_1: 0.09915  loss_mask_1: 0.1488  loss_dice_1: 0.1407  loss_ce_2: 0.099  loss_mask_2: 0.1648  loss_dice_2: 0.1413  loss_ce_3: 0.09904  loss_mask_3: 0.1534  loss_dice_3: 0.1469  loss_ce_4: 0.09908  loss_mask_4: 0.1634  loss_dice_4: 0.1403  loss_ce_5: 0.09896  loss_mask_5: 0.1589  loss_dice_5: 0.1403  loss_ce_6: 0.099  loss_mask_6: 0.1653  loss_dice_6: 0.1445  loss_ce_7: 0.099  loss_mask_7: 0.1634  loss_dice_7: 0.1425  loss_ce_8: 0.09904  loss_mask_8: 0.1625  loss_dice_8: 0.1396  time: 0.5817  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:18:54] d2.utils.events INFO:  eta: 0:16:18  iter: 28279  total_loss: 4.631  loss_ce: 0.09904  loss_mask: 0.2173  loss_dice: 0.1431  loss_ce_0: 0.05231  loss_mask_0: 0.2055  loss_dice_0: 0.1419  loss_ce_1: 0.09936  loss_mask_1: 0.2033  loss_dice_1: 0.1435  loss_ce_2: 0.09928  loss_mask_2: 0.2124  loss_dice_2: 0.138  loss_ce_3: 0.09922  loss_mask_3: 0.2122  loss_dice_3: 0.1392  loss_ce_4: 0.09923  loss_mask_4: 0.2146  loss_dice_4: 0.1392  loss_ce_5: 0.09911  loss_mask_5: 0.2124  loss_dice_5: 0.1389  loss_ce_6: 0.09915  loss_mask_6: 0.2181  loss_dice_6: 0.1412  loss_ce_7: 0.09904  loss_mask_7: 0.2041  loss_dice_7: 0.1353  loss_ce_8: 0.09915  loss_mask_8: 0.2137  loss_dice_8: 0.1431  time: 0.5817  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:19:05] d2.utils.events INFO:  eta: 0:16:07  iter: 28299  total_loss: 4.674  loss_ce: 0.09908  loss_mask: 0.1605  loss_dice: 0.146  loss_ce_0: 0.0523  loss_mask_0: 0.1506  loss_dice_0: 0.1475  loss_ce_1: 0.09953  loss_mask_1: 0.1584  loss_dice_1: 0.1489  loss_ce_2: 0.09924  loss_mask_2: 0.1516  loss_dice_2: 0.1509  loss_ce_3: 0.09922  loss_mask_3: 0.1476  loss_dice_3: 0.1444  loss_ce_4: 0.0993  loss_mask_4: 0.1664  loss_dice_4: 0.151  loss_ce_5: 0.09919  loss_mask_5: 0.157  loss_dice_5: 0.1503  loss_ce_6: 0.09934  loss_mask_6: 0.1608  loss_dice_6: 0.1434  loss_ce_7: 0.099  loss_mask_7: 0.1466  loss_dice_7: 0.143  loss_ce_8: 0.09911  loss_mask_8: 0.1592  loss_dice_8: 0.1477  time: 0.5817  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 16:19:16] d2.utils.events INFO:  eta: 0:15:55  iter: 28319  total_loss: 4.464  loss_ce: 0.09885  loss_mask: 0.1936  loss_dice: 0.1362  loss_ce_0: 0.0523  loss_mask_0: 0.1997  loss_dice_0: 0.1343  loss_ce_1: 0.09898  loss_mask_1: 0.1983  loss_dice_1: 0.1395  loss_ce_2: 0.09891  loss_mask_2: 0.199  loss_dice_2: 0.13  loss_ce_3: 0.09889  loss_mask_3: 0.2001  loss_dice_3: 0.1399  loss_ce_4: 0.099  loss_mask_4: 0.2057  loss_dice_4: 0.1378  loss_ce_5: 0.09896  loss_mask_5: 0.2058  loss_dice_5: 0.1378  loss_ce_6: 0.09889  loss_mask_6: 0.1939  loss_dice_6: 0.1378  loss_ce_7: 0.09889  loss_mask_7: 0.1959  loss_dice_7: 0.1366  loss_ce_8: 0.09893  loss_mask_8: 0.1955  loss_dice_8: 0.1304  time: 0.5816  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 16:19:28] d2.utils.events INFO:  eta: 0:15:43  iter: 28339  total_loss: 4.577  loss_ce: 0.1608  loss_mask: 0.1797  loss_dice: 0.1339  loss_ce_0: 0.07507  loss_mask_0: 0.1745  loss_dice_0: 0.1334  loss_ce_1: 0.1601  loss_mask_1: 0.1824  loss_dice_1: 0.1405  loss_ce_2: 0.1603  loss_mask_2: 0.1743  loss_dice_2: 0.1346  loss_ce_3: 0.1605  loss_mask_3: 0.1766  loss_dice_3: 0.1369  loss_ce_4: 0.1605  loss_mask_4: 0.1757  loss_dice_4: 0.1368  loss_ce_5: 0.1607  loss_mask_5: 0.1716  loss_dice_5: 0.1395  loss_ce_6: 0.1605  loss_mask_6: 0.1803  loss_dice_6: 0.1336  loss_ce_7: 0.1607  loss_mask_7: 0.1765  loss_dice_7: 0.1315  loss_ce_8: 0.1606  loss_mask_8: 0.1763  loss_dice_8: 0.1356  time: 0.5816  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:19:39] d2.utils.events INFO:  eta: 0:15:31  iter: 28359  total_loss: 4.635  loss_ce: 0.09878  loss_mask: 0.1882  loss_dice: 0.1537  loss_ce_0: 0.05229  loss_mask_0: 0.1838  loss_dice_0: 0.1489  loss_ce_1: 0.09882  loss_mask_1: 0.18  loss_dice_1: 0.1459  loss_ce_2: 0.09889  loss_mask_2: 0.1743  loss_dice_2: 0.142  loss_ce_3: 0.09889  loss_mask_3: 0.1891  loss_dice_3: 0.1511  loss_ce_4: 0.09881  loss_mask_4: 0.1889  loss_dice_4: 0.1528  loss_ce_5: 0.09889  loss_mask_5: 0.1854  loss_dice_5: 0.1528  loss_ce_6: 0.09896  loss_mask_6: 0.1864  loss_dice_6: 0.1538  loss_ce_7: 0.09874  loss_mask_7: 0.1794  loss_dice_7: 0.1462  loss_ce_8: 0.09889  loss_mask_8: 0.1758  loss_dice_8: 0.1449  time: 0.5816  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:19:51] d2.utils.events INFO:  eta: 0:15:20  iter: 28379  total_loss: 4.667  loss_ce: 0.09889  loss_mask: 0.2352  loss_dice: 0.1451  loss_ce_0: 0.05229  loss_mask_0: 0.2307  loss_dice_0: 0.1435  loss_ce_1: 0.09928  loss_mask_1: 0.2293  loss_dice_1: 0.1435  loss_ce_2: 0.09906  loss_mask_2: 0.2189  loss_dice_2: 0.151  loss_ce_3: 0.099  loss_mask_3: 0.2274  loss_dice_3: 0.1445  loss_ce_4: 0.09911  loss_mask_4: 0.235  loss_dice_4: 0.1428  loss_ce_5: 0.09904  loss_mask_5: 0.2262  loss_dice_5: 0.1464  loss_ce_6: 0.09904  loss_mask_6: 0.2373  loss_dice_6: 0.1526  loss_ce_7: 0.09893  loss_mask_7: 0.2339  loss_dice_7: 0.1424  loss_ce_8: 0.09897  loss_mask_8: 0.2265  loss_dice_8: 0.1478  time: 0.5816  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:20:02] d2.utils.events INFO:  eta: 0:15:09  iter: 28399  total_loss: 4.523  loss_ce: 0.09878  loss_mask: 0.1759  loss_dice: 0.1821  loss_ce_0: 0.05228  loss_mask_0: 0.16  loss_dice_0: 0.1719  loss_ce_1: 0.09876  loss_mask_1: 0.1684  loss_dice_1: 0.1736  loss_ce_2: 0.09874  loss_mask_2: 0.1611  loss_dice_2: 0.1681  loss_ce_3: 0.09889  loss_mask_3: 0.1707  loss_dice_3: 0.1766  loss_ce_4: 0.09874  loss_mask_4: 0.1644  loss_dice_4: 0.1702  loss_ce_5: 0.09874  loss_mask_5: 0.1639  loss_dice_5: 0.1749  loss_ce_6: 0.09885  loss_mask_6: 0.1685  loss_dice_6: 0.1676  loss_ce_7: 0.09867  loss_mask_7: 0.1708  loss_dice_7: 0.1705  loss_ce_8: 0.09882  loss_mask_8: 0.1677  loss_dice_8: 0.1763  time: 0.5816  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:20:14] d2.utils.events INFO:  eta: 0:14:57  iter: 28419  total_loss: 4.542  loss_ce: 0.09867  loss_mask: 0.1815  loss_dice: 0.1461  loss_ce_0: 0.05226  loss_mask_0: 0.1857  loss_dice_0: 0.1454  loss_ce_1: 0.09882  loss_mask_1: 0.1852  loss_dice_1: 0.1432  loss_ce_2: 0.0987  loss_mask_2: 0.1958  loss_dice_2: 0.1507  loss_ce_3: 0.09866  loss_mask_3: 0.181  loss_dice_3: 0.146  loss_ce_4: 0.09881  loss_mask_4: 0.1787  loss_dice_4: 0.1467  loss_ce_5: 0.09867  loss_mask_5: 0.1763  loss_dice_5: 0.1479  loss_ce_6: 0.09874  loss_mask_6: 0.1826  loss_dice_6: 0.1464  loss_ce_7: 0.09867  loss_mask_7: 0.1908  loss_dice_7: 0.1455  loss_ce_8: 0.09874  loss_mask_8: 0.1772  loss_dice_8: 0.1509  time: 0.5816  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:20:25] d2.utils.events INFO:  eta: 0:14:46  iter: 28439  total_loss: 4.657  loss_ce: 0.09867  loss_mask: 0.1942  loss_dice: 0.1455  loss_ce_0: 0.05224  loss_mask_0: 0.1984  loss_dice_0: 0.1497  loss_ce_1: 0.09921  loss_mask_1: 0.1909  loss_dice_1: 0.1489  loss_ce_2: 0.09896  loss_mask_2: 0.1941  loss_dice_2: 0.1471  loss_ce_3: 0.09889  loss_mask_3: 0.1969  loss_dice_3: 0.148  loss_ce_4: 0.099  loss_mask_4: 0.1975  loss_dice_4: 0.1461  loss_ce_5: 0.09881  loss_mask_5: 0.1898  loss_dice_5: 0.1436  loss_ce_6: 0.09896  loss_mask_6: 0.1998  loss_dice_6: 0.1512  loss_ce_7: 0.0987  loss_mask_7: 0.1979  loss_dice_7: 0.1499  loss_ce_8: 0.09885  loss_mask_8: 0.1839  loss_dice_8: 0.1484  time: 0.5816  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:20:37] d2.utils.events INFO:  eta: 0:14:35  iter: 28459  total_loss: 4.483  loss_ce: 0.09878  loss_mask: 0.1676  loss_dice: 0.128  loss_ce_0: 0.05223  loss_mask_0: 0.1575  loss_dice_0: 0.1241  loss_ce_1: 0.09958  loss_mask_1: 0.1647  loss_dice_1: 0.1214  loss_ce_2: 0.09909  loss_mask_2: 0.1745  loss_dice_2: 0.1293  loss_ce_3: 0.09896  loss_mask_3: 0.1732  loss_dice_3: 0.1256  loss_ce_4: 0.09915  loss_mask_4: 0.1659  loss_dice_4: 0.1291  loss_ce_5: 0.09874  loss_mask_5: 0.1795  loss_dice_5: 0.126  loss_ce_6: 0.09911  loss_mask_6: 0.1669  loss_dice_6: 0.1341  loss_ce_7: 0.09874  loss_mask_7: 0.1588  loss_dice_7: 0.1263  loss_ce_8: 0.09893  loss_mask_8: 0.1721  loss_dice_8: 0.1265  time: 0.5816  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:20:48] d2.utils.events INFO:  eta: 0:14:24  iter: 28479  total_loss: 4.794  loss_ce: 0.09852  loss_mask: 0.1923  loss_dice: 0.1514  loss_ce_0: 0.05222  loss_mask_0: 0.1959  loss_dice_0: 0.1496  loss_ce_1: 0.09841  loss_mask_1: 0.1855  loss_dice_1: 0.1526  loss_ce_2: 0.0985  loss_mask_2: 0.1919  loss_dice_2: 0.1542  loss_ce_3: 0.09859  loss_mask_3: 0.1883  loss_dice_3: 0.1539  loss_ce_4: 0.09855  loss_mask_4: 0.1879  loss_dice_4: 0.1541  loss_ce_5: 0.09852  loss_mask_5: 0.1844  loss_dice_5: 0.1518  loss_ce_6: 0.09855  loss_mask_6: 0.189  loss_dice_6: 0.158  loss_ce_7: 0.09848  loss_mask_7: 0.1854  loss_dice_7: 0.1646  loss_ce_8: 0.09859  loss_mask_8: 0.1875  loss_dice_8: 0.1625  time: 0.5816  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:20:59] d2.utils.events INFO:  eta: 0:14:13  iter: 28499  total_loss: 4.937  loss_ce: 0.1612  loss_mask: 0.171  loss_dice: 0.1422  loss_ce_0: 0.07517  loss_mask_0: 0.1701  loss_dice_0: 0.1342  loss_ce_1: 0.1603  loss_mask_1: 0.1674  loss_dice_1: 0.1377  loss_ce_2: 0.1608  loss_mask_2: 0.1712  loss_dice_2: 0.1394  loss_ce_3: 0.1611  loss_mask_3: 0.1716  loss_dice_3: 0.139  loss_ce_4: 0.1609  loss_mask_4: 0.1701  loss_dice_4: 0.142  loss_ce_5: 0.1609  loss_mask_5: 0.1788  loss_dice_5: 0.1358  loss_ce_6: 0.161  loss_mask_6: 0.1618  loss_dice_6: 0.1434  loss_ce_7: 0.161  loss_mask_7: 0.1733  loss_dice_7: 0.1422  loss_ce_8: 0.161  loss_mask_8: 0.1768  loss_dice_8: 0.1446  time: 0.5816  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:21:11] d2.utils.events INFO:  eta: 0:14:01  iter: 28519  total_loss: 4.451  loss_ce: 0.09848  loss_mask: 0.1794  loss_dice: 0.1375  loss_ce_0: 0.05222  loss_mask_0: 0.1793  loss_dice_0: 0.1376  loss_ce_1: 0.09854  loss_mask_1: 0.1813  loss_dice_1: 0.1282  loss_ce_2: 0.09852  loss_mask_2: 0.1794  loss_dice_2: 0.1362  loss_ce_3: 0.09859  loss_mask_3: 0.1743  loss_dice_3: 0.1273  loss_ce_4: 0.09855  loss_mask_4: 0.1854  loss_dice_4: 0.1332  loss_ce_5: 0.09852  loss_mask_5: 0.1812  loss_dice_5: 0.1281  loss_ce_6: 0.09855  loss_mask_6: 0.1791  loss_dice_6: 0.1334  loss_ce_7: 0.09848  loss_mask_7: 0.1798  loss_dice_7: 0.1288  loss_ce_8: 0.09848  loss_mask_8: 0.1856  loss_dice_8: 0.137  time: 0.5816  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:21:22] d2.utils.events INFO:  eta: 0:13:51  iter: 28539  total_loss: 4.511  loss_ce: 0.1611  loss_mask: 0.177  loss_dice: 0.131  loss_ce_0: 0.07514  loss_mask_0: 0.174  loss_dice_0: 0.1322  loss_ce_1: 0.1607  loss_mask_1: 0.1745  loss_dice_1: 0.1369  loss_ce_2: 0.161  loss_mask_2: 0.1919  loss_dice_2: 0.134  loss_ce_3: 0.1611  loss_mask_3: 0.1714  loss_dice_3: 0.1348  loss_ce_4: 0.1609  loss_mask_4: 0.1848  loss_dice_4: 0.135  loss_ce_5: 0.1612  loss_mask_5: 0.1817  loss_dice_5: 0.1335  loss_ce_6: 0.161  loss_mask_6: 0.1666  loss_dice_6: 0.1322  loss_ce_7: 0.1611  loss_mask_7: 0.1764  loss_dice_7: 0.1354  loss_ce_8: 0.161  loss_mask_8: 0.1749  loss_dice_8: 0.1301  time: 0.5816  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:21:34] d2.utils.events INFO:  eta: 0:13:40  iter: 28559  total_loss: 4.601  loss_ce: 0.161  loss_mask: 0.1399  loss_dice: 0.1703  loss_ce_0: 0.07511  loss_mask_0: 0.131  loss_dice_0: 0.1715  loss_ce_1: 0.1608  loss_mask_1: 0.1355  loss_dice_1: 0.1648  loss_ce_2: 0.161  loss_mask_2: 0.1346  loss_dice_2: 0.1726  loss_ce_3: 0.1608  loss_mask_3: 0.1313  loss_dice_3: 0.1559  loss_ce_4: 0.1608  loss_mask_4: 0.1366  loss_dice_4: 0.1648  loss_ce_5: 0.161  loss_mask_5: 0.1341  loss_dice_5: 0.1755  loss_ce_6: 0.1608  loss_mask_6: 0.1401  loss_dice_6: 0.1779  loss_ce_7: 0.1609  loss_mask_7: 0.1371  loss_dice_7: 0.1729  loss_ce_8: 0.1611  loss_mask_8: 0.1375  loss_dice_8: 0.1783  time: 0.5816  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:21:45] d2.utils.events INFO:  eta: 0:13:29  iter: 28579  total_loss: 4.762  loss_ce: 0.1607  loss_mask: 0.2006  loss_dice: 0.1413  loss_ce_0: 0.07502  loss_mask_0: 0.2034  loss_dice_0: 0.1418  loss_ce_1: 0.1602  loss_mask_1: 0.2  loss_dice_1: 0.144  loss_ce_2: 0.1604  loss_mask_2: 0.2077  loss_dice_2: 0.1511  loss_ce_3: 0.1605  loss_mask_3: 0.208  loss_dice_3: 0.1388  loss_ce_4: 0.1604  loss_mask_4: 0.2036  loss_dice_4: 0.1425  loss_ce_5: 0.1604  loss_mask_5: 0.2043  loss_dice_5: 0.1458  loss_ce_6: 0.1603  loss_mask_6: 0.2075  loss_dice_6: 0.1485  loss_ce_7: 0.1607  loss_mask_7: 0.2044  loss_dice_7: 0.1435  loss_ce_8: 0.1604  loss_mask_8: 0.2092  loss_dice_8: 0.1498  time: 0.5815  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:21:57] d2.utils.events INFO:  eta: 0:13:18  iter: 28599  total_loss: 4.498  loss_ce: 0.09915  loss_mask: 0.1889  loss_dice: 0.1304  loss_ce_0: 0.05231  loss_mask_0: 0.2  loss_dice_0: 0.1365  loss_ce_1: 0.09947  loss_mask_1: 0.1992  loss_dice_1: 0.1334  loss_ce_2: 0.09934  loss_mask_2: 0.1895  loss_dice_2: 0.1359  loss_ce_3: 0.09937  loss_mask_3: 0.1845  loss_dice_3: 0.1355  loss_ce_4: 0.09937  loss_mask_4: 0.1958  loss_dice_4: 0.1345  loss_ce_5: 0.09919  loss_mask_5: 0.2011  loss_dice_5: 0.1385  loss_ce_6: 0.09941  loss_mask_6: 0.192  loss_dice_6: 0.1402  loss_ce_7: 0.09911  loss_mask_7: 0.1904  loss_dice_7: 0.1371  loss_ce_8: 0.09919  loss_mask_8: 0.1977  loss_dice_8: 0.1377  time: 0.5815  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:22:08] d2.utils.events INFO:  eta: 0:13:07  iter: 28619  total_loss: 4.629  loss_ce: 0.09904  loss_mask: 0.1835  loss_dice: 0.1558  loss_ce_0: 0.05231  loss_mask_0: 0.1858  loss_dice_0: 0.1527  loss_ce_1: 0.09947  loss_mask_1: 0.1833  loss_dice_1: 0.1511  loss_ce_2: 0.09917  loss_mask_2: 0.1847  loss_dice_2: 0.1508  loss_ce_3: 0.09926  loss_mask_3: 0.1827  loss_dice_3: 0.1525  loss_ce_4: 0.09926  loss_mask_4: 0.1868  loss_dice_4: 0.1607  loss_ce_5: 0.09919  loss_mask_5: 0.1823  loss_dice_5: 0.1533  loss_ce_6: 0.09926  loss_mask_6: 0.1782  loss_dice_6: 0.1475  loss_ce_7: 0.09919  loss_mask_7: 0.1795  loss_dice_7: 0.1493  loss_ce_8: 0.09908  loss_mask_8: 0.1834  loss_dice_8: 0.1541  time: 0.5815  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:22:20] d2.utils.events INFO:  eta: 0:12:56  iter: 28639  total_loss: 4.722  loss_ce: 0.09949  loss_mask: 0.2266  loss_dice: 0.1303  loss_ce_0: 0.05234  loss_mask_0: 0.2222  loss_dice_0: 0.1324  loss_ce_1: 0.1005  loss_mask_1: 0.2113  loss_dice_1: 0.1268  loss_ce_2: 0.09992  loss_mask_2: 0.2228  loss_dice_2: 0.1314  loss_ce_3: 0.09975  loss_mask_3: 0.2119  loss_dice_3: 0.1313  loss_ce_4: 0.09994  loss_mask_4: 0.2207  loss_dice_4: 0.1296  loss_ce_5: 0.09971  loss_mask_5: 0.2239  loss_dice_5: 0.1303  loss_ce_6: 0.09994  loss_mask_6: 0.221  loss_dice_6: 0.1298  loss_ce_7: 0.09952  loss_mask_7: 0.2099  loss_dice_7: 0.1244  loss_ce_8: 0.09992  loss_mask_8: 0.2112  loss_dice_8: 0.1313  time: 0.5815  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:22:31] d2.utils.events INFO:  eta: 0:12:45  iter: 28659  total_loss: 4.443  loss_ce: 0.09908  loss_mask: 0.1541  loss_dice: 0.1497  loss_ce_0: 0.05232  loss_mask_0: 0.1575  loss_dice_0: 0.1581  loss_ce_1: 0.09941  loss_mask_1: 0.1479  loss_dice_1: 0.1553  loss_ce_2: 0.09924  loss_mask_2: 0.1448  loss_dice_2: 0.1494  loss_ce_3: 0.09926  loss_mask_3: 0.149  loss_dice_3: 0.1416  loss_ce_4: 0.09941  loss_mask_4: 0.1529  loss_dice_4: 0.1477  loss_ce_5: 0.09911  loss_mask_5: 0.1454  loss_dice_5: 0.1505  loss_ce_6: 0.09922  loss_mask_6: 0.1544  loss_dice_6: 0.149  loss_ce_7: 0.09926  loss_mask_7: 0.1489  loss_dice_7: 0.1491  loss_ce_8: 0.09911  loss_mask_8: 0.1548  loss_dice_8: 0.1515  time: 0.5815  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:22:42] d2.utils.events INFO:  eta: 0:12:34  iter: 28679  total_loss: 4.519  loss_ce: 0.09908  loss_mask: 0.1997  loss_dice: 0.1363  loss_ce_0: 0.0523  loss_mask_0: 0.2018  loss_dice_0: 0.1388  loss_ce_1: 0.09966  loss_mask_1: 0.1935  loss_dice_1: 0.1381  loss_ce_2: 0.09939  loss_mask_2: 0.1967  loss_dice_2: 0.1433  loss_ce_3: 0.09919  loss_mask_3: 0.1952  loss_dice_3: 0.1354  loss_ce_4: 0.09934  loss_mask_4: 0.1916  loss_dice_4: 0.1423  loss_ce_5: 0.09911  loss_mask_5: 0.1946  loss_dice_5: 0.1302  loss_ce_6: 0.0993  loss_mask_6: 0.1983  loss_dice_6: 0.1364  loss_ce_7: 0.09911  loss_mask_7: 0.1916  loss_dice_7: 0.1317  loss_ce_8: 0.09926  loss_mask_8: 0.199  loss_dice_8: 0.1366  time: 0.5815  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:22:54] d2.utils.events INFO:  eta: 0:12:22  iter: 28699  total_loss: 4.38  loss_ce: 0.09896  loss_mask: 0.1684  loss_dice: 0.1538  loss_ce_0: 0.05227  loss_mask_0: 0.1661  loss_dice_0: 0.1459  loss_ce_1: 0.09926  loss_mask_1: 0.1663  loss_dice_1: 0.149  loss_ce_2: 0.09906  loss_mask_2: 0.172  loss_dice_2: 0.1507  loss_ce_3: 0.09919  loss_mask_3: 0.1766  loss_dice_3: 0.1473  loss_ce_4: 0.09911  loss_mask_4: 0.1787  loss_dice_4: 0.1495  loss_ce_5: 0.09896  loss_mask_5: 0.1657  loss_dice_5: 0.1487  loss_ce_6: 0.09911  loss_mask_6: 0.1661  loss_dice_6: 0.1535  loss_ce_7: 0.09896  loss_mask_7: 0.1714  loss_dice_7: 0.1491  loss_ce_8: 0.09904  loss_mask_8: 0.1699  loss_dice_8: 0.149  time: 0.5815  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:23:05] d2.utils.events INFO:  eta: 0:12:11  iter: 28719  total_loss: 4.768  loss_ce: 0.09882  loss_mask: 0.179  loss_dice: 0.1455  loss_ce_0: 0.05226  loss_mask_0: 0.1949  loss_dice_0: 0.1417  loss_ce_1: 0.09913  loss_mask_1: 0.1874  loss_dice_1: 0.143  loss_ce_2: 0.09904  loss_mask_2: 0.188  loss_dice_2: 0.1424  loss_ce_3: 0.099  loss_mask_3: 0.1947  loss_dice_3: 0.153  loss_ce_4: 0.09904  loss_mask_4: 0.19  loss_dice_4: 0.1472  loss_ce_5: 0.09896  loss_mask_5: 0.1905  loss_dice_5: 0.1507  loss_ce_6: 0.09893  loss_mask_6: 0.1801  loss_dice_6: 0.1429  loss_ce_7: 0.09893  loss_mask_7: 0.1838  loss_dice_7: 0.145  loss_ce_8: 0.099  loss_mask_8: 0.1834  loss_dice_8: 0.1432  time: 0.5815  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:23:17] d2.utils.events INFO:  eta: 0:11:59  iter: 28739  total_loss: 4.695  loss_ce: 0.09867  loss_mask: 0.1775  loss_dice: 0.1506  loss_ce_0: 0.05224  loss_mask_0: 0.1714  loss_dice_0: 0.1506  loss_ce_1: 0.09891  loss_mask_1: 0.1796  loss_dice_1: 0.155  loss_ce_2: 0.09883  loss_mask_2: 0.1776  loss_dice_2: 0.1486  loss_ce_3: 0.09889  loss_mask_3: 0.1754  loss_dice_3: 0.1477  loss_ce_4: 0.09889  loss_mask_4: 0.1814  loss_dice_4: 0.1461  loss_ce_5: 0.09881  loss_mask_5: 0.1783  loss_dice_5: 0.1474  loss_ce_6: 0.09892  loss_mask_6: 0.1682  loss_dice_6: 0.1464  loss_ce_7: 0.09874  loss_mask_7: 0.1751  loss_dice_7: 0.1439  loss_ce_8: 0.09882  loss_mask_8: 0.1732  loss_dice_8: 0.1449  time: 0.5815  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:23:28] d2.utils.events INFO:  eta: 0:11:47  iter: 28759  total_loss: 4.435  loss_ce: 0.09885  loss_mask: 0.2022  loss_dice: 0.1296  loss_ce_0: 0.05222  loss_mask_0: 0.1939  loss_dice_0: 0.1262  loss_ce_1: 0.09945  loss_mask_1: 0.2008  loss_dice_1: 0.1276  loss_ce_2: 0.09913  loss_mask_2: 0.1901  loss_dice_2: 0.1268  loss_ce_3: 0.09907  loss_mask_3: 0.1936  loss_dice_3: 0.1236  loss_ce_4: 0.09915  loss_mask_4: 0.194  loss_dice_4: 0.1239  loss_ce_5: 0.09896  loss_mask_5: 0.1971  loss_dice_5: 0.1269  loss_ce_6: 0.09915  loss_mask_6: 0.1943  loss_dice_6: 0.1259  loss_ce_7: 0.09885  loss_mask_7: 0.1962  loss_dice_7: 0.1245  loss_ce_8: 0.099  loss_mask_8: 0.1982  loss_dice_8: 0.1231  time: 0.5815  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 16:23:40] d2.utils.events INFO:  eta: 0:11:36  iter: 28779  total_loss: 4.284  loss_ce: 0.09874  loss_mask: 0.1717  loss_dice: 0.1269  loss_ce_0: 0.05222  loss_mask_0: 0.1712  loss_dice_0: 0.133  loss_ce_1: 0.09921  loss_mask_1: 0.1704  loss_dice_1: 0.1298  loss_ce_2: 0.09898  loss_mask_2: 0.1607  loss_dice_2: 0.1326  loss_ce_3: 0.09881  loss_mask_3: 0.1698  loss_dice_3: 0.1286  loss_ce_4: 0.09896  loss_mask_4: 0.1678  loss_dice_4: 0.1306  loss_ce_5: 0.09889  loss_mask_5: 0.1648  loss_dice_5: 0.1283  loss_ce_6: 0.09889  loss_mask_6: 0.1648  loss_dice_6: 0.134  loss_ce_7: 0.09874  loss_mask_7: 0.1642  loss_dice_7: 0.1305  loss_ce_8: 0.099  loss_mask_8: 0.1626  loss_dice_8: 0.1279  time: 0.5815  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:23:51] d2.utils.events INFO:  eta: 0:11:24  iter: 28799  total_loss: 4.63  loss_ce: 0.161  loss_mask: 0.1603  loss_dice: 0.1358  loss_ce_0: 0.07514  loss_mask_0: 0.1614  loss_dice_0: 0.1366  loss_ce_1: 0.1609  loss_mask_1: 0.1564  loss_dice_1: 0.1397  loss_ce_2: 0.1609  loss_mask_2: 0.1653  loss_dice_2: 0.1437  loss_ce_3: 0.1609  loss_mask_3: 0.1623  loss_dice_3: 0.1369  loss_ce_4: 0.1608  loss_mask_4: 0.1647  loss_dice_4: 0.1402  loss_ce_5: 0.161  loss_mask_5: 0.161  loss_dice_5: 0.1329  loss_ce_6: 0.1608  loss_mask_6: 0.1653  loss_dice_6: 0.1358  loss_ce_7: 0.161  loss_mask_7: 0.1598  loss_dice_7: 0.1389  loss_ce_8: 0.1609  loss_mask_8: 0.1553  loss_dice_8: 0.1344  time: 0.5815  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:24:03] d2.utils.events INFO:  eta: 0:11:13  iter: 28819  total_loss: 4.819  loss_ce: 0.1297  loss_mask: 0.1834  loss_dice: 0.1471  loss_ce_0: 0.06366  loss_mask_0: 0.1854  loss_dice_0: 0.1425  loss_ce_1: 0.1293  loss_mask_1: 0.1792  loss_dice_1: 0.1424  loss_ce_2: 0.1296  loss_mask_2: 0.1784  loss_dice_2: 0.148  loss_ce_3: 0.1297  loss_mask_3: 0.1878  loss_dice_3: 0.1386  loss_ce_4: 0.1295  loss_mask_4: 0.1876  loss_dice_4: 0.1418  loss_ce_5: 0.1295  loss_mask_5: 0.1789  loss_dice_5: 0.137  loss_ce_6: 0.1295  loss_mask_6: 0.1978  loss_dice_6: 0.1395  loss_ce_7: 0.1298  loss_mask_7: 0.1769  loss_dice_7: 0.1404  loss_ce_8: 0.1295  loss_mask_8: 0.1878  loss_dice_8: 0.1396  time: 0.5815  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:24:14] d2.utils.events INFO:  eta: 0:11:01  iter: 28839  total_loss: 5.083  loss_ce: 0.09889  loss_mask: 0.1952  loss_dice: 0.1477  loss_ce_0: 0.0523  loss_mask_0: 0.1959  loss_dice_0: 0.1465  loss_ce_1: 0.09936  loss_mask_1: 0.2077  loss_dice_1: 0.1564  loss_ce_2: 0.09919  loss_mask_2: 0.1936  loss_dice_2: 0.1502  loss_ce_3: 0.09918  loss_mask_3: 0.2016  loss_dice_3: 0.1511  loss_ce_4: 0.09915  loss_mask_4: 0.2048  loss_dice_4: 0.1551  loss_ce_5: 0.09911  loss_mask_5: 0.2051  loss_dice_5: 0.1484  loss_ce_6: 0.09919  loss_mask_6: 0.2008  loss_dice_6: 0.1525  loss_ce_7: 0.09904  loss_mask_7: 0.204  loss_dice_7: 0.1534  loss_ce_8: 0.09915  loss_mask_8: 0.1944  loss_dice_8: 0.1516  time: 0.5815  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:24:25] d2.utils.events INFO:  eta: 0:10:50  iter: 28859  total_loss: 4.594  loss_ce: 0.099  loss_mask: 0.1928  loss_dice: 0.1575  loss_ce_0: 0.05229  loss_mask_0: 0.1779  loss_dice_0: 0.162  loss_ce_1: 0.09958  loss_mask_1: 0.1907  loss_dice_1: 0.156  loss_ce_2: 0.09924  loss_mask_2: 0.1842  loss_dice_2: 0.1621  loss_ce_3: 0.09919  loss_mask_3: 0.1884  loss_dice_3: 0.156  loss_ce_4: 0.09926  loss_mask_4: 0.1948  loss_dice_4: 0.1585  loss_ce_5: 0.09911  loss_mask_5: 0.1947  loss_dice_5: 0.1605  loss_ce_6: 0.09923  loss_mask_6: 0.1923  loss_dice_6: 0.1622  loss_ce_7: 0.09904  loss_mask_7: 0.1928  loss_dice_7: 0.1622  loss_ce_8: 0.09915  loss_mask_8: 0.1874  loss_dice_8: 0.1668  time: 0.5814  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:24:37] d2.utils.events INFO:  eta: 0:10:39  iter: 28879  total_loss: 4.53  loss_ce: 0.09885  loss_mask: 0.1668  loss_dice: 0.1445  loss_ce_0: 0.05228  loss_mask_0: 0.1845  loss_dice_0: 0.1491  loss_ce_1: 0.09921  loss_mask_1: 0.1769  loss_dice_1: 0.1447  loss_ce_2: 0.09904  loss_mask_2: 0.1718  loss_dice_2: 0.1449  loss_ce_3: 0.09892  loss_mask_3: 0.1669  loss_dice_3: 0.1468  loss_ce_4: 0.09908  loss_mask_4: 0.1733  loss_dice_4: 0.1447  loss_ce_5: 0.09896  loss_mask_5: 0.1727  loss_dice_5: 0.1422  loss_ce_6: 0.09896  loss_mask_6: 0.1748  loss_dice_6: 0.138  loss_ce_7: 0.09889  loss_mask_7: 0.1674  loss_dice_7: 0.1426  loss_ce_8: 0.099  loss_mask_8: 0.1693  loss_dice_8: 0.1472  time: 0.5814  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:24:48] d2.utils.events INFO:  eta: 0:10:27  iter: 28899  total_loss: 4.886  loss_ce: 0.1608  loss_mask: 0.2198  loss_dice: 0.148  loss_ce_0: 0.07507  loss_mask_0: 0.2165  loss_dice_0: 0.144  loss_ce_1: 0.1602  loss_mask_1: 0.2145  loss_dice_1: 0.1467  loss_ce_2: 0.1606  loss_mask_2: 0.2163  loss_dice_2: 0.1468  loss_ce_3: 0.1606  loss_mask_3: 0.2197  loss_dice_3: 0.1493  loss_ce_4: 0.1605  loss_mask_4: 0.2101  loss_dice_4: 0.1413  loss_ce_5: 0.1606  loss_mask_5: 0.2186  loss_dice_5: 0.1472  loss_ce_6: 0.1605  loss_mask_6: 0.2217  loss_dice_6: 0.1472  loss_ce_7: 0.1607  loss_mask_7: 0.2279  loss_dice_7: 0.1457  loss_ce_8: 0.1607  loss_mask_8: 0.2172  loss_dice_8: 0.1476  time: 0.5814  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:25:00] d2.utils.events INFO:  eta: 0:10:16  iter: 28919  total_loss: 4.351  loss_ce: 0.09874  loss_mask: 0.1836  loss_dice: 0.1287  loss_ce_0: 0.05228  loss_mask_0: 0.1829  loss_dice_0: 0.1297  loss_ce_1: 0.09898  loss_mask_1: 0.1789  loss_dice_1: 0.1343  loss_ce_2: 0.09889  loss_mask_2: 0.1795  loss_dice_2: 0.1288  loss_ce_3: 0.09881  loss_mask_3: 0.1732  loss_dice_3: 0.1339  loss_ce_4: 0.09893  loss_mask_4: 0.1726  loss_dice_4: 0.1341  loss_ce_5: 0.09881  loss_mask_5: 0.18  loss_dice_5: 0.1327  loss_ce_6: 0.09889  loss_mask_6: 0.1766  loss_dice_6: 0.134  loss_ce_7: 0.09885  loss_mask_7: 0.1767  loss_dice_7: 0.1362  loss_ce_8: 0.09878  loss_mask_8: 0.1817  loss_dice_8: 0.1362  time: 0.5814  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:25:11] d2.utils.events INFO:  eta: 0:10:04  iter: 28939  total_loss: 4.332  loss_ce: 0.09885  loss_mask: 0.1703  loss_dice: 0.1367  loss_ce_0: 0.05226  loss_mask_0: 0.1835  loss_dice_0: 0.1429  loss_ce_1: 0.09934  loss_mask_1: 0.1773  loss_dice_1: 0.1403  loss_ce_2: 0.09902  loss_mask_2: 0.1792  loss_dice_2: 0.1412  loss_ce_3: 0.09896  loss_mask_3: 0.1754  loss_dice_3: 0.1428  loss_ce_4: 0.09907  loss_mask_4: 0.1713  loss_dice_4: 0.1402  loss_ce_5: 0.09896  loss_mask_5: 0.1743  loss_dice_5: 0.1394  loss_ce_6: 0.099  loss_mask_6: 0.1756  loss_dice_6: 0.1401  loss_ce_7: 0.09881  loss_mask_7: 0.1723  loss_dice_7: 0.1435  loss_ce_8: 0.09893  loss_mask_8: 0.1781  loss_dice_8: 0.1422  time: 0.5814  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:25:22] d2.utils.events INFO:  eta: 0:09:53  iter: 28959  total_loss: 4.501  loss_ce: 0.09878  loss_mask: 0.1949  loss_dice: 0.1345  loss_ce_0: 0.05224  loss_mask_0: 0.1809  loss_dice_0: 0.1363  loss_ce_1: 0.09925  loss_mask_1: 0.1849  loss_dice_1: 0.1333  loss_ce_2: 0.099  loss_mask_2: 0.1897  loss_dice_2: 0.1356  loss_ce_3: 0.09889  loss_mask_3: 0.1818  loss_dice_3: 0.1348  loss_ce_4: 0.09904  loss_mask_4: 0.185  loss_dice_4: 0.1366  loss_ce_5: 0.09889  loss_mask_5: 0.2007  loss_dice_5: 0.1355  loss_ce_6: 0.09893  loss_mask_6: 0.186  loss_dice_6: 0.1317  loss_ce_7: 0.09874  loss_mask_7: 0.1751  loss_dice_7: 0.1313  loss_ce_8: 0.09885  loss_mask_8: 0.1945  loss_dice_8: 0.1349  time: 0.5814  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:25:34] d2.utils.events INFO:  eta: 0:09:42  iter: 28979  total_loss: 4.592  loss_ce: 0.09885  loss_mask: 0.1584  loss_dice: 0.1434  loss_ce_0: 0.05224  loss_mask_0: 0.1434  loss_dice_0: 0.143  loss_ce_1: 0.09956  loss_mask_1: 0.1561  loss_dice_1: 0.1472  loss_ce_2: 0.09902  loss_mask_2: 0.1666  loss_dice_2: 0.1548  loss_ce_3: 0.09889  loss_mask_3: 0.1495  loss_dice_3: 0.1426  loss_ce_4: 0.09919  loss_mask_4: 0.1588  loss_dice_4: 0.1455  loss_ce_5: 0.09889  loss_mask_5: 0.1558  loss_dice_5: 0.1507  loss_ce_6: 0.099  loss_mask_6: 0.1517  loss_dice_6: 0.1535  loss_ce_7: 0.09885  loss_mask_7: 0.1535  loss_dice_7: 0.1388  loss_ce_8: 0.099  loss_mask_8: 0.1523  loss_dice_8: 0.1448  time: 0.5814  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:25:45] d2.utils.events INFO:  eta: 0:09:30  iter: 28999  total_loss: 4.498  loss_ce: 0.161  loss_mask: 0.1971  loss_dice: 0.1243  loss_ce_0: 0.0751  loss_mask_0: 0.1887  loss_dice_0: 0.1267  loss_ce_1: 0.1604  loss_mask_1: 0.1924  loss_dice_1: 0.127  loss_ce_2: 0.1608  loss_mask_2: 0.1987  loss_dice_2: 0.1284  loss_ce_3: 0.161  loss_mask_3: 0.1989  loss_dice_3: 0.1278  loss_ce_4: 0.1607  loss_mask_4: 0.1966  loss_dice_4: 0.1223  loss_ce_5: 0.1609  loss_mask_5: 0.1991  loss_dice_5: 0.1264  loss_ce_6: 0.161  loss_mask_6: 0.2029  loss_dice_6: 0.1276  loss_ce_7: 0.161  loss_mask_7: 0.1999  loss_dice_7: 0.1265  loss_ce_8: 0.1609  loss_mask_8: 0.1886  loss_dice_8: 0.1255  time: 0.5814  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:25:57] d2.utils.events INFO:  eta: 0:09:19  iter: 29019  total_loss: 4.389  loss_ce: 0.09893  loss_mask: 0.1624  loss_dice: 0.1239  loss_ce_0: 0.05227  loss_mask_0: 0.1803  loss_dice_0: 0.1226  loss_ce_1: 0.09956  loss_mask_1: 0.1637  loss_dice_1: 0.1265  loss_ce_2: 0.09922  loss_mask_2: 0.1657  loss_dice_2: 0.1217  loss_ce_3: 0.09907  loss_mask_3: 0.167  loss_dice_3: 0.1273  loss_ce_4: 0.09919  loss_mask_4: 0.1671  loss_dice_4: 0.1252  loss_ce_5: 0.09911  loss_mask_5: 0.1683  loss_dice_5: 0.1246  loss_ce_6: 0.09915  loss_mask_6: 0.165  loss_dice_6: 0.1201  loss_ce_7: 0.09896  loss_mask_7: 0.162  loss_dice_7: 0.1201  loss_ce_8: 0.09915  loss_mask_8: 0.1621  loss_dice_8: 0.1225  time: 0.5814  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:26:08] d2.utils.events INFO:  eta: 0:09:07  iter: 29039  total_loss: 4.41  loss_ce: 0.09874  loss_mask: 0.1732  loss_dice: 0.1529  loss_ce_0: 0.05227  loss_mask_0: 0.1746  loss_dice_0: 0.1559  loss_ce_1: 0.09887  loss_mask_1: 0.1743  loss_dice_1: 0.1586  loss_ce_2: 0.09868  loss_mask_2: 0.1704  loss_dice_2: 0.1643  loss_ce_3: 0.09885  loss_mask_3: 0.1782  loss_dice_3: 0.1571  loss_ce_4: 0.09881  loss_mask_4: 0.1735  loss_dice_4: 0.1668  loss_ce_5: 0.09866  loss_mask_5: 0.1719  loss_dice_5: 0.1574  loss_ce_6: 0.09881  loss_mask_6: 0.1798  loss_dice_6: 0.158  loss_ce_7: 0.09874  loss_mask_7: 0.1848  loss_dice_7: 0.1601  loss_ce_8: 0.09882  loss_mask_8: 0.1769  loss_dice_8: 0.1597  time: 0.5814  data_time: 0.0026  lr: 1e-06  max_mem: 2811M
[07/11 16:26:19] d2.utils.events INFO:  eta: 0:08:56  iter: 29059  total_loss: 4.549  loss_ce: 0.1299  loss_mask: 0.2325  loss_dice: 0.1286  loss_ce_0: 0.06368  loss_mask_0: 0.2247  loss_dice_0: 0.1358  loss_ce_1: 0.1299  loss_mask_1: 0.222  loss_dice_1: 0.1347  loss_ce_2: 0.13  loss_mask_2: 0.2223  loss_dice_2: 0.1387  loss_ce_3: 0.1299  loss_mask_3: 0.2273  loss_dice_3: 0.1363  loss_ce_4: 0.13  loss_mask_4: 0.2314  loss_dice_4: 0.1355  loss_ce_5: 0.1297  loss_mask_5: 0.2292  loss_dice_5: 0.1393  loss_ce_6: 0.13  loss_mask_6: 0.2235  loss_dice_6: 0.1379  loss_ce_7: 0.1299  loss_mask_7: 0.2304  loss_dice_7: 0.1359  loss_ce_8: 0.1297  loss_mask_8: 0.2276  loss_dice_8: 0.1373  time: 0.5814  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:26:31] d2.utils.events INFO:  eta: 0:08:44  iter: 29079  total_loss: 4.767  loss_ce: 0.09859  loss_mask: 0.216  loss_dice: 0.1436  loss_ce_0: 0.05222  loss_mask_0: 0.2262  loss_dice_0: 0.1406  loss_ce_1: 0.09874  loss_mask_1: 0.2065  loss_dice_1: 0.1443  loss_ce_2: 0.09855  loss_mask_2: 0.2128  loss_dice_2: 0.1417  loss_ce_3: 0.09855  loss_mask_3: 0.2001  loss_dice_3: 0.1441  loss_ce_4: 0.09863  loss_mask_4: 0.2129  loss_dice_4: 0.1455  loss_ce_5: 0.09852  loss_mask_5: 0.2166  loss_dice_5: 0.147  loss_ce_6: 0.09855  loss_mask_6: 0.2101  loss_dice_6: 0.1439  loss_ce_7: 0.09852  loss_mask_7: 0.2176  loss_dice_7: 0.1395  loss_ce_8: 0.09863  loss_mask_8: 0.2112  loss_dice_8: 0.1397  time: 0.5814  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:26:42] d2.utils.events INFO:  eta: 0:08:33  iter: 29099  total_loss: 4.469  loss_ce: 0.09852  loss_mask: 0.1691  loss_dice: 0.1383  loss_ce_0: 0.05218  loss_mask_0: 0.1611  loss_dice_0: 0.1359  loss_ce_1: 0.09885  loss_mask_1: 0.1651  loss_dice_1: 0.1416  loss_ce_2: 0.09866  loss_mask_2: 0.1613  loss_dice_2: 0.1403  loss_ce_3: 0.09863  loss_mask_3: 0.1635  loss_dice_3: 0.1398  loss_ce_4: 0.0987  loss_mask_4: 0.1701  loss_dice_4: 0.1364  loss_ce_5: 0.09859  loss_mask_5: 0.1532  loss_dice_5: 0.1329  loss_ce_6: 0.0987  loss_mask_6: 0.1552  loss_dice_6: 0.1318  loss_ce_7: 0.09855  loss_mask_7: 0.1656  loss_dice_7: 0.1359  loss_ce_8: 0.09859  loss_mask_8: 0.163  loss_dice_8: 0.1422  time: 0.5813  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:26:54] d2.utils.events INFO:  eta: 0:08:21  iter: 29119  total_loss: 4.676  loss_ce: 0.1613  loss_mask: 0.2016  loss_dice: 0.1361  loss_ce_0: 0.0752  loss_mask_0: 0.2005  loss_dice_0: 0.1435  loss_ce_1: 0.1607  loss_mask_1: 0.1956  loss_dice_1: 0.1432  loss_ce_2: 0.161  loss_mask_2: 0.1974  loss_dice_2: 0.1398  loss_ce_3: 0.1612  loss_mask_3: 0.2042  loss_dice_3: 0.1466  loss_ce_4: 0.161  loss_mask_4: 0.2027  loss_dice_4: 0.1374  loss_ce_5: 0.1612  loss_mask_5: 0.2051  loss_dice_5: 0.1401  loss_ce_6: 0.1611  loss_mask_6: 0.1975  loss_dice_6: 0.1405  loss_ce_7: 0.1613  loss_mask_7: 0.2057  loss_dice_7: 0.1406  loss_ce_8: 0.1611  loss_mask_8: 0.204  loss_dice_8: 0.137  time: 0.5813  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:27:05] d2.utils.events INFO:  eta: 0:08:10  iter: 29139  total_loss: 4.24  loss_ce: 0.09844  loss_mask: 0.1391  loss_dice: 0.1287  loss_ce_0: 0.05219  loss_mask_0: 0.134  loss_dice_0: 0.124  loss_ce_1: 0.09837  loss_mask_1: 0.1317  loss_dice_1: 0.1219  loss_ce_2: 0.09842  loss_mask_2: 0.1359  loss_dice_2: 0.1273  loss_ce_3: 0.09851  loss_mask_3: 0.1333  loss_dice_3: 0.133  loss_ce_4: 0.0984  loss_mask_4: 0.1334  loss_dice_4: 0.1253  loss_ce_5: 0.09837  loss_mask_5: 0.1407  loss_dice_5: 0.1305  loss_ce_6: 0.09851  loss_mask_6: 0.1232  loss_dice_6: 0.1302  loss_ce_7: 0.0984  loss_mask_7: 0.1314  loss_dice_7: 0.119  loss_ce_8: 0.09844  loss_mask_8: 0.1374  loss_dice_8: 0.1275  time: 0.5813  data_time: 0.0020  lr: 1e-06  max_mem: 2811M
[07/11 16:27:17] d2.utils.events INFO:  eta: 0:07:58  iter: 29159  total_loss: 4.575  loss_ce: 0.09852  loss_mask: 0.1556  loss_dice: 0.1373  loss_ce_0: 0.05217  loss_mask_0: 0.1455  loss_dice_0: 0.1382  loss_ce_1: 0.09876  loss_mask_1: 0.1559  loss_dice_1: 0.1434  loss_ce_2: 0.09857  loss_mask_2: 0.1607  loss_dice_2: 0.1401  loss_ce_3: 0.09848  loss_mask_3: 0.1501  loss_dice_3: 0.1438  loss_ce_4: 0.09859  loss_mask_4: 0.1493  loss_dice_4: 0.1382  loss_ce_5: 0.09852  loss_mask_5: 0.1513  loss_dice_5: 0.1344  loss_ce_6: 0.09859  loss_mask_6: 0.1495  loss_dice_6: 0.1361  loss_ce_7: 0.09837  loss_mask_7: 0.1616  loss_dice_7: 0.1358  loss_ce_8: 0.09867  loss_mask_8: 0.1616  loss_dice_8: 0.1371  time: 0.5813  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:27:28] d2.utils.events INFO:  eta: 0:07:47  iter: 29179  total_loss: 4.924  loss_ce: 0.09815  loss_mask: 0.1509  loss_dice: 0.1693  loss_ce_0: 0.05215  loss_mask_0: 0.1519  loss_dice_0: 0.165  loss_ce_1: 0.09842  loss_mask_1: 0.1495  loss_dice_1: 0.1597  loss_ce_2: 0.09811  loss_mask_2: 0.1492  loss_dice_2: 0.1731  loss_ce_3: 0.09811  loss_mask_3: 0.1442  loss_dice_3: 0.1595  loss_ce_4: 0.09826  loss_mask_4: 0.1504  loss_dice_4: 0.1541  loss_ce_5: 0.09822  loss_mask_5: 0.1466  loss_dice_5: 0.1658  loss_ce_6: 0.09825  loss_mask_6: 0.1528  loss_dice_6: 0.1591  loss_ce_7: 0.09826  loss_mask_7: 0.1617  loss_dice_7: 0.1653  loss_ce_8: 0.09829  loss_mask_8: 0.1501  loss_dice_8: 0.157  time: 0.5813  data_time: 0.0019  lr: 1e-06  max_mem: 2811M
[07/11 16:27:39] d2.utils.events INFO:  eta: 0:07:36  iter: 29199  total_loss: 4.56  loss_ce: 0.13  loss_mask: 0.1896  loss_dice: 0.1451  loss_ce_0: 0.06368  loss_mask_0: 0.1934  loss_dice_0: 0.151  loss_ce_1: 0.13  loss_mask_1: 0.1966  loss_dice_1: 0.1528  loss_ce_2: 0.1299  loss_mask_2: 0.2038  loss_dice_2: 0.1513  loss_ce_3: 0.1299  loss_mask_3: 0.1925  loss_dice_3: 0.1509  loss_ce_4: 0.13  loss_mask_4: 0.186  loss_dice_4: 0.1478  loss_ce_5: 0.1299  loss_mask_5: 0.193  loss_dice_5: 0.1487  loss_ce_6: 0.13  loss_mask_6: 0.1982  loss_dice_6: 0.1499  loss_ce_7: 0.1299  loss_mask_7: 0.1941  loss_dice_7: 0.1574  loss_ce_8: 0.1299  loss_mask_8: 0.1911  loss_dice_8: 0.1514  time: 0.5813  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:27:51] d2.utils.events INFO:  eta: 0:07:24  iter: 29219  total_loss: 4.896  loss_ce: 0.1617  loss_mask: 0.1507  loss_dice: 0.154  loss_ce_0: 0.07523  loss_mask_0: 0.1453  loss_dice_0: 0.1544  loss_ce_1: 0.1611  loss_mask_1: 0.1486  loss_dice_1: 0.1524  loss_ce_2: 0.1616  loss_mask_2: 0.1455  loss_dice_2: 0.151  loss_ce_3: 0.1616  loss_mask_3: 0.1464  loss_dice_3: 0.156  loss_ce_4: 0.1615  loss_mask_4: 0.151  loss_dice_4: 0.1497  loss_ce_5: 0.1617  loss_mask_5: 0.1426  loss_dice_5: 0.1546  loss_ce_6: 0.1618  loss_mask_6: 0.1418  loss_dice_6: 0.1557  loss_ce_7: 0.1615  loss_mask_7: 0.1352  loss_dice_7: 0.1547  loss_ce_8: 0.1615  loss_mask_8: 0.1547  loss_dice_8: 0.1531  time: 0.5813  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:28:03] d2.utils.events INFO:  eta: 0:07:13  iter: 29239  total_loss: 4.926  loss_ce: 0.1615  loss_mask: 0.2  loss_dice: 0.1557  loss_ce_0: 0.07522  loss_mask_0: 0.194  loss_dice_0: 0.1522  loss_ce_1: 0.1612  loss_mask_1: 0.1951  loss_dice_1: 0.1549  loss_ce_2: 0.1614  loss_mask_2: 0.1952  loss_dice_2: 0.1551  loss_ce_3: 0.1614  loss_mask_3: 0.1952  loss_dice_3: 0.1554  loss_ce_4: 0.1613  loss_mask_4: 0.1975  loss_dice_4: 0.155  loss_ce_5: 0.1614  loss_mask_5: 0.1954  loss_dice_5: 0.1542  loss_ce_6: 0.1613  loss_mask_6: 0.1914  loss_dice_6: 0.1551  loss_ce_7: 0.1614  loss_mask_7: 0.1972  loss_dice_7: 0.1521  loss_ce_8: 0.1614  loss_mask_8: 0.2034  loss_dice_8: 0.1512  time: 0.5813  data_time: 0.0023  lr: 1e-06  max_mem: 2811M
[07/11 16:28:14] d2.utils.events INFO:  eta: 0:07:02  iter: 29259  total_loss: 4.784  loss_ce: 0.09841  loss_mask: 0.2101  loss_dice: 0.1665  loss_ce_0: 0.05218  loss_mask_0: 0.2221  loss_dice_0: 0.167  loss_ce_1: 0.09855  loss_mask_1: 0.2164  loss_dice_1: 0.1639  loss_ce_2: 0.09846  loss_mask_2: 0.224  loss_dice_2: 0.1682  loss_ce_3: 0.0984  loss_mask_3: 0.2196  loss_dice_3: 0.1614  loss_ce_4: 0.09855  loss_mask_4: 0.2202  loss_dice_4: 0.1614  loss_ce_5: 0.09844  loss_mask_5: 0.2269  loss_dice_5: 0.1701  loss_ce_6: 0.09851  loss_mask_6: 0.2169  loss_dice_6: 0.1659  loss_ce_7: 0.09837  loss_mask_7: 0.2182  loss_dice_7: 0.1644  loss_ce_8: 0.09852  loss_mask_8: 0.2175  loss_dice_8: 0.1677  time: 0.5813  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:28:26] d2.utils.events INFO:  eta: 0:06:50  iter: 29279  total_loss: 4.433  loss_ce: 0.09844  loss_mask: 0.1556  loss_dice: 0.1495  loss_ce_0: 0.05218  loss_mask_0: 0.1553  loss_dice_0: 0.1446  loss_ce_1: 0.09911  loss_mask_1: 0.1684  loss_dice_1: 0.1476  loss_ce_2: 0.09885  loss_mask_2: 0.1597  loss_dice_2: 0.1482  loss_ce_3: 0.09859  loss_mask_3: 0.1588  loss_dice_3: 0.142  loss_ce_4: 0.09881  loss_mask_4: 0.1536  loss_dice_4: 0.1395  loss_ce_5: 0.09852  loss_mask_5: 0.1656  loss_dice_5: 0.1423  loss_ce_6: 0.09863  loss_mask_6: 0.1616  loss_dice_6: 0.1418  loss_ce_7: 0.09844  loss_mask_7: 0.1594  loss_dice_7: 0.1451  loss_ce_8: 0.09863  loss_mask_8: 0.1628  loss_dice_8: 0.1474  time: 0.5813  data_time: 0.0018  lr: 1e-06  max_mem: 2811M
[07/11 16:28:37] d2.utils.events INFO:  eta: 0:06:39  iter: 29299  total_loss: 4.573  loss_ce: 0.1614  loss_mask: 0.1867  loss_dice: 0.1338  loss_ce_0: 0.07519  loss_mask_0: 0.1865  loss_dice_0: 0.1336  loss_ce_1: 0.1609  loss_mask_1: 0.1913  loss_dice_1: 0.1359  loss_ce_2: 0.1612  loss_mask_2: 0.1922  loss_dice_2: 0.1351  loss_ce_3: 0.1614  loss_mask_3: 0.1879  loss_dice_3: 0.1282  loss_ce_4: 0.1611  loss_mask_4: 0.1848  loss_dice_4: 0.13  loss_ce_5: 0.1613  loss_mask_5: 0.1897  loss_dice_5: 0.1375  loss_ce_6: 0.1613  loss_mask_6: 0.1914  loss_dice_6: 0.1339  loss_ce_7: 0.1613  loss_mask_7: 0.192  loss_dice_7: 0.1278  loss_ce_8: 0.1612  loss_mask_8: 0.1948  loss_dice_8: 0.1311  time: 0.5813  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:28:49] d2.utils.events INFO:  eta: 0:06:28  iter: 29319  total_loss: 4.217  loss_ce: 0.1611  loss_mask: 0.1602  loss_dice: 0.1316  loss_ce_0: 0.07516  loss_mask_0: 0.1465  loss_dice_0: 0.1277  loss_ce_1: 0.1599  loss_mask_1: 0.148  loss_dice_1: 0.1244  loss_ce_2: 0.1607  loss_mask_2: 0.1533  loss_dice_2: 0.1269  loss_ce_3: 0.161  loss_mask_3: 0.1505  loss_dice_3: 0.1245  loss_ce_4: 0.1607  loss_mask_4: 0.1551  loss_dice_4: 0.1273  loss_ce_5: 0.1608  loss_mask_5: 0.1507  loss_dice_5: 0.1232  loss_ce_6: 0.1607  loss_mask_6: 0.1538  loss_dice_6: 0.1287  loss_ce_7: 0.1609  loss_mask_7: 0.1543  loss_dice_7: 0.1278  loss_ce_8: 0.1608  loss_mask_8: 0.1513  loss_dice_8: 0.1293  time: 0.5813  data_time: 0.0021  lr: 1e-06  max_mem: 2811M
[07/11 16:29:00] d2.utils.events INFO:  eta: 0:06:16  iter: 29339  total_loss: 4.563  loss_ce: 0.1609  loss_mask: 0.195  loss_dice: 0.1546  loss_ce_0: 0.07514  loss_mask_0: 0.1944  loss_dice_0: 0.1512  loss_ce_1: 0.1602  loss_mask_1: 0.2015  loss_dice_1: 0.1556  loss_ce_2: 0.1606  loss_mask_2: 0.1949  loss_dice_2: 0.1532  loss_ce_3: 0.1607  loss_mask_3: 0.1962  loss_dice_3: 0.154  loss_ce_4: 0.1605  loss_mask_4: 0.2001  loss_dice_4: 0.1534  loss_ce_5: 0.1606  loss_mask_5: 0.2036  loss_dice_5: 0.1531  loss_ce_6: 0.1606  loss_mask_6: 0.1946  loss_dice_6: 0.1567  loss_ce_7: 0.1608  loss_mask_7: 0.1984  loss_dice_7: 0.1469  loss_ce_8: 0.1608  loss_mask_8: 0.1962  loss_dice_8: 0.149  time: 0.5813  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:29:11] d2.utils.events INFO:  eta: 0:06:05  iter: 29359  total_loss: 4.586  loss_ce: 0.09885  loss_mask: 0.2051  loss_dice: 0.1335  loss_ce_0: 0.05224  loss_mask_0: 0.2074  loss_dice_0: 0.1354  loss_ce_1: 0.09923  loss_mask_1: 0.2112  loss_dice_1: 0.1372  loss_ce_2: 0.09906  loss_mask_2: 0.203  loss_dice_2: 0.1376  loss_ce_3: 0.09889  loss_mask_3: 0.2005  loss_dice_3: 0.1366  loss_ce_4: 0.09915  loss_mask_4: 0.2072  loss_dice_4: 0.1306  loss_ce_5: 0.09896  loss_mask_5: 0.2086  loss_dice_5: 0.1363  loss_ce_6: 0.09908  loss_mask_6: 0.2052  loss_dice_6: 0.1329  loss_ce_7: 0.09889  loss_mask_7: 0.2074  loss_dice_7: 0.1341  loss_ce_8: 0.099  loss_mask_8: 0.204  loss_dice_8: 0.1329  time: 0.5813  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:29:23] d2.utils.events INFO:  eta: 0:05:54  iter: 29379  total_loss: 4.373  loss_ce: 0.09878  loss_mask: 0.1817  loss_dice: 0.1477  loss_ce_0: 0.05224  loss_mask_0: 0.1823  loss_dice_0: 0.1504  loss_ce_1: 0.09893  loss_mask_1: 0.1948  loss_dice_1: 0.149  loss_ce_2: 0.09885  loss_mask_2: 0.1801  loss_dice_2: 0.147  loss_ce_3: 0.09889  loss_mask_3: 0.1801  loss_dice_3: 0.1395  loss_ce_4: 0.09889  loss_mask_4: 0.1843  loss_dice_4: 0.1438  loss_ce_5: 0.09889  loss_mask_5: 0.1828  loss_dice_5: 0.1412  loss_ce_6: 0.09881  loss_mask_6: 0.185  loss_dice_6: 0.1481  loss_ce_7: 0.09878  loss_mask_7: 0.1791  loss_dice_7: 0.1486  loss_ce_8: 0.09878  loss_mask_8: 0.1815  loss_dice_8: 0.1495  time: 0.5813  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:29:34] d2.utils.events INFO:  eta: 0:05:42  iter: 29399  total_loss: 4.424  loss_ce: 0.161  loss_mask: 0.1864  loss_dice: 0.1334  loss_ce_0: 0.07514  loss_mask_0: 0.1769  loss_dice_0: 0.1317  loss_ce_1: 0.1607  loss_mask_1: 0.1796  loss_dice_1: 0.1271  loss_ce_2: 0.1609  loss_mask_2: 0.183  loss_dice_2: 0.13  loss_ce_3: 0.1608  loss_mask_3: 0.1827  loss_dice_3: 0.1269  loss_ce_4: 0.1607  loss_mask_4: 0.1908  loss_dice_4: 0.131  loss_ce_5: 0.161  loss_mask_5: 0.1836  loss_dice_5: 0.1301  loss_ce_6: 0.1609  loss_mask_6: 0.1793  loss_dice_6: 0.1306  loss_ce_7: 0.161  loss_mask_7: 0.1855  loss_dice_7: 0.1295  loss_ce_8: 0.161  loss_mask_8: 0.1742  loss_dice_8: 0.1287  time: 0.5813  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:29:46] d2.utils.events INFO:  eta: 0:05:31  iter: 29419  total_loss: 4.748  loss_ce: 0.09867  loss_mask: 0.2045  loss_dice: 0.1393  loss_ce_0: 0.05222  loss_mask_0: 0.2039  loss_dice_0: 0.1371  loss_ce_1: 0.09897  loss_mask_1: 0.2029  loss_dice_1: 0.1366  loss_ce_2: 0.09876  loss_mask_2: 0.2022  loss_dice_2: 0.135  loss_ce_3: 0.09877  loss_mask_3: 0.1913  loss_dice_3: 0.1359  loss_ce_4: 0.09888  loss_mask_4: 0.2007  loss_dice_4: 0.1371  loss_ce_5: 0.09866  loss_mask_5: 0.198  loss_dice_5: 0.1349  loss_ce_6: 0.09881  loss_mask_6: 0.2005  loss_dice_6: 0.1416  loss_ce_7: 0.09874  loss_mask_7: 0.2058  loss_dice_7: 0.1346  loss_ce_8: 0.09882  loss_mask_8: 0.1927  loss_dice_8: 0.1378  time: 0.5812  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:29:57] d2.utils.events INFO:  eta: 0:05:19  iter: 29439  total_loss: 4.568  loss_ce: 0.0987  loss_mask: 0.1647  loss_dice: 0.152  loss_ce_0: 0.05222  loss_mask_0: 0.173  loss_dice_0: 0.161  loss_ce_1: 0.09941  loss_mask_1: 0.1692  loss_dice_1: 0.1612  loss_ce_2: 0.09909  loss_mask_2: 0.1578  loss_dice_2: 0.1635  loss_ce_3: 0.09881  loss_mask_3: 0.1696  loss_dice_3: 0.1539  loss_ce_4: 0.09893  loss_mask_4: 0.161  loss_dice_4: 0.153  loss_ce_5: 0.09896  loss_mask_5: 0.1681  loss_dice_5: 0.1561  loss_ce_6: 0.09889  loss_mask_6: 0.1587  loss_dice_6: 0.1593  loss_ce_7: 0.09881  loss_mask_7: 0.1543  loss_dice_7: 0.1512  loss_ce_8: 0.09896  loss_mask_8: 0.1642  loss_dice_8: 0.1602  time: 0.5812  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:30:09] d2.utils.events INFO:  eta: 0:05:08  iter: 29459  total_loss: 4.457  loss_ce: 0.09867  loss_mask: 0.1856  loss_dice: 0.1497  loss_ce_0: 0.05221  loss_mask_0: 0.1917  loss_dice_0: 0.1534  loss_ce_1: 0.09902  loss_mask_1: 0.185  loss_dice_1: 0.1514  loss_ce_2: 0.09878  loss_mask_2: 0.1902  loss_dice_2: 0.1461  loss_ce_3: 0.09874  loss_mask_3: 0.196  loss_dice_3: 0.1488  loss_ce_4: 0.09885  loss_mask_4: 0.184  loss_dice_4: 0.1464  loss_ce_5: 0.09874  loss_mask_5: 0.1963  loss_dice_5: 0.1456  loss_ce_6: 0.09885  loss_mask_6: 0.1901  loss_dice_6: 0.1512  loss_ce_7: 0.09867  loss_mask_7: 0.1941  loss_dice_7: 0.1466  loss_ce_8: 0.09878  loss_mask_8: 0.1921  loss_dice_8: 0.1468  time: 0.5812  data_time: 0.0017  lr: 1e-06  max_mem: 2811M
[07/11 16:30:20] d2.utils.events INFO:  eta: 0:04:56  iter: 29479  total_loss: 4.771  loss_ce: 0.1298  loss_mask: 0.2119  loss_dice: 0.1365  loss_ce_0: 0.06367  loss_mask_0: 0.201  loss_dice_0: 0.1368  loss_ce_1: 0.1295  loss_mask_1: 0.2154  loss_dice_1: 0.1387  loss_ce_2: 0.1296  loss_mask_2: 0.2108  loss_dice_2: 0.1397  loss_ce_3: 0.1297  loss_mask_3: 0.2096  loss_dice_3: 0.1379  loss_ce_4: 0.1297  loss_mask_4: 0.2165  loss_dice_4: 0.1407  loss_ce_5: 0.1297  loss_mask_5: 0.2166  loss_dice_5: 0.139  loss_ce_6: 0.1297  loss_mask_6: 0.2053  loss_dice_6: 0.1388  loss_ce_7: 0.1298  loss_mask_7: 0.2099  loss_dice_7: 0.1388  loss_ce_8: 0.1298  loss_mask_8: 0.2043  loss_dice_8: 0.1331  time: 0.5812  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:30:33] d2.utils.events INFO:  eta: 0:04:45  iter: 29499  total_loss: 4.581  loss_ce: 0.09885  loss_mask: 0.1865  loss_dice: 0.1353  loss_ce_0: 0.05225  loss_mask_0: 0.1875  loss_dice_0: 0.1403  loss_ce_1: 0.09932  loss_mask_1: 0.1898  loss_dice_1: 0.1349  loss_ce_2: 0.09906  loss_mask_2: 0.1946  loss_dice_2: 0.1391  loss_ce_3: 0.09907  loss_mask_3: 0.1986  loss_dice_3: 0.1334  loss_ce_4: 0.09907  loss_mask_4: 0.1902  loss_dice_4: 0.142  loss_ce_5: 0.09904  loss_mask_5: 0.1872  loss_dice_5: 0.1382  loss_ce_6: 0.09911  loss_mask_6: 0.1931  loss_dice_6: 0.1392  loss_ce_7: 0.09885  loss_mask_7: 0.194  loss_dice_7: 0.1394  loss_ce_8: 0.09897  loss_mask_8: 0.1894  loss_dice_8: 0.1343  time: 0.5813  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:30:45] d2.utils.events INFO:  eta: 0:04:35  iter: 29519  total_loss: 4.626  loss_ce: 0.1607  loss_mask: 0.1313  loss_dice: 0.1373  loss_ce_0: 0.07505  loss_mask_0: 0.1278  loss_dice_0: 0.141  loss_ce_1: 0.1595  loss_mask_1: 0.1331  loss_dice_1: 0.1399  loss_ce_2: 0.16  loss_mask_2: 0.13  loss_dice_2: 0.1386  loss_ce_3: 0.1601  loss_mask_3: 0.1292  loss_dice_3: 0.1385  loss_ce_4: 0.1599  loss_mask_4: 0.1207  loss_dice_4: 0.145  loss_ce_5: 0.1604  loss_mask_5: 0.1273  loss_dice_5: 0.1409  loss_ce_6: 0.1602  loss_mask_6: 0.1314  loss_dice_6: 0.1383  loss_ce_7: 0.1606  loss_mask_7: 0.127  loss_dice_7: 0.139  loss_ce_8: 0.1604  loss_mask_8: 0.1241  loss_dice_8: 0.1353  time: 0.5813  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:30:58] d2.utils.events INFO:  eta: 0:04:24  iter: 29539  total_loss: 4.822  loss_ce: 0.09908  loss_mask: 0.2324  loss_dice: 0.1435  loss_ce_0: 0.05228  loss_mask_0: 0.2228  loss_dice_0: 0.1384  loss_ce_1: 0.09947  loss_mask_1: 0.2144  loss_dice_1: 0.1397  loss_ce_2: 0.09917  loss_mask_2: 0.2316  loss_dice_2: 0.143  loss_ce_3: 0.09904  loss_mask_3: 0.232  loss_dice_3: 0.1417  loss_ce_4: 0.09908  loss_mask_4: 0.2391  loss_dice_4: 0.1432  loss_ce_5: 0.09911  loss_mask_5: 0.2328  loss_dice_5: 0.1447  loss_ce_6: 0.09919  loss_mask_6: 0.2305  loss_dice_6: 0.141  loss_ce_7: 0.09896  loss_mask_7: 0.2309  loss_dice_7: 0.141  loss_ce_8: 0.09915  loss_mask_8: 0.2339  loss_dice_8: 0.146  time: 0.5813  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 16:31:11] d2.utils.events INFO:  eta: 0:04:13  iter: 29559  total_loss: 4.433  loss_ce: 0.09882  loss_mask: 0.2065  loss_dice: 0.1374  loss_ce_0: 0.05227  loss_mask_0: 0.204  loss_dice_0: 0.1362  loss_ce_1: 0.09902  loss_mask_1: 0.2002  loss_dice_1: 0.1356  loss_ce_2: 0.09881  loss_mask_2: 0.2103  loss_dice_2: 0.1359  loss_ce_3: 0.09889  loss_mask_3: 0.2033  loss_dice_3: 0.1387  loss_ce_4: 0.09889  loss_mask_4: 0.2077  loss_dice_4: 0.1339  loss_ce_5: 0.09881  loss_mask_5: 0.205  loss_dice_5: 0.1434  loss_ce_6: 0.09889  loss_mask_6: 0.2051  loss_dice_6: 0.1388  loss_ce_7: 0.09878  loss_mask_7: 0.2061  loss_dice_7: 0.1345  loss_ce_8: 0.09889  loss_mask_8: 0.2154  loss_dice_8: 0.1356  time: 0.5814  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 16:31:23] d2.utils.events INFO:  eta: 0:04:02  iter: 29579  total_loss: 4.373  loss_ce: 0.09859  loss_mask: 0.172  loss_dice: 0.1561  loss_ce_0: 0.05221  loss_mask_0: 0.1628  loss_dice_0: 0.1501  loss_ce_1: 0.09854  loss_mask_1: 0.168  loss_dice_1: 0.152  loss_ce_2: 0.09857  loss_mask_2: 0.1589  loss_dice_2: 0.1529  loss_ce_3: 0.09859  loss_mask_3: 0.1689  loss_dice_3: 0.1511  loss_ce_4: 0.09866  loss_mask_4: 0.1677  loss_dice_4: 0.1561  loss_ce_5: 0.09859  loss_mask_5: 0.152  loss_dice_5: 0.1514  loss_ce_6: 0.09855  loss_mask_6: 0.156  loss_dice_6: 0.1489  loss_ce_7: 0.09855  loss_mask_7: 0.1622  loss_dice_7: 0.1491  loss_ce_8: 0.09848  loss_mask_8: 0.1669  loss_dice_8: 0.1564  time: 0.5814  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:31:36] d2.utils.events INFO:  eta: 0:03:51  iter: 29599  total_loss: 4.727  loss_ce: 0.1613  loss_mask: 0.1999  loss_dice: 0.1376  loss_ce_0: 0.07517  loss_mask_0: 0.192  loss_dice_0: 0.132  loss_ce_1: 0.1612  loss_mask_1: 0.1986  loss_dice_1: 0.1341  loss_ce_2: 0.1613  loss_mask_2: 0.1929  loss_dice_2: 0.1337  loss_ce_3: 0.1611  loss_mask_3: 0.2022  loss_dice_3: 0.1379  loss_ce_4: 0.1611  loss_mask_4: 0.1942  loss_dice_4: 0.1362  loss_ce_5: 0.1611  loss_mask_5: 0.2033  loss_dice_5: 0.1337  loss_ce_6: 0.1612  loss_mask_6: 0.1986  loss_dice_6: 0.1378  loss_ce_7: 0.1613  loss_mask_7: 0.1954  loss_dice_7: 0.1352  loss_ce_8: 0.1613  loss_mask_8: 0.1953  loss_dice_8: 0.1348  time: 0.5814  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:31:48] d2.utils.events INFO:  eta: 0:03:40  iter: 29619  total_loss: 4.78  loss_ce: 0.09856  loss_mask: 0.2116  loss_dice: 0.1646  loss_ce_0: 0.05221  loss_mask_0: 0.1942  loss_dice_0: 0.1622  loss_ce_1: 0.09891  loss_mask_1: 0.2001  loss_dice_1: 0.1627  loss_ce_2: 0.09876  loss_mask_2: 0.2103  loss_dice_2: 0.1604  loss_ce_3: 0.09866  loss_mask_3: 0.1935  loss_dice_3: 0.159  loss_ce_4: 0.09881  loss_mask_4: 0.2032  loss_dice_4: 0.1622  loss_ce_5: 0.09874  loss_mask_5: 0.2033  loss_dice_5: 0.1478  loss_ce_6: 0.09866  loss_mask_6: 0.2002  loss_dice_6: 0.1469  loss_ce_7: 0.09867  loss_mask_7: 0.1998  loss_dice_7: 0.1616  loss_ce_8: 0.0987  loss_mask_8: 0.2076  loss_dice_8: 0.1601  time: 0.5815  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:32:01] d2.utils.events INFO:  eta: 0:03:29  iter: 29639  total_loss: 4.502  loss_ce: 0.09844  loss_mask: 0.1833  loss_dice: 0.1325  loss_ce_0: 0.05219  loss_mask_0: 0.188  loss_dice_0: 0.1293  loss_ce_1: 0.0985  loss_mask_1: 0.1886  loss_dice_1: 0.1351  loss_ce_2: 0.09846  loss_mask_2: 0.1837  loss_dice_2: 0.1303  loss_ce_3: 0.09855  loss_mask_3: 0.1897  loss_dice_3: 0.1346  loss_ce_4: 0.09855  loss_mask_4: 0.1904  loss_dice_4: 0.1375  loss_ce_5: 0.09851  loss_mask_5: 0.1913  loss_dice_5: 0.1333  loss_ce_6: 0.09855  loss_mask_6: 0.1894  loss_dice_6: 0.1311  loss_ce_7: 0.09852  loss_mask_7: 0.1832  loss_dice_7: 0.1323  loss_ce_8: 0.09856  loss_mask_8: 0.1854  loss_dice_8: 0.1264  time: 0.5815  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 16:32:14] d2.utils.events INFO:  eta: 0:03:18  iter: 29659  total_loss: 4.773  loss_ce: 0.09829  loss_mask: 0.2128  loss_dice: 0.1532  loss_ce_0: 0.05217  loss_mask_0: 0.2141  loss_dice_0: 0.1518  loss_ce_1: 0.09828  loss_mask_1: 0.211  loss_dice_1: 0.1518  loss_ce_2: 0.09813  loss_mask_2: 0.2139  loss_dice_2: 0.1533  loss_ce_3: 0.09836  loss_mask_3: 0.1979  loss_dice_3: 0.1455  loss_ce_4: 0.09829  loss_mask_4: 0.2087  loss_dice_4: 0.1518  loss_ce_5: 0.09822  loss_mask_5: 0.2071  loss_dice_5: 0.1567  loss_ce_6: 0.09833  loss_mask_6: 0.2061  loss_dice_6: 0.1447  loss_ce_7: 0.09826  loss_mask_7: 0.2123  loss_dice_7: 0.155  loss_ce_8: 0.09818  loss_mask_8: 0.2072  loss_dice_8: 0.1522  time: 0.5815  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:32:27] d2.utils.events INFO:  eta: 0:03:07  iter: 29679  total_loss: 4.651  loss_ce: 0.1613  loss_mask: 0.1607  loss_dice: 0.1563  loss_ce_0: 0.07522  loss_mask_0: 0.1583  loss_dice_0: 0.1579  loss_ce_1: 0.1605  loss_mask_1: 0.1548  loss_dice_1: 0.1582  loss_ce_2: 0.1609  loss_mask_2: 0.1504  loss_dice_2: 0.1534  loss_ce_3: 0.1612  loss_mask_3: 0.1568  loss_dice_3: 0.1568  loss_ce_4: 0.161  loss_mask_4: 0.1678  loss_dice_4: 0.1607  loss_ce_5: 0.1614  loss_mask_5: 0.1651  loss_dice_5: 0.1628  loss_ce_6: 0.161  loss_mask_6: 0.1662  loss_dice_6: 0.1546  loss_ce_7: 0.1614  loss_mask_7: 0.1582  loss_dice_7: 0.1622  loss_ce_8: 0.1611  loss_mask_8: 0.1665  loss_dice_8: 0.1607  time: 0.5816  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:32:39] d2.utils.events INFO:  eta: 0:02:55  iter: 29699  total_loss: 4.372  loss_ce: 0.09841  loss_mask: 0.1747  loss_dice: 0.1309  loss_ce_0: 0.05215  loss_mask_0: 0.1683  loss_dice_0: 0.1242  loss_ce_1: 0.09887  loss_mask_1: 0.1672  loss_dice_1: 0.1258  loss_ce_2: 0.09857  loss_mask_2: 0.173  loss_dice_2: 0.1265  loss_ce_3: 0.09844  loss_mask_3: 0.1653  loss_dice_3: 0.1234  loss_ce_4: 0.0987  loss_mask_4: 0.1709  loss_dice_4: 0.1271  loss_ce_5: 0.09852  loss_mask_5: 0.1708  loss_dice_5: 0.124  loss_ce_6: 0.09855  loss_mask_6: 0.1723  loss_dice_6: 0.1242  loss_ce_7: 0.0984  loss_mask_7: 0.1725  loss_dice_7: 0.1279  loss_ce_8: 0.09852  loss_mask_8: 0.1643  loss_dice_8: 0.1262  time: 0.5816  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:32:52] d2.utils.events INFO:  eta: 0:02:44  iter: 29719  total_loss: 4.583  loss_ce: 0.09833  loss_mask: 0.2055  loss_dice: 0.1265  loss_ce_0: 0.05214  loss_mask_0: 0.2143  loss_dice_0: 0.129  loss_ce_1: 0.09885  loss_mask_1: 0.2066  loss_dice_1: 0.1296  loss_ce_2: 0.0984  loss_mask_2: 0.2136  loss_dice_2: 0.1285  loss_ce_3: 0.09848  loss_mask_3: 0.2007  loss_dice_3: 0.1341  loss_ce_4: 0.09855  loss_mask_4: 0.2126  loss_dice_4: 0.1298  loss_ce_5: 0.09829  loss_mask_5: 0.2068  loss_dice_5: 0.13  loss_ce_6: 0.0984  loss_mask_6: 0.2036  loss_dice_6: 0.1267  loss_ce_7: 0.09833  loss_mask_7: 0.2055  loss_dice_7: 0.1302  loss_ce_8: 0.09863  loss_mask_8: 0.2009  loss_dice_8: 0.1292  time: 0.5816  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:33:05] d2.utils.events INFO:  eta: 0:02:33  iter: 29739  total_loss: 4.723  loss_ce: 0.1612  loss_mask: 0.1708  loss_dice: 0.1438  loss_ce_0: 0.07522  loss_mask_0: 0.167  loss_dice_0: 0.1573  loss_ce_1: 0.1602  loss_mask_1: 0.1708  loss_dice_1: 0.1508  loss_ce_2: 0.1609  loss_mask_2: 0.1733  loss_dice_2: 0.1482  loss_ce_3: 0.1611  loss_mask_3: 0.1764  loss_dice_3: 0.1511  loss_ce_4: 0.1609  loss_mask_4: 0.1693  loss_dice_4: 0.146  loss_ce_5: 0.161  loss_mask_5: 0.1739  loss_dice_5: 0.1507  loss_ce_6: 0.1609  loss_mask_6: 0.1809  loss_dice_6: 0.1459  loss_ce_7: 0.1613  loss_mask_7: 0.1736  loss_dice_7: 0.1584  loss_ce_8: 0.1612  loss_mask_8: 0.1759  loss_dice_8: 0.1439  time: 0.5817  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:33:17] d2.utils.events INFO:  eta: 0:02:21  iter: 29759  total_loss: 4.429  loss_ce: 0.0984  loss_mask: 0.1602  loss_dice: 0.1567  loss_ce_0: 0.05217  loss_mask_0: 0.1532  loss_dice_0: 0.1529  loss_ce_1: 0.09874  loss_mask_1: 0.1645  loss_dice_1: 0.1555  loss_ce_2: 0.09857  loss_mask_2: 0.1652  loss_dice_2: 0.1619  loss_ce_3: 0.09851  loss_mask_3: 0.1573  loss_dice_3: 0.1535  loss_ce_4: 0.09863  loss_mask_4: 0.1607  loss_dice_4: 0.1552  loss_ce_5: 0.09844  loss_mask_5: 0.1621  loss_dice_5: 0.1567  loss_ce_6: 0.09859  loss_mask_6: 0.1687  loss_dice_6: 0.1549  loss_ce_7: 0.09837  loss_mask_7: 0.1563  loss_dice_7: 0.1569  loss_ce_8: 0.09859  loss_mask_8: 0.1691  loss_dice_8: 0.162  time: 0.5817  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:33:30] d2.utils.events INFO:  eta: 0:02:10  iter: 29779  total_loss: 4.867  loss_ce: 0.09844  loss_mask: 0.2023  loss_dice: 0.1641  loss_ce_0: 0.05216  loss_mask_0: 0.2072  loss_dice_0: 0.1558  loss_ce_1: 0.0987  loss_mask_1: 0.2095  loss_dice_1: 0.1647  loss_ce_2: 0.0984  loss_mask_2: 0.2072  loss_dice_2: 0.1621  loss_ce_3: 0.09844  loss_mask_3: 0.201  loss_dice_3: 0.1609  loss_ce_4: 0.09851  loss_mask_4: 0.2079  loss_dice_4: 0.1643  loss_ce_5: 0.09844  loss_mask_5: 0.2069  loss_dice_5: 0.161  loss_ce_6: 0.09844  loss_mask_6: 0.2073  loss_dice_6: 0.1647  loss_ce_7: 0.09837  loss_mask_7: 0.2119  loss_dice_7: 0.1632  loss_ce_8: 0.09844  loss_mask_8: 0.2072  loss_dice_8: 0.1629  time: 0.5817  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:33:43] d2.utils.events INFO:  eta: 0:01:58  iter: 29799  total_loss: 4.525  loss_ce: 0.161  loss_mask: 0.1589  loss_dice: 0.1495  loss_ce_0: 0.07525  loss_mask_0: 0.1502  loss_dice_0: 0.1483  loss_ce_1: 0.1594  loss_mask_1: 0.1559  loss_dice_1: 0.1512  loss_ce_2: 0.1603  loss_mask_2: 0.1568  loss_dice_2: 0.1433  loss_ce_3: 0.1607  loss_mask_3: 0.161  loss_dice_3: 0.1457  loss_ce_4: 0.1604  loss_mask_4: 0.1602  loss_dice_4: 0.144  loss_ce_5: 0.1608  loss_mask_5: 0.1569  loss_dice_5: 0.1495  loss_ce_6: 0.1603  loss_mask_6: 0.1592  loss_dice_6: 0.1469  loss_ce_7: 0.161  loss_mask_7: 0.1564  loss_dice_7: 0.145  loss_ce_8: 0.1608  loss_mask_8: 0.1576  loss_dice_8: 0.1497  time: 0.5818  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:33:55] d2.utils.events INFO:  eta: 0:01:47  iter: 29819  total_loss: 4.286  loss_ce: 0.1297  loss_mask: 0.1713  loss_dice: 0.1423  loss_ce_0: 0.06369  loss_mask_0: 0.1711  loss_dice_0: 0.1456  loss_ce_1: 0.1292  loss_mask_1: 0.1682  loss_dice_1: 0.1365  loss_ce_2: 0.1292  loss_mask_2: 0.1761  loss_dice_2: 0.1387  loss_ce_3: 0.1295  loss_mask_3: 0.1728  loss_dice_3: 0.1377  loss_ce_4: 0.1294  loss_mask_4: 0.1766  loss_dice_4: 0.1374  loss_ce_5: 0.1294  loss_mask_5: 0.1794  loss_dice_5: 0.1356  loss_ce_6: 0.1294  loss_mask_6: 0.1706  loss_dice_6: 0.1414  loss_ce_7: 0.1298  loss_mask_7: 0.1718  loss_dice_7: 0.1368  loss_ce_8: 0.1299  loss_mask_8: 0.1678  loss_dice_8: 0.1349  time: 0.5818  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:34:08] d2.utils.events INFO:  eta: 0:01:35  iter: 29839  total_loss: 4.477  loss_ce: 0.1298  loss_mask: 0.153  loss_dice: 0.1501  loss_ce_0: 0.06366  loss_mask_0: 0.1516  loss_dice_0: 0.1478  loss_ce_1: 0.1296  loss_mask_1: 0.15  loss_dice_1: 0.1524  loss_ce_2: 0.1297  loss_mask_2: 0.1523  loss_dice_2: 0.1471  loss_ce_3: 0.1298  loss_mask_3: 0.1572  loss_dice_3: 0.1484  loss_ce_4: 0.1297  loss_mask_4: 0.1496  loss_dice_4: 0.1526  loss_ce_5: 0.1298  loss_mask_5: 0.1435  loss_dice_5: 0.1479  loss_ce_6: 0.1296  loss_mask_6: 0.152  loss_dice_6: 0.1515  loss_ce_7: 0.1299  loss_mask_7: 0.1506  loss_dice_7: 0.1482  loss_ce_8: 0.1298  loss_mask_8: 0.1479  loss_dice_8: 0.1458  time: 0.5818  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:34:21] d2.utils.events INFO:  eta: 0:01:23  iter: 29859  total_loss: 4.277  loss_ce: 0.09874  loss_mask: 0.1744  loss_dice: 0.1304  loss_ce_0: 0.05216  loss_mask_0: 0.1835  loss_dice_0: 0.1238  loss_ce_1: 0.09945  loss_mask_1: 0.1752  loss_dice_1: 0.1253  loss_ce_2: 0.09913  loss_mask_2: 0.1812  loss_dice_2: 0.1268  loss_ce_3: 0.09896  loss_mask_3: 0.1829  loss_dice_3: 0.1306  loss_ce_4: 0.09907  loss_mask_4: 0.1842  loss_dice_4: 0.1303  loss_ce_5: 0.09896  loss_mask_5: 0.1792  loss_dice_5: 0.1269  loss_ce_6: 0.09915  loss_mask_6: 0.176  loss_dice_6: 0.1304  loss_ce_7: 0.09881  loss_mask_7: 0.1809  loss_dice_7: 0.1282  loss_ce_8: 0.09893  loss_mask_8: 0.1795  loss_dice_8: 0.126  time: 0.5819  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:34:33] d2.utils.events INFO:  eta: 0:01:11  iter: 29879  total_loss: 4.613  loss_ce: 0.09852  loss_mask: 0.1976  loss_dice: 0.1391  loss_ce_0: 0.05216  loss_mask_0: 0.199  loss_dice_0: 0.1345  loss_ce_1: 0.09857  loss_mask_1: 0.2058  loss_dice_1: 0.1366  loss_ce_2: 0.09867  loss_mask_2: 0.2012  loss_dice_2: 0.1306  loss_ce_3: 0.09851  loss_mask_3: 0.2069  loss_dice_3: 0.1397  loss_ce_4: 0.09859  loss_mask_4: 0.1956  loss_dice_4: 0.1279  loss_ce_5: 0.09867  loss_mask_5: 0.2051  loss_dice_5: 0.1397  loss_ce_6: 0.09859  loss_mask_6: 0.1978  loss_dice_6: 0.1327  loss_ce_7: 0.09855  loss_mask_7: 0.2057  loss_dice_7: 0.1331  loss_ce_8: 0.09863  loss_mask_8: 0.2012  loss_dice_8: 0.1359  time: 0.5819  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:34:46] d2.utils.events INFO:  eta: 0:01:00  iter: 29899  total_loss: 4.452  loss_ce: 0.09881  loss_mask: 0.1934  loss_dice: 0.139  loss_ce_0: 0.05216  loss_mask_0: 0.1963  loss_dice_0: 0.1369  loss_ce_1: 0.09949  loss_mask_1: 0.1954  loss_dice_1: 0.1404  loss_ce_2: 0.09904  loss_mask_2: 0.2022  loss_dice_2: 0.1297  loss_ce_3: 0.099  loss_mask_3: 0.2015  loss_dice_3: 0.1312  loss_ce_4: 0.09911  loss_mask_4: 0.1911  loss_dice_4: 0.1335  loss_ce_5: 0.09904  loss_mask_5: 0.1892  loss_dice_5: 0.1377  loss_ce_6: 0.09915  loss_mask_6: 0.1912  loss_dice_6: 0.138  loss_ce_7: 0.09878  loss_mask_7: 0.1976  loss_dice_7: 0.1326  loss_ce_8: 0.09896  loss_mask_8: 0.1963  loss_dice_8: 0.1333  time: 0.5819  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:34:59] d2.utils.events INFO:  eta: 0:00:48  iter: 29919  total_loss: 4.675  loss_ce: 0.09844  loss_mask: 0.1673  loss_dice: 0.1574  loss_ce_0: 0.05216  loss_mask_0: 0.1624  loss_dice_0: 0.1508  loss_ce_1: 0.09863  loss_mask_1: 0.1599  loss_dice_1: 0.1518  loss_ce_2: 0.09852  loss_mask_2: 0.1698  loss_dice_2: 0.1544  loss_ce_3: 0.09848  loss_mask_3: 0.1671  loss_dice_3: 0.1578  loss_ce_4: 0.09855  loss_mask_4: 0.1702  loss_dice_4: 0.1543  loss_ce_5: 0.09852  loss_mask_5: 0.1618  loss_dice_5: 0.152  loss_ce_6: 0.09852  loss_mask_6: 0.1684  loss_dice_6: 0.1587  loss_ce_7: 0.09848  loss_mask_7: 0.1673  loss_dice_7: 0.1551  loss_ce_8: 0.09852  loss_mask_8: 0.163  loss_dice_8: 0.15  time: 0.5820  data_time: 0.0016  lr: 1e-06  max_mem: 2811M
[07/11 16:35:11] d2.utils.events INFO:  eta: 0:00:36  iter: 29939  total_loss: 4.577  loss_ce: 0.09844  loss_mask: 0.2026  loss_dice: 0.124  loss_ce_0: 0.05214  loss_mask_0: 0.2067  loss_dice_0: 0.1301  loss_ce_1: 0.09889  loss_mask_1: 0.1939  loss_dice_1: 0.1252  loss_ce_2: 0.09855  loss_mask_2: 0.1979  loss_dice_2: 0.1291  loss_ce_3: 0.09851  loss_mask_3: 0.2074  loss_dice_3: 0.1242  loss_ce_4: 0.0987  loss_mask_4: 0.1998  loss_dice_4: 0.1252  loss_ce_5: 0.09852  loss_mask_5: 0.2032  loss_dice_5: 0.1279  loss_ce_6: 0.09859  loss_mask_6: 0.2051  loss_dice_6: 0.1291  loss_ce_7: 0.09844  loss_mask_7: 0.2015  loss_dice_7: 0.1263  loss_ce_8: 0.09852  loss_mask_8: 0.1957  loss_dice_8: 0.1288  time: 0.5820  data_time: 0.0014  lr: 1e-06  max_mem: 2811M
[07/11 16:35:24] d2.utils.events INFO:  eta: 0:00:24  iter: 29959  total_loss: 4.75  loss_ce: 0.09818  loss_mask: 0.1773  loss_dice: 0.15  loss_ce_0: 0.0521  loss_mask_0: 0.177  loss_dice_0: 0.1507  loss_ce_1: 0.09839  loss_mask_1: 0.1842  loss_dice_1: 0.1505  loss_ce_2: 0.09829  loss_mask_2: 0.1767  loss_dice_2: 0.1448  loss_ce_3: 0.09818  loss_mask_3: 0.1842  loss_dice_3: 0.1462  loss_ce_4: 0.0984  loss_mask_4: 0.1801  loss_dice_4: 0.1487  loss_ce_5: 0.09822  loss_mask_5: 0.1799  loss_dice_5: 0.1507  loss_ce_6: 0.09822  loss_mask_6: 0.1785  loss_dice_6: 0.1444  loss_ce_7: 0.09822  loss_mask_7: 0.1823  loss_dice_7: 0.1443  loss_ce_8: 0.09829  loss_mask_8: 0.1813  loss_dice_8: 0.1483  time: 0.5820  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:35:37] d2.utils.events INFO:  eta: 0:00:12  iter: 29979  total_loss: 4.653  loss_ce: 0.09807  loss_mask: 0.1929  loss_dice: 0.1619  loss_ce_0: 0.05209  loss_mask_0: 0.1939  loss_dice_0: 0.1571  loss_ce_1: 0.09792  loss_mask_1: 0.1878  loss_dice_1: 0.1648  loss_ce_2: 0.098  loss_mask_2: 0.1949  loss_dice_2: 0.1617  loss_ce_3: 0.09803  loss_mask_3: 0.1878  loss_dice_3: 0.1577  loss_ce_4: 0.09803  loss_mask_4: 0.1935  loss_dice_4: 0.1618  loss_ce_5: 0.098  loss_mask_5: 0.1968  loss_dice_5: 0.1592  loss_ce_6: 0.09807  loss_mask_6: 0.1925  loss_dice_6: 0.1611  loss_ce_7: 0.09803  loss_mask_7: 0.1967  loss_dice_7: 0.1575  loss_ce_8: 0.09811  loss_mask_8: 0.1983  loss_dice_8: 0.1701  time: 0.5821  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:35:49] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256to256_instance/model_0029999.pth
[07/11 16:35:50] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256to256_instance/model_final.pth
[07/11 16:35:50] d2.utils.events INFO:  eta: 0:00:00  iter: 29999  total_loss: 4.556  loss_ce: 0.09781  loss_mask: 0.2217  loss_dice: 0.1396  loss_ce_0: 0.05205  loss_mask_0: 0.2121  loss_dice_0: 0.1364  loss_ce_1: 0.09761  loss_mask_1: 0.2156  loss_dice_1: 0.1366  loss_ce_2: 0.09754  loss_mask_2: 0.2216  loss_dice_2: 0.1339  loss_ce_3: 0.09748  loss_mask_3: 0.2187  loss_dice_3: 0.1361  loss_ce_4: 0.0977  loss_mask_4: 0.2228  loss_dice_4: 0.1358  loss_ce_5: 0.09763  loss_mask_5: 0.2236  loss_dice_5: 0.1424  loss_ce_6: 0.09751  loss_mask_6: 0.2129  loss_dice_6: 0.1313  loss_ce_7: 0.09763  loss_mask_7: 0.2233  loss_dice_7: 0.1379  loss_ce_8: 0.09774  loss_mask_8: 0.218  loss_dice_8: 0.1345  time: 0.5821  data_time: 0.0015  lr: 1e-06  max_mem: 2811M
[07/11 16:35:50] d2.engine.hooks INFO: Overall training speed: 29998 iterations in 4:51:02 (0.5821 s / it)
[07/11 16:35:50] d2.engine.hooks INFO: Total training time: 5:18:57 (0:27:55 on hooks)
[07/11 16:35:50] d2.data.datasets.coco INFO: Loaded 754 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_val2017.json
[07/11 16:35:50] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/11 16:35:50] d2.data.common INFO: Serializing 754 elements to byte tensors and concatenating them all ...
[07/11 16:35:50] d2.data.common INFO: Serialized dataset takes 2.49 MiB
[07/11 16:35:50] d2.evaluation.evaluator INFO: Start inference on 754 batches
[07/11 16:35:55] d2.evaluation.evaluator INFO: Inference done 11/754. Dataloading: 0.0005 s/iter. Inference: 0.1415 s/iter. Eval: 0.2754 s/iter. Total: 0.4174 s/iter. ETA=0:05:10
[07/11 16:36:00] d2.evaluation.evaluator INFO: Inference done 24/754. Dataloading: 0.0007 s/iter. Inference: 0.1435 s/iter. Eval: 0.2716 s/iter. Total: 0.4158 s/iter. ETA=0:05:03
[07/11 16:36:05] d2.evaluation.evaluator INFO: Inference done 36/754. Dataloading: 0.0008 s/iter. Inference: 0.1453 s/iter. Eval: 0.2701 s/iter. Total: 0.4163 s/iter. ETA=0:04:58
[07/11 16:36:10] d2.evaluation.evaluator INFO: Inference done 49/754. Dataloading: 0.0009 s/iter. Inference: 0.1454 s/iter. Eval: 0.2689 s/iter. Total: 0.4153 s/iter. ETA=0:04:52
[07/11 16:36:16] d2.evaluation.evaluator INFO: Inference done 61/754. Dataloading: 0.0009 s/iter. Inference: 0.1461 s/iter. Eval: 0.2696 s/iter. Total: 0.4167 s/iter. ETA=0:04:48
[07/11 16:36:21] d2.evaluation.evaluator INFO: Inference done 73/754. Dataloading: 0.0008 s/iter. Inference: 0.1459 s/iter. Eval: 0.2712 s/iter. Total: 0.4180 s/iter. ETA=0:04:44
[07/11 16:36:26] d2.evaluation.evaluator INFO: Inference done 85/754. Dataloading: 0.0008 s/iter. Inference: 0.1462 s/iter. Eval: 0.2709 s/iter. Total: 0.4181 s/iter. ETA=0:04:39
[07/11 16:36:31] d2.evaluation.evaluator INFO: Inference done 97/754. Dataloading: 0.0008 s/iter. Inference: 0.1469 s/iter. Eval: 0.2708 s/iter. Total: 0.4186 s/iter. ETA=0:04:35
[07/11 16:36:36] d2.evaluation.evaluator INFO: Inference done 109/754. Dataloading: 0.0008 s/iter. Inference: 0.1469 s/iter. Eval: 0.2713 s/iter. Total: 0.4190 s/iter. ETA=0:04:30
[07/11 16:36:41] d2.evaluation.evaluator INFO: Inference done 121/754. Dataloading: 0.0008 s/iter. Inference: 0.1467 s/iter. Eval: 0.2722 s/iter. Total: 0.4197 s/iter. ETA=0:04:25
[07/11 16:36:46] d2.evaluation.evaluator INFO: Inference done 133/754. Dataloading: 0.0008 s/iter. Inference: 0.1466 s/iter. Eval: 0.2722 s/iter. Total: 0.4196 s/iter. ETA=0:04:20
[07/11 16:36:51] d2.evaluation.evaluator INFO: Inference done 146/754. Dataloading: 0.0008 s/iter. Inference: 0.1464 s/iter. Eval: 0.2721 s/iter. Total: 0.4193 s/iter. ETA=0:04:14
[07/11 16:36:56] d2.evaluation.evaluator INFO: Inference done 158/754. Dataloading: 0.0008 s/iter. Inference: 0.1462 s/iter. Eval: 0.2729 s/iter. Total: 0.4200 s/iter. ETA=0:04:10
[07/11 16:37:01] d2.evaluation.evaluator INFO: Inference done 170/754. Dataloading: 0.0008 s/iter. Inference: 0.1462 s/iter. Eval: 0.2729 s/iter. Total: 0.4200 s/iter. ETA=0:04:05
[07/11 16:37:06] d2.evaluation.evaluator INFO: Inference done 182/754. Dataloading: 0.0008 s/iter. Inference: 0.1462 s/iter. Eval: 0.2727 s/iter. Total: 0.4198 s/iter. ETA=0:04:00
[07/11 16:37:12] d2.evaluation.evaluator INFO: Inference done 195/754. Dataloading: 0.0008 s/iter. Inference: 0.1459 s/iter. Eval: 0.2725 s/iter. Total: 0.4192 s/iter. ETA=0:03:54
[07/11 16:37:17] d2.evaluation.evaluator INFO: Inference done 207/754. Dataloading: 0.0008 s/iter. Inference: 0.1459 s/iter. Eval: 0.2727 s/iter. Total: 0.4195 s/iter. ETA=0:03:49
[07/11 16:37:22] d2.evaluation.evaluator INFO: Inference done 219/754. Dataloading: 0.0008 s/iter. Inference: 0.1461 s/iter. Eval: 0.2726 s/iter. Total: 0.4195 s/iter. ETA=0:03:44
[07/11 16:37:27] d2.evaluation.evaluator INFO: Inference done 232/754. Dataloading: 0.0008 s/iter. Inference: 0.1458 s/iter. Eval: 0.2726 s/iter. Total: 0.4192 s/iter. ETA=0:03:38
[07/11 16:37:33] d2.evaluation.evaluator INFO: Inference done 245/754. Dataloading: 0.0008 s/iter. Inference: 0.1457 s/iter. Eval: 0.2723 s/iter. Total: 0.4188 s/iter. ETA=0:03:33
[07/11 16:37:38] d2.evaluation.evaluator INFO: Inference done 257/754. Dataloading: 0.0008 s/iter. Inference: 0.1458 s/iter. Eval: 0.2722 s/iter. Total: 0.4189 s/iter. ETA=0:03:28
[07/11 16:37:43] d2.evaluation.evaluator INFO: Inference done 269/754. Dataloading: 0.0008 s/iter. Inference: 0.1458 s/iter. Eval: 0.2723 s/iter. Total: 0.4189 s/iter. ETA=0:03:23
[07/11 16:37:48] d2.evaluation.evaluator INFO: Inference done 281/754. Dataloading: 0.0008 s/iter. Inference: 0.1460 s/iter. Eval: 0.2722 s/iter. Total: 0.4191 s/iter. ETA=0:03:18
[07/11 16:37:53] d2.evaluation.evaluator INFO: Inference done 293/754. Dataloading: 0.0008 s/iter. Inference: 0.1461 s/iter. Eval: 0.2723 s/iter. Total: 0.4193 s/iter. ETA=0:03:13
[07/11 16:37:58] d2.evaluation.evaluator INFO: Inference done 305/754. Dataloading: 0.0008 s/iter. Inference: 0.1464 s/iter. Eval: 0.2724 s/iter. Total: 0.4195 s/iter. ETA=0:03:08
[07/11 16:38:04] d2.evaluation.evaluator INFO: Inference done 318/754. Dataloading: 0.0008 s/iter. Inference: 0.1460 s/iter. Eval: 0.2727 s/iter. Total: 0.4196 s/iter. ETA=0:03:02
[07/11 16:38:09] d2.evaluation.evaluator INFO: Inference done 330/754. Dataloading: 0.0008 s/iter. Inference: 0.1460 s/iter. Eval: 0.2729 s/iter. Total: 0.4197 s/iter. ETA=0:02:57
[07/11 16:38:14] d2.evaluation.evaluator INFO: Inference done 342/754. Dataloading: 0.0008 s/iter. Inference: 0.1461 s/iter. Eval: 0.2728 s/iter. Total: 0.4197 s/iter. ETA=0:02:52
[07/11 16:38:19] d2.evaluation.evaluator INFO: Inference done 354/754. Dataloading: 0.0008 s/iter. Inference: 0.1463 s/iter. Eval: 0.2727 s/iter. Total: 0.4198 s/iter. ETA=0:02:47
[07/11 16:38:24] d2.evaluation.evaluator INFO: Inference done 366/754. Dataloading: 0.0008 s/iter. Inference: 0.1464 s/iter. Eval: 0.2726 s/iter. Total: 0.4199 s/iter. ETA=0:02:42
[07/11 16:38:29] d2.evaluation.evaluator INFO: Inference done 378/754. Dataloading: 0.0008 s/iter. Inference: 0.1467 s/iter. Eval: 0.2729 s/iter. Total: 0.4204 s/iter. ETA=0:02:38
[07/11 16:38:34] d2.evaluation.evaluator INFO: Inference done 390/754. Dataloading: 0.0008 s/iter. Inference: 0.1468 s/iter. Eval: 0.2731 s/iter. Total: 0.4208 s/iter. ETA=0:02:33
[07/11 16:38:39] d2.evaluation.evaluator INFO: Inference done 402/754. Dataloading: 0.0008 s/iter. Inference: 0.1470 s/iter. Eval: 0.2731 s/iter. Total: 0.4210 s/iter. ETA=0:02:28
[07/11 16:38:45] d2.evaluation.evaluator INFO: Inference done 415/754. Dataloading: 0.0008 s/iter. Inference: 0.1469 s/iter. Eval: 0.2729 s/iter. Total: 0.4206 s/iter. ETA=0:02:22
[07/11 16:38:50] d2.evaluation.evaluator INFO: Inference done 428/754. Dataloading: 0.0008 s/iter. Inference: 0.1469 s/iter. Eval: 0.2727 s/iter. Total: 0.4205 s/iter. ETA=0:02:17
[07/11 16:38:55] d2.evaluation.evaluator INFO: Inference done 441/754. Dataloading: 0.0008 s/iter. Inference: 0.1468 s/iter. Eval: 0.2726 s/iter. Total: 0.4202 s/iter. ETA=0:02:11
[07/11 16:39:01] d2.evaluation.evaluator INFO: Inference done 454/754. Dataloading: 0.0008 s/iter. Inference: 0.1467 s/iter. Eval: 0.2725 s/iter. Total: 0.4201 s/iter. ETA=0:02:06
[07/11 16:39:06] d2.evaluation.evaluator INFO: Inference done 466/754. Dataloading: 0.0007 s/iter. Inference: 0.1469 s/iter. Eval: 0.2726 s/iter. Total: 0.4204 s/iter. ETA=0:02:01
[07/11 16:39:11] d2.evaluation.evaluator INFO: Inference done 478/754. Dataloading: 0.0007 s/iter. Inference: 0.1469 s/iter. Eval: 0.2726 s/iter. Total: 0.4204 s/iter. ETA=0:01:56
[07/11 16:39:16] d2.evaluation.evaluator INFO: Inference done 490/754. Dataloading: 0.0007 s/iter. Inference: 0.1469 s/iter. Eval: 0.2728 s/iter. Total: 0.4205 s/iter. ETA=0:01:51
[07/11 16:39:21] d2.evaluation.evaluator INFO: Inference done 502/754. Dataloading: 0.0007 s/iter. Inference: 0.1469 s/iter. Eval: 0.2729 s/iter. Total: 0.4206 s/iter. ETA=0:01:45
[07/11 16:39:27] d2.evaluation.evaluator INFO: Inference done 515/754. Dataloading: 0.0007 s/iter. Inference: 0.1467 s/iter. Eval: 0.2728 s/iter. Total: 0.4203 s/iter. ETA=0:01:40
[07/11 16:39:32] d2.evaluation.evaluator INFO: Inference done 527/754. Dataloading: 0.0007 s/iter. Inference: 0.1467 s/iter. Eval: 0.2727 s/iter. Total: 0.4203 s/iter. ETA=0:01:35
[07/11 16:39:37] d2.evaluation.evaluator INFO: Inference done 540/754. Dataloading: 0.0007 s/iter. Inference: 0.1465 s/iter. Eval: 0.2727 s/iter. Total: 0.4200 s/iter. ETA=0:01:29
[07/11 16:39:42] d2.evaluation.evaluator INFO: Inference done 553/754. Dataloading: 0.0007 s/iter. Inference: 0.1464 s/iter. Eval: 0.2726 s/iter. Total: 0.4198 s/iter. ETA=0:01:24
[07/11 16:39:47] d2.evaluation.evaluator INFO: Inference done 565/754. Dataloading: 0.0007 s/iter. Inference: 0.1464 s/iter. Eval: 0.2727 s/iter. Total: 0.4199 s/iter. ETA=0:01:19
[07/11 16:39:52] d2.evaluation.evaluator INFO: Inference done 577/754. Dataloading: 0.0007 s/iter. Inference: 0.1465 s/iter. Eval: 0.2726 s/iter. Total: 0.4199 s/iter. ETA=0:01:14
[07/11 16:39:58] d2.evaluation.evaluator INFO: Inference done 589/754. Dataloading: 0.0007 s/iter. Inference: 0.1465 s/iter. Eval: 0.2727 s/iter. Total: 0.4201 s/iter. ETA=0:01:09
[07/11 16:40:03] d2.evaluation.evaluator INFO: Inference done 601/754. Dataloading: 0.0007 s/iter. Inference: 0.1467 s/iter. Eval: 0.2730 s/iter. Total: 0.4206 s/iter. ETA=0:01:04
[07/11 16:40:08] d2.evaluation.evaluator INFO: Inference done 614/754. Dataloading: 0.0007 s/iter. Inference: 0.1465 s/iter. Eval: 0.2730 s/iter. Total: 0.4203 s/iter. ETA=0:00:58
[07/11 16:40:14] d2.evaluation.evaluator INFO: Inference done 627/754. Dataloading: 0.0007 s/iter. Inference: 0.1465 s/iter. Eval: 0.2728 s/iter. Total: 0.4202 s/iter. ETA=0:00:53
[07/11 16:40:19] d2.evaluation.evaluator INFO: Inference done 640/754. Dataloading: 0.0007 s/iter. Inference: 0.1465 s/iter. Eval: 0.2726 s/iter. Total: 0.4200 s/iter. ETA=0:00:47
[07/11 16:40:24] d2.evaluation.evaluator INFO: Inference done 653/754. Dataloading: 0.0007 s/iter. Inference: 0.1465 s/iter. Eval: 0.2725 s/iter. Total: 0.4199 s/iter. ETA=0:00:42
[07/11 16:40:30] d2.evaluation.evaluator INFO: Inference done 666/754. Dataloading: 0.0007 s/iter. Inference: 0.1465 s/iter. Eval: 0.2725 s/iter. Total: 0.4199 s/iter. ETA=0:00:36
[07/11 16:40:35] d2.evaluation.evaluator INFO: Inference done 678/754. Dataloading: 0.0007 s/iter. Inference: 0.1466 s/iter. Eval: 0.2727 s/iter. Total: 0.4201 s/iter. ETA=0:00:31
[07/11 16:40:40] d2.evaluation.evaluator INFO: Inference done 691/754. Dataloading: 0.0007 s/iter. Inference: 0.1466 s/iter. Eval: 0.2725 s/iter. Total: 0.4199 s/iter. ETA=0:00:26
[07/11 16:40:46] d2.evaluation.evaluator INFO: Inference done 704/754. Dataloading: 0.0007 s/iter. Inference: 0.1465 s/iter. Eval: 0.2724 s/iter. Total: 0.4198 s/iter. ETA=0:00:20
[07/11 16:40:51] d2.evaluation.evaluator INFO: Inference done 716/754. Dataloading: 0.0007 s/iter. Inference: 0.1466 s/iter. Eval: 0.2724 s/iter. Total: 0.4199 s/iter. ETA=0:00:15
[07/11 16:40:56] d2.evaluation.evaluator INFO: Inference done 729/754. Dataloading: 0.0007 s/iter. Inference: 0.1466 s/iter. Eval: 0.2724 s/iter. Total: 0.4198 s/iter. ETA=0:00:10
[07/11 16:41:01] d2.evaluation.evaluator INFO: Inference done 741/754. Dataloading: 0.0007 s/iter. Inference: 0.1467 s/iter. Eval: 0.2724 s/iter. Total: 0.4199 s/iter. ETA=0:00:05
[07/11 16:41:07] d2.evaluation.evaluator INFO: Inference done 754/754. Dataloading: 0.0007 s/iter. Inference: 0.1467 s/iter. Eval: 0.2722 s/iter. Total: 0.4197 s/iter. ETA=0:00:00
[07/11 16:41:07] d2.evaluation.evaluator INFO: Total inference time: 0:05:14.494454 (0.419886 s / iter per device, on 1 devices)
[07/11 16:41:07] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:01:49 (0.146701 s / iter per device, on 1 devices)
[07/11 16:41:08] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/11 16:41:08] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/11 16:41:09] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/11 16:41:11] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/11 16:41:11] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 16:41:11] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/11 16:41:19] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 66.238 | 66.238 | 66.238 |  nan  |  nan  | 66.238 |
[07/11 16:41:19] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/11 16:41:19] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 79.559 | defect     | 52.918 |
[07/11 16:41:19] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_new in csv format:
[07/11 16:41:19] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/11 16:41:19] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 16:41:19] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/11 16:41:19] d2.evaluation.testing INFO: copypaste: Task: segm
[07/11 16:41:19] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/11 16:41:19] d2.evaluation.testing INFO: copypaste: 66.2385,66.2385,66.2385,nan,nan,66.2385
[07/11 16:41:19] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256to256_instance/model_best.pth
[07/11 16:41:19] d2.engine.hooks INFO: Saved best model as latest eval score for total_loss is4.17332, better than last best score 4.45740 @ iteration 4999.
[07/12 14:47:28] detectron2 INFO: Rank of current process: 0. World size: 1
[07/12 14:47:29] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/12 14:47:29] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256to256_instance/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth'], resume=False)
[07/12 14:47:29] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256to256_instance/config.yaml:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_val_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m&id002[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m&id001[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id001[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m*id002[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 14:47:29] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_val_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 14:47:29] detectron2 INFO: Full config saved to ./output_coco_256to256_instance/config.yaml
[07/12 14:47:29] d2.utils.env INFO: Using a generated random seed 33375056
[07/12 14:47:32] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[07/12 14:47:32] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth ...
[07/12 14:47:32] d2.data.datasets.coco INFO: Loaded 754 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_val2017.json
[07/12 14:47:32] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 355          |   defect   | 399          |
|            |              |            |              |
|   total    | 754          |            |              |[0m
[07/12 14:47:32] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/12 14:47:32] d2.data.common INFO: Serializing 754 elements to byte tensors and concatenating them all ...
[07/12 14:47:32] d2.data.common INFO: Serialized dataset takes 2.49 MiB
[07/12 14:47:32] d2.evaluation.evaluator INFO: Start inference on 754 batches
[07/12 14:47:36] d2.evaluation.evaluator INFO: Inference done 11/754. Dataloading: 0.0005 s/iter. Inference: 0.0387 s/iter. Eval: 0.2281 s/iter. Total: 0.2673 s/iter. ETA=0:03:18
[07/12 14:47:41] d2.evaluation.evaluator INFO: Inference done 29/754. Dataloading: 0.0006 s/iter. Inference: 0.0473 s/iter. Eval: 0.2279 s/iter. Total: 0.2759 s/iter. ETA=0:03:20
[07/12 14:47:46] d2.evaluation.evaluator INFO: Inference done 47/754. Dataloading: 0.0006 s/iter. Inference: 0.0495 s/iter. Eval: 0.2282 s/iter. Total: 0.2784 s/iter. ETA=0:03:16
[07/12 14:47:51] d2.evaluation.evaluator INFO: Inference done 66/754. Dataloading: 0.0006 s/iter. Inference: 0.0482 s/iter. Eval: 0.2283 s/iter. Total: 0.2772 s/iter. ETA=0:03:10
[07/12 14:47:56] d2.evaluation.evaluator INFO: Inference done 85/754. Dataloading: 0.0006 s/iter. Inference: 0.0488 s/iter. Eval: 0.2284 s/iter. Total: 0.2779 s/iter. ETA=0:03:05
[07/12 14:48:01] d2.evaluation.evaluator INFO: Inference done 104/754. Dataloading: 0.0006 s/iter. Inference: 0.0483 s/iter. Eval: 0.2285 s/iter. Total: 0.2775 s/iter. ETA=0:03:00
[07/12 14:48:04] detectron2 INFO: Rank of current process: 0. World size: 1
[07/12 14:48:05] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/12 14:48:05] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256to256_instance/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256to256_instance/model_best.pth'], resume=False)
[07/12 14:48:05] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256to256_instance/config.yaml:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_val_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 14:48:05] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_val_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_best.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 14:48:05] detectron2 INFO: Full config saved to ./output_coco_256to256_instance/config.yaml
[07/12 14:48:05] d2.utils.env INFO: Using a generated random seed 8964347
[07/12 14:48:07] d2.evaluation.evaluator INFO: Inference done 122/754. Dataloading: 0.0006 s/iter. Inference: 0.0482 s/iter. Eval: 0.2291 s/iter. Total: 0.2781 s/iter. ETA=0:02:55
[07/12 14:48:07] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[07/12 14:48:07] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256to256_instance/model_best.pth ...
[07/12 14:48:07] d2.data.datasets.coco INFO: Loaded 754 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_val2017.json
[07/12 14:48:08] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 355          |   defect   | 399          |
|            |              |            |              |
|   total    | 754          |            |              |[0m
[07/12 14:48:08] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/12 14:48:08] d2.data.common INFO: Serializing 754 elements to byte tensors and concatenating them all ...
[07/12 14:48:08] d2.data.common INFO: Serialized dataset takes 2.49 MiB
[07/12 14:48:08] d2.evaluation.evaluator INFO: Start inference on 754 batches
[07/12 14:48:11] d2.evaluation.evaluator INFO: Inference done 11/754. Dataloading: 0.0005 s/iter. Inference: 0.0590 s/iter. Eval: 0.2302 s/iter. Total: 0.2896 s/iter. ETA=0:03:35
[07/12 14:48:12] d2.evaluation.evaluator INFO: Inference done 140/754. Dataloading: 0.0006 s/iter. Inference: 0.0488 s/iter. Eval: 0.2297 s/iter. Total: 0.2792 s/iter. ETA=0:02:51
[07/12 14:48:16] d2.evaluation.evaluator INFO: Inference done 29/754. Dataloading: 0.0006 s/iter. Inference: 0.0492 s/iter. Eval: 0.2324 s/iter. Total: 0.2823 s/iter. ETA=0:03:24
[07/12 14:48:17] d2.evaluation.evaluator INFO: Inference done 158/754. Dataloading: 0.0006 s/iter. Inference: 0.0488 s/iter. Eval: 0.2299 s/iter. Total: 0.2794 s/iter. ETA=0:02:46
[07/12 14:48:22] d2.evaluation.evaluator INFO: Inference done 47/754. Dataloading: 0.0006 s/iter. Inference: 0.0506 s/iter. Eval: 0.2335 s/iter. Total: 0.2847 s/iter. ETA=0:03:21
[07/12 14:48:22] d2.evaluation.evaluator INFO: Inference done 176/754. Dataloading: 0.0006 s/iter. Inference: 0.0490 s/iter. Eval: 0.2302 s/iter. Total: 0.2799 s/iter. ETA=0:02:41
[07/12 14:48:27] d2.evaluation.evaluator INFO: Inference done 65/754. Dataloading: 0.0006 s/iter. Inference: 0.0506 s/iter. Eval: 0.2341 s/iter. Total: 0.2854 s/iter. ETA=0:03:16
[07/12 14:48:27] d2.evaluation.evaluator INFO: Inference done 194/754. Dataloading: 0.0006 s/iter. Inference: 0.0495 s/iter. Eval: 0.2305 s/iter. Total: 0.2807 s/iter. ETA=0:02:37
[07/12 14:48:32] d2.evaluation.evaluator INFO: Inference done 83/754. Dataloading: 0.0006 s/iter. Inference: 0.0505 s/iter. Eval: 0.2345 s/iter. Total: 0.2857 s/iter. ETA=0:03:11
[07/12 14:48:32] d2.evaluation.evaluator INFO: Inference done 212/754. Dataloading: 0.0006 s/iter. Inference: 0.0498 s/iter. Eval: 0.2306 s/iter. Total: 0.2811 s/iter. ETA=0:02:32
[07/12 14:48:37] d2.evaluation.evaluator INFO: Inference done 101/754. Dataloading: 0.0006 s/iter. Inference: 0.0499 s/iter. Eval: 0.2345 s/iter. Total: 0.2851 s/iter. ETA=0:03:06
[07/12 14:48:37] d2.evaluation.evaluator INFO: Inference done 230/754. Dataloading: 0.0006 s/iter. Inference: 0.0499 s/iter. Eval: 0.2307 s/iter. Total: 0.2814 s/iter. ETA=0:02:27
[07/12 14:48:42] d2.evaluation.evaluator INFO: Inference done 119/754. Dataloading: 0.0006 s/iter. Inference: 0.0503 s/iter. Eval: 0.2348 s/iter. Total: 0.2857 s/iter. ETA=0:03:01
[07/12 14:48:42] d2.evaluation.evaluator INFO: Inference done 248/754. Dataloading: 0.0006 s/iter. Inference: 0.0501 s/iter. Eval: 0.2309 s/iter. Total: 0.2817 s/iter. ETA=0:02:22
[07/12 14:48:47] d2.evaluation.evaluator INFO: Inference done 136/754. Dataloading: 0.0006 s/iter. Inference: 0.0506 s/iter. Eval: 0.2355 s/iter. Total: 0.2868 s/iter. ETA=0:02:57
[07/12 14:48:48] d2.evaluation.evaluator INFO: Inference done 266/754. Dataloading: 0.0006 s/iter. Inference: 0.0505 s/iter. Eval: 0.2312 s/iter. Total: 0.2824 s/iter. ETA=0:02:17
[07/12 14:48:52] d2.evaluation.evaluator INFO: Inference done 154/754. Dataloading: 0.0006 s/iter. Inference: 0.0513 s/iter. Eval: 0.2353 s/iter. Total: 0.2873 s/iter. ETA=0:02:52
[07/12 14:48:53] d2.evaluation.evaluator INFO: Inference done 284/754. Dataloading: 0.0006 s/iter. Inference: 0.0506 s/iter. Eval: 0.2315 s/iter. Total: 0.2829 s/iter. ETA=0:02:12
[07/12 14:48:58] d2.evaluation.evaluator INFO: Inference done 172/754. Dataloading: 0.0006 s/iter. Inference: 0.0521 s/iter. Eval: 0.2352 s/iter. Total: 0.2880 s/iter. ETA=0:02:47
[07/12 14:48:58] d2.evaluation.evaluator INFO: Inference done 302/754. Dataloading: 0.0006 s/iter. Inference: 0.0508 s/iter. Eval: 0.2318 s/iter. Total: 0.2833 s/iter. ETA=0:02:08
[07/12 14:49:03] d2.evaluation.evaluator INFO: Inference done 190/754. Dataloading: 0.0006 s/iter. Inference: 0.0517 s/iter. Eval: 0.2348 s/iter. Total: 0.2872 s/iter. ETA=0:02:41
[07/12 14:49:03] d2.evaluation.evaluator INFO: Inference done 320/754. Dataloading: 0.0006 s/iter. Inference: 0.0505 s/iter. Eval: 0.2320 s/iter. Total: 0.2832 s/iter. ETA=0:02:02
[07/12 14:49:08] d2.evaluation.evaluator INFO: Inference done 208/754. Dataloading: 0.0006 s/iter. Inference: 0.0518 s/iter. Eval: 0.2345 s/iter. Total: 0.2870 s/iter. ETA=0:02:36
[07/12 14:49:08] d2.evaluation.evaluator INFO: Inference done 338/754. Dataloading: 0.0006 s/iter. Inference: 0.0504 s/iter. Eval: 0.2321 s/iter. Total: 0.2833 s/iter. ETA=0:01:57
[07/12 14:49:13] d2.evaluation.evaluator INFO: Inference done 226/754. Dataloading: 0.0006 s/iter. Inference: 0.0520 s/iter. Eval: 0.2342 s/iter. Total: 0.2869 s/iter. ETA=0:02:31
[07/12 14:49:14] d2.evaluation.evaluator INFO: Inference done 356/754. Dataloading: 0.0006 s/iter. Inference: 0.0504 s/iter. Eval: 0.2323 s/iter. Total: 0.2835 s/iter. ETA=0:01:52
[07/12 14:49:18] d2.evaluation.evaluator INFO: Inference done 244/754. Dataloading: 0.0006 s/iter. Inference: 0.0521 s/iter. Eval: 0.2346 s/iter. Total: 0.2874 s/iter. ETA=0:02:26
[07/12 14:49:19] d2.evaluation.evaluator INFO: Inference done 373/754. Dataloading: 0.0006 s/iter. Inference: 0.0508 s/iter. Eval: 0.2327 s/iter. Total: 0.2842 s/iter. ETA=0:01:48
[07/12 14:49:23] d2.evaluation.evaluator INFO: Inference done 262/754. Dataloading: 0.0006 s/iter. Inference: 0.0518 s/iter. Eval: 0.2347 s/iter. Total: 0.2872 s/iter. ETA=0:02:21
[07/12 14:49:24] d2.evaluation.evaluator INFO: Inference done 391/754. Dataloading: 0.0006 s/iter. Inference: 0.0510 s/iter. Eval: 0.2327 s/iter. Total: 0.2844 s/iter. ETA=0:01:43
[07/12 14:49:29] d2.evaluation.evaluator INFO: Inference done 280/754. Dataloading: 0.0006 s/iter. Inference: 0.0520 s/iter. Eval: 0.2348 s/iter. Total: 0.2875 s/iter. ETA=0:02:16
[07/12 14:49:29] d2.evaluation.evaluator INFO: Inference done 409/754. Dataloading: 0.0006 s/iter. Inference: 0.0513 s/iter. Eval: 0.2328 s/iter. Total: 0.2847 s/iter. ETA=0:01:38
[07/12 14:49:34] d2.evaluation.evaluator INFO: Inference done 298/754. Dataloading: 0.0006 s/iter. Inference: 0.0518 s/iter. Eval: 0.2349 s/iter. Total: 0.2873 s/iter. ETA=0:02:11
[07/12 14:49:34] d2.evaluation.evaluator INFO: Inference done 427/754. Dataloading: 0.0006 s/iter. Inference: 0.0512 s/iter. Eval: 0.2327 s/iter. Total: 0.2846 s/iter. ETA=0:01:33
[07/12 14:49:39] d2.evaluation.evaluator INFO: Inference done 316/754. Dataloading: 0.0006 s/iter. Inference: 0.0520 s/iter. Eval: 0.2350 s/iter. Total: 0.2877 s/iter. ETA=0:02:06
[07/12 14:49:39] d2.evaluation.evaluator INFO: Inference done 445/754. Dataloading: 0.0006 s/iter. Inference: 0.0514 s/iter. Eval: 0.2328 s/iter. Total: 0.2849 s/iter. ETA=0:01:28
[07/12 14:49:44] d2.evaluation.evaluator INFO: Inference done 334/754. Dataloading: 0.0006 s/iter. Inference: 0.0520 s/iter. Eval: 0.2350 s/iter. Total: 0.2877 s/iter. ETA=0:02:00
[07/12 14:49:45] d2.evaluation.evaluator INFO: Inference done 463/754. Dataloading: 0.0006 s/iter. Inference: 0.0516 s/iter. Eval: 0.2328 s/iter. Total: 0.2851 s/iter. ETA=0:01:22
[07/12 14:49:49] d2.evaluation.evaluator INFO: Inference done 352/754. Dataloading: 0.0006 s/iter. Inference: 0.0520 s/iter. Eval: 0.2352 s/iter. Total: 0.2878 s/iter. ETA=0:01:55
[07/12 14:49:50] d2.evaluation.evaluator INFO: Inference done 481/754. Dataloading: 0.0006 s/iter. Inference: 0.0518 s/iter. Eval: 0.2328 s/iter. Total: 0.2853 s/iter. ETA=0:01:17
[07/12 14:49:55] d2.evaluation.evaluator INFO: Inference done 370/754. Dataloading: 0.0006 s/iter. Inference: 0.0520 s/iter. Eval: 0.2353 s/iter. Total: 0.2881 s/iter. ETA=0:01:50
[07/12 14:49:55] d2.evaluation.evaluator INFO: Inference done 499/754. Dataloading: 0.0006 s/iter. Inference: 0.0518 s/iter. Eval: 0.2330 s/iter. Total: 0.2855 s/iter. ETA=0:01:12
[07/12 14:50:00] d2.evaluation.evaluator INFO: Inference done 388/754. Dataloading: 0.0006 s/iter. Inference: 0.0522 s/iter. Eval: 0.2353 s/iter. Total: 0.2883 s/iter. ETA=0:01:45
[07/12 14:50:00] d2.evaluation.evaluator INFO: Inference done 517/754. Dataloading: 0.0006 s/iter. Inference: 0.0518 s/iter. Eval: 0.2332 s/iter. Total: 0.2856 s/iter. ETA=0:01:07
[07/12 14:50:05] d2.evaluation.evaluator INFO: Inference done 406/754. Dataloading: 0.0006 s/iter. Inference: 0.0525 s/iter. Eval: 0.2353 s/iter. Total: 0.2885 s/iter. ETA=0:01:40
[07/12 14:50:06] d2.evaluation.evaluator INFO: Inference done 535/754. Dataloading: 0.0006 s/iter. Inference: 0.0518 s/iter. Eval: 0.2334 s/iter. Total: 0.2859 s/iter. ETA=0:01:02
[07/12 14:50:11] d2.evaluation.evaluator INFO: Inference done 552/754. Dataloading: 0.0006 s/iter. Inference: 0.0520 s/iter. Eval: 0.2335 s/iter. Total: 0.2863 s/iter. ETA=0:00:57
[07/12 14:50:11] d2.evaluation.evaluator INFO: Inference done 424/754. Dataloading: 0.0006 s/iter. Inference: 0.0529 s/iter. Eval: 0.2353 s/iter. Total: 0.2889 s/iter. ETA=0:01:35
[07/12 14:50:16] d2.evaluation.evaluator INFO: Inference done 570/754. Dataloading: 0.0006 s/iter. Inference: 0.0520 s/iter. Eval: 0.2337 s/iter. Total: 0.2864 s/iter. ETA=0:00:52
[07/12 14:50:16] d2.evaluation.evaluator INFO: Inference done 442/754. Dataloading: 0.0006 s/iter. Inference: 0.0530 s/iter. Eval: 0.2353 s/iter. Total: 0.2890 s/iter. ETA=0:01:30
[07/12 14:50:21] d2.evaluation.evaluator INFO: Inference done 587/754. Dataloading: 0.0006 s/iter. Inference: 0.0520 s/iter. Eval: 0.2340 s/iter. Total: 0.2867 s/iter. ETA=0:00:47
[07/12 14:50:21] d2.evaluation.evaluator INFO: Inference done 460/754. Dataloading: 0.0006 s/iter. Inference: 0.0530 s/iter. Eval: 0.2353 s/iter. Total: 0.2890 s/iter. ETA=0:01:24
[07/12 14:50:26] d2.evaluation.evaluator INFO: Inference done 477/754. Dataloading: 0.0006 s/iter. Inference: 0.0531 s/iter. Eval: 0.2354 s/iter. Total: 0.2892 s/iter. ETA=0:01:20
[07/12 14:50:26] d2.evaluation.evaluator INFO: Inference done 605/754. Dataloading: 0.0006 s/iter. Inference: 0.0522 s/iter. Eval: 0.2340 s/iter. Total: 0.2869 s/iter. ETA=0:00:42
[07/12 14:50:31] d2.evaluation.evaluator INFO: Inference done 495/754. Dataloading: 0.0006 s/iter. Inference: 0.0531 s/iter. Eval: 0.2356 s/iter. Total: 0.2893 s/iter. ETA=0:01:14
[07/12 14:50:31] d2.evaluation.evaluator INFO: Inference done 623/754. Dataloading: 0.0006 s/iter. Inference: 0.0522 s/iter. Eval: 0.2341 s/iter. Total: 0.2870 s/iter. ETA=0:00:37
[07/12 14:50:37] d2.evaluation.evaluator INFO: Inference done 513/754. Dataloading: 0.0006 s/iter. Inference: 0.0529 s/iter. Eval: 0.2357 s/iter. Total: 0.2894 s/iter. ETA=0:01:09
[07/12 14:50:37] d2.evaluation.evaluator INFO: Inference done 641/754. Dataloading: 0.0006 s/iter. Inference: 0.0523 s/iter. Eval: 0.2342 s/iter. Total: 0.2872 s/iter. ETA=0:00:32
[07/12 14:50:42] d2.evaluation.evaluator INFO: Inference done 530/754. Dataloading: 0.0006 s/iter. Inference: 0.0530 s/iter. Eval: 0.2359 s/iter. Total: 0.2896 s/iter. ETA=0:01:04
[07/12 14:50:42] d2.evaluation.evaluator INFO: Inference done 658/754. Dataloading: 0.0006 s/iter. Inference: 0.0524 s/iter. Eval: 0.2343 s/iter. Total: 0.2874 s/iter. ETA=0:00:27
[07/12 14:50:47] d2.evaluation.evaluator INFO: Inference done 548/754. Dataloading: 0.0006 s/iter. Inference: 0.0528 s/iter. Eval: 0.2361 s/iter. Total: 0.2896 s/iter. ETA=0:00:59
[07/12 14:50:47] d2.evaluation.evaluator INFO: Inference done 676/754. Dataloading: 0.0006 s/iter. Inference: 0.0524 s/iter. Eval: 0.2343 s/iter. Total: 0.2874 s/iter. ETA=0:00:22
[07/12 14:50:52] d2.evaluation.evaluator INFO: Inference done 566/754. Dataloading: 0.0006 s/iter. Inference: 0.0528 s/iter. Eval: 0.2362 s/iter. Total: 0.2897 s/iter. ETA=0:00:54
[07/12 14:50:52] d2.evaluation.evaluator INFO: Inference done 694/754. Dataloading: 0.0006 s/iter. Inference: 0.0524 s/iter. Eval: 0.2344 s/iter. Total: 0.2876 s/iter. ETA=0:00:17
[07/12 14:50:57] d2.evaluation.evaluator INFO: Inference done 711/754. Dataloading: 0.0006 s/iter. Inference: 0.0526 s/iter. Eval: 0.2345 s/iter. Total: 0.2878 s/iter. ETA=0:00:12
[07/12 14:50:57] d2.evaluation.evaluator INFO: Inference done 583/754. Dataloading: 0.0006 s/iter. Inference: 0.0529 s/iter. Eval: 0.2364 s/iter. Total: 0.2900 s/iter. ETA=0:00:49
[07/12 14:51:02] d2.evaluation.evaluator INFO: Inference done 729/754. Dataloading: 0.0006 s/iter. Inference: 0.0526 s/iter. Eval: 0.2346 s/iter. Total: 0.2879 s/iter. ETA=0:00:07
[07/12 14:51:02] d2.evaluation.evaluator INFO: Inference done 601/754. Dataloading: 0.0006 s/iter. Inference: 0.0530 s/iter. Eval: 0.2364 s/iter. Total: 0.2900 s/iter. ETA=0:00:44
[07/12 14:51:07] d2.evaluation.evaluator INFO: Inference done 746/754. Dataloading: 0.0006 s/iter. Inference: 0.0526 s/iter. Eval: 0.2347 s/iter. Total: 0.2881 s/iter. ETA=0:00:02
[07/12 14:51:07] d2.evaluation.evaluator INFO: Inference done 618/754. Dataloading: 0.0006 s/iter. Inference: 0.0531 s/iter. Eval: 0.2364 s/iter. Total: 0.2902 s/iter. ETA=0:00:39
[07/12 14:51:10] d2.evaluation.evaluator INFO: Total inference time: 0:03:35.777617 (0.288088 s / iter per device, on 1 devices)
[07/12 14:51:10] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:39 (0.052532 s / iter per device, on 1 devices)
[07/12 14:51:11] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/12 14:51:11] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/12 14:51:11] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/12 14:51:13] d2.evaluation.evaluator INFO: Inference done 636/754. Dataloading: 0.0006 s/iter. Inference: 0.0530 s/iter. Eval: 0.2364 s/iter. Total: 0.2901 s/iter. ETA=0:00:34
[07/12 14:51:14] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/12 14:51:14] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 14:51:14] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/12 14:51:18] d2.evaluation.evaluator INFO: Inference done 654/754. Dataloading: 0.0006 s/iter. Inference: 0.0529 s/iter. Eval: 0.2364 s/iter. Total: 0.2900 s/iter. ETA=0:00:29
[07/12 14:51:21] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 66.238 | 66.238 | 66.238 |  nan  |  nan  | 66.238 |
[07/12 14:51:21] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 14:51:21] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 79.559 | defect     | 52.918 |
[07/12 14:51:21] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_new in csv format:
[07/12 14:51:21] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/12 14:51:21] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 14:51:21] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/12 14:51:21] d2.evaluation.testing INFO: copypaste: Task: segm
[07/12 14:51:21] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 14:51:21] d2.evaluation.testing INFO: copypaste: 66.2385,66.2385,66.2385,nan,nan,66.2385
[07/12 14:51:23] d2.evaluation.evaluator INFO: Inference done 672/754. Dataloading: 0.0006 s/iter. Inference: 0.0528 s/iter. Eval: 0.2365 s/iter. Total: 0.2900 s/iter. ETA=0:00:23
[07/12 14:51:28] d2.evaluation.evaluator INFO: Inference done 690/754. Dataloading: 0.0006 s/iter. Inference: 0.0526 s/iter. Eval: 0.2367 s/iter. Total: 0.2900 s/iter. ETA=0:00:18
[07/12 14:51:34] d2.evaluation.evaluator INFO: Inference done 708/754. Dataloading: 0.0006 s/iter. Inference: 0.0526 s/iter. Eval: 0.2368 s/iter. Total: 0.2901 s/iter. ETA=0:00:13
[07/12 14:51:39] d2.evaluation.evaluator INFO: Inference done 726/754. Dataloading: 0.0006 s/iter. Inference: 0.0524 s/iter. Eval: 0.2370 s/iter. Total: 0.2901 s/iter. ETA=0:00:08
[07/12 14:51:44] d2.evaluation.evaluator INFO: Inference done 743/754. Dataloading: 0.0006 s/iter. Inference: 0.0522 s/iter. Eval: 0.2373 s/iter. Total: 0.2902 s/iter. ETA=0:00:03
[07/12 14:51:47] d2.evaluation.evaluator INFO: Total inference time: 0:03:37.479978 (0.290360 s / iter per device, on 1 devices)
[07/12 14:51:47] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:39 (0.052080 s / iter per device, on 1 devices)
[07/12 14:51:48] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/12 14:51:48] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/12 14:51:49] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/12 14:51:51] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/12 14:51:51] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 14:51:51] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/12 14:51:58] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 66.238 | 66.238 | 66.238 |  nan  |  nan  | 66.238 |
[07/12 14:51:58] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 14:51:58] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 79.559 | defect     | 52.918 |
[07/12 14:51:58] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_new in csv format:
[07/12 14:51:58] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/12 14:51:58] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 14:51:58] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/12 14:51:58] d2.evaluation.testing INFO: copypaste: Task: segm
[07/12 14:51:58] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 14:51:58] d2.evaluation.testing INFO: copypaste: 66.2385,66.2385,66.2385,nan,nan,66.2385
[07/12 15:00:08] detectron2 INFO: Rank of current process: 0. World size: 1
[07/12 15:00:09] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/12 15:00:09] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256to256_instance/config_single_instance_test.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth'], resume=False)
[07/12 15:00:09] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256to256_instance/config_single_instance_test.yaml:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_test_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_best.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:00:09] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_test_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:00:09] detectron2 INFO: Full config saved to ./output_coco_256to256_instance/config.yaml
[07/12 15:00:09] d2.utils.env INFO: Using a generated random seed 13423745
[07/12 15:00:12] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[07/12 15:00:12] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth ...
[07/12 15:05:26] detectron2 INFO: Rank of current process: 0. World size: 1
[07/12 15:05:27] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/12 15:05:27] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256to256_instance/config_single_instance_test.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth'], resume=False)
[07/12 15:05:27] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256to256_instance/config_single_instance_test.yaml:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_test_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_best.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:05:27] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_test_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:05:27] detectron2 INFO: Full config saved to ./output_coco_256to256_instance/config.yaml
[07/12 15:05:27] d2.utils.env INFO: Using a generated random seed 30986323
[07/12 15:05:29] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[07/12 15:05:29] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth ...
[07/12 15:05:29] d2.data.datasets.coco INFO: Loaded 508 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_COCO/annotations/instances_test2017.json
[07/12 15:05:29] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 320          |   defect   | 188          |
|            |              |            |              |
|   total    | 508          |            |              |[0m
[07/12 15:05:29] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/12 15:05:29] d2.data.common INFO: Serializing 508 elements to byte tensors and concatenating them all ...
[07/12 15:05:29] d2.data.common INFO: Serialized dataset takes 1.94 MiB
[07/12 15:05:29] d2.evaluation.evaluator INFO: Start inference on 508 batches
[07/12 15:05:33] d2.evaluation.evaluator INFO: Inference done 11/508. Dataloading: 0.0004 s/iter. Inference: 0.0571 s/iter. Eval: 0.2273 s/iter. Total: 0.2849 s/iter. ETA=0:02:21
[07/12 15:05:38] d2.evaluation.evaluator INFO: Inference done 30/508. Dataloading: 0.0006 s/iter. Inference: 0.0481 s/iter. Eval: 0.2276 s/iter. Total: 0.2764 s/iter. ETA=0:02:12
[07/12 15:05:44] d2.evaluation.evaluator INFO: Inference done 49/508. Dataloading: 0.0006 s/iter. Inference: 0.0498 s/iter. Eval: 0.2275 s/iter. Total: 0.2780 s/iter. ETA=0:02:07
[07/12 15:05:49] d2.evaluation.evaluator INFO: Inference done 68/508. Dataloading: 0.0006 s/iter. Inference: 0.0486 s/iter. Eval: 0.2275 s/iter. Total: 0.2768 s/iter. ETA=0:02:01
[07/12 15:05:54] d2.evaluation.evaluator INFO: Inference done 87/508. Dataloading: 0.0006 s/iter. Inference: 0.0482 s/iter. Eval: 0.2275 s/iter. Total: 0.2764 s/iter. ETA=0:01:56
[07/12 15:05:59] d2.evaluation.evaluator INFO: Inference done 106/508. Dataloading: 0.0006 s/iter. Inference: 0.0481 s/iter. Eval: 0.2276 s/iter. Total: 0.2765 s/iter. ETA=0:01:51
[07/12 15:06:05] d2.evaluation.evaluator INFO: Inference done 125/508. Dataloading: 0.0006 s/iter. Inference: 0.0476 s/iter. Eval: 0.2277 s/iter. Total: 0.2760 s/iter. ETA=0:01:45
[07/12 15:06:10] d2.evaluation.evaluator INFO: Inference done 144/508. Dataloading: 0.0006 s/iter. Inference: 0.0470 s/iter. Eval: 0.2277 s/iter. Total: 0.2754 s/iter. ETA=0:01:40
[07/12 15:06:15] d2.evaluation.evaluator INFO: Inference done 162/508. Dataloading: 0.0006 s/iter. Inference: 0.0471 s/iter. Eval: 0.2281 s/iter. Total: 0.2759 s/iter. ETA=0:01:35
[07/12 15:06:20] d2.evaluation.evaluator INFO: Inference done 181/508. Dataloading: 0.0006 s/iter. Inference: 0.0475 s/iter. Eval: 0.2282 s/iter. Total: 0.2764 s/iter. ETA=0:01:30
[07/12 15:06:25] d2.evaluation.evaluator INFO: Inference done 200/508. Dataloading: 0.0006 s/iter. Inference: 0.0473 s/iter. Eval: 0.2282 s/iter. Total: 0.2762 s/iter. ETA=0:01:25
[07/12 15:06:31] d2.evaluation.evaluator INFO: Inference done 219/508. Dataloading: 0.0006 s/iter. Inference: 0.0472 s/iter. Eval: 0.2283 s/iter. Total: 0.2762 s/iter. ETA=0:01:19
[07/12 15:06:36] d2.evaluation.evaluator INFO: Inference done 237/508. Dataloading: 0.0006 s/iter. Inference: 0.0475 s/iter. Eval: 0.2283 s/iter. Total: 0.2765 s/iter. ETA=0:01:14
[07/12 15:06:41] d2.evaluation.evaluator INFO: Inference done 255/508. Dataloading: 0.0006 s/iter. Inference: 0.0478 s/iter. Eval: 0.2283 s/iter. Total: 0.2768 s/iter. ETA=0:01:10
[07/12 15:06:46] d2.evaluation.evaluator INFO: Inference done 273/508. Dataloading: 0.0006 s/iter. Inference: 0.0480 s/iter. Eval: 0.2284 s/iter. Total: 0.2771 s/iter. ETA=0:01:05
[07/12 15:06:51] d2.evaluation.evaluator INFO: Inference done 292/508. Dataloading: 0.0006 s/iter. Inference: 0.0480 s/iter. Eval: 0.2284 s/iter. Total: 0.2771 s/iter. ETA=0:00:59
[07/12 15:06:56] d2.evaluation.evaluator INFO: Inference done 311/508. Dataloading: 0.0006 s/iter. Inference: 0.0481 s/iter. Eval: 0.2284 s/iter. Total: 0.2772 s/iter. ETA=0:00:54
[07/12 15:07:01] d2.evaluation.evaluator INFO: Inference done 330/508. Dataloading: 0.0006 s/iter. Inference: 0.0478 s/iter. Eval: 0.2284 s/iter. Total: 0.2770 s/iter. ETA=0:00:49
[07/12 15:07:07] d2.evaluation.evaluator INFO: Inference done 349/508. Dataloading: 0.0006 s/iter. Inference: 0.0478 s/iter. Eval: 0.2284 s/iter. Total: 0.2769 s/iter. ETA=0:00:44
[07/12 15:07:12] d2.evaluation.evaluator INFO: Inference done 367/508. Dataloading: 0.0006 s/iter. Inference: 0.0479 s/iter. Eval: 0.2286 s/iter. Total: 0.2772 s/iter. ETA=0:00:39
[07/12 15:07:17] d2.evaluation.evaluator INFO: Inference done 386/508. Dataloading: 0.0006 s/iter. Inference: 0.0478 s/iter. Eval: 0.2286 s/iter. Total: 0.2772 s/iter. ETA=0:00:33
[07/12 15:07:22] d2.evaluation.evaluator INFO: Inference done 404/508. Dataloading: 0.0006 s/iter. Inference: 0.0480 s/iter. Eval: 0.2287 s/iter. Total: 0.2774 s/iter. ETA=0:00:28
[07/12 15:07:27] d2.evaluation.evaluator INFO: Inference done 423/508. Dataloading: 0.0006 s/iter. Inference: 0.0478 s/iter. Eval: 0.2288 s/iter. Total: 0.2774 s/iter. ETA=0:00:23
[07/12 15:07:32] d2.evaluation.evaluator INFO: Inference done 441/508. Dataloading: 0.0006 s/iter. Inference: 0.0479 s/iter. Eval: 0.2289 s/iter. Total: 0.2775 s/iter. ETA=0:00:18
[07/12 15:07:38] d2.evaluation.evaluator INFO: Inference done 460/508. Dataloading: 0.0006 s/iter. Inference: 0.0477 s/iter. Eval: 0.2289 s/iter. Total: 0.2774 s/iter. ETA=0:00:13
[07/12 15:07:43] d2.evaluation.evaluator INFO: Inference done 479/508. Dataloading: 0.0006 s/iter. Inference: 0.0476 s/iter. Eval: 0.2289 s/iter. Total: 0.2773 s/iter. ETA=0:00:08
[07/12 15:07:48] d2.evaluation.evaluator INFO: Inference done 497/508. Dataloading: 0.0006 s/iter. Inference: 0.0478 s/iter. Eval: 0.2289 s/iter. Total: 0.2774 s/iter. ETA=0:00:03
[07/12 15:07:51] d2.evaluation.evaluator INFO: Total inference time: 0:02:19.542638 (0.277421 s / iter per device, on 1 devices)
[07/12 15:07:51] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:23 (0.047660 s / iter per device, on 1 devices)
[07/12 15:07:52] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/12 15:07:52] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/12 15:07:52] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/12 15:07:54] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/12 15:07:54] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 15:07:54] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/12 15:07:59] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 68.478 | 68.478 | 68.478 |  nan  |  nan  | 68.478 |
[07/12 15:07:59] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 15:07:59] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 99.948 | defect     | 37.008 |
[07/12 15:07:59] d2.engine.defaults INFO: Evaluation results for chick_dataset_test_new in csv format:
[07/12 15:07:59] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/12 15:07:59] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 15:07:59] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/12 15:07:59] d2.evaluation.testing INFO: copypaste: Task: segm
[07/12 15:07:59] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 15:07:59] d2.evaluation.testing INFO: copypaste: 68.4778,68.4778,68.4778,nan,nan,68.4778
[07/12 15:13:57] detectron2 INFO: Rank of current process: 0. World size: 1
[07/12 15:13:58] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/12 15:13:58] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256to256_instance/config_multiple_instance_val.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth'], resume=False)
[07/12 15:13:58] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256to256_instance/config_multiple_instance_val.yaml:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_val_syn_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_best.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:13:58] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_val_syn_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:13:58] detectron2 INFO: Full config saved to ./output_coco_256to256_instance/config.yaml
[07/12 15:13:58] d2.utils.env INFO: Using a generated random seed 62421829
[07/12 15:14:01] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[07/12 15:14:01] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth ...
[07/12 15:14:01] d2.data.datasets.coco INFO: Loaded 470 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_syn_bg/annotations/instances_val2017.json
[07/12 15:14:01] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 548          |   defect   | 501          |
|            |              |            |              |
|   total    | 1049         |            |              |[0m
[07/12 15:14:01] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/12 15:14:01] d2.data.common INFO: Serializing 470 elements to byte tensors and concatenating them all ...
[07/12 15:14:01] d2.data.common INFO: Serialized dataset takes 8.16 MiB
[07/12 15:14:01] d2.evaluation.evaluator INFO: Start inference on 470 batches
[07/12 15:14:10] d2.evaluation.evaluator INFO: Inference done 11/470. Dataloading: 0.0006 s/iter. Inference: 0.0607 s/iter. Eval: 0.6865 s/iter. Total: 0.7477 s/iter. ETA=0:05:43
[07/12 15:14:16] d2.evaluation.evaluator INFO: Inference done 18/470. Dataloading: 0.0006 s/iter. Inference: 0.0677 s/iter. Eval: 0.6865 s/iter. Total: 0.7549 s/iter. ETA=0:05:41
[07/12 15:14:21] d2.evaluation.evaluator INFO: Inference done 25/470. Dataloading: 0.0006 s/iter. Inference: 0.0665 s/iter. Eval: 0.6867 s/iter. Total: 0.7539 s/iter. ETA=0:05:35
[07/12 15:14:26] d2.evaluation.evaluator INFO: Inference done 32/470. Dataloading: 0.0006 s/iter. Inference: 0.0673 s/iter. Eval: 0.6876 s/iter. Total: 0.7556 s/iter. ETA=0:05:30
[07/12 15:14:29] detectron2 INFO: Rank of current process: 0. World size: 1
[07/12 15:14:30] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/12 15:14:30] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256to256_instance/config_multiple_instance_test.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth'], resume=False)
[07/12 15:14:30] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256to256_instance/config_multiple_instance_test.yaml:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_test_syn_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_best.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:14:30] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_test_syn_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:14:30] detectron2 INFO: Full config saved to ./output_coco_256to256_instance/config.yaml
[07/12 15:14:30] d2.utils.env INFO: Using a generated random seed 34234508
[07/12 15:14:32] d2.evaluation.evaluator INFO: Inference done 39/470. Dataloading: 0.0006 s/iter. Inference: 0.0680 s/iter. Eval: 0.6908 s/iter. Total: 0.7595 s/iter. ETA=0:05:27
[07/12 15:14:33] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[07/12 15:14:33] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth ...
[07/12 15:14:33] d2.data.datasets.coco INFO: Loaded 369 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_syn_bg/annotations/instances_test2017.json
[07/12 15:14:33] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 457          |   defect   | 377          |
|            |              |            |              |
|   total    | 834          |            |              |[0m
[07/12 15:14:33] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/12 15:14:33] d2.data.common INFO: Serializing 369 elements to byte tensors and concatenating them all ...
[07/12 15:14:33] d2.data.common INFO: Serialized dataset takes 7.74 MiB
[07/12 15:14:33] d2.evaluation.evaluator INFO: Start inference on 369 batches
[07/12 15:14:34] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:35] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:36] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:37] d2.evaluation.evaluator INFO: Inference done 45/470. Dataloading: 0.0008 s/iter. Inference: 0.0682 s/iter. Eval: 0.7027 s/iter. Total: 0.7717 s/iter. ETA=0:05:27
[07/12 15:14:37] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:39] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:40] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:41] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:42] d2.evaluation.evaluator INFO: Inference done 51/470. Dataloading: 0.0008 s/iter. Inference: 0.0705 s/iter. Eval: 0.7180 s/iter. Total: 0.7893 s/iter. ETA=0:05:30
[07/12 15:14:42] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:43] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:44] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:46] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:47] d2.evaluation.evaluator INFO: Inference done 11/369. Dataloading: 0.0007 s/iter. Inference: 0.8061 s/iter. Eval: 0.3708 s/iter. Total: 1.1776 s/iter. ETA=0:07:01
[07/12 15:14:47] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:47] d2.evaluation.evaluator INFO: Inference done 57/470. Dataloading: 0.0008 s/iter. Inference: 0.0707 s/iter. Eval: 0.7304 s/iter. Total: 0.8020 s/iter. ETA=0:05:31
[07/12 15:14:48] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:49] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:50] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:52] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:53] d2.evaluation.evaluator INFO: Inference done 16/369. Dataloading: 0.0007 s/iter. Inference: 0.8053 s/iter. Eval: 0.3720 s/iter. Total: 1.1781 s/iter. ETA=0:06:55
[07/12 15:14:53] d2.evaluation.evaluator INFO: Inference done 63/470. Dataloading: 0.0008 s/iter. Inference: 0.0716 s/iter. Eval: 0.7374 s/iter. Total: 0.8098 s/iter. ETA=0:05:29
[07/12 15:14:53] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:54] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:55] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:56] d2.utils.memory INFO: Attempting to copy inputs of <bound method MaskFormer.instance_inference of MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)> to CPU due to CUDA OOM
[07/12 15:14:58] d2.evaluation.evaluator INFO: Inference done 69/470. Dataloading: 0.0008 s/iter. Inference: 0.0726 s/iter. Eval: 0.7415 s/iter. Total: 0.8150 s/iter. ETA=0:05:26
[07/12 15:15:03] d2.evaluation.evaluator INFO: Inference done 76/470. Dataloading: 0.0008 s/iter. Inference: 0.0721 s/iter. Eval: 0.7367 s/iter. Total: 0.8096 s/iter. ETA=0:05:18
[07/12 15:15:09] d2.evaluation.evaluator INFO: Inference done 83/470. Dataloading: 0.0008 s/iter. Inference: 0.0722 s/iter. Eval: 0.7327 s/iter. Total: 0.8058 s/iter. ETA=0:05:11
[07/12 15:15:14] d2.evaluation.evaluator INFO: Inference done 90/470. Dataloading: 0.0008 s/iter. Inference: 0.0722 s/iter. Eval: 0.7294 s/iter. Total: 0.8024 s/iter. ETA=0:05:04
[07/12 15:15:19] d2.evaluation.evaluator INFO: Inference done 97/470. Dataloading: 0.0008 s/iter. Inference: 0.0716 s/iter. Eval: 0.7264 s/iter. Total: 0.7989 s/iter. ETA=0:04:58
[07/12 15:15:25] d2.evaluation.evaluator INFO: Inference done 104/470. Dataloading: 0.0008 s/iter. Inference: 0.0715 s/iter. Eval: 0.7239 s/iter. Total: 0.7962 s/iter. ETA=0:04:51
[07/12 15:15:30] d2.evaluation.evaluator INFO: Inference done 111/470. Dataloading: 0.0008 s/iter. Inference: 0.0711 s/iter. Eval: 0.7218 s/iter. Total: 0.7937 s/iter. ETA=0:04:44
[07/12 15:15:32] detectron2 INFO: Rank of current process: 0. World size: 1
[07/12 15:15:33] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/12 15:15:33] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256to256_instance/config_multiple_instance_val.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth'], resume=False)
[07/12 15:15:33] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256to256_instance/config_multiple_instance_val.yaml:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_val_syn_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_best.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:15:33] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_val_syn_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:15:33] detectron2 INFO: Full config saved to ./output_coco_256to256_instance/config.yaml
[07/12 15:15:33] d2.utils.env INFO: Using a generated random seed 37498601
[07/12 15:15:35] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[07/12 15:15:35] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth ...
[07/12 15:15:35] d2.evaluation.evaluator INFO: Inference done 118/470. Dataloading: 0.0007 s/iter. Inference: 0.0710 s/iter. Eval: 0.7206 s/iter. Total: 0.7924 s/iter. ETA=0:04:38
[07/12 15:15:36] d2.data.datasets.coco INFO: Loaded 470 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_syn_bg/annotations/instances_val2017.json
[07/12 15:15:36] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 548          |   defect   | 501          |
|            |              |            |              |
|   total    | 1049         |            |              |[0m
[07/12 15:15:36] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/12 15:15:36] d2.data.common INFO: Serializing 470 elements to byte tensors and concatenating them all ...
[07/12 15:15:36] d2.data.common INFO: Serialized dataset takes 8.16 MiB
[07/12 15:15:36] d2.evaluation.evaluator INFO: Start inference on 470 batches
[07/12 15:15:41] d2.evaluation.evaluator INFO: Inference done 125/470. Dataloading: 0.0007 s/iter. Inference: 0.0705 s/iter. Eval: 0.7196 s/iter. Total: 0.7910 s/iter. ETA=0:04:32
[07/12 15:15:45] d2.evaluation.evaluator INFO: Inference done 11/470. Dataloading: 0.0005 s/iter. Inference: 0.0456 s/iter. Eval: 0.7097 s/iter. Total: 0.7558 s/iter. ETA=0:05:46
[07/12 15:15:46] d2.evaluation.evaluator INFO: Inference done 132/470. Dataloading: 0.0007 s/iter. Inference: 0.0702 s/iter. Eval: 0.7195 s/iter. Total: 0.7905 s/iter. ETA=0:04:27
[07/12 15:15:50] d2.evaluation.evaluator INFO: Inference done 18/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7077 s/iter. Total: 0.7535 s/iter. ETA=0:05:40
[07/12 15:15:52] d2.evaluation.evaluator INFO: Inference done 139/470. Dataloading: 0.0007 s/iter. Inference: 0.0702 s/iter. Eval: 0.7190 s/iter. Total: 0.7900 s/iter. ETA=0:04:21
[07/12 15:15:55] d2.evaluation.evaluator INFO: Inference done 25/470. Dataloading: 0.0006 s/iter. Inference: 0.0450 s/iter. Eval: 0.7048 s/iter. Total: 0.7505 s/iter. ETA=0:05:33
[07/12 15:15:57] d2.evaluation.evaluator INFO: Inference done 146/470. Dataloading: 0.0007 s/iter. Inference: 0.0697 s/iter. Eval: 0.7178 s/iter. Total: 0.7883 s/iter. ETA=0:04:15
[07/12 15:16:00] d2.evaluation.evaluator INFO: Inference done 32/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7029 s/iter. Total: 0.7487 s/iter. ETA=0:05:27
[07/12 15:16:02] d2.evaluation.evaluator INFO: Inference done 153/470. Dataloading: 0.0007 s/iter. Inference: 0.0695 s/iter. Eval: 0.7170 s/iter. Total: 0.7873 s/iter. ETA=0:04:09
[07/12 15:16:06] d2.evaluation.evaluator INFO: Inference done 39/470. Dataloading: 0.0006 s/iter. Inference: 0.0450 s/iter. Eval: 0.7026 s/iter. Total: 0.7483 s/iter. ETA=0:05:22
[07/12 15:16:08] d2.evaluation.evaluator INFO: Inference done 160/470. Dataloading: 0.0007 s/iter. Inference: 0.0698 s/iter. Eval: 0.7171 s/iter. Total: 0.7877 s/iter. ETA=0:04:04
[07/12 15:16:11] d2.evaluation.evaluator INFO: Inference done 46/470. Dataloading: 0.0006 s/iter. Inference: 0.0450 s/iter. Eval: 0.7052 s/iter. Total: 0.7509 s/iter. ETA=0:05:18
[07/12 15:16:13] d2.evaluation.evaluator INFO: Inference done 167/470. Dataloading: 0.0007 s/iter. Inference: 0.0703 s/iter. Eval: 0.7167 s/iter. Total: 0.7877 s/iter. ETA=0:03:58
[07/12 15:16:16] d2.evaluation.evaluator INFO: Inference done 53/470. Dataloading: 0.0006 s/iter. Inference: 0.0450 s/iter. Eval: 0.7053 s/iter. Total: 0.7510 s/iter. ETA=0:05:13
[07/12 15:16:19] d2.evaluation.evaluator INFO: Inference done 174/470. Dataloading: 0.0007 s/iter. Inference: 0.0702 s/iter. Eval: 0.7158 s/iter. Total: 0.7868 s/iter. ETA=0:03:52
[07/12 15:16:21] d2.evaluation.evaluator INFO: Inference done 60/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7047 s/iter. Total: 0.7504 s/iter. ETA=0:05:07
[07/12 15:16:24] d2.evaluation.evaluator INFO: Inference done 181/470. Dataloading: 0.0007 s/iter. Inference: 0.0702 s/iter. Eval: 0.7153 s/iter. Total: 0.7864 s/iter. ETA=0:03:47
[07/12 15:16:27] d2.evaluation.evaluator INFO: Inference done 67/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7046 s/iter. Total: 0.7504 s/iter. ETA=0:05:02
[07/12 15:16:30] d2.evaluation.evaluator INFO: Inference done 188/470. Dataloading: 0.0007 s/iter. Inference: 0.0699 s/iter. Eval: 0.7155 s/iter. Total: 0.7863 s/iter. ETA=0:03:41
[07/12 15:16:32] d2.evaluation.evaluator INFO: Inference done 74/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7061 s/iter. Total: 0.7519 s/iter. ETA=0:04:57
[07/12 15:16:35] d2.evaluation.evaluator INFO: Inference done 195/470. Dataloading: 0.0007 s/iter. Inference: 0.0696 s/iter. Eval: 0.7155 s/iter. Total: 0.7860 s/iter. ETA=0:03:36
[07/12 15:16:37] d2.evaluation.evaluator INFO: Inference done 81/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7069 s/iter. Total: 0.7527 s/iter. ETA=0:04:52
[07/12 15:16:41] d2.evaluation.evaluator INFO: Inference done 202/470. Dataloading: 0.0007 s/iter. Inference: 0.0698 s/iter. Eval: 0.7152 s/iter. Total: 0.7858 s/iter. ETA=0:03:30
[07/12 15:16:43] d2.evaluation.evaluator INFO: Inference done 88/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7065 s/iter. Total: 0.7523 s/iter. ETA=0:04:47
[07/12 15:16:46] d2.evaluation.evaluator INFO: Inference done 209/470. Dataloading: 0.0007 s/iter. Inference: 0.0701 s/iter. Eval: 0.7144 s/iter. Total: 0.7854 s/iter. ETA=0:03:24
[07/12 15:16:48] d2.evaluation.evaluator INFO: Inference done 95/470. Dataloading: 0.0006 s/iter. Inference: 0.0450 s/iter. Eval: 0.7057 s/iter. Total: 0.7515 s/iter. ETA=0:04:41
[07/12 15:16:51] d2.evaluation.evaluator INFO: Inference done 216/470. Dataloading: 0.0007 s/iter. Inference: 0.0699 s/iter. Eval: 0.7145 s/iter. Total: 0.7852 s/iter. ETA=0:03:19
[07/12 15:16:53] d2.evaluation.evaluator INFO: Inference done 102/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7062 s/iter. Total: 0.7520 s/iter. ETA=0:04:36
[07/12 15:16:57] d2.evaluation.evaluator INFO: Inference done 223/470. Dataloading: 0.0007 s/iter. Inference: 0.0699 s/iter. Eval: 0.7145 s/iter. Total: 0.7852 s/iter. ETA=0:03:13
[07/12 15:16:58] d2.evaluation.evaluator INFO: Inference done 109/470. Dataloading: 0.0006 s/iter. Inference: 0.0452 s/iter. Eval: 0.7066 s/iter. Total: 0.7525 s/iter. ETA=0:04:31
[07/12 15:17:02] d2.evaluation.evaluator INFO: Inference done 230/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7141 s/iter. Total: 0.7850 s/iter. ETA=0:03:08
[07/12 15:17:04] d2.evaluation.evaluator INFO: Inference done 116/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7064 s/iter. Total: 0.7522 s/iter. ETA=0:04:26
[07/12 15:17:08] d2.evaluation.evaluator INFO: Inference done 237/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7136 s/iter. Total: 0.7843 s/iter. ETA=0:03:02
[07/12 15:17:09] d2.evaluation.evaluator INFO: Inference done 123/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7058 s/iter. Total: 0.7517 s/iter. ETA=0:04:20
[07/12 15:17:13] d2.evaluation.evaluator INFO: Inference done 244/470. Dataloading: 0.0007 s/iter. Inference: 0.0699 s/iter. Eval: 0.7136 s/iter. Total: 0.7843 s/iter. ETA=0:02:57
[07/12 15:17:14] d2.evaluation.evaluator INFO: Inference done 130/470. Dataloading: 0.0006 s/iter. Inference: 0.0451 s/iter. Eval: 0.7058 s/iter. Total: 0.7517 s/iter. ETA=0:04:15
[07/12 15:17:19] d2.evaluation.evaluator INFO: Inference done 251/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7136 s/iter. Total: 0.7844 s/iter. ETA=0:02:51
[07/12 15:17:19] d2.evaluation.evaluator INFO: Inference done 137/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7068 s/iter. Total: 0.7527 s/iter. ETA=0:04:10
[07/12 15:17:24] d2.evaluation.evaluator INFO: Inference done 258/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7134 s/iter. Total: 0.7842 s/iter. ETA=0:02:46
[07/12 15:17:25] d2.evaluation.evaluator INFO: Inference done 144/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7073 s/iter. Total: 0.7532 s/iter. ETA=0:04:05
[07/12 15:17:30] d2.evaluation.evaluator INFO: Inference done 265/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7131 s/iter. Total: 0.7839 s/iter. ETA=0:02:40
[07/12 15:17:30] d2.evaluation.evaluator INFO: Inference done 151/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7073 s/iter. Total: 0.7531 s/iter. ETA=0:04:00
[07/12 15:17:35] d2.evaluation.evaluator INFO: Inference done 272/470. Dataloading: 0.0007 s/iter. Inference: 0.0703 s/iter. Eval: 0.7128 s/iter. Total: 0.7839 s/iter. ETA=0:02:35
[07/12 15:17:35] d2.evaluation.evaluator INFO: Inference done 158/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7070 s/iter. Total: 0.7529 s/iter. ETA=0:03:54
[07/12 15:17:41] d2.evaluation.evaluator INFO: Inference done 279/470. Dataloading: 0.0007 s/iter. Inference: 0.0701 s/iter. Eval: 0.7129 s/iter. Total: 0.7838 s/iter. ETA=0:02:29
[07/12 15:17:41] d2.evaluation.evaluator INFO: Inference done 165/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7071 s/iter. Total: 0.7530 s/iter. ETA=0:03:49
[07/12 15:17:46] d2.evaluation.evaluator INFO: Inference done 172/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7076 s/iter. Total: 0.7534 s/iter. ETA=0:03:44
[07/12 15:17:46] d2.evaluation.evaluator INFO: Inference done 286/470. Dataloading: 0.0007 s/iter. Inference: 0.0701 s/iter. Eval: 0.7130 s/iter. Total: 0.7839 s/iter. ETA=0:02:24
[07/12 15:17:51] d2.evaluation.evaluator INFO: Inference done 179/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7077 s/iter. Total: 0.7535 s/iter. ETA=0:03:39
[07/12 15:17:51] d2.evaluation.evaluator INFO: Inference done 293/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7128 s/iter. Total: 0.7836 s/iter. ETA=0:02:18
[07/12 15:17:56] d2.evaluation.evaluator INFO: Inference done 186/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7075 s/iter. Total: 0.7534 s/iter. ETA=0:03:33
[07/12 15:17:57] d2.evaluation.evaluator INFO: Inference done 300/470. Dataloading: 0.0007 s/iter. Inference: 0.0699 s/iter. Eval: 0.7124 s/iter. Total: 0.7831 s/iter. ETA=0:02:13
[07/12 15:18:02] d2.evaluation.evaluator INFO: Inference done 193/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7075 s/iter. Total: 0.7533 s/iter. ETA=0:03:28
[07/12 15:18:02] d2.evaluation.evaluator INFO: Inference done 307/470. Dataloading: 0.0007 s/iter. Inference: 0.0698 s/iter. Eval: 0.7120 s/iter. Total: 0.7827 s/iter. ETA=0:02:07
[07/12 15:18:07] d2.evaluation.evaluator INFO: Inference done 200/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7072 s/iter. Total: 0.7531 s/iter. ETA=0:03:23
[07/12 15:18:08] d2.evaluation.evaluator INFO: Inference done 314/470. Dataloading: 0.0007 s/iter. Inference: 0.0697 s/iter. Eval: 0.7121 s/iter. Total: 0.7826 s/iter. ETA=0:02:02
[07/12 15:18:12] d2.evaluation.evaluator INFO: Inference done 207/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7077 s/iter. Total: 0.7535 s/iter. ETA=0:03:18
[07/12 15:18:13] d2.evaluation.evaluator INFO: Inference done 321/470. Dataloading: 0.0007 s/iter. Inference: 0.0698 s/iter. Eval: 0.7124 s/iter. Total: 0.7829 s/iter. ETA=0:01:56
[07/12 15:18:18] d2.evaluation.evaluator INFO: Inference done 214/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7082 s/iter. Total: 0.7541 s/iter. ETA=0:03:13
[07/12 15:18:19] d2.evaluation.evaluator INFO: Inference done 328/470. Dataloading: 0.0007 s/iter. Inference: 0.0696 s/iter. Eval: 0.7124 s/iter. Total: 0.7828 s/iter. ETA=0:01:51
[07/12 15:18:23] d2.evaluation.evaluator INFO: Inference done 221/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7084 s/iter. Total: 0.7542 s/iter. ETA=0:03:07
[07/12 15:18:24] d2.evaluation.evaluator INFO: Inference done 335/470. Dataloading: 0.0007 s/iter. Inference: 0.0697 s/iter. Eval: 0.7121 s/iter. Total: 0.7826 s/iter. ETA=0:01:45
[07/12 15:18:28] d2.evaluation.evaluator INFO: Inference done 228/470. Dataloading: 0.0007 s/iter. Inference: 0.0451 s/iter. Eval: 0.7083 s/iter. Total: 0.7542 s/iter. ETA=0:03:02
[07/12 15:18:29] d2.evaluation.evaluator INFO: Inference done 342/470. Dataloading: 0.0007 s/iter. Inference: 0.0698 s/iter. Eval: 0.7119 s/iter. Total: 0.7824 s/iter. ETA=0:01:40
[07/12 15:18:34] d2.evaluation.evaluator INFO: Inference done 235/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7082 s/iter. Total: 0.7541 s/iter. ETA=0:02:57
[07/12 15:18:35] d2.evaluation.evaluator INFO: Inference done 349/470. Dataloading: 0.0007 s/iter. Inference: 0.0697 s/iter. Eval: 0.7120 s/iter. Total: 0.7825 s/iter. ETA=0:01:34
[07/12 15:18:39] d2.evaluation.evaluator INFO: Inference done 242/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7085 s/iter. Total: 0.7544 s/iter. ETA=0:02:52
[07/12 15:18:41] d2.evaluation.evaluator INFO: Inference done 356/470. Dataloading: 0.0007 s/iter. Inference: 0.0695 s/iter. Eval: 0.7126 s/iter. Total: 0.7828 s/iter. ETA=0:01:29
[07/12 15:18:44] d2.evaluation.evaluator INFO: Inference done 249/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7088 s/iter. Total: 0.7548 s/iter. ETA=0:02:46
[07/12 15:18:46] d2.evaluation.evaluator INFO: Inference done 363/470. Dataloading: 0.0007 s/iter. Inference: 0.0695 s/iter. Eval: 0.7124 s/iter. Total: 0.7828 s/iter. ETA=0:01:23
[07/12 15:18:50] d2.evaluation.evaluator INFO: Inference done 256/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7089 s/iter. Total: 0.7548 s/iter. ETA=0:02:41
[07/12 15:18:52] d2.evaluation.evaluator INFO: Inference done 370/470. Dataloading: 0.0007 s/iter. Inference: 0.0698 s/iter. Eval: 0.7122 s/iter. Total: 0.7828 s/iter. ETA=0:01:18
[07/12 15:18:55] d2.evaluation.evaluator INFO: Inference done 263/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7087 s/iter. Total: 0.7546 s/iter. ETA=0:02:36
[07/12 15:18:57] d2.evaluation.evaluator INFO: Inference done 377/470. Dataloading: 0.0007 s/iter. Inference: 0.0696 s/iter. Eval: 0.7124 s/iter. Total: 0.7828 s/iter. ETA=0:01:12
[07/12 15:19:00] d2.evaluation.evaluator INFO: Inference done 270/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7089 s/iter. Total: 0.7549 s/iter. ETA=0:02:30
[07/12 15:19:03] d2.evaluation.evaluator INFO: Inference done 384/470. Dataloading: 0.0007 s/iter. Inference: 0.0698 s/iter. Eval: 0.7124 s/iter. Total: 0.7830 s/iter. ETA=0:01:07
[07/12 15:19:05] d2.evaluation.evaluator INFO: Inference done 277/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7092 s/iter. Total: 0.7551 s/iter. ETA=0:02:25
[07/12 15:19:08] d2.evaluation.evaluator INFO: Inference done 391/470. Dataloading: 0.0007 s/iter. Inference: 0.0699 s/iter. Eval: 0.7123 s/iter. Total: 0.7830 s/iter. ETA=0:01:01
[07/12 15:19:11] d2.evaluation.evaluator INFO: Inference done 284/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7092 s/iter. Total: 0.7552 s/iter. ETA=0:02:20
[07/12 15:19:13] d2.evaluation.evaluator INFO: Inference done 398/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7121 s/iter. Total: 0.7829 s/iter. ETA=0:00:56
[07/12 15:19:16] d2.evaluation.evaluator INFO: Inference done 291/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7092 s/iter. Total: 0.7551 s/iter. ETA=0:02:15
[07/12 15:19:19] d2.evaluation.evaluator INFO: Inference done 405/470. Dataloading: 0.0007 s/iter. Inference: 0.0699 s/iter. Eval: 0.7121 s/iter. Total: 0.7828 s/iter. ETA=0:00:50
[07/12 15:19:21] d2.evaluation.evaluator INFO: Inference done 298/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7091 s/iter. Total: 0.7550 s/iter. ETA=0:02:09
[07/12 15:19:24] d2.evaluation.evaluator INFO: Inference done 412/470. Dataloading: 0.0007 s/iter. Inference: 0.0699 s/iter. Eval: 0.7123 s/iter. Total: 0.7830 s/iter. ETA=0:00:45
[07/12 15:19:27] d2.evaluation.evaluator INFO: Inference done 305/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7094 s/iter. Total: 0.7554 s/iter. ETA=0:02:04
[07/12 15:19:30] d2.evaluation.evaluator INFO: Inference done 419/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7123 s/iter. Total: 0.7830 s/iter. ETA=0:00:39
[07/12 15:19:32] d2.evaluation.evaluator INFO: Inference done 312/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7098 s/iter. Total: 0.7557 s/iter. ETA=0:01:59
[07/12 15:19:35] d2.evaluation.evaluator INFO: Inference done 426/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7122 s/iter. Total: 0.7830 s/iter. ETA=0:00:34
[07/12 15:19:37] d2.evaluation.evaluator INFO: Inference done 319/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7099 s/iter. Total: 0.7558 s/iter. ETA=0:01:54
[07/12 15:19:41] d2.evaluation.evaluator INFO: Inference done 433/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7120 s/iter. Total: 0.7828 s/iter. ETA=0:00:28
[07/12 15:19:43] d2.evaluation.evaluator INFO: Inference done 326/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7099 s/iter. Total: 0.7558 s/iter. ETA=0:01:48
[07/12 15:19:46] d2.evaluation.evaluator INFO: Inference done 440/470. Dataloading: 0.0007 s/iter. Inference: 0.0702 s/iter. Eval: 0.7120 s/iter. Total: 0.7830 s/iter. ETA=0:00:23
[07/12 15:19:48] d2.evaluation.evaluator INFO: Inference done 333/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7098 s/iter. Total: 0.7558 s/iter. ETA=0:01:43
[07/12 15:19:52] d2.evaluation.evaluator INFO: Inference done 447/470. Dataloading: 0.0007 s/iter. Inference: 0.0701 s/iter. Eval: 0.7122 s/iter. Total: 0.7831 s/iter. ETA=0:00:18
[07/12 15:19:53] d2.evaluation.evaluator INFO: Inference done 340/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7101 s/iter. Total: 0.7561 s/iter. ETA=0:01:38
[07/12 15:19:57] d2.evaluation.evaluator INFO: Inference done 454/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7123 s/iter. Total: 0.7830 s/iter. ETA=0:00:12
[07/12 15:19:59] d2.evaluation.evaluator INFO: Inference done 347/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7104 s/iter. Total: 0.7563 s/iter. ETA=0:01:33
[07/12 15:20:03] d2.evaluation.evaluator INFO: Inference done 461/470. Dataloading: 0.0007 s/iter. Inference: 0.0699 s/iter. Eval: 0.7122 s/iter. Total: 0.7829 s/iter. ETA=0:00:07
[07/12 15:20:04] d2.evaluation.evaluator INFO: Inference done 354/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7105 s/iter. Total: 0.7565 s/iter. ETA=0:01:27
[07/12 15:20:08] d2.evaluation.evaluator INFO: Inference done 468/470. Dataloading: 0.0007 s/iter. Inference: 0.0700 s/iter. Eval: 0.7121 s/iter. Total: 0.7829 s/iter. ETA=0:00:01
[07/12 15:20:09] d2.evaluation.evaluator INFO: Inference done 361/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7108 s/iter. Total: 0.7567 s/iter. ETA=0:01:22
[07/12 15:20:10] d2.evaluation.evaluator INFO: Total inference time: 0:06:04.055422 (0.782915 s / iter per device, on 1 devices)
[07/12 15:20:10] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:32 (0.069964 s / iter per device, on 1 devices)
[07/12 15:20:11] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/12 15:20:11] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/12 15:20:11] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/12 15:20:13] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/12 15:20:13] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 15:20:13] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/12 15:20:15] d2.evaluation.evaluator INFO: Inference done 368/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7110 s/iter. Total: 0.7570 s/iter. ETA=0:01:17
[07/12 15:20:20] d2.evaluation.evaluator INFO: Inference done 375/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7112 s/iter. Total: 0.7571 s/iter. ETA=0:01:11
[07/12 15:20:22] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 1.630 | 3.266  | 1.552  |  nan  |  nan  | 1.630 |
[07/12 15:20:22] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 15:20:22] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 2.580 | defect     | 0.680 |
[07/12 15:20:22] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_syn_new in csv format:
[07/12 15:20:22] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/12 15:20:22] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 15:20:22] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/12 15:20:22] d2.evaluation.testing INFO: copypaste: Task: segm
[07/12 15:20:22] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 15:20:22] d2.evaluation.testing INFO: copypaste: 1.6298,3.2658,1.5519,nan,nan,1.6298
[07/12 15:20:26] d2.evaluation.evaluator INFO: Inference done 382/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7116 s/iter. Total: 0.7576 s/iter. ETA=0:01:06
[07/12 15:20:31] d2.evaluation.evaluator INFO: Inference done 389/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7121 s/iter. Total: 0.7581 s/iter. ETA=0:01:01
[07/12 15:20:37] d2.evaluation.evaluator INFO: Inference done 396/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7126 s/iter. Total: 0.7585 s/iter. ETA=0:00:56
[07/12 15:20:42] d2.evaluation.evaluator INFO: Inference done 403/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7131 s/iter. Total: 0.7591 s/iter. ETA=0:00:50
[07/12 15:20:48] d2.evaluation.evaluator INFO: Inference done 410/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7136 s/iter. Total: 0.7595 s/iter. ETA=0:00:45
[07/12 15:20:53] d2.evaluation.evaluator INFO: Inference done 417/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7140 s/iter. Total: 0.7600 s/iter. ETA=0:00:40
[07/12 15:20:59] d2.evaluation.evaluator INFO: Inference done 424/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7145 s/iter. Total: 0.7605 s/iter. ETA=0:00:34
[07/12 15:21:04] d2.evaluation.evaluator INFO: Inference done 431/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7149 s/iter. Total: 0.7609 s/iter. ETA=0:00:29
[07/12 15:21:10] d2.evaluation.evaluator INFO: Inference done 438/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7153 s/iter. Total: 0.7613 s/iter. ETA=0:00:24
[07/12 15:21:15] d2.evaluation.evaluator INFO: Inference done 445/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7157 s/iter. Total: 0.7617 s/iter. ETA=0:00:19
[07/12 15:21:21] d2.evaluation.evaluator INFO: Inference done 452/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7161 s/iter. Total: 0.7620 s/iter. ETA=0:00:13
[07/12 15:21:26] d2.evaluation.evaluator INFO: Inference done 459/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7164 s/iter. Total: 0.7624 s/iter. ETA=0:00:08
[07/12 15:21:32] d2.evaluation.evaluator INFO: Inference done 466/470. Dataloading: 0.0007 s/iter. Inference: 0.0452 s/iter. Eval: 0.7168 s/iter. Total: 0.7627 s/iter. ETA=0:00:03
[07/12 15:21:35] d2.evaluation.evaluator INFO: Total inference time: 0:05:54.804892 (0.763021 s / iter per device, on 1 devices)
[07/12 15:21:35] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:21 (0.045196 s / iter per device, on 1 devices)
[07/12 15:21:36] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/12 15:21:36] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/12 15:21:37] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/12 15:21:39] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/12 15:21:39] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 15:21:39] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/12 15:21:47] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 1.630 | 3.266  | 1.552  |  nan  |  nan  | 1.630 |
[07/12 15:21:47] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 15:21:47] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 2.580 | defect     | 0.680 |
[07/12 15:21:47] d2.engine.defaults INFO: Evaluation results for chick_dataset_val_syn_new in csv format:
[07/12 15:21:47] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/12 15:21:47] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 15:21:47] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/12 15:21:47] d2.evaluation.testing INFO: copypaste: Task: segm
[07/12 15:21:47] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 15:21:47] d2.evaluation.testing INFO: copypaste: 1.6298,3.2658,1.5519,nan,nan,1.6298
[07/12 15:23:28] detectron2 INFO: Rank of current process: 0. World size: 1
[07/12 15:23:29] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/12 15:23:29] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256to256_instance/config_multiple_instance_test.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth'], resume=False)
[07/12 15:23:29] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256to256_instance/config_multiple_instance_test.yaml:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_test_syn_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_best.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:23:29] detectron2 INFO: Running with full config:
[38;5;197mCUDNN_BENCHMARK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;197mDATALOADER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mASPECT_RATIO_GROUPING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mFILTER_EMPTY_ANNOTATIONS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mNUM_WORKERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m  [39m[38;5;197mREPEAT_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSAMPLER_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mTrainingSampler[39m
[38;5;197mDATASETS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mPRECOMPUTED_PROPOSAL_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPROPOSAL_FILES_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_test_syn_new[39m
[38;5;15m  [39m[38;5;197mTRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mchick_dataset_train_new[39m
[38;5;197mGLOBAL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mHACK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;197mINPUT[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mCOLOR_AUG_SSD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mCROP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSINGLE_CATEGORY_MAX_AREA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mSIZE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m    [39m[38;5;197mTYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mrelative_range[39m
[38;5;15m  [39m[38;5;197mDATASET_MAPPER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcoco_instance_lsj[39m
[38;5;15m  [39m[38;5;197mFORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRGB[39m
[38;5;15m  [39m[38;5;197mIMAGE_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMASK_FORMAT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mpolygon[39m
[38;5;15m  [39m[38;5;197mMAX_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMAX_SIZE_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mMIN_SIZE_TRAIN_SAMPLING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mchoice[39m
[38;5;15m  [39m[38;5;197mRANDOM_FLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhorizontal[39m
[38;5;15m  [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mMODEL[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mANCHOR_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mANGLES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-90[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m90[39m
[38;5;15m    [39m[38;5;197mASPECT_RATIOS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mDefaultAnchorGenerator[39m
[38;5;15m    [39m[38;5;197mOFFSET[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mSIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m128[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m  [39m[38;5;197mBACKBONE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFREEZE_AT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbuild_resnet_backbone[39m
[38;5;15m  [39m[38;5;197mDEVICE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mcuda[39m
[38;5;15m  [39m[38;5;197mFPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mFUSE_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msum[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mOUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m  [39m[38;5;197mKEYPOINT_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mLOAD_PROPOSALS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMASK_FORMER[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLASS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mDEC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m    [39m[38;5;197mDEEP_SUPERVISION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mDICE_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mDIM_FEEDFORWARD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2048[39m
[38;5;15m    [39m[38;5;197mDROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mENFORCE_INPUT_PROJ[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mHIDDEN_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mIMPORTANCE_SAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.75[39m
[38;5;15m    [39m[38;5;197mMASK_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mNHEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mNO_OBJECT_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mNUM_OBJECT_QUERIES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m    [39m[38;5;197mOVERSAMPLE_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m3.0[39m
[38;5;15m    [39m[38;5;197mPRE_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mSIZE_DIVISIBILITY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m32[39m
[38;5;15m    [39m[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mINSTANCE_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mOBJECT_MASK_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESHOLD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.8[39m
[38;5;15m      [39m[38;5;197mPANOPTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEMANTIC_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m      [39m[38;5;197mSEM_SEG_POSTPROCESSING_BEFORE_INFERENCE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mTRAIN_NUM_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12544[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMultiScaleMaskedTransformerDecoder[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_IN_FEATURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mmulti_scale_pixel_decoder[39m
[38;5;15m  [39m[38;5;197mMASK_ON[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mMETA_ARCHITECTURE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormer[39m
[38;5;15m  [39m[38;5;197mPANOPTIC_FPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCOMBINE[39m[38;5;15m:[39m
[38;5;15m      [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m      [39m[38;5;197mINSTANCES_CONFIDENCE_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mOVERLAP_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m      [39m[38;5;197mSTUFF_AREA_LIMIT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4096[39m
[38;5;15m    [39m[38;5;197mINSTANCE_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mPIXEL_MEAN[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m123.675[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m116.28[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m103.53[39m
[38;5;15m  [39m[38;5;197mPIXEL_STD[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m58.395[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.12[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m57.375[39m
[38;5;15m  [39m[38;5;197mPROPOSAL_GENERATOR[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mMIN_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRPN[39m
[38;5;15m  [39m[38;5;197mRESNETS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mDEFORM_MODULATED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEFORM_NUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mDEFORM_ON_PER_STAGE[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mDEPTH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m50[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mFrozenBN[39m
[38;5;15m    [39m[38;5;197mNUM_GROUPS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mRES2_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mRES4_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_DILATION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mRES5_MULTI_GRID[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mSTEM_OUT_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m    [39m[38;5;197mSTEM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mbasic[39m
[38;5;15m    [39m[38;5;197mSTRIDE_IN_1X1[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWIDTH_PER_GROUP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m64[39m
[38;5;15m  [39m[38;5;197mRETINANET[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_ALPHA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mFOCAL_LOSS_GAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mp7[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mNUM_CONVS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRIOR_PROB[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_LOSS_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mTOPK_CANDIDATES_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m  [39m[38;5;197mROI_BOX_CASCADE_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m30.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m      [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m15.0[39m
[38;5;15m    [39m[38;5;197mIOUS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m  [39m[38;5;197mROI_BOX_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m5.0[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_BBOX_REG[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mFC_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1024[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mNUM_FC[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mTRAIN_ON_PRED_BOXES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mROI_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mRes5ROIHeads[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m80[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.25[39m
[38;5;15m    [39m[38;5;197mPROPOSAL_APPEND_GT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mSCORE_THRESH_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mROI_KEYPOINT_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMIN_KEYPOINTS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mKRCNNConvDeconvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNUM_KEYPOINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m17[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mROI_MASK_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLS_AGNOSTIC_MASK[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mCONV_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskRCNNConvUpsampleHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;186m'[39m[38;5;186m'[39m
[38;5;15m    [39m[38;5;197mNUM_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_RESOLUTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m14[39m
[38;5;15m    [39m[38;5;197mPOOLER_SAMPLING_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;197mPOOLER_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mROIAlignV2[39m
[38;5;15m  [39m[38;5;197mRPN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mBATCH_SIZE_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141msmooth_l1[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_LOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBBOX_REG_WEIGHTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mBOUNDARY_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mCONV_DIMS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;197mHEAD_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mStandardRPNHead[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;197mIOU_LABELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m    [39m[38;5;197mIOU_THRESHOLDS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mNMS_THRESH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.7[39m
[38;5;15m    [39m[38;5;197mPOSITIVE_FRACTION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.5[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;197mPOST_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TEST[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6000[39m
[38;5;15m    [39m[38;5;197mPRE_NMS_TOPK_TRAIN[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m12000[39m
[38;5;15m    [39m[38;5;197mSMOOTH_L1_BETA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mSEM_SEG_HEAD[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mASPP_CHANNELS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mASPP_DILATIONS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m18[39m
[38;5;15m    [39m[38;5;197mASPP_DROPOUT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m    [39m[38;5;197mCOMMON_STRIDE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mCONVS_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_HEADS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m8[39m
[38;5;15m    [39m[38;5;197mDEFORMABLE_TRANSFORMER_ENCODER_N_POINTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mIGNORE_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m255[39m
[38;5;15m    [39m[38;5;197mIN_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mLOSS_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mhard_pixel_mining[39m
[38;5;15m    [39m[38;5;197mLOSS_WEIGHT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m    [39m[38;5;197mMASK_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m256[39m
[38;5;15m    [39m[38;5;197mNAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMaskFormerHead[39m
[38;5;15m    [39m[38;5;197mNORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mGN[39m
[38;5;15m    [39m[38;5;197mNUM_CLASSES[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mPIXEL_DECODER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mMSDeformAttnPixelDecoder[39m
[38;5;15m    [39m[38;5;197mPROJECT_CHANNELS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m48[39m
[38;5;15m    [39m[38;5;197mPROJECT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;197mTRANSFORMER_ENC_LAYERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;197mUSE_DEPTHWISE_SEPARABLE_CONV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mSWIN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mAPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mATTN_DROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mDEPTHS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;15m    [39m[38;5;197mDROP_PATH_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.3[39m
[38;5;15m    [39m[38;5;197mDROP_RATE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m    [39m[38;5;197mEMBED_DIM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m96[39m
[38;5;15m    [39m[38;5;197mMLP_RATIO[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4.0[39m
[38;5;15m    [39m[38;5;197mNUM_HEADS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m12[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m24[39m
[38;5;15m    [39m[38;5;197mOUT_FEATURES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres2[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres3[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres4[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141mres5[39m
[38;5;15m    [39m[38;5;197mPATCH_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mPATCH_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15m    [39m[38;5;197mPRETRAIN_IMG_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m224[39m
[38;5;15m    [39m[38;5;197mQKV_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mQK_SCALE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m    [39m[38;5;197mUSE_CHECKPOINT[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mWINDOW_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m7[39m
[38;5;15m  [39m[38;5;197mWEIGHTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m/home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth[39m
[38;5;197mOUTPUT_DIR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m./output_coco_256to256_instance[39m
[38;5;197mSEED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m-1[39m
[38;5;197mSOLVER[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAMP[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m  [39m[38;5;197mBACKBONE_MULTIPLIER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mBASE_LR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0001[39m
[38;5;15m  [39m[38;5;197mBIAS_LR_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mCHECKPOINT_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mCLIP_GRADIENTS[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mCLIP_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfull_model[39m
[38;5;15m    [39m[38;5;197mCLIP_VALUE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.01[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mNORM_TYPE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2.0[39m
[38;5;15m  [39m[38;5;197mGAMMA[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.1[39m
[38;5;15m  [39m[38;5;197mIMS_PER_BATCH[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1[39m
[38;5;15m  [39m[38;5;197mLR_SCHEDULER_NAME[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mWarmupMultiStepLR[39m
[38;5;15m  [39m[38;5;197mMAX_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m30000[39m
[38;5;15m  [39m[38;5;197mMOMENTUM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mNESTEROV[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m  [39m[38;5;197mOPTIMIZER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mADAMW[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_CONSTANT_ENDING[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mPOLY_LR_POWER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.9[39m
[38;5;15m  [39m[38;5;197mREFERENCE_WORLD_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15m  [39m[38;5;197mSTEPS[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m10000[39m
[38;5;15m  [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m20000[39m
[38;5;15m  [39m[38;5;197mWARMUP_FACTOR[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m1.0[39m
[38;5;15m  [39m[38;5;197mWARMUP_ITERS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15m  [39m[38;5;197mWARMUP_METHOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mlinear[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.05[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_BIAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mnull[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_EMBED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15m  [39m[38;5;197mWEIGHT_DECAY_NORM[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;197mTEST[39m[38;5;15m:[39m
[38;5;15m  [39m[38;5;197mAUG[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mFLIP[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mtrue[39m
[38;5;15m    [39m[38;5;197mMAX_SIZE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m4000[39m
[38;5;15m    [39m[38;5;197mMIN_SIZES[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m400[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m500[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m600[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m700[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m800[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m900[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1100[39m
[38;5;15m    [39m[38;5;15m-[39m[38;5;15m [39m[38;5;141m1200[39m
[38;5;15m  [39m[38;5;197mDETECTIONS_PER_IMAGE[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m100[39m
[38;5;15m  [39m[38;5;197mEVAL_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m5000[39m
[38;5;15m  [39m[38;5;197mEXPECTED_RESULTS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mKEYPOINT_OKS_SIGMAS[39m[38;5;15m:[39m[38;5;15m [39m[38;5;15m[[39m[38;5;15m][39m
[38;5;15m  [39m[38;5;197mPRECISE_BN[39m[38;5;15m:[39m
[38;5;15m    [39m[38;5;197mENABLED[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141mfalse[39m
[38;5;15m    [39m[38;5;197mNUM_ITER[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m200[39m
[38;5;197mVERSION[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m2[39m
[38;5;197mVIS_PERIOD[39m[38;5;15m:[39m[38;5;15m [39m[38;5;141m0[39m

[07/12 15:23:29] detectron2 INFO: Full config saved to ./output_coco_256to256_instance/config.yaml
[07/12 15:23:29] d2.utils.env INFO: Using a generated random seed 33011729
[07/12 15:23:31] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (deconv_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (deconv2_mask_features): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[07/12 15:23:31] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256to256_instance/model_final.pth ...
[07/12 15:23:31] d2.data.datasets.coco INFO: Loaded 369 images in COCO format from /home/tqsang/V100/tqsang/crop_obj/front_2_class_new_syn_bg/annotations/instances_test2017.json
[07/12 15:23:31] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 457          |   defect   | 377          |
|            |              |            |              |
|   total    | 834          |            |              |[0m
[07/12 15:23:31] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(256, 256), max_size=256, sample_style='choice')]
[07/12 15:23:31] d2.data.common INFO: Serializing 369 elements to byte tensors and concatenating them all ...
[07/12 15:23:31] d2.data.common INFO: Serialized dataset takes 7.74 MiB
[07/12 15:23:31] d2.evaluation.evaluator INFO: Start inference on 369 batches
[07/12 15:23:40] d2.evaluation.evaluator INFO: Inference done 11/369. Dataloading: 0.0006 s/iter. Inference: 0.0448 s/iter. Eval: 0.6872 s/iter. Total: 0.7326 s/iter. ETA=0:04:22
[07/12 15:23:45] d2.evaluation.evaluator INFO: Inference done 18/369. Dataloading: 0.0006 s/iter. Inference: 0.0448 s/iter. Eval: 0.6879 s/iter. Total: 0.7333 s/iter. ETA=0:04:17
[07/12 15:23:50] d2.evaluation.evaluator INFO: Inference done 25/369. Dataloading: 0.0007 s/iter. Inference: 0.0447 s/iter. Eval: 0.6871 s/iter. Total: 0.7326 s/iter. ETA=0:04:12
[07/12 15:23:55] d2.evaluation.evaluator INFO: Inference done 32/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6871 s/iter. Total: 0.7328 s/iter. ETA=0:04:06
[07/12 15:24:00] d2.evaluation.evaluator INFO: Inference done 39/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6873 s/iter. Total: 0.7329 s/iter. ETA=0:04:01
[07/12 15:24:06] d2.evaluation.evaluator INFO: Inference done 46/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6872 s/iter. Total: 0.7329 s/iter. ETA=0:03:56
[07/12 15:24:11] d2.evaluation.evaluator INFO: Inference done 53/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6871 s/iter. Total: 0.7328 s/iter. ETA=0:03:51
[07/12 15:24:16] d2.evaluation.evaluator INFO: Inference done 60/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6872 s/iter. Total: 0.7328 s/iter. ETA=0:03:46
[07/12 15:24:21] d2.evaluation.evaluator INFO: Inference done 67/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6873 s/iter. Total: 0.7329 s/iter. ETA=0:03:41
[07/12 15:24:26] d2.evaluation.evaluator INFO: Inference done 74/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6877 s/iter. Total: 0.7333 s/iter. ETA=0:03:36
[07/12 15:24:31] d2.evaluation.evaluator INFO: Inference done 81/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6879 s/iter. Total: 0.7335 s/iter. ETA=0:03:31
[07/12 15:24:36] d2.evaluation.evaluator INFO: Inference done 88/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6879 s/iter. Total: 0.7335 s/iter. ETA=0:03:26
[07/12 15:24:42] d2.evaluation.evaluator INFO: Inference done 95/369. Dataloading: 0.0007 s/iter. Inference: 0.0447 s/iter. Eval: 0.6881 s/iter. Total: 0.7336 s/iter. ETA=0:03:21
[07/12 15:24:47] d2.evaluation.evaluator INFO: Inference done 102/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6879 s/iter. Total: 0.7335 s/iter. ETA=0:03:15
[07/12 15:24:52] d2.evaluation.evaluator INFO: Inference done 109/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6880 s/iter. Total: 0.7335 s/iter. ETA=0:03:10
[07/12 15:24:57] d2.evaluation.evaluator INFO: Inference done 116/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6880 s/iter. Total: 0.7336 s/iter. ETA=0:03:05
[07/12 15:25:02] d2.evaluation.evaluator INFO: Inference done 123/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6881 s/iter. Total: 0.7337 s/iter. ETA=0:03:00
[07/12 15:25:07] d2.evaluation.evaluator INFO: Inference done 130/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6882 s/iter. Total: 0.7338 s/iter. ETA=0:02:55
[07/12 15:25:12] d2.evaluation.evaluator INFO: Inference done 137/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6882 s/iter. Total: 0.7338 s/iter. ETA=0:02:50
[07/12 15:25:18] d2.evaluation.evaluator INFO: Inference done 144/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6887 s/iter. Total: 0.7343 s/iter. ETA=0:02:45
[07/12 15:25:23] d2.evaluation.evaluator INFO: Inference done 151/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6887 s/iter. Total: 0.7343 s/iter. ETA=0:02:40
[07/12 15:25:28] d2.evaluation.evaluator INFO: Inference done 158/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6887 s/iter. Total: 0.7343 s/iter. ETA=0:02:34
[07/12 15:25:33] d2.evaluation.evaluator INFO: Inference done 165/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6888 s/iter. Total: 0.7345 s/iter. ETA=0:02:29
[07/12 15:25:38] d2.evaluation.evaluator INFO: Inference done 172/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6888 s/iter. Total: 0.7345 s/iter. ETA=0:02:24
[07/12 15:25:43] d2.evaluation.evaluator INFO: Inference done 179/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6889 s/iter. Total: 0.7346 s/iter. ETA=0:02:19
[07/12 15:25:49] d2.evaluation.evaluator INFO: Inference done 186/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6890 s/iter. Total: 0.7347 s/iter. ETA=0:02:14
[07/12 15:25:54] d2.evaluation.evaluator INFO: Inference done 193/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6891 s/iter. Total: 0.7348 s/iter. ETA=0:02:09
[07/12 15:25:59] d2.evaluation.evaluator INFO: Inference done 200/369. Dataloading: 0.0007 s/iter. Inference: 0.0449 s/iter. Eval: 0.6892 s/iter. Total: 0.7348 s/iter. ETA=0:02:04
[07/12 15:26:04] d2.evaluation.evaluator INFO: Inference done 207/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6892 s/iter. Total: 0.7349 s/iter. ETA=0:01:59
[07/12 15:26:09] d2.evaluation.evaluator INFO: Inference done 214/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6893 s/iter. Total: 0.7349 s/iter. ETA=0:01:53
[07/12 15:26:14] d2.evaluation.evaluator INFO: Inference done 221/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6893 s/iter. Total: 0.7349 s/iter. ETA=0:01:48
[07/12 15:26:19] d2.evaluation.evaluator INFO: Inference done 228/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6894 s/iter. Total: 0.7350 s/iter. ETA=0:01:43
[07/12 15:26:25] d2.evaluation.evaluator INFO: Inference done 235/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6895 s/iter. Total: 0.7351 s/iter. ETA=0:01:38
[07/12 15:26:30] d2.evaluation.evaluator INFO: Inference done 242/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6895 s/iter. Total: 0.7351 s/iter. ETA=0:01:33
[07/12 15:26:35] d2.evaluation.evaluator INFO: Inference done 249/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6895 s/iter. Total: 0.7350 s/iter. ETA=0:01:28
[07/12 15:26:40] d2.evaluation.evaluator INFO: Inference done 256/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6895 s/iter. Total: 0.7351 s/iter. ETA=0:01:23
[07/12 15:26:45] d2.evaluation.evaluator INFO: Inference done 263/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6896 s/iter. Total: 0.7351 s/iter. ETA=0:01:17
[07/12 15:26:50] d2.evaluation.evaluator INFO: Inference done 270/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6896 s/iter. Total: 0.7352 s/iter. ETA=0:01:12
[07/12 15:26:56] d2.evaluation.evaluator INFO: Inference done 277/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6896 s/iter. Total: 0.7352 s/iter. ETA=0:01:07
[07/12 15:27:01] d2.evaluation.evaluator INFO: Inference done 284/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6896 s/iter. Total: 0.7352 s/iter. ETA=0:01:02
[07/12 15:27:06] d2.evaluation.evaluator INFO: Inference done 291/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6896 s/iter. Total: 0.7352 s/iter. ETA=0:00:57
[07/12 15:27:11] d2.evaluation.evaluator INFO: Inference done 298/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6897 s/iter. Total: 0.7353 s/iter. ETA=0:00:52
[07/12 15:27:16] d2.evaluation.evaluator INFO: Inference done 305/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6898 s/iter. Total: 0.7353 s/iter. ETA=0:00:47
[07/12 15:27:21] d2.evaluation.evaluator INFO: Inference done 312/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6898 s/iter. Total: 0.7354 s/iter. ETA=0:00:41
[07/12 15:27:26] d2.evaluation.evaluator INFO: Inference done 319/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6898 s/iter. Total: 0.7354 s/iter. ETA=0:00:36
[07/12 15:27:32] d2.evaluation.evaluator INFO: Inference done 326/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6899 s/iter. Total: 0.7355 s/iter. ETA=0:00:31
[07/12 15:27:37] d2.evaluation.evaluator INFO: Inference done 333/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6900 s/iter. Total: 0.7355 s/iter. ETA=0:00:26
[07/12 15:27:42] d2.evaluation.evaluator INFO: Inference done 340/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6900 s/iter. Total: 0.7356 s/iter. ETA=0:00:21
[07/12 15:27:47] d2.evaluation.evaluator INFO: Inference done 347/369. Dataloading: 0.0007 s/iter. Inference: 0.0448 s/iter. Eval: 0.6904 s/iter. Total: 0.7360 s/iter. ETA=0:00:16
[07/12 15:27:53] d2.evaluation.evaluator INFO: Inference done 354/369. Dataloading: 0.0007 s/iter. Inference: 0.0450 s/iter. Eval: 0.6907 s/iter. Total: 0.7364 s/iter. ETA=0:00:11
[07/12 15:27:58] d2.evaluation.evaluator INFO: Inference done 361/369. Dataloading: 0.0007 s/iter. Inference: 0.0450 s/iter. Eval: 0.6910 s/iter. Total: 0.7368 s/iter. ETA=0:00:05
[07/12 15:28:03] d2.evaluation.evaluator INFO: Inference done 368/369. Dataloading: 0.0007 s/iter. Inference: 0.0450 s/iter. Eval: 0.6916 s/iter. Total: 0.7374 s/iter. ETA=0:00:00
[07/12 15:28:04] d2.evaluation.evaluator INFO: Total inference time: 0:04:28.517020 (0.737684 s / iter per device, on 1 devices)
[07/12 15:28:04] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:16 (0.045087 s / iter per device, on 1 devices)
[07/12 15:28:05] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[07/12 15:28:05] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256to256_instance/inference/coco_instances_results.json
[07/12 15:28:05] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[07/12 15:28:07] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[07/12 15:28:07] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 15:28:07] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[07/12 15:28:14] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 1.660 | 2.694  | 1.684  |  nan  |  nan  | 1.660 |
[07/12 15:28:14] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[07/12 15:28:14] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 2.771 | defect     | 0.548 |
[07/12 15:28:14] d2.engine.defaults INFO: Evaluation results for chick_dataset_test_syn_new in csv format:
[07/12 15:28:14] d2.evaluation.testing INFO: copypaste: Task: bbox
[07/12 15:28:14] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 15:28:14] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[07/12 15:28:14] d2.evaluation.testing INFO: copypaste: Task: segm
[07/12 15:28:14] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[07/12 15:28:14] d2.evaluation.testing INFO: copypaste: 1.6595,2.6939,1.6836,nan,nan,1.6595
