[04/13 14:32:03] detectron2 INFO: Rank of current process: 0. World size: 1
[04/13 14:32:05] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/13 14:32:05] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco_front2class/instance-segmentation/maskformer2_R50_bs16_50ep_front_2class_256x256.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=False, machine_rank=0, num_gpus=1, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '1', 'SOLVER.BASE_LR', '0.0001', 'OUTPUT_DIR', './output_coco_256_100'], resume=False)
[04/13 14:32:05] detectron2 INFO: Contents of args.config_file=configs/coco_front2class/instance-segmentation/maskformer2_R50_bs16_50ep_front_2class_256x256.yaml:
_BASE_: Base-COCO-InstanceSegmentation-256x256.yaml
MODEL:
  META_ARCHITECTURE: "MaskFormer"
  SEM_SEG_HEAD:
    NAME: "MaskFormerHead"
    IGNORE_VALUE: 255
    NUM_CLASSES: 2
    LOSS_WEIGHT: 1.0
    CONVS_DIM: 256
    MASK_DIM: 256
    NORM: "GN"
    # pixel decoder
    PIXEL_DECODER_NAME: "MSDeformAttnPixelDecoder"
    IN_FEATURES: ["res2", "res3", "res4", "res5"]
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES: ["res3", "res4", "res5"]
    COMMON_STRIDE: 4
    TRANSFORMER_ENC_LAYERS: 6
  MASK_FORMER:
    TRANSFORMER_DECODER_NAME: "MultiScaleMaskedTransformerDecoder"
    TRANSFORMER_IN_FEATURE: "multi_scale_pixel_decoder"
    DEEP_SUPERVISION: True
    NO_OBJECT_WEIGHT: 0.1
    CLASS_WEIGHT: 2.0
    MASK_WEIGHT: 5.0
    DICE_WEIGHT: 5.0
    HIDDEN_DIM: 256
    NUM_OBJECT_QUERIES: 100 #100
    NHEADS: 8
    DROPOUT: 0.0
    DIM_FEEDFORWARD: 2048
    ENC_LAYERS: 0
    PRE_NORM: False
    ENFORCE_INPUT_PROJ: False
    SIZE_DIVISIBILITY: 32
    DEC_LAYERS: 10  # 9 decoder layers, add one for the loss on learnable query
    TRAIN_NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
    TEST:
      SEMANTIC_ON: False
      INSTANCE_ON: True
      PANOPTIC_ON: False
      OVERLAP_THRESHOLD: 0.8
      OBJECT_MASK_THRESHOLD: 0.8

[04/13 14:32:05] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ''
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/13 14:32:05] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/13 14:32:05] d2.utils.env INFO: Using a generated random seed 7476632
[04/13 14:32:18] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/13 14:32:18] mask2former.data.dataset_mappers.coco_instance_new_baseline_dataset_mapper INFO: [COCOInstanceNewBaselineDatasetMapper] Full TransformGens used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=256, target_width=256), FixedSizeCrop(crop_size=(256, 256))]
[04/13 14:32:18] d2.data.datasets.coco INFO: Loaded 3424 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_train2017.json
[04/13 14:32:18] d2.data.build INFO: Removed 0 images with no usable annotations. 3424 images left.
[04/13 14:32:18] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 1571         |   defect   | 1853         |
|            |              |            |              |
|   total    | 3424         |            |              |[0m
[04/13 14:32:18] d2.data.build INFO: Using training sampler TrainingSampler
[04/13 14:32:18] d2.data.common INFO: Serializing 3424 elements to byte tensors and concatenating them all ...
[04/13 14:32:18] d2.data.common INFO: Serialized dataset takes 11.57 MiB
[04/13 14:32:18] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[04/13 14:32:18] d2.engine.train_loop INFO: Starting training from iteration 0
[04/13 14:32:28] d2.utils.events INFO:  eta: 1:39:57  iter: 19  total_loss: 85.7  loss_ce: 0.797  loss_mask: 3.412  loss_dice: 4.269  loss_ce_0: 2.255  loss_mask_0: 2.965  loss_dice_0: 3.376  loss_ce_1: 0.8974  loss_mask_1: 3.31  loss_dice_1: 4.067  loss_ce_2: 0.8005  loss_mask_2: 3.884  loss_dice_2: 4.334  loss_ce_3: 0.7914  loss_mask_3: 3.603  loss_dice_3: 4.474  loss_ce_4: 0.7957  loss_mask_4: 3.432  loss_dice_4: 4.27  loss_ce_5: 0.7725  loss_mask_5: 3.385  loss_dice_5: 4.348  loss_ce_6: 0.8113  loss_mask_6: 3.513  loss_dice_6: 3.733  loss_ce_7: 0.8206  loss_mask_7: 3.532  loss_dice_7: 4.265  loss_ce_8: 0.8777  loss_mask_8: 3.411  loss_dice_8: 4.204  time: 0.1719  data_time: 0.0081  lr: 0.0001  max_mem: 1071M
[04/13 14:32:30] d2.utils.events INFO:  eta: 1:42:19  iter: 39  total_loss: 52.64  loss_ce: 0.7712  loss_mask: 2.378  loss_dice: 2.613  loss_ce_0: 2.243  loss_mask_0: 1.556  loss_dice_0: 1.978  loss_ce_1: 0.6548  loss_mask_1: 1.592  loss_dice_1: 2.15  loss_ce_2: 0.7017  loss_mask_2: 1.92  loss_dice_2: 1.738  loss_ce_3: 0.6927  loss_mask_3: 2.142  loss_dice_3: 1.823  loss_ce_4: 0.7289  loss_mask_4: 2.189  loss_dice_4: 2.13  loss_ce_5: 0.7624  loss_mask_5: 2.192  loss_dice_5: 2.172  loss_ce_6: 0.7252  loss_mask_6: 2.161  loss_dice_6: 1.951  loss_ce_7: 0.7264  loss_mask_7: 2.262  loss_dice_7: 2.154  loss_ce_8: 0.7129  loss_mask_8: 2.568  loss_dice_8: 2.528  time: 0.1466  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:32:33] d2.utils.events INFO:  eta: 1:42:11  iter: 59  total_loss: 26.07  loss_ce: 0.7228  loss_mask: 0.977  loss_dice: 0.9247  loss_ce_0: 2.185  loss_mask_0: 0.8084  loss_dice_0: 1.185  loss_ce_1: 0.5891  loss_mask_1: 0.8899  loss_dice_1: 0.9048  loss_ce_2: 0.6571  loss_mask_2: 0.8912  loss_dice_2: 0.8157  loss_ce_3: 0.7257  loss_mask_3: 0.9335  loss_dice_3: 0.8596  loss_ce_4: 0.7224  loss_mask_4: 0.9746  loss_dice_4: 0.9102  loss_ce_5: 0.7317  loss_mask_5: 0.9916  loss_dice_5: 0.9055  loss_ce_6: 0.7294  loss_mask_6: 1.066  loss_dice_6: 0.8871  loss_ce_7: 0.7352  loss_mask_7: 0.9792  loss_dice_7: 0.8619  loss_ce_8: 0.747  loss_mask_8: 0.9768  loss_dice_8: 0.8513  time: 0.1379  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:32:35] d2.utils.events INFO:  eta: 1:42:38  iter: 79  total_loss: 26.71  loss_ce: 0.7385  loss_mask: 1.088  loss_dice: 1.14  loss_ce_0: 2.136  loss_mask_0: 0.6775  loss_dice_0: 1.133  loss_ce_1: 0.4908  loss_mask_1: 0.6377  loss_dice_1: 0.7689  loss_ce_2: 0.56  loss_mask_2: 0.7016  loss_dice_2: 0.9473  loss_ce_3: 0.6589  loss_mask_3: 0.8147  loss_dice_3: 0.9218  loss_ce_4: 0.7328  loss_mask_4: 1.045  loss_dice_4: 1.163  loss_ce_5: 0.7439  loss_mask_5: 1.156  loss_dice_5: 1.168  loss_ce_6: 0.7551  loss_mask_6: 1.14  loss_dice_6: 1.079  loss_ce_7: 0.7491  loss_mask_7: 1.174  loss_dice_7: 1.023  loss_ce_8: 0.7402  loss_mask_8: 1.194  loss_dice_8: 1.113  time: 0.1347  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:32:38] d2.utils.events INFO:  eta: 1:42:42  iter: 99  total_loss: 22.56  loss_ce: 0.6691  loss_mask: 0.7553  loss_dice: 0.5914  loss_ce_0: 2.088  loss_mask_0: 0.6338  loss_dice_0: 0.5054  loss_ce_1: 0.2999  loss_mask_1: 0.7518  loss_dice_1: 0.5332  loss_ce_2: 0.3352  loss_mask_2: 0.7474  loss_dice_2: 0.4413  loss_ce_3: 0.539  loss_mask_3: 0.6984  loss_dice_3: 0.4996  loss_ce_4: 0.6145  loss_mask_4: 0.7341  loss_dice_4: 0.6336  loss_ce_5: 0.6771  loss_mask_5: 0.8326  loss_dice_5: 0.6642  loss_ce_6: 0.6719  loss_mask_6: 0.8083  loss_dice_6: 0.6458  loss_ce_7: 0.6776  loss_mask_7: 0.8169  loss_dice_7: 0.65  loss_ce_8: 0.6751  loss_mask_8: 0.7958  loss_dice_8: 0.6166  time: 0.1326  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:32:40] d2.utils.events INFO:  eta: 1:42:58  iter: 119  total_loss: 17.22  loss_ce: 0.751  loss_mask: 0.4464  loss_dice: 0.4719  loss_ce_0: 2.037  loss_mask_0: 0.41  loss_dice_0: 0.558  loss_ce_1: 0.1295  loss_mask_1: 0.4376  loss_dice_1: 0.5379  loss_ce_2: 0.109  loss_mask_2: 0.45  loss_dice_2: 0.4414  loss_ce_3: 0.2159  loss_mask_3: 0.4046  loss_dice_3: 0.4804  loss_ce_4: 0.4026  loss_mask_4: 0.435  loss_dice_4: 0.4841  loss_ce_5: 0.6409  loss_mask_5: 0.4563  loss_dice_5: 0.4809  loss_ce_6: 0.7278  loss_mask_6: 0.4375  loss_dice_6: 0.492  loss_ce_7: 0.7487  loss_mask_7: 0.4365  loss_dice_7: 0.4888  loss_ce_8: 0.7361  loss_mask_8: 0.4349  loss_dice_8: 0.4622  time: 0.1315  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:32:43] d2.utils.events INFO:  eta: 1:43:20  iter: 139  total_loss: 22.05  loss_ce: 0.6668  loss_mask: 0.3679  loss_dice: 1.129  loss_ce_0: 1.981  loss_mask_0: 0.3202  loss_dice_0: 1.091  loss_ce_1: 0.1088  loss_mask_1: 0.3708  loss_dice_1: 1.231  loss_ce_2: 0.1351  loss_mask_2: 0.3746  loss_dice_2: 1.122  loss_ce_3: 0.1412  loss_mask_3: 0.3607  loss_dice_3: 1.181  loss_ce_4: 0.1361  loss_mask_4: 0.3534  loss_dice_4: 1.215  loss_ce_5: 0.2463  loss_mask_5: 0.3581  loss_dice_5: 1.059  loss_ce_6: 0.4725  loss_mask_6: 0.3579  loss_dice_6: 1.05  loss_ce_7: 0.621  loss_mask_7: 0.3745  loss_dice_7: 1.091  loss_ce_8: 0.6584  loss_mask_8: 0.3729  loss_dice_8: 1.038  time: 0.1307  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:32:45] d2.utils.events INFO:  eta: 1:43:03  iter: 159  total_loss: 15.84  loss_ce: 0.6251  loss_mask: 0.5367  loss_dice: 0.5808  loss_ce_0: 1.911  loss_mask_0: 0.5356  loss_dice_0: 0.5614  loss_ce_1: 0.1369  loss_mask_1: 0.567  loss_dice_1: 0.5995  loss_ce_2: 0.1352  loss_mask_2: 0.5669  loss_dice_2: 0.6869  loss_ce_3: 0.149  loss_mask_3: 0.5525  loss_dice_3: 0.7115  loss_ce_4: 0.1418  loss_mask_4: 0.5392  loss_dice_4: 0.7387  loss_ce_5: 0.1531  loss_mask_5: 0.5239  loss_dice_5: 0.6135  loss_ce_6: 0.1479  loss_mask_6: 0.4911  loss_dice_6: 0.6423  loss_ce_7: 0.1966  loss_mask_7: 0.5259  loss_dice_7: 0.597  loss_ce_8: 0.4039  loss_mask_8: 0.5365  loss_dice_8: 0.569  time: 0.1298  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:32:48] d2.utils.events INFO:  eta: 1:42:56  iter: 179  total_loss: 12.69  loss_ce: 0.2306  loss_mask: 0.5533  loss_dice: 0.3663  loss_ce_0: 1.843  loss_mask_0: 0.5832  loss_dice_0: 0.337  loss_ce_1: 0.1235  loss_mask_1: 0.5529  loss_dice_1: 0.3494  loss_ce_2: 0.1098  loss_mask_2: 0.5571  loss_dice_2: 0.3415  loss_ce_3: 0.09903  loss_mask_3: 0.5635  loss_dice_3: 0.3591  loss_ce_4: 0.1005  loss_mask_4: 0.6099  loss_dice_4: 0.3825  loss_ce_5: 0.1225  loss_mask_5: 0.6004  loss_dice_5: 0.3836  loss_ce_6: 0.1265  loss_mask_6: 0.6207  loss_dice_6: 0.4032  loss_ce_7: 0.1531  loss_mask_7: 0.6004  loss_dice_7: 0.3922  loss_ce_8: 0.1495  loss_mask_8: 0.6009  loss_dice_8: 0.3502  time: 0.1290  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:32:50] d2.utils.events INFO:  eta: 1:43:02  iter: 199  total_loss: 10.78  loss_ce: 0.1887  loss_mask: 0.3525  loss_dice: 0.3411  loss_ce_0: 1.776  loss_mask_0: 0.3785  loss_dice_0: 0.3381  loss_ce_1: 0.1538  loss_mask_1: 0.3633  loss_dice_1: 0.3555  loss_ce_2: 0.1464  loss_mask_2: 0.3528  loss_dice_2: 0.3396  loss_ce_3: 0.1485  loss_mask_3: 0.3494  loss_dice_3: 0.3498  loss_ce_4: 0.1409  loss_mask_4: 0.357  loss_dice_4: 0.3621  loss_ce_5: 0.1179  loss_mask_5: 0.3568  loss_dice_5: 0.3621  loss_ce_6: 0.1045  loss_mask_6: 0.3602  loss_dice_6: 0.3473  loss_ce_7: 0.1081  loss_mask_7: 0.3795  loss_dice_7: 0.353  loss_ce_8: 0.1551  loss_mask_8: 0.3563  loss_dice_8: 0.3425  time: 0.1286  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:32:53] d2.utils.events INFO:  eta: 1:43:07  iter: 219  total_loss: 10.68  loss_ce: 0.2287  loss_mask: 0.404  loss_dice: 0.2954  loss_ce_0: 1.687  loss_mask_0: 0.3955  loss_dice_0: 0.3102  loss_ce_1: 0.1215  loss_mask_1: 0.4165  loss_dice_1: 0.2991  loss_ce_2: 0.1389  loss_mask_2: 0.4231  loss_dice_2: 0.2937  loss_ce_3: 0.143  loss_mask_3: 0.4157  loss_dice_3: 0.294  loss_ce_4: 0.1428  loss_mask_4: 0.4135  loss_dice_4: 0.2919  loss_ce_5: 0.1715  loss_mask_5: 0.4248  loss_dice_5: 0.3073  loss_ce_6: 0.1789  loss_mask_6: 0.4378  loss_dice_6: 0.3045  loss_ce_7: 0.1999  loss_mask_7: 0.4379  loss_dice_7: 0.3068  loss_ce_8: 0.2055  loss_mask_8: 0.4104  loss_dice_8: 0.2878  time: 0.1283  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:32:55] d2.utils.events INFO:  eta: 1:43:22  iter: 239  total_loss: 11.8  loss_ce: 0.1705  loss_mask: 0.4256  loss_dice: 0.4509  loss_ce_0: 1.618  loss_mask_0: 0.3803  loss_dice_0: 0.4528  loss_ce_1: 0.1529  loss_mask_1: 0.4215  loss_dice_1: 0.4508  loss_ce_2: 0.167  loss_mask_2: 0.4256  loss_dice_2: 0.4711  loss_ce_3: 0.1663  loss_mask_3: 0.4158  loss_dice_3: 0.4458  loss_ce_4: 0.1574  loss_mask_4: 0.4224  loss_dice_4: 0.4703  loss_ce_5: 0.1622  loss_mask_5: 0.4074  loss_dice_5: 0.4874  loss_ce_6: 0.1656  loss_mask_6: 0.4211  loss_dice_6: 0.4687  loss_ce_7: 0.175  loss_mask_7: 0.4138  loss_dice_7: 0.4909  loss_ce_8: 0.1701  loss_mask_8: 0.4525  loss_dice_8: 0.4434  time: 0.1283  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:32:58] d2.utils.events INFO:  eta: 1:43:45  iter: 259  total_loss: 10.02  loss_ce: 0.1292  loss_mask: 0.3954  loss_dice: 0.2833  loss_ce_0: 1.543  loss_mask_0: 0.4005  loss_dice_0: 0.2746  loss_ce_1: 0.116  loss_mask_1: 0.3806  loss_dice_1: 0.278  loss_ce_2: 0.1155  loss_mask_2: 0.3946  loss_dice_2: 0.2955  loss_ce_3: 0.1137  loss_mask_3: 0.4216  loss_dice_3: 0.3076  loss_ce_4: 0.1117  loss_mask_4: 0.3968  loss_dice_4: 0.2766  loss_ce_5: 0.1103  loss_mask_5: 0.3858  loss_dice_5: 0.268  loss_ce_6: 0.11  loss_mask_6: 0.3977  loss_dice_6: 0.289  loss_ce_7: 0.11  loss_mask_7: 0.4019  loss_dice_7: 0.282  loss_ce_8: 0.1193  loss_mask_8: 0.3945  loss_dice_8: 0.2853  time: 0.1282  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:00] d2.utils.events INFO:  eta: 1:43:50  iter: 279  total_loss: 9.689  loss_ce: 0.1442  loss_mask: 0.3762  loss_dice: 0.3752  loss_ce_0: 1.442  loss_mask_0: 0.3634  loss_dice_0: 0.3757  loss_ce_1: 0.1412  loss_mask_1: 0.3812  loss_dice_1: 0.3652  loss_ce_2: 0.1356  loss_mask_2: 0.3651  loss_dice_2: 0.3603  loss_ce_3: 0.1396  loss_mask_3: 0.3785  loss_dice_3: 0.3713  loss_ce_4: 0.1382  loss_mask_4: 0.3694  loss_dice_4: 0.3595  loss_ce_5: 0.1337  loss_mask_5: 0.3715  loss_dice_5: 0.3518  loss_ce_6: 0.1287  loss_mask_6: 0.3732  loss_dice_6: 0.3754  loss_ce_7: 0.1374  loss_mask_7: 0.3796  loss_dice_7: 0.3849  loss_ce_8: 0.1398  loss_mask_8: 0.3814  loss_dice_8: 0.3816  time: 0.1280  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:03] d2.utils.events INFO:  eta: 1:44:06  iter: 299  total_loss: 9.087  loss_ce: 0.137  loss_mask: 0.3041  loss_dice: 0.2829  loss_ce_0: 1.372  loss_mask_0: 0.2714  loss_dice_0: 0.2797  loss_ce_1: 0.1252  loss_mask_1: 0.271  loss_dice_1: 0.276  loss_ce_2: 0.1294  loss_mask_2: 0.2613  loss_dice_2: 0.262  loss_ce_3: 0.1322  loss_mask_3: 0.2994  loss_dice_3: 0.2702  loss_ce_4: 0.1325  loss_mask_4: 0.2652  loss_dice_4: 0.2742  loss_ce_5: 0.1335  loss_mask_5: 0.2667  loss_dice_5: 0.2756  loss_ce_6: 0.1324  loss_mask_6: 0.2936  loss_dice_6: 0.2838  loss_ce_7: 0.1321  loss_mask_7: 0.273  loss_dice_7: 0.2826  loss_ce_8: 0.13  loss_mask_8: 0.2804  loss_dice_8: 0.28  time: 0.1280  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:05] d2.utils.events INFO:  eta: 1:44:05  iter: 319  total_loss: 10.94  loss_ce: 0.1348  loss_mask: 0.3732  loss_dice: 0.3374  loss_ce_0: 1.295  loss_mask_0: 0.386  loss_dice_0: 0.3671  loss_ce_1: 0.1324  loss_mask_1: 0.3935  loss_dice_1: 0.3405  loss_ce_2: 0.1348  loss_mask_2: 0.3897  loss_dice_2: 0.3174  loss_ce_3: 0.1343  loss_mask_3: 0.416  loss_dice_3: 0.3354  loss_ce_4: 0.1309  loss_mask_4: 0.4005  loss_dice_4: 0.3292  loss_ce_5: 0.1331  loss_mask_5: 0.4004  loss_dice_5: 0.3484  loss_ce_6: 0.1324  loss_mask_6: 0.4086  loss_dice_6: 0.3418  loss_ce_7: 0.1339  loss_mask_7: 0.4108  loss_dice_7: 0.3627  loss_ce_8: 0.1337  loss_mask_8: 0.4076  loss_dice_8: 0.3457  time: 0.1279  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:08] d2.utils.events INFO:  eta: 1:44:08  iter: 339  total_loss: 8.272  loss_ce: 0.09642  loss_mask: 0.3098  loss_dice: 0.3087  loss_ce_0: 1.223  loss_mask_0: 0.325  loss_dice_0: 0.2888  loss_ce_1: 0.103  loss_mask_1: 0.3071  loss_dice_1: 0.2821  loss_ce_2: 0.1004  loss_mask_2: 0.3029  loss_dice_2: 0.2832  loss_ce_3: 0.1038  loss_mask_3: 0.323  loss_dice_3: 0.2828  loss_ce_4: 0.1022  loss_mask_4: 0.3005  loss_dice_4: 0.2788  loss_ce_5: 0.1028  loss_mask_5: 0.3067  loss_dice_5: 0.2963  loss_ce_6: 0.1035  loss_mask_6: 0.3129  loss_dice_6: 0.2962  loss_ce_7: 0.1043  loss_mask_7: 0.3181  loss_dice_7: 0.289  loss_ce_8: 0.09963  loss_mask_8: 0.3046  loss_dice_8: 0.2757  time: 0.1278  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:10] d2.utils.events INFO:  eta: 1:44:06  iter: 359  total_loss: 8.253  loss_ce: 0.1202  loss_mask: 0.3049  loss_dice: 0.27  loss_ce_0: 1.145  loss_mask_0: 0.3074  loss_dice_0: 0.2623  loss_ce_1: 0.108  loss_mask_1: 0.2981  loss_dice_1: 0.2559  loss_ce_2: 0.1167  loss_mask_2: 0.2873  loss_dice_2: 0.2535  loss_ce_3: 0.1175  loss_mask_3: 0.3077  loss_dice_3: 0.2568  loss_ce_4: 0.1205  loss_mask_4: 0.3142  loss_dice_4: 0.2571  loss_ce_5: 0.1189  loss_mask_5: 0.3098  loss_dice_5: 0.2638  loss_ce_6: 0.1193  loss_mask_6: 0.2975  loss_dice_6: 0.257  loss_ce_7: 0.1215  loss_mask_7: 0.2952  loss_dice_7: 0.2664  loss_ce_8: 0.1174  loss_mask_8: 0.31  loss_dice_8: 0.2488  time: 0.1277  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:13] d2.utils.events INFO:  eta: 1:43:57  iter: 379  total_loss: 9.045  loss_ce: 0.1217  loss_mask: 0.2172  loss_dice: 0.2808  loss_ce_0: 1.069  loss_mask_0: 0.2125  loss_dice_0: 0.2651  loss_ce_1: 0.1077  loss_mask_1: 0.2105  loss_dice_1: 0.2688  loss_ce_2: 0.1113  loss_mask_2: 0.2143  loss_dice_2: 0.2635  loss_ce_3: 0.1164  loss_mask_3: 0.2094  loss_dice_3: 0.2746  loss_ce_4: 0.1104  loss_mask_4: 0.2119  loss_dice_4: 0.2701  loss_ce_5: 0.1154  loss_mask_5: 0.2208  loss_dice_5: 0.2804  loss_ce_6: 0.1151  loss_mask_6: 0.208  loss_dice_6: 0.2689  loss_ce_7: 0.1109  loss_mask_7: 0.2071  loss_dice_7: 0.2778  loss_ce_8: 0.1221  loss_mask_8: 0.2129  loss_dice_8: 0.2689  time: 0.1275  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:16] d2.utils.events INFO:  eta: 1:43:53  iter: 399  total_loss: 8.075  loss_ce: 0.1501  loss_mask: 0.3297  loss_dice: 0.2194  loss_ce_0: 0.998  loss_mask_0: 0.321  loss_dice_0: 0.2219  loss_ce_1: 0.1408  loss_mask_1: 0.329  loss_dice_1: 0.2167  loss_ce_2: 0.1391  loss_mask_2: 0.3272  loss_dice_2: 0.2318  loss_ce_3: 0.1387  loss_mask_3: 0.3214  loss_dice_3: 0.2298  loss_ce_4: 0.1419  loss_mask_4: 0.3265  loss_dice_4: 0.2258  loss_ce_5: 0.1373  loss_mask_5: 0.3388  loss_dice_5: 0.2326  loss_ce_6: 0.1376  loss_mask_6: 0.3305  loss_dice_6: 0.2314  loss_ce_7: 0.1418  loss_mask_7: 0.3248  loss_dice_7: 0.2261  loss_ce_8: 0.1429  loss_mask_8: 0.3303  loss_dice_8: 0.241  time: 0.1274  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:18] d2.utils.events INFO:  eta: 1:43:48  iter: 419  total_loss: 8.606  loss_ce: 0.1429  loss_mask: 0.3235  loss_dice: 0.2985  loss_ce_0: 0.9382  loss_mask_0: 0.3079  loss_dice_0: 0.2728  loss_ce_1: 0.1391  loss_mask_1: 0.3087  loss_dice_1: 0.302  loss_ce_2: 0.1392  loss_mask_2: 0.3213  loss_dice_2: 0.3205  loss_ce_3: 0.1378  loss_mask_3: 0.3212  loss_dice_3: 0.293  loss_ce_4: 0.1359  loss_mask_4: 0.3124  loss_dice_4: 0.2973  loss_ce_5: 0.1343  loss_mask_5: 0.3075  loss_dice_5: 0.3148  loss_ce_6: 0.1338  loss_mask_6: 0.3194  loss_dice_6: 0.3164  loss_ce_7: 0.1351  loss_mask_7: 0.3147  loss_dice_7: 0.3088  loss_ce_8: 0.1383  loss_mask_8: 0.3135  loss_dice_8: 0.3201  time: 0.1273  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:21] d2.utils.events INFO:  eta: 1:43:52  iter: 439  total_loss: 7.899  loss_ce: 0.1262  loss_mask: 0.3184  loss_dice: 0.2328  loss_ce_0: 0.8761  loss_mask_0: 0.3123  loss_dice_0: 0.2286  loss_ce_1: 0.1221  loss_mask_1: 0.3159  loss_dice_1: 0.2324  loss_ce_2: 0.1247  loss_mask_2: 0.3065  loss_dice_2: 0.2382  loss_ce_3: 0.1263  loss_mask_3: 0.3175  loss_dice_3: 0.222  loss_ce_4: 0.1261  loss_mask_4: 0.3347  loss_dice_4: 0.2431  loss_ce_5: 0.1268  loss_mask_5: 0.3095  loss_dice_5: 0.2305  loss_ce_6: 0.1271  loss_mask_6: 0.3331  loss_dice_6: 0.2363  loss_ce_7: 0.1247  loss_mask_7: 0.3174  loss_dice_7: 0.2281  loss_ce_8: 0.1226  loss_mask_8: 0.3244  loss_dice_8: 0.231  time: 0.1274  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:23] d2.utils.events INFO:  eta: 1:43:50  iter: 459  total_loss: 7.656  loss_ce: 0.1015  loss_mask: 0.2912  loss_dice: 0.2401  loss_ce_0: 0.808  loss_mask_0: 0.2848  loss_dice_0: 0.2176  loss_ce_1: 0.1075  loss_mask_1: 0.2952  loss_dice_1: 0.217  loss_ce_2: 0.1017  loss_mask_2: 0.2915  loss_dice_2: 0.2192  loss_ce_3: 0.09984  loss_mask_3: 0.2913  loss_dice_3: 0.2279  loss_ce_4: 0.09953  loss_mask_4: 0.2967  loss_dice_4: 0.2249  loss_ce_5: 0.09902  loss_mask_5: 0.3037  loss_dice_5: 0.233  loss_ce_6: 0.09974  loss_mask_6: 0.2872  loss_dice_6: 0.2345  loss_ce_7: 0.09957  loss_mask_7: 0.2968  loss_dice_7: 0.2393  loss_ce_8: 0.101  loss_mask_8: 0.29  loss_dice_8: 0.2255  time: 0.1274  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:26] d2.utils.events INFO:  eta: 1:43:51  iter: 479  total_loss: 7.858  loss_ce: 0.139  loss_mask: 0.2877  loss_dice: 0.2382  loss_ce_0: 0.7396  loss_mask_0: 0.2788  loss_dice_0: 0.2406  loss_ce_1: 0.1375  loss_mask_1: 0.3002  loss_dice_1: 0.2454  loss_ce_2: 0.1373  loss_mask_2: 0.2869  loss_dice_2: 0.2384  loss_ce_3: 0.1374  loss_mask_3: 0.269  loss_dice_3: 0.2444  loss_ce_4: 0.1365  loss_mask_4: 0.2785  loss_dice_4: 0.2391  loss_ce_5: 0.1377  loss_mask_5: 0.2994  loss_dice_5: 0.2465  loss_ce_6: 0.1396  loss_mask_6: 0.2841  loss_dice_6: 0.2397  loss_ce_7: 0.1383  loss_mask_7: 0.2907  loss_dice_7: 0.25  loss_ce_8: 0.1365  loss_mask_8: 0.2796  loss_dice_8: 0.2429  time: 0.1273  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:28] d2.utils.events INFO:  eta: 1:43:55  iter: 499  total_loss: 8.269  loss_ce: 0.1031  loss_mask: 0.3095  loss_dice: 0.3075  loss_ce_0: 0.6739  loss_mask_0: 0.2871  loss_dice_0: 0.2993  loss_ce_1: 0.08746  loss_mask_1: 0.3093  loss_dice_1: 0.311  loss_ce_2: 0.08737  loss_mask_2: 0.3001  loss_dice_2: 0.3026  loss_ce_3: 0.08871  loss_mask_3: 0.2954  loss_dice_3: 0.3223  loss_ce_4: 0.09194  loss_mask_4: 0.305  loss_dice_4: 0.3029  loss_ce_5: 0.09203  loss_mask_5: 0.3027  loss_dice_5: 0.3063  loss_ce_6: 0.09145  loss_mask_6: 0.3005  loss_dice_6: 0.3181  loss_ce_7: 0.0916  loss_mask_7: 0.3046  loss_dice_7: 0.3134  loss_ce_8: 0.09482  loss_mask_8: 0.2993  loss_dice_8: 0.3112  time: 0.1273  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:31] d2.utils.events INFO:  eta: 1:43:56  iter: 519  total_loss: 6.727  loss_ce: 0.06  loss_mask: 0.2495  loss_dice: 0.282  loss_ce_0: 0.6342  loss_mask_0: 0.2489  loss_dice_0: 0.2887  loss_ce_1: 0.06178  loss_mask_1: 0.2462  loss_dice_1: 0.3072  loss_ce_2: 0.06104  loss_mask_2: 0.2392  loss_dice_2: 0.2643  loss_ce_3: 0.05994  loss_mask_3: 0.2457  loss_dice_3: 0.296  loss_ce_4: 0.06084  loss_mask_4: 0.246  loss_dice_4: 0.2805  loss_ce_5: 0.05909  loss_mask_5: 0.2533  loss_dice_5: 0.2789  loss_ce_6: 0.0608  loss_mask_6: 0.2499  loss_dice_6: 0.2878  loss_ce_7: 0.05964  loss_mask_7: 0.2452  loss_dice_7: 0.2963  loss_ce_8: 0.05944  loss_mask_8: 0.2593  loss_dice_8: 0.2983  time: 0.1274  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:33] d2.utils.events INFO:  eta: 1:43:49  iter: 539  total_loss: 7.413  loss_ce: 0.1326  loss_mask: 0.24  loss_dice: 0.2523  loss_ce_0: 0.5874  loss_mask_0: 0.2394  loss_dice_0: 0.2505  loss_ce_1: 0.1306  loss_mask_1: 0.2461  loss_dice_1: 0.2457  loss_ce_2: 0.1306  loss_mask_2: 0.2476  loss_dice_2: 0.2406  loss_ce_3: 0.1301  loss_mask_3: 0.244  loss_dice_3: 0.2553  loss_ce_4: 0.1311  loss_mask_4: 0.2448  loss_dice_4: 0.2479  loss_ce_5: 0.1309  loss_mask_5: 0.2498  loss_dice_5: 0.2535  loss_ce_6: 0.1303  loss_mask_6: 0.243  loss_dice_6: 0.2567  loss_ce_7: 0.1307  loss_mask_7: 0.2512  loss_dice_7: 0.2629  loss_ce_8: 0.1305  loss_mask_8: 0.2411  loss_dice_8: 0.2426  time: 0.1271  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:36] d2.utils.events INFO:  eta: 1:43:38  iter: 559  total_loss: 7.312  loss_ce: 0.128  loss_mask: 0.2812  loss_dice: 0.2195  loss_ce_0: 0.5446  loss_mask_0: 0.2677  loss_dice_0: 0.212  loss_ce_1: 0.1289  loss_mask_1: 0.2873  loss_dice_1: 0.2225  loss_ce_2: 0.1293  loss_mask_2: 0.2736  loss_dice_2: 0.2137  loss_ce_3: 0.128  loss_mask_3: 0.2805  loss_dice_3: 0.218  loss_ce_4: 0.1288  loss_mask_4: 0.278  loss_dice_4: 0.2208  loss_ce_5: 0.128  loss_mask_5: 0.2702  loss_dice_5: 0.2191  loss_ce_6: 0.1284  loss_mask_6: 0.2782  loss_dice_6: 0.2214  loss_ce_7: 0.129  loss_mask_7: 0.28  loss_dice_7: 0.2333  loss_ce_8: 0.1284  loss_mask_8: 0.2774  loss_dice_8: 0.2216  time: 0.1268  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:38] d2.utils.events INFO:  eta: 1:43:30  iter: 579  total_loss: 6.838  loss_ce: 0.163  loss_mask: 0.2529  loss_dice: 0.2418  loss_ce_0: 0.5075  loss_mask_0: 0.2542  loss_dice_0: 0.2429  loss_ce_1: 0.1633  loss_mask_1: 0.24  loss_dice_1: 0.2343  loss_ce_2: 0.1663  loss_mask_2: 0.2602  loss_dice_2: 0.2411  loss_ce_3: 0.1653  loss_mask_3: 0.265  loss_dice_3: 0.245  loss_ce_4: 0.1654  loss_mask_4: 0.2683  loss_dice_4: 0.2363  loss_ce_5: 0.1665  loss_mask_5: 0.2582  loss_dice_5: 0.2247  loss_ce_6: 0.1647  loss_mask_6: 0.2548  loss_dice_6: 0.2406  loss_ce_7: 0.166  loss_mask_7: 0.2559  loss_dice_7: 0.2323  loss_ce_8: 0.1653  loss_mask_8: 0.2516  loss_dice_8: 0.2216  time: 0.1266  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:41] d2.utils.events INFO:  eta: 1:43:18  iter: 599  total_loss: 6.079  loss_ce: 0.1287  loss_mask: 0.2372  loss_dice: 0.2244  loss_ce_0: 0.4736  loss_mask_0: 0.2237  loss_dice_0: 0.2099  loss_ce_1: 0.1299  loss_mask_1: 0.2218  loss_dice_1: 0.2055  loss_ce_2: 0.1297  loss_mask_2: 0.2208  loss_dice_2: 0.2142  loss_ce_3: 0.1302  loss_mask_3: 0.2287  loss_dice_3: 0.2151  loss_ce_4: 0.1298  loss_mask_4: 0.231  loss_dice_4: 0.2066  loss_ce_5: 0.129  loss_mask_5: 0.2318  loss_dice_5: 0.2266  loss_ce_6: 0.1305  loss_mask_6: 0.2267  loss_dice_6: 0.2092  loss_ce_7: 0.1291  loss_mask_7: 0.2263  loss_dice_7: 0.2231  loss_ce_8: 0.1295  loss_mask_8: 0.2266  loss_dice_8: 0.2156  time: 0.1264  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:43] d2.utils.events INFO:  eta: 1:43:09  iter: 619  total_loss: 6.429  loss_ce: 0.0898  loss_mask: 0.2621  loss_dice: 0.2553  loss_ce_0: 0.44  loss_mask_0: 0.2696  loss_dice_0: 0.2563  loss_ce_1: 0.09289  loss_mask_1: 0.2546  loss_dice_1: 0.257  loss_ce_2: 0.0926  loss_mask_2: 0.2578  loss_dice_2: 0.2418  loss_ce_3: 0.09329  loss_mask_3: 0.2662  loss_dice_3: 0.2625  loss_ce_4: 0.09415  loss_mask_4: 0.2678  loss_dice_4: 0.2448  loss_ce_5: 0.094  loss_mask_5: 0.2566  loss_dice_5: 0.2567  loss_ce_6: 0.09508  loss_mask_6: 0.2695  loss_dice_6: 0.2527  loss_ce_7: 0.09458  loss_mask_7: 0.2685  loss_dice_7: 0.2612  loss_ce_8: 0.09262  loss_mask_8: 0.2632  loss_dice_8: 0.2534  time: 0.1263  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:45] d2.utils.events INFO:  eta: 1:43:03  iter: 639  total_loss: 7.705  loss_ce: 0.1674  loss_mask: 0.3275  loss_dice: 0.2139  loss_ce_0: 0.4119  loss_mask_0: 0.3286  loss_dice_0: 0.2104  loss_ce_1: 0.1668  loss_mask_1: 0.3221  loss_dice_1: 0.2117  loss_ce_2: 0.167  loss_mask_2: 0.3357  loss_dice_2: 0.2132  loss_ce_3: 0.1654  loss_mask_3: 0.3216  loss_dice_3: 0.2174  loss_ce_4: 0.1659  loss_mask_4: 0.3344  loss_dice_4: 0.2144  loss_ce_5: 0.1645  loss_mask_5: 0.3353  loss_dice_5: 0.2245  loss_ce_6: 0.1641  loss_mask_6: 0.3293  loss_dice_6: 0.2267  loss_ce_7: 0.1646  loss_mask_7: 0.3455  loss_dice_7: 0.2372  loss_ce_8: 0.1651  loss_mask_8: 0.3398  loss_dice_8: 0.2123  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:48] d2.utils.events INFO:  eta: 1:43:04  iter: 659  total_loss: 7.301  loss_ce: 0.1421  loss_mask: 0.2399  loss_dice: 0.2726  loss_ce_0: 0.387  loss_mask_0: 0.2496  loss_dice_0: 0.2649  loss_ce_1: 0.1425  loss_mask_1: 0.2487  loss_dice_1: 0.2678  loss_ce_2: 0.1418  loss_mask_2: 0.2394  loss_dice_2: 0.2694  loss_ce_3: 0.1458  loss_mask_3: 0.2342  loss_dice_3: 0.2748  loss_ce_4: 0.1424  loss_mask_4: 0.2362  loss_dice_4: 0.2699  loss_ce_5: 0.1404  loss_mask_5: 0.2385  loss_dice_5: 0.2786  loss_ce_6: 0.1376  loss_mask_6: 0.232  loss_dice_6: 0.2707  loss_ce_7: 0.1447  loss_mask_7: 0.247  loss_dice_7: 0.2953  loss_ce_8: 0.1435  loss_mask_8: 0.2456  loss_dice_8: 0.2713  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:51] d2.utils.events INFO:  eta: 1:43:02  iter: 679  total_loss: 7.028  loss_ce: 0.1302  loss_mask: 0.2715  loss_dice: 0.2207  loss_ce_0: 0.3637  loss_mask_0: 0.2616  loss_dice_0: 0.2105  loss_ce_1: 0.1324  loss_mask_1: 0.255  loss_dice_1: 0.2  loss_ce_2: 0.1289  loss_mask_2: 0.2547  loss_dice_2: 0.1963  loss_ce_3: 0.1293  loss_mask_3: 0.2538  loss_dice_3: 0.2089  loss_ce_4: 0.1303  loss_mask_4: 0.252  loss_dice_4: 0.1883  loss_ce_5: 0.1293  loss_mask_5: 0.2446  loss_dice_5: 0.1973  loss_ce_6: 0.1317  loss_mask_6: 0.246  loss_dice_6: 0.1991  loss_ce_7: 0.1299  loss_mask_7: 0.245  loss_dice_7: 0.2042  loss_ce_8: 0.1334  loss_mask_8: 0.2712  loss_dice_8: 0.2055  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:33:53] d2.utils.events INFO:  eta: 1:42:59  iter: 699  total_loss: 7.074  loss_ce: 0.1699  loss_mask: 0.2561  loss_dice: 0.2334  loss_ce_0: 0.348  loss_mask_0: 0.2611  loss_dice_0: 0.2235  loss_ce_1: 0.1781  loss_mask_1: 0.2596  loss_dice_1: 0.231  loss_ce_2: 0.1771  loss_mask_2: 0.2649  loss_dice_2: 0.2222  loss_ce_3: 0.1763  loss_mask_3: 0.2718  loss_dice_3: 0.2243  loss_ce_4: 0.173  loss_mask_4: 0.2611  loss_dice_4: 0.2279  loss_ce_5: 0.1751  loss_mask_5: 0.2527  loss_dice_5: 0.2243  loss_ce_6: 0.1768  loss_mask_6: 0.2715  loss_dice_6: 0.2242  loss_ce_7: 0.1714  loss_mask_7: 0.2632  loss_dice_7: 0.2347  loss_ce_8: 0.1705  loss_mask_8: 0.2747  loss_dice_8: 0.2171  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:56] d2.utils.events INFO:  eta: 1:42:58  iter: 719  total_loss: 7.126  loss_ce: 0.1216  loss_mask: 0.2675  loss_dice: 0.265  loss_ce_0: 0.3296  loss_mask_0: 0.2726  loss_dice_0: 0.2716  loss_ce_1: 0.1147  loss_mask_1: 0.2662  loss_dice_1: 0.2715  loss_ce_2: 0.1159  loss_mask_2: 0.2688  loss_dice_2: 0.2607  loss_ce_3: 0.1157  loss_mask_3: 0.2585  loss_dice_3: 0.2563  loss_ce_4: 0.1179  loss_mask_4: 0.2652  loss_dice_4: 0.2569  loss_ce_5: 0.116  loss_mask_5: 0.269  loss_dice_5: 0.2732  loss_ce_6: 0.1132  loss_mask_6: 0.2668  loss_dice_6: 0.2707  loss_ce_7: 0.117  loss_mask_7: 0.2499  loss_dice_7: 0.2774  loss_ce_8: 0.1176  loss_mask_8: 0.2489  loss_dice_8: 0.2737  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:33:58] d2.utils.events INFO:  eta: 1:42:56  iter: 739  total_loss: 6.622  loss_ce: 0.139  loss_mask: 0.2703  loss_dice: 0.1998  loss_ce_0: 0.3125  loss_mask_0: 0.268  loss_dice_0: 0.1813  loss_ce_1: 0.138  loss_mask_1: 0.2597  loss_dice_1: 0.2032  loss_ce_2: 0.1381  loss_mask_2: 0.2592  loss_dice_2: 0.1954  loss_ce_3: 0.1379  loss_mask_3: 0.2624  loss_dice_3: 0.1984  loss_ce_4: 0.138  loss_mask_4: 0.2652  loss_dice_4: 0.1835  loss_ce_5: 0.138  loss_mask_5: 0.2613  loss_dice_5: 0.1948  loss_ce_6: 0.1379  loss_mask_6: 0.2759  loss_dice_6: 0.1931  loss_ce_7: 0.138  loss_mask_7: 0.255  loss_dice_7: 0.1925  loss_ce_8: 0.1384  loss_mask_8: 0.2751  loss_dice_8: 0.1987  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:01] d2.utils.events INFO:  eta: 1:42:55  iter: 759  total_loss: 6.239  loss_ce: 0.1348  loss_mask: 0.2151  loss_dice: 0.183  loss_ce_0: 0.2962  loss_mask_0: 0.2142  loss_dice_0: 0.1768  loss_ce_1: 0.1309  loss_mask_1: 0.2211  loss_dice_1: 0.1847  loss_ce_2: 0.1329  loss_mask_2: 0.2148  loss_dice_2: 0.1753  loss_ce_3: 0.1331  loss_mask_3: 0.2196  loss_dice_3: 0.1825  loss_ce_4: 0.1335  loss_mask_4: 0.2193  loss_dice_4: 0.1859  loss_ce_5: 0.1325  loss_mask_5: 0.214  loss_dice_5: 0.1839  loss_ce_6: 0.1323  loss_mask_6: 0.2102  loss_dice_6: 0.1823  loss_ce_7: 0.1312  loss_mask_7: 0.2141  loss_dice_7: 0.184  loss_ce_8: 0.1327  loss_mask_8: 0.2081  loss_dice_8: 0.1802  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:34:03] d2.utils.events INFO:  eta: 1:42:57  iter: 779  total_loss: 7.451  loss_ce: 0.1192  loss_mask: 0.2642  loss_dice: 0.3053  loss_ce_0: 0.2783  loss_mask_0: 0.2641  loss_dice_0: 0.3356  loss_ce_1: 0.1193  loss_mask_1: 0.2638  loss_dice_1: 0.3097  loss_ce_2: 0.1192  loss_mask_2: 0.2664  loss_dice_2: 0.2868  loss_ce_3: 0.1198  loss_mask_3: 0.2617  loss_dice_3: 0.3182  loss_ce_4: 0.1193  loss_mask_4: 0.2641  loss_dice_4: 0.3004  loss_ce_5: 0.1197  loss_mask_5: 0.2729  loss_dice_5: 0.297  loss_ce_6: 0.1201  loss_mask_6: 0.2596  loss_dice_6: 0.2924  loss_ce_7: 0.1197  loss_mask_7: 0.2633  loss_dice_7: 0.3386  loss_ce_8: 0.1192  loss_mask_8: 0.267  loss_dice_8: 0.3072  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:34:06] d2.utils.events INFO:  eta: 1:42:58  iter: 799  total_loss: 5.841  loss_ce: 0.1314  loss_mask: 0.2215  loss_dice: 0.2152  loss_ce_0: 0.2639  loss_mask_0: 0.2226  loss_dice_0: 0.2278  loss_ce_1: 0.1314  loss_mask_1: 0.217  loss_dice_1: 0.2198  loss_ce_2: 0.1315  loss_mask_2: 0.2093  loss_dice_2: 0.2281  loss_ce_3: 0.1316  loss_mask_3: 0.2042  loss_dice_3: 0.2347  loss_ce_4: 0.1311  loss_mask_4: 0.2271  loss_dice_4: 0.2249  loss_ce_5: 0.1315  loss_mask_5: 0.2137  loss_dice_5: 0.2259  loss_ce_6: 0.1314  loss_mask_6: 0.2117  loss_dice_6: 0.231  loss_ce_7: 0.1315  loss_mask_7: 0.2088  loss_dice_7: 0.244  loss_ce_8: 0.1312  loss_mask_8: 0.1993  loss_dice_8: 0.209  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:34:08] d2.utils.events INFO:  eta: 1:43:00  iter: 819  total_loss: 5.824  loss_ce: 0.1393  loss_mask: 0.2139  loss_dice: 0.1878  loss_ce_0: 0.2504  loss_mask_0: 0.201  loss_dice_0: 0.1921  loss_ce_1: 0.1396  loss_mask_1: 0.211  loss_dice_1: 0.195  loss_ce_2: 0.1403  loss_mask_2: 0.2091  loss_dice_2: 0.1903  loss_ce_3: 0.1384  loss_mask_3: 0.204  loss_dice_3: 0.1981  loss_ce_4: 0.1384  loss_mask_4: 0.2129  loss_dice_4: 0.1976  loss_ce_5: 0.1374  loss_mask_5: 0.2095  loss_dice_5: 0.1995  loss_ce_6: 0.1384  loss_mask_6: 0.2199  loss_dice_6: 0.201  loss_ce_7: 0.1359  loss_mask_7: 0.2191  loss_dice_7: 0.2225  loss_ce_8: 0.1394  loss_mask_8: 0.207  loss_dice_8: 0.1879  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:11] d2.utils.events INFO:  eta: 1:42:59  iter: 839  total_loss: 7.696  loss_ce: 0.1287  loss_mask: 0.2643  loss_dice: 0.3503  loss_ce_0: 0.2387  loss_mask_0: 0.2665  loss_dice_0: 0.3336  loss_ce_1: 0.1284  loss_mask_1: 0.2609  loss_dice_1: 0.3574  loss_ce_2: 0.1286  loss_mask_2: 0.2571  loss_dice_2: 0.3593  loss_ce_3: 0.1286  loss_mask_3: 0.2654  loss_dice_3: 0.3309  loss_ce_4: 0.1285  loss_mask_4: 0.2556  loss_dice_4: 0.3406  loss_ce_5: 0.127  loss_mask_5: 0.2636  loss_dice_5: 0.3332  loss_ce_6: 0.1278  loss_mask_6: 0.2651  loss_dice_6: 0.3483  loss_ce_7: 0.128  loss_mask_7: 0.267  loss_dice_7: 0.3686  loss_ce_8: 0.1288  loss_mask_8: 0.2636  loss_dice_8: 0.3437  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:13] d2.utils.events INFO:  eta: 1:42:57  iter: 859  total_loss: 6.498  loss_ce: 0.1298  loss_mask: 0.2392  loss_dice: 0.1732  loss_ce_0: 0.2296  loss_mask_0: 0.258  loss_dice_0: 0.171  loss_ce_1: 0.1306  loss_mask_1: 0.2443  loss_dice_1: 0.1804  loss_ce_2: 0.13  loss_mask_2: 0.2431  loss_dice_2: 0.17  loss_ce_3: 0.1295  loss_mask_3: 0.2594  loss_dice_3: 0.173  loss_ce_4: 0.1301  loss_mask_4: 0.2535  loss_dice_4: 0.1772  loss_ce_5: 0.1303  loss_mask_5: 0.2445  loss_dice_5: 0.1667  loss_ce_6: 0.1294  loss_mask_6: 0.2614  loss_dice_6: 0.1779  loss_ce_7: 0.1304  loss_mask_7: 0.2361  loss_dice_7: 0.1691  loss_ce_8: 0.1299  loss_mask_8: 0.2513  loss_dice_8: 0.1689  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:16] d2.utils.events INFO:  eta: 1:42:53  iter: 879  total_loss: 6.914  loss_ce: 0.1188  loss_mask: 0.2566  loss_dice: 0.2371  loss_ce_0: 0.2179  loss_mask_0: 0.2556  loss_dice_0: 0.2303  loss_ce_1: 0.118  loss_mask_1: 0.2662  loss_dice_1: 0.2429  loss_ce_2: 0.1182  loss_mask_2: 0.2623  loss_dice_2: 0.2339  loss_ce_3: 0.119  loss_mask_3: 0.2499  loss_dice_3: 0.2424  loss_ce_4: 0.1186  loss_mask_4: 0.2757  loss_dice_4: 0.2371  loss_ce_5: 0.1188  loss_mask_5: 0.268  loss_dice_5: 0.2396  loss_ce_6: 0.1187  loss_mask_6: 0.2655  loss_dice_6: 0.2361  loss_ce_7: 0.1185  loss_mask_7: 0.2525  loss_dice_7: 0.2363  loss_ce_8: 0.1189  loss_mask_8: 0.2651  loss_dice_8: 0.2324  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:18] d2.utils.events INFO:  eta: 1:42:50  iter: 899  total_loss: 7.507  loss_ce: 0.06649  loss_mask: 0.297  loss_dice: 0.2815  loss_ce_0: 0.2063  loss_mask_0: 0.3034  loss_dice_0: 0.2715  loss_ce_1: 0.06589  loss_mask_1: 0.3135  loss_dice_1: 0.2712  loss_ce_2: 0.06675  loss_mask_2: 0.2978  loss_dice_2: 0.2726  loss_ce_3: 0.06625  loss_mask_3: 0.296  loss_dice_3: 0.2734  loss_ce_4: 0.06668  loss_mask_4: 0.2928  loss_dice_4: 0.272  loss_ce_5: 0.06652  loss_mask_5: 0.3022  loss_dice_5: 0.2647  loss_ce_6: 0.06603  loss_mask_6: 0.3084  loss_dice_6: 0.2708  loss_ce_7: 0.06625  loss_mask_7: 0.3018  loss_dice_7: 0.2786  loss_ce_8: 0.06655  loss_mask_8: 0.2969  loss_dice_8: 0.2795  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:34:21] d2.utils.events INFO:  eta: 1:42:49  iter: 919  total_loss: 6.096  loss_ce: 0.06012  loss_mask: 0.2299  loss_dice: 0.2227  loss_ce_0: 0.1961  loss_mask_0: 0.2285  loss_dice_0: 0.2298  loss_ce_1: 0.05952  loss_mask_1: 0.232  loss_dice_1: 0.2273  loss_ce_2: 0.06076  loss_mask_2: 0.2318  loss_dice_2: 0.232  loss_ce_3: 0.06004  loss_mask_3: 0.2354  loss_dice_3: 0.2267  loss_ce_4: 0.06032  loss_mask_4: 0.2292  loss_dice_4: 0.2328  loss_ce_5: 0.06032  loss_mask_5: 0.2286  loss_dice_5: 0.23  loss_ce_6: 0.05984  loss_mask_6: 0.2261  loss_dice_6: 0.2318  loss_ce_7: 0.05989  loss_mask_7: 0.2345  loss_dice_7: 0.2346  loss_ce_8: 0.0606  loss_mask_8: 0.2418  loss_dice_8: 0.2196  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:24] d2.utils.events INFO:  eta: 1:42:47  iter: 939  total_loss: 5.646  loss_ce: 0.1026  loss_mask: 0.2348  loss_dice: 0.2227  loss_ce_0: 0.1846  loss_mask_0: 0.2259  loss_dice_0: 0.211  loss_ce_1: 0.1033  loss_mask_1: 0.2374  loss_dice_1: 0.2265  loss_ce_2: 0.1039  loss_mask_2: 0.2341  loss_dice_2: 0.2089  loss_ce_3: 0.1022  loss_mask_3: 0.2249  loss_dice_3: 0.2156  loss_ce_4: 0.1025  loss_mask_4: 0.2228  loss_dice_4: 0.2138  loss_ce_5: 0.1019  loss_mask_5: 0.215  loss_dice_5: 0.223  loss_ce_6: 0.1012  loss_mask_6: 0.2202  loss_dice_6: 0.2216  loss_ce_7: 0.1014  loss_mask_7: 0.229  loss_dice_7: 0.222  loss_ce_8: 0.1021  loss_mask_8: 0.2356  loss_dice_8: 0.2098  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:26] d2.utils.events INFO:  eta: 1:42:47  iter: 959  total_loss: 5.742  loss_ce: 0.06443  loss_mask: 0.2298  loss_dice: 0.1901  loss_ce_0: 0.1734  loss_mask_0: 0.2408  loss_dice_0: 0.1878  loss_ce_1: 0.06327  loss_mask_1: 0.2395  loss_dice_1: 0.1857  loss_ce_2: 0.06161  loss_mask_2: 0.2289  loss_dice_2: 0.1866  loss_ce_3: 0.06217  loss_mask_3: 0.2339  loss_dice_3: 0.1913  loss_ce_4: 0.06151  loss_mask_4: 0.2339  loss_dice_4: 0.1899  loss_ce_5: 0.06366  loss_mask_5: 0.233  loss_dice_5: 0.1833  loss_ce_6: 0.06316  loss_mask_6: 0.2425  loss_dice_6: 0.1869  loss_ce_7: 0.0648  loss_mask_7: 0.2401  loss_dice_7: 0.196  loss_ce_8: 0.06379  loss_mask_8: 0.2351  loss_dice_8: 0.1865  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:29] d2.utils.events INFO:  eta: 1:42:46  iter: 979  total_loss: 6.232  loss_ce: 0.06619  loss_mask: 0.2468  loss_dice: 0.222  loss_ce_0: 0.1644  loss_mask_0: 0.2464  loss_dice_0: 0.2202  loss_ce_1: 0.06638  loss_mask_1: 0.2576  loss_dice_1: 0.2234  loss_ce_2: 0.06762  loss_mask_2: 0.2512  loss_dice_2: 0.217  loss_ce_3: 0.06706  loss_mask_3: 0.2443  loss_dice_3: 0.2206  loss_ce_4: 0.06797  loss_mask_4: 0.2429  loss_dice_4: 0.2202  loss_ce_5: 0.06657  loss_mask_5: 0.2431  loss_dice_5: 0.2191  loss_ce_6: 0.06581  loss_mask_6: 0.2369  loss_dice_6: 0.2167  loss_ce_7: 0.06608  loss_mask_7: 0.2476  loss_dice_7: 0.2217  loss_ce_8: 0.06522  loss_mask_8: 0.2455  loss_dice_8: 0.2166  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:31] d2.utils.events INFO:  eta: 1:42:46  iter: 999  total_loss: 6.668  loss_ce: 0.09155  loss_mask: 0.2724  loss_dice: 0.179  loss_ce_0: 0.1572  loss_mask_0: 0.2861  loss_dice_0: 0.1775  loss_ce_1: 0.09295  loss_mask_1: 0.2819  loss_dice_1: 0.1787  loss_ce_2: 0.09462  loss_mask_2: 0.2679  loss_dice_2: 0.1868  loss_ce_3: 0.09368  loss_mask_3: 0.2765  loss_dice_3: 0.1687  loss_ce_4: 0.09361  loss_mask_4: 0.2754  loss_dice_4: 0.1694  loss_ce_5: 0.09225  loss_mask_5: 0.2677  loss_dice_5: 0.1782  loss_ce_6: 0.0912  loss_mask_6: 0.2666  loss_dice_6: 0.174  loss_ce_7: 0.09134  loss_mask_7: 0.2626  loss_dice_7: 0.1721  loss_ce_8: 0.09065  loss_mask_8: 0.2717  loss_dice_8: 0.1745  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:34] d2.utils.events INFO:  eta: 1:42:45  iter: 1019  total_loss: 5.423  loss_ce: 0.1267  loss_mask: 0.2398  loss_dice: 0.1712  loss_ce_0: 0.1838  loss_mask_0: 0.2368  loss_dice_0: 0.1774  loss_ce_1: 0.1232  loss_mask_1: 0.2262  loss_dice_1: 0.1868  loss_ce_2: 0.1248  loss_mask_2: 0.2349  loss_dice_2: 0.1824  loss_ce_3: 0.1257  loss_mask_3: 0.2307  loss_dice_3: 0.18  loss_ce_4: 0.125  loss_mask_4: 0.2259  loss_dice_4: 0.1734  loss_ce_5: 0.1259  loss_mask_5: 0.2405  loss_dice_5: 0.1843  loss_ce_6: 0.1267  loss_mask_6: 0.2351  loss_dice_6: 0.175  loss_ce_7: 0.1253  loss_mask_7: 0.2184  loss_dice_7: 0.179  loss_ce_8: 0.1268  loss_mask_8: 0.2378  loss_dice_8: 0.1781  time: 0.1262  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:36] d2.utils.events INFO:  eta: 1:42:42  iter: 1039  total_loss: 6.457  loss_ce: 0.15  loss_mask: 0.2306  loss_dice: 0.189  loss_ce_0: 0.1478  loss_mask_0: 0.229  loss_dice_0: 0.1874  loss_ce_1: 0.1468  loss_mask_1: 0.2287  loss_dice_1: 0.1785  loss_ce_2: 0.1484  loss_mask_2: 0.2134  loss_dice_2: 0.1734  loss_ce_3: 0.1495  loss_mask_3: 0.2276  loss_dice_3: 0.1814  loss_ce_4: 0.1485  loss_mask_4: 0.2225  loss_dice_4: 0.1748  loss_ce_5: 0.15  loss_mask_5: 0.235  loss_dice_5: 0.1895  loss_ce_6: 0.1497  loss_mask_6: 0.2173  loss_dice_6: 0.1726  loss_ce_7: 0.1491  loss_mask_7: 0.218  loss_dice_7: 0.1811  loss_ce_8: 0.1505  loss_mask_8: 0.2249  loss_dice_8: 0.1804  time: 0.1262  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:34:39] d2.utils.events INFO:  eta: 1:42:41  iter: 1059  total_loss: 6.041  loss_ce: 0.1319  loss_mask: 0.2397  loss_dice: 0.1849  loss_ce_0: 0.1667  loss_mask_0: 0.2427  loss_dice_0: 0.1716  loss_ce_1: 0.1323  loss_mask_1: 0.2388  loss_dice_1: 0.1818  loss_ce_2: 0.1318  loss_mask_2: 0.2382  loss_dice_2: 0.1767  loss_ce_3: 0.1315  loss_mask_3: 0.238  loss_dice_3: 0.1808  loss_ce_4: 0.1311  loss_mask_4: 0.2358  loss_dice_4: 0.1794  loss_ce_5: 0.1311  loss_mask_5: 0.2412  loss_dice_5: 0.1845  loss_ce_6: 0.1317  loss_mask_6: 0.2328  loss_dice_6: 0.176  loss_ce_7: 0.1311  loss_mask_7: 0.2407  loss_dice_7: 0.1798  loss_ce_8: 0.1314  loss_mask_8: 0.236  loss_dice_8: 0.1798  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:34:41] d2.utils.events INFO:  eta: 1:42:39  iter: 1079  total_loss: 5.737  loss_ce: 0.136  loss_mask: 0.2483  loss_dice: 0.1747  loss_ce_0: 0.1611  loss_mask_0: 0.2417  loss_dice_0: 0.1645  loss_ce_1: 0.1392  loss_mask_1: 0.2398  loss_dice_1: 0.1684  loss_ce_2: 0.1351  loss_mask_2: 0.2432  loss_dice_2: 0.1638  loss_ce_3: 0.1364  loss_mask_3: 0.2504  loss_dice_3: 0.1755  loss_ce_4: 0.1383  loss_mask_4: 0.2398  loss_dice_4: 0.1777  loss_ce_5: 0.1348  loss_mask_5: 0.2475  loss_dice_5: 0.1661  loss_ce_6: 0.1349  loss_mask_6: 0.2426  loss_dice_6: 0.171  loss_ce_7: 0.1374  loss_mask_7: 0.2395  loss_dice_7: 0.1676  loss_ce_8: 0.1348  loss_mask_8: 0.2403  loss_dice_8: 0.173  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:34:44] d2.utils.events INFO:  eta: 1:42:40  iter: 1099  total_loss: 5.732  loss_ce: 0.1297  loss_mask: 0.2212  loss_dice: 0.1863  loss_ce_0: 0.1531  loss_mask_0: 0.222  loss_dice_0: 0.1805  loss_ce_1: 0.1294  loss_mask_1: 0.222  loss_dice_1: 0.1806  loss_ce_2: 0.1291  loss_mask_2: 0.2297  loss_dice_2: 0.1822  loss_ce_3: 0.1292  loss_mask_3: 0.2174  loss_dice_3: 0.1881  loss_ce_4: 0.1286  loss_mask_4: 0.2174  loss_dice_4: 0.1796  loss_ce_5: 0.1296  loss_mask_5: 0.2104  loss_dice_5: 0.1801  loss_ce_6: 0.1301  loss_mask_6: 0.2274  loss_dice_6: 0.1841  loss_ce_7: 0.1297  loss_mask_7: 0.2155  loss_dice_7: 0.1858  loss_ce_8: 0.1294  loss_mask_8: 0.2323  loss_dice_8: 0.1798  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:46] d2.utils.events INFO:  eta: 1:42:39  iter: 1119  total_loss: 5.794  loss_ce: 0.101  loss_mask: 0.2521  loss_dice: 0.1562  loss_ce_0: 0.1459  loss_mask_0: 0.2414  loss_dice_0: 0.1644  loss_ce_1: 0.1095  loss_mask_1: 0.2445  loss_dice_1: 0.1641  loss_ce_2: 0.1019  loss_mask_2: 0.2408  loss_dice_2: 0.1635  loss_ce_3: 0.1013  loss_mask_3: 0.2428  loss_dice_3: 0.162  loss_ce_4: 0.1069  loss_mask_4: 0.2343  loss_dice_4: 0.1562  loss_ce_5: 0.1024  loss_mask_5: 0.2305  loss_dice_5: 0.1647  loss_ce_6: 0.09947  loss_mask_6: 0.2382  loss_dice_6: 0.1654  loss_ce_7: 0.1076  loss_mask_7: 0.2356  loss_dice_7: 0.1579  loss_ce_8: 0.1014  loss_mask_8: 0.2348  loss_dice_8: 0.1556  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:49] d2.utils.events INFO:  eta: 1:42:32  iter: 1139  total_loss: 5.869  loss_ce: 0.1332  loss_mask: 0.2208  loss_dice: 0.1778  loss_ce_0: 0.1388  loss_mask_0: 0.2153  loss_dice_0: 0.1828  loss_ce_1: 0.1336  loss_mask_1: 0.2182  loss_dice_1: 0.1831  loss_ce_2: 0.1329  loss_mask_2: 0.2176  loss_dice_2: 0.1815  loss_ce_3: 0.1325  loss_mask_3: 0.2223  loss_dice_3: 0.1822  loss_ce_4: 0.1331  loss_mask_4: 0.2189  loss_dice_4: 0.1859  loss_ce_5: 0.1327  loss_mask_5: 0.2208  loss_dice_5: 0.1832  loss_ce_6: 0.133  loss_mask_6: 0.2254  loss_dice_6: 0.18  loss_ce_7: 0.1332  loss_mask_7: 0.2237  loss_dice_7: 0.1839  loss_ce_8: 0.1327  loss_mask_8: 0.2126  loss_dice_8: 0.1748  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:34:51] d2.utils.events INFO:  eta: 1:42:30  iter: 1159  total_loss: 6.345  loss_ce: 0.127  loss_mask: 0.2602  loss_dice: 0.1619  loss_ce_0: 0.1367  loss_mask_0: 0.2692  loss_dice_0: 0.1536  loss_ce_1: 0.1278  loss_mask_1: 0.2528  loss_dice_1: 0.1633  loss_ce_2: 0.1282  loss_mask_2: 0.2644  loss_dice_2: 0.1626  loss_ce_3: 0.1278  loss_mask_3: 0.2593  loss_dice_3: 0.1649  loss_ce_4: 0.1279  loss_mask_4: 0.2666  loss_dice_4: 0.1699  loss_ce_5: 0.1278  loss_mask_5: 0.2598  loss_dice_5: 0.164  loss_ce_6: 0.1272  loss_mask_6: 0.2668  loss_dice_6: 0.164  loss_ce_7: 0.128  loss_mask_7: 0.2698  loss_dice_7: 0.1671  loss_ce_8: 0.1273  loss_mask_8: 0.2567  loss_dice_8: 0.1662  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:54] d2.utils.events INFO:  eta: 1:42:27  iter: 1179  total_loss: 6.074  loss_ce: 0.1406  loss_mask: 0.2736  loss_dice: 0.1552  loss_ce_0: 0.1344  loss_mask_0: 0.2741  loss_dice_0: 0.1601  loss_ce_1: 0.1435  loss_mask_1: 0.2708  loss_dice_1: 0.1649  loss_ce_2: 0.1449  loss_mask_2: 0.2831  loss_dice_2: 0.162  loss_ce_3: 0.1435  loss_mask_3: 0.2748  loss_dice_3: 0.159  loss_ce_4: 0.1414  loss_mask_4: 0.2709  loss_dice_4: 0.1662  loss_ce_5: 0.1411  loss_mask_5: 0.2869  loss_dice_5: 0.1629  loss_ce_6: 0.141  loss_mask_6: 0.278  loss_dice_6: 0.1637  loss_ce_7: 0.1395  loss_mask_7: 0.2752  loss_dice_7: 0.1619  loss_ce_8: 0.1408  loss_mask_8: 0.2814  loss_dice_8: 0.1601  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:34:56] d2.utils.events INFO:  eta: 1:42:24  iter: 1199  total_loss: 6.115  loss_ce: 0.1228  loss_mask: 0.2661  loss_dice: 0.1761  loss_ce_0: 0.1316  loss_mask_0: 0.2672  loss_dice_0: 0.1771  loss_ce_1: 0.1221  loss_mask_1: 0.2584  loss_dice_1: 0.1787  loss_ce_2: 0.1225  loss_mask_2: 0.265  loss_dice_2: 0.1798  loss_ce_3: 0.1225  loss_mask_3: 0.2591  loss_dice_3: 0.1814  loss_ce_4: 0.1225  loss_mask_4: 0.2598  loss_dice_4: 0.1811  loss_ce_5: 0.1226  loss_mask_5: 0.2594  loss_dice_5: 0.1786  loss_ce_6: 0.1228  loss_mask_6: 0.2578  loss_dice_6: 0.1795  loss_ce_7: 0.1226  loss_mask_7: 0.2671  loss_dice_7: 0.1785  loss_ce_8: 0.1228  loss_mask_8: 0.2618  loss_dice_8: 0.177  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:34:59] d2.utils.events INFO:  eta: 1:42:19  iter: 1219  total_loss: 6.075  loss_ce: 0.1131  loss_mask: 0.2466  loss_dice: 0.1871  loss_ce_0: 0.128  loss_mask_0: 0.2502  loss_dice_0: 0.1903  loss_ce_1: 0.1131  loss_mask_1: 0.2419  loss_dice_1: 0.1886  loss_ce_2: 0.1078  loss_mask_2: 0.2429  loss_dice_2: 0.1897  loss_ce_3: 0.1083  loss_mask_3: 0.2487  loss_dice_3: 0.194  loss_ce_4: 0.1137  loss_mask_4: 0.2506  loss_dice_4: 0.1922  loss_ce_5: 0.1118  loss_mask_5: 0.2462  loss_dice_5: 0.1855  loss_ce_6: 0.113  loss_mask_6: 0.2448  loss_dice_6: 0.1934  loss_ce_7: 0.1174  loss_mask_7: 0.2422  loss_dice_7: 0.1893  loss_ce_8: 0.1129  loss_mask_8: 0.242  loss_dice_8: 0.1782  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:01] d2.utils.events INFO:  eta: 1:42:10  iter: 1239  total_loss: 5.952  loss_ce: 0.09257  loss_mask: 0.2325  loss_dice: 0.1675  loss_ce_0: 0.1269  loss_mask_0: 0.2284  loss_dice_0: 0.1621  loss_ce_1: 0.09214  loss_mask_1: 0.2231  loss_dice_1: 0.1687  loss_ce_2: 0.09115  loss_mask_2: 0.2228  loss_dice_2: 0.1651  loss_ce_3: 0.09115  loss_mask_3: 0.2211  loss_dice_3: 0.1738  loss_ce_4: 0.09171  loss_mask_4: 0.219  loss_dice_4: 0.1705  loss_ce_5: 0.09213  loss_mask_5: 0.2204  loss_dice_5: 0.1654  loss_ce_6: 0.09234  loss_mask_6: 0.2294  loss_dice_6: 0.1632  loss_ce_7: 0.09263  loss_mask_7: 0.2255  loss_dice_7: 0.1688  loss_ce_8: 0.0922  loss_mask_8: 0.2338  loss_dice_8: 0.1604  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:04] d2.utils.events INFO:  eta: 1:42:07  iter: 1259  total_loss: 5.87  loss_ce: 0.1335  loss_mask: 0.2712  loss_dice: 0.1575  loss_ce_0: 0.1263  loss_mask_0: 0.2555  loss_dice_0: 0.1612  loss_ce_1: 0.1319  loss_mask_1: 0.2555  loss_dice_1: 0.1614  loss_ce_2: 0.1318  loss_mask_2: 0.2563  loss_dice_2: 0.156  loss_ce_3: 0.1318  loss_mask_3: 0.2634  loss_dice_3: 0.1604  loss_ce_4: 0.1321  loss_mask_4: 0.2684  loss_dice_4: 0.1613  loss_ce_5: 0.132  loss_mask_5: 0.2595  loss_dice_5: 0.1609  loss_ce_6: 0.1319  loss_mask_6: 0.263  loss_dice_6: 0.1638  loss_ce_7: 0.1335  loss_mask_7: 0.2564  loss_dice_7: 0.1692  loss_ce_8: 0.1332  loss_mask_8: 0.2663  loss_dice_8: 0.1616  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:06] d2.utils.events INFO:  eta: 1:42:04  iter: 1279  total_loss: 5.694  loss_ce: 0.1306  loss_mask: 0.279  loss_dice: 0.1534  loss_ce_0: 0.1189  loss_mask_0: 0.268  loss_dice_0: 0.1478  loss_ce_1: 0.1334  loss_mask_1: 0.2717  loss_dice_1: 0.1492  loss_ce_2: 0.1339  loss_mask_2: 0.2655  loss_dice_2: 0.1481  loss_ce_3: 0.1338  loss_mask_3: 0.2733  loss_dice_3: 0.1499  loss_ce_4: 0.1321  loss_mask_4: 0.2729  loss_dice_4: 0.1532  loss_ce_5: 0.1317  loss_mask_5: 0.2781  loss_dice_5: 0.1533  loss_ce_6: 0.1315  loss_mask_6: 0.2727  loss_dice_6: 0.1513  loss_ce_7: 0.1311  loss_mask_7: 0.2721  loss_dice_7: 0.1535  loss_ce_8: 0.1307  loss_mask_8: 0.2745  loss_dice_8: 0.1498  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:09] d2.utils.events INFO:  eta: 1:41:57  iter: 1299  total_loss: 5.358  loss_ce: 0.1279  loss_mask: 0.2084  loss_dice: 0.1457  loss_ce_0: 0.1215  loss_mask_0: 0.2081  loss_dice_0: 0.1531  loss_ce_1: 0.1304  loss_mask_1: 0.2159  loss_dice_1: 0.151  loss_ce_2: 0.13  loss_mask_2: 0.2041  loss_dice_2: 0.1502  loss_ce_3: 0.1295  loss_mask_3: 0.2237  loss_dice_3: 0.1503  loss_ce_4: 0.129  loss_mask_4: 0.2161  loss_dice_4: 0.1499  loss_ce_5: 0.1288  loss_mask_5: 0.2084  loss_dice_5: 0.1465  loss_ce_6: 0.1293  loss_mask_6: 0.2064  loss_dice_6: 0.1467  loss_ce_7: 0.1291  loss_mask_7: 0.2133  loss_dice_7: 0.1536  loss_ce_8: 0.1287  loss_mask_8: 0.2235  loss_dice_8: 0.1416  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:11] d2.utils.events INFO:  eta: 1:41:53  iter: 1319  total_loss: 5.715  loss_ce: 0.1402  loss_mask: 0.2123  loss_dice: 0.1541  loss_ce_0: 0.1142  loss_mask_0: 0.2379  loss_dice_0: 0.1571  loss_ce_1: 0.141  loss_mask_1: 0.2251  loss_dice_1: 0.1572  loss_ce_2: 0.1375  loss_mask_2: 0.2311  loss_dice_2: 0.1551  loss_ce_3: 0.1419  loss_mask_3: 0.2281  loss_dice_3: 0.1571  loss_ce_4: 0.1405  loss_mask_4: 0.2278  loss_dice_4: 0.1593  loss_ce_5: 0.1389  loss_mask_5: 0.2213  loss_dice_5: 0.1498  loss_ce_6: 0.1407  loss_mask_6: 0.2282  loss_dice_6: 0.1578  loss_ce_7: 0.1426  loss_mask_7: 0.2167  loss_dice_7: 0.1556  loss_ce_8: 0.1405  loss_mask_8: 0.2252  loss_dice_8: 0.1532  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:14] d2.utils.events INFO:  eta: 1:41:45  iter: 1339  total_loss: 5.644  loss_ce: 0.1365  loss_mask: 0.2332  loss_dice: 0.1817  loss_ce_0: 0.1168  loss_mask_0: 0.233  loss_dice_0: 0.1839  loss_ce_1: 0.1418  loss_mask_1: 0.2665  loss_dice_1: 0.1929  loss_ce_2: 0.1375  loss_mask_2: 0.2536  loss_dice_2: 0.1801  loss_ce_3: 0.1369  loss_mask_3: 0.2434  loss_dice_3: 0.1811  loss_ce_4: 0.1416  loss_mask_4: 0.2396  loss_dice_4: 0.1818  loss_ce_5: 0.1374  loss_mask_5: 0.2495  loss_dice_5: 0.1858  loss_ce_6: 0.1367  loss_mask_6: 0.2508  loss_dice_6: 0.1838  loss_ce_7: 0.1422  loss_mask_7: 0.2458  loss_dice_7: 0.1881  loss_ce_8: 0.1373  loss_mask_8: 0.2395  loss_dice_8: 0.1791  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:16] d2.utils.events INFO:  eta: 1:41:40  iter: 1359  total_loss: 5.495  loss_ce: 0.1291  loss_mask: 0.2635  loss_dice: 0.1547  loss_ce_0: 0.1111  loss_mask_0: 0.2552  loss_dice_0: 0.1475  loss_ce_1: 0.129  loss_mask_1: 0.2541  loss_dice_1: 0.1606  loss_ce_2: 0.1294  loss_mask_2: 0.2687  loss_dice_2: 0.1564  loss_ce_3: 0.1293  loss_mask_3: 0.2663  loss_dice_3: 0.1516  loss_ce_4: 0.1286  loss_mask_4: 0.267  loss_dice_4: 0.1572  loss_ce_5: 0.1287  loss_mask_5: 0.2702  loss_dice_5: 0.1562  loss_ce_6: 0.1292  loss_mask_6: 0.263  loss_dice_6: 0.1552  loss_ce_7: 0.128  loss_mask_7: 0.2675  loss_dice_7: 0.1616  loss_ce_8: 0.1288  loss_mask_8: 0.2564  loss_dice_8: 0.1609  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:19] d2.utils.events INFO:  eta: 1:41:41  iter: 1379  total_loss: 5.499  loss_ce: 0.1348  loss_mask: 0.2122  loss_dice: 0.1501  loss_ce_0: 0.1119  loss_mask_0: 0.2066  loss_dice_0: 0.1523  loss_ce_1: 0.1341  loss_mask_1: 0.2088  loss_dice_1: 0.1543  loss_ce_2: 0.1347  loss_mask_2: 0.2094  loss_dice_2: 0.1538  loss_ce_3: 0.1344  loss_mask_3: 0.2011  loss_dice_3: 0.1535  loss_ce_4: 0.1338  loss_mask_4: 0.2052  loss_dice_4: 0.1514  loss_ce_5: 0.1338  loss_mask_5: 0.1958  loss_dice_5: 0.1568  loss_ce_6: 0.1359  loss_mask_6: 0.2104  loss_dice_6: 0.149  loss_ce_7: 0.1336  loss_mask_7: 0.2028  loss_dice_7: 0.1564  loss_ce_8: 0.1341  loss_mask_8: 0.1978  loss_dice_8: 0.1482  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:21] d2.utils.events INFO:  eta: 1:41:42  iter: 1399  total_loss: 5.895  loss_ce: 0.1204  loss_mask: 0.2223  loss_dice: 0.1829  loss_ce_0: 0.1107  loss_mask_0: 0.2318  loss_dice_0: 0.1815  loss_ce_1: 0.1187  loss_mask_1: 0.2115  loss_dice_1: 0.1779  loss_ce_2: 0.1182  loss_mask_2: 0.2324  loss_dice_2: 0.183  loss_ce_3: 0.119  loss_mask_3: 0.2342  loss_dice_3: 0.1808  loss_ce_4: 0.1197  loss_mask_4: 0.225  loss_dice_4: 0.1832  loss_ce_5: 0.1199  loss_mask_5: 0.2222  loss_dice_5: 0.183  loss_ce_6: 0.1203  loss_mask_6: 0.225  loss_dice_6: 0.1802  loss_ce_7: 0.1207  loss_mask_7: 0.2438  loss_dice_7: 0.1844  loss_ce_8: 0.1206  loss_mask_8: 0.2226  loss_dice_8: 0.1899  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:24] d2.utils.events INFO:  eta: 1:41:41  iter: 1419  total_loss: 5.805  loss_ce: 0.1329  loss_mask: 0.2082  loss_dice: 0.1788  loss_ce_0: 0.1078  loss_mask_0: 0.2113  loss_dice_0: 0.1794  loss_ce_1: 0.1309  loss_mask_1: 0.2052  loss_dice_1: 0.1836  loss_ce_2: 0.1332  loss_mask_2: 0.2066  loss_dice_2: 0.1801  loss_ce_3: 0.1331  loss_mask_3: 0.2134  loss_dice_3: 0.1833  loss_ce_4: 0.1309  loss_mask_4: 0.2021  loss_dice_4: 0.1914  loss_ce_5: 0.1327  loss_mask_5: 0.1987  loss_dice_5: 0.1869  loss_ce_6: 0.1346  loss_mask_6: 0.2114  loss_dice_6: 0.1847  loss_ce_7: 0.1311  loss_mask_7: 0.2119  loss_dice_7: 0.1807  loss_ce_8: 0.1331  loss_mask_8: 0.2028  loss_dice_8: 0.1839  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:26] d2.utils.events INFO:  eta: 1:41:35  iter: 1439  total_loss: 6.705  loss_ce: 0.1279  loss_mask: 0.2303  loss_dice: 0.1979  loss_ce_0: 0.1063  loss_mask_0: 0.2216  loss_dice_0: 0.2023  loss_ce_1: 0.1295  loss_mask_1: 0.2193  loss_dice_1: 0.1964  loss_ce_2: 0.1289  loss_mask_2: 0.2288  loss_dice_2: 0.1901  loss_ce_3: 0.1284  loss_mask_3: 0.21  loss_dice_3: 0.2016  loss_ce_4: 0.1294  loss_mask_4: 0.2307  loss_dice_4: 0.2  loss_ce_5: 0.1291  loss_mask_5: 0.2157  loss_dice_5: 0.2032  loss_ce_6: 0.1287  loss_mask_6: 0.2266  loss_dice_6: 0.1999  loss_ce_7: 0.1288  loss_mask_7: 0.2302  loss_dice_7: 0.2058  loss_ce_8: 0.1281  loss_mask_8: 0.228  loss_dice_8: 0.2053  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:29] d2.utils.events INFO:  eta: 1:41:35  iter: 1459  total_loss: 5.94  loss_ce: 0.1262  loss_mask: 0.2051  loss_dice: 0.1861  loss_ce_0: 0.1029  loss_mask_0: 0.2119  loss_dice_0: 0.1871  loss_ce_1: 0.1271  loss_mask_1: 0.2031  loss_dice_1: 0.189  loss_ce_2: 0.1283  loss_mask_2: 0.206  loss_dice_2: 0.1855  loss_ce_3: 0.1273  loss_mask_3: 0.217  loss_dice_3: 0.195  loss_ce_4: 0.1273  loss_mask_4: 0.2077  loss_dice_4: 0.1821  loss_ce_5: 0.1275  loss_mask_5: 0.2239  loss_dice_5: 0.1886  loss_ce_6: 0.1251  loss_mask_6: 0.2193  loss_dice_6: 0.1885  loss_ce_7: 0.1259  loss_mask_7: 0.2047  loss_dice_7: 0.1926  loss_ce_8: 0.1273  loss_mask_8: 0.2149  loss_dice_8: 0.1904  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:32] d2.utils.events INFO:  eta: 1:41:33  iter: 1479  total_loss: 6.505  loss_ce: 0.09895  loss_mask: 0.2  loss_dice: 0.2264  loss_ce_0: 0.09925  loss_mask_0: 0.2085  loss_dice_0: 0.2351  loss_ce_1: 0.09723  loss_mask_1: 0.2056  loss_dice_1: 0.2242  loss_ce_2: 0.1013  loss_mask_2: 0.2082  loss_dice_2: 0.2198  loss_ce_3: 0.09881  loss_mask_3: 0.197  loss_dice_3: 0.2272  loss_ce_4: 0.09888  loss_mask_4: 0.2001  loss_dice_4: 0.2452  loss_ce_5: 0.1005  loss_mask_5: 0.2186  loss_dice_5: 0.2383  loss_ce_6: 0.09963  loss_mask_6: 0.2154  loss_dice_6: 0.2211  loss_ce_7: 0.09865  loss_mask_7: 0.2124  loss_dice_7: 0.2257  loss_ce_8: 0.0997  loss_mask_8: 0.2102  loss_dice_8: 0.2206  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:34] d2.utils.events INFO:  eta: 1:41:29  iter: 1499  total_loss: 6.076  loss_ce: 0.06867  loss_mask: 0.1912  loss_dice: 0.1947  loss_ce_0: 0.09514  loss_mask_0: 0.1956  loss_dice_0: 0.204  loss_ce_1: 0.06856  loss_mask_1: 0.2004  loss_dice_1: 0.2002  loss_ce_2: 0.06856  loss_mask_2: 0.193  loss_dice_2: 0.1931  loss_ce_3: 0.0685  loss_mask_3: 0.2028  loss_dice_3: 0.203  loss_ce_4: 0.06878  loss_mask_4: 0.2  loss_dice_4: 0.2086  loss_ce_5: 0.06884  loss_mask_5: 0.1876  loss_dice_5: 0.1985  loss_ce_6: 0.06834  loss_mask_6: 0.2025  loss_dice_6: 0.1994  loss_ce_7: 0.06912  loss_mask_7: 0.1922  loss_dice_7: 0.1948  loss_ce_8: 0.06901  loss_mask_8: 0.1914  loss_dice_8: 0.1925  time: 0.1259  data_time: 0.0014  lr: 0.0001  max_mem: 1071M
[04/13 14:35:37] d2.utils.events INFO:  eta: 1:41:25  iter: 1519  total_loss: 5.523  loss_ce: 0.1273  loss_mask: 0.2179  loss_dice: 0.1627  loss_ce_0: 0.1013  loss_mask_0: 0.2207  loss_dice_0: 0.1637  loss_ce_1: 0.1259  loss_mask_1: 0.2219  loss_dice_1: 0.1665  loss_ce_2: 0.1259  loss_mask_2: 0.2084  loss_dice_2: 0.1616  loss_ce_3: 0.1258  loss_mask_3: 0.2241  loss_dice_3: 0.1729  loss_ce_4: 0.1259  loss_mask_4: 0.2239  loss_dice_4: 0.1675  loss_ce_5: 0.126  loss_mask_5: 0.2256  loss_dice_5: 0.1767  loss_ce_6: 0.1268  loss_mask_6: 0.2205  loss_dice_6: 0.1628  loss_ce_7: 0.1269  loss_mask_7: 0.2189  loss_dice_7: 0.1662  loss_ce_8: 0.1262  loss_mask_8: 0.2195  loss_dice_8: 0.1628  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:39] d2.utils.events INFO:  eta: 1:41:25  iter: 1539  total_loss: 5.261  loss_ce: 0.1246  loss_mask: 0.2195  loss_dice: 0.1369  loss_ce_0: 0.1085  loss_mask_0: 0.2199  loss_dice_0: 0.1279  loss_ce_1: 0.1246  loss_mask_1: 0.2273  loss_dice_1: 0.1381  loss_ce_2: 0.1217  loss_mask_2: 0.2209  loss_dice_2: 0.1318  loss_ce_3: 0.1231  loss_mask_3: 0.2251  loss_dice_3: 0.1334  loss_ce_4: 0.1236  loss_mask_4: 0.2185  loss_dice_4: 0.1346  loss_ce_5: 0.1243  loss_mask_5: 0.2226  loss_dice_5: 0.1362  loss_ce_6: 0.1242  loss_mask_6: 0.2279  loss_dice_6: 0.1343  loss_ce_7: 0.1249  loss_mask_7: 0.227  loss_dice_7: 0.1347  loss_ce_8: 0.1243  loss_mask_8: 0.2138  loss_dice_8: 0.1278  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:42] d2.utils.events INFO:  eta: 1:41:28  iter: 1559  total_loss: 6.263  loss_ce: 0.1366  loss_mask: 0.2532  loss_dice: 0.1853  loss_ce_0: 0.09221  loss_mask_0: 0.24  loss_dice_0: 0.1805  loss_ce_1: 0.1367  loss_mask_1: 0.2601  loss_dice_1: 0.1815  loss_ce_2: 0.1369  loss_mask_2: 0.2622  loss_dice_2: 0.181  loss_ce_3: 0.1367  loss_mask_3: 0.252  loss_dice_3: 0.1852  loss_ce_4: 0.1387  loss_mask_4: 0.2589  loss_dice_4: 0.189  loss_ce_5: 0.1379  loss_mask_5: 0.2459  loss_dice_5: 0.1879  loss_ce_6: 0.1366  loss_mask_6: 0.2472  loss_dice_6: 0.1807  loss_ce_7: 0.1363  loss_mask_7: 0.2556  loss_dice_7: 0.1896  loss_ce_8: 0.1368  loss_mask_8: 0.2495  loss_dice_8: 0.1794  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:44] d2.utils.events INFO:  eta: 1:41:27  iter: 1579  total_loss: 5.722  loss_ce: 0.1282  loss_mask: 0.2394  loss_dice: 0.1703  loss_ce_0: 0.09743  loss_mask_0: 0.2332  loss_dice_0: 0.1652  loss_ce_1: 0.1284  loss_mask_1: 0.222  loss_dice_1: 0.1616  loss_ce_2: 0.1287  loss_mask_2: 0.2288  loss_dice_2: 0.1632  loss_ce_3: 0.1285  loss_mask_3: 0.221  loss_dice_3: 0.166  loss_ce_4: 0.1288  loss_mask_4: 0.2333  loss_dice_4: 0.171  loss_ce_5: 0.1286  loss_mask_5: 0.2283  loss_dice_5: 0.1666  loss_ce_6: 0.1283  loss_mask_6: 0.218  loss_dice_6: 0.1603  loss_ce_7: 0.1282  loss_mask_7: 0.2089  loss_dice_7: 0.1595  loss_ce_8: 0.1287  loss_mask_8: 0.2297  loss_dice_8: 0.1601  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:47] d2.utils.events INFO:  eta: 1:41:27  iter: 1599  total_loss: 5.595  loss_ce: 0.1105  loss_mask: 0.2252  loss_dice: 0.19  loss_ce_0: 0.09128  loss_mask_0: 0.232  loss_dice_0: 0.1871  loss_ce_1: 0.1102  loss_mask_1: 0.2194  loss_dice_1: 0.184  loss_ce_2: 0.1094  loss_mask_2: 0.2232  loss_dice_2: 0.1886  loss_ce_3: 0.1103  loss_mask_3: 0.2318  loss_dice_3: 0.19  loss_ce_4: 0.1108  loss_mask_4: 0.2202  loss_dice_4: 0.1925  loss_ce_5: 0.1101  loss_mask_5: 0.2285  loss_dice_5: 0.1856  loss_ce_6: 0.1105  loss_mask_6: 0.2227  loss_dice_6: 0.1782  loss_ce_7: 0.111  loss_mask_7: 0.2326  loss_dice_7: 0.1886  loss_ce_8: 0.1104  loss_mask_8: 0.2241  loss_dice_8: 0.1844  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:49] d2.utils.events INFO:  eta: 1:41:30  iter: 1619  total_loss: 5.628  loss_ce: 0.1011  loss_mask: 0.2411  loss_dice: 0.1603  loss_ce_0: 0.08922  loss_mask_0: 0.2502  loss_dice_0: 0.1551  loss_ce_1: 0.1005  loss_mask_1: 0.2468  loss_dice_1: 0.1545  loss_ce_2: 0.1013  loss_mask_2: 0.2388  loss_dice_2: 0.1653  loss_ce_3: 0.101  loss_mask_3: 0.2316  loss_dice_3: 0.1598  loss_ce_4: 0.1011  loss_mask_4: 0.2389  loss_dice_4: 0.1619  loss_ce_5: 0.1014  loss_mask_5: 0.2405  loss_dice_5: 0.1615  loss_ce_6: 0.1008  loss_mask_6: 0.2459  loss_dice_6: 0.1599  loss_ce_7: 0.1007  loss_mask_7: 0.2495  loss_dice_7: 0.1622  loss_ce_8: 0.1014  loss_mask_8: 0.2423  loss_dice_8: 0.1583  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:52] d2.utils.events INFO:  eta: 1:41:32  iter: 1639  total_loss: 6.887  loss_ce: 0.1008  loss_mask: 0.2327  loss_dice: 0.2758  loss_ce_0: 0.08674  loss_mask_0: 0.2328  loss_dice_0: 0.2817  loss_ce_1: 0.1011  loss_mask_1: 0.222  loss_dice_1: 0.2854  loss_ce_2: 0.1017  loss_mask_2: 0.2294  loss_dice_2: 0.2749  loss_ce_3: 0.1011  loss_mask_3: 0.2259  loss_dice_3: 0.2802  loss_ce_4: 0.1017  loss_mask_4: 0.2283  loss_dice_4: 0.283  loss_ce_5: 0.1014  loss_mask_5: 0.2346  loss_dice_5: 0.2848  loss_ce_6: 0.1006  loss_mask_6: 0.2274  loss_dice_6: 0.2885  loss_ce_7: 0.1008  loss_mask_7: 0.2269  loss_dice_7: 0.3015  loss_ce_8: 0.1011  loss_mask_8: 0.2218  loss_dice_8: 0.2769  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:54] d2.utils.events INFO:  eta: 1:41:29  iter: 1659  total_loss: 6.314  loss_ce: 0.1453  loss_mask: 0.2356  loss_dice: 0.2087  loss_ce_0: 0.1036  loss_mask_0: 0.2312  loss_dice_0: 0.2089  loss_ce_1: 0.1443  loss_mask_1: 0.2378  loss_dice_1: 0.1984  loss_ce_2: 0.1442  loss_mask_2: 0.2434  loss_dice_2: 0.2083  loss_ce_3: 0.1443  loss_mask_3: 0.239  loss_dice_3: 0.2066  loss_ce_4: 0.1445  loss_mask_4: 0.2281  loss_dice_4: 0.2111  loss_ce_5: 0.1454  loss_mask_5: 0.2342  loss_dice_5: 0.2118  loss_ce_6: 0.1461  loss_mask_6: 0.2324  loss_dice_6: 0.2092  loss_ce_7: 0.1462  loss_mask_7: 0.2445  loss_dice_7: 0.2087  loss_ce_8: 0.1454  loss_mask_8: 0.2315  loss_dice_8: 0.206  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:35:57] d2.utils.events INFO:  eta: 1:41:23  iter: 1679  total_loss: 5.552  loss_ce: 0.1364  loss_mask: 0.2157  loss_dice: 0.1413  loss_ce_0: 0.09289  loss_mask_0: 0.2172  loss_dice_0: 0.1444  loss_ce_1: 0.1353  loss_mask_1: 0.2197  loss_dice_1: 0.1423  loss_ce_2: 0.1348  loss_mask_2: 0.2076  loss_dice_2: 0.1451  loss_ce_3: 0.1352  loss_mask_3: 0.2154  loss_dice_3: 0.1413  loss_ce_4: 0.1354  loss_mask_4: 0.2159  loss_dice_4: 0.1407  loss_ce_5: 0.1362  loss_mask_5: 0.2222  loss_dice_5: 0.1375  loss_ce_6: 0.1365  loss_mask_6: 0.2203  loss_dice_6: 0.1456  loss_ce_7: 0.1365  loss_mask_7: 0.2158  loss_dice_7: 0.1475  loss_ce_8: 0.1363  loss_mask_8: 0.2128  loss_dice_8: 0.1461  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:35:59] d2.utils.events INFO:  eta: 1:41:23  iter: 1699  total_loss: 5.755  loss_ce: 0.1269  loss_mask: 0.2248  loss_dice: 0.1638  loss_ce_0: 0.09724  loss_mask_0: 0.2319  loss_dice_0: 0.1687  loss_ce_1: 0.1277  loss_mask_1: 0.219  loss_dice_1: 0.1627  loss_ce_2: 0.1282  loss_mask_2: 0.2191  loss_dice_2: 0.1628  loss_ce_3: 0.1278  loss_mask_3: 0.2268  loss_dice_3: 0.1709  loss_ce_4: 0.1274  loss_mask_4: 0.2332  loss_dice_4: 0.1693  loss_ce_5: 0.1271  loss_mask_5: 0.2163  loss_dice_5: 0.171  loss_ce_6: 0.1271  loss_mask_6: 0.2268  loss_dice_6: 0.172  loss_ce_7: 0.1271  loss_mask_7: 0.2252  loss_dice_7: 0.1721  loss_ce_8: 0.1268  loss_mask_8: 0.2231  loss_dice_8: 0.1661  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:02] d2.utils.events INFO:  eta: 1:41:22  iter: 1719  total_loss: 5.722  loss_ce: 0.08331  loss_mask: 0.221  loss_dice: 0.1606  loss_ce_0: 0.09118  loss_mask_0: 0.2219  loss_dice_0: 0.168  loss_ce_1: 0.08329  loss_mask_1: 0.2321  loss_dice_1: 0.1689  loss_ce_2: 0.08301  loss_mask_2: 0.2206  loss_dice_2: 0.1661  loss_ce_3: 0.0832  loss_mask_3: 0.226  loss_dice_3: 0.1672  loss_ce_4: 0.08333  loss_mask_4: 0.2404  loss_dice_4: 0.1569  loss_ce_5: 0.0832  loss_mask_5: 0.2342  loss_dice_5: 0.1658  loss_ce_6: 0.08353  loss_mask_6: 0.2298  loss_dice_6: 0.1563  loss_ce_7: 0.08333  loss_mask_7: 0.2188  loss_dice_7: 0.1615  loss_ce_8: 0.08353  loss_mask_8: 0.2231  loss_dice_8: 0.1522  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:04] d2.utils.events INFO:  eta: 1:41:22  iter: 1739  total_loss: 5.338  loss_ce: 0.1774  loss_mask: 0.2566  loss_dice: 0.1416  loss_ce_0: 0.0935  loss_mask_0: 0.2619  loss_dice_0: 0.1456  loss_ce_1: 0.1761  loss_mask_1: 0.2524  loss_dice_1: 0.1423  loss_ce_2: 0.1767  loss_mask_2: 0.267  loss_dice_2: 0.1381  loss_ce_3: 0.1768  loss_mask_3: 0.2604  loss_dice_3: 0.1435  loss_ce_4: 0.1771  loss_mask_4: 0.2636  loss_dice_4: 0.1465  loss_ce_5: 0.1772  loss_mask_5: 0.2517  loss_dice_5: 0.141  loss_ce_6: 0.1776  loss_mask_6: 0.2736  loss_dice_6: 0.141  loss_ce_7: 0.1774  loss_mask_7: 0.2556  loss_dice_7: 0.1503  loss_ce_8: 0.1777  loss_mask_8: 0.2662  loss_dice_8: 0.144  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:07] d2.utils.events INFO:  eta: 1:41:21  iter: 1759  total_loss: 5.632  loss_ce: 0.1272  loss_mask: 0.1958  loss_dice: 0.1877  loss_ce_0: 0.09242  loss_mask_0: 0.1993  loss_dice_0: 0.1837  loss_ce_1: 0.1262  loss_mask_1: 0.1987  loss_dice_1: 0.1932  loss_ce_2: 0.1253  loss_mask_2: 0.2071  loss_dice_2: 0.2017  loss_ce_3: 0.1253  loss_mask_3: 0.2081  loss_dice_3: 0.1986  loss_ce_4: 0.1249  loss_mask_4: 0.2062  loss_dice_4: 0.2047  loss_ce_5: 0.1268  loss_mask_5: 0.2075  loss_dice_5: 0.2139  loss_ce_6: 0.1263  loss_mask_6: 0.1992  loss_dice_6: 0.1981  loss_ce_7: 0.1268  loss_mask_7: 0.2014  loss_dice_7: 0.2059  loss_ce_8: 0.1266  loss_mask_8: 0.1982  loss_dice_8: 0.194  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:10] d2.utils.events INFO:  eta: 1:41:19  iter: 1779  total_loss: 6.12  loss_ce: 0.1138  loss_mask: 0.2332  loss_dice: 0.1799  loss_ce_0: 0.08961  loss_mask_0: 0.2367  loss_dice_0: 0.1847  loss_ce_1: 0.1104  loss_mask_1: 0.2296  loss_dice_1: 0.1785  loss_ce_2: 0.1119  loss_mask_2: 0.229  loss_dice_2: 0.1789  loss_ce_3: 0.1146  loss_mask_3: 0.2428  loss_dice_3: 0.1839  loss_ce_4: 0.1131  loss_mask_4: 0.234  loss_dice_4: 0.1822  loss_ce_5: 0.114  loss_mask_5: 0.2331  loss_dice_5: 0.177  loss_ce_6: 0.1137  loss_mask_6: 0.2376  loss_dice_6: 0.1828  loss_ce_7: 0.1132  loss_mask_7: 0.2314  loss_dice_7: 0.1892  loss_ce_8: 0.1131  loss_mask_8: 0.2414  loss_dice_8: 0.1849  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:12] d2.utils.events INFO:  eta: 1:41:16  iter: 1799  total_loss: 5.606  loss_ce: 0.1102  loss_mask: 0.2306  loss_dice: 0.1519  loss_ce_0: 0.08751  loss_mask_0: 0.2328  loss_dice_0: 0.1545  loss_ce_1: 0.1206  loss_mask_1: 0.2238  loss_dice_1: 0.1521  loss_ce_2: 0.1209  loss_mask_2: 0.2183  loss_dice_2: 0.1473  loss_ce_3: 0.116  loss_mask_3: 0.2205  loss_dice_3: 0.1475  loss_ce_4: 0.1104  loss_mask_4: 0.2165  loss_dice_4: 0.1571  loss_ce_5: 0.1089  loss_mask_5: 0.2321  loss_dice_5: 0.1492  loss_ce_6: 0.1084  loss_mask_6: 0.2155  loss_dice_6: 0.1475  loss_ce_7: 0.1071  loss_mask_7: 0.2257  loss_dice_7: 0.1522  loss_ce_8: 0.1102  loss_mask_8: 0.2102  loss_dice_8: 0.143  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:15] d2.utils.events INFO:  eta: 1:41:11  iter: 1819  total_loss: 5.301  loss_ce: 0.1225  loss_mask: 0.2171  loss_dice: 0.1625  loss_ce_0: 0.08965  loss_mask_0: 0.2232  loss_dice_0: 0.169  loss_ce_1: 0.1257  loss_mask_1: 0.2255  loss_dice_1: 0.1678  loss_ce_2: 0.1243  loss_mask_2: 0.2158  loss_dice_2: 0.1686  loss_ce_3: 0.123  loss_mask_3: 0.2199  loss_dice_3: 0.1724  loss_ce_4: 0.1222  loss_mask_4: 0.2112  loss_dice_4: 0.1706  loss_ce_5: 0.1224  loss_mask_5: 0.2223  loss_dice_5: 0.1678  loss_ce_6: 0.1223  loss_mask_6: 0.2064  loss_dice_6: 0.1701  loss_ce_7: 0.1224  loss_mask_7: 0.2233  loss_dice_7: 0.1686  loss_ce_8: 0.1225  loss_mask_8: 0.2181  loss_dice_8: 0.1659  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:17] d2.utils.events INFO:  eta: 1:41:06  iter: 1839  total_loss: 5.64  loss_ce: 0.1623  loss_mask: 0.2454  loss_dice: 0.1596  loss_ce_0: 0.0871  loss_mask_0: 0.2485  loss_dice_0: 0.1548  loss_ce_1: 0.1611  loss_mask_1: 0.2497  loss_dice_1: 0.1638  loss_ce_2: 0.1651  loss_mask_2: 0.2519  loss_dice_2: 0.1635  loss_ce_3: 0.1644  loss_mask_3: 0.251  loss_dice_3: 0.1612  loss_ce_4: 0.1623  loss_mask_4: 0.2391  loss_dice_4: 0.1608  loss_ce_5: 0.1628  loss_mask_5: 0.242  loss_dice_5: 0.1646  loss_ce_6: 0.1629  loss_mask_6: 0.2527  loss_dice_6: 0.1564  loss_ce_7: 0.1612  loss_mask_7: 0.2308  loss_dice_7: 0.1606  loss_ce_8: 0.1626  loss_mask_8: 0.2398  loss_dice_8: 0.1552  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:20] d2.utils.events INFO:  eta: 1:41:01  iter: 1859  total_loss: 5.031  loss_ce: 0.1316  loss_mask: 0.248  loss_dice: 0.1302  loss_ce_0: 0.08676  loss_mask_0: 0.246  loss_dice_0: 0.1346  loss_ce_1: 0.1352  loss_mask_1: 0.2394  loss_dice_1: 0.1313  loss_ce_2: 0.1351  loss_mask_2: 0.2383  loss_dice_2: 0.1347  loss_ce_3: 0.135  loss_mask_3: 0.2397  loss_dice_3: 0.1334  loss_ce_4: 0.1351  loss_mask_4: 0.2492  loss_dice_4: 0.1319  loss_ce_5: 0.1335  loss_mask_5: 0.2394  loss_dice_5: 0.1396  loss_ce_6: 0.1326  loss_mask_6: 0.2424  loss_dice_6: 0.1327  loss_ce_7: 0.1318  loss_mask_7: 0.241  loss_dice_7: 0.1311  loss_ce_8: 0.1323  loss_mask_8: 0.237  loss_dice_8: 0.1325  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:22] d2.utils.events INFO:  eta: 1:41:02  iter: 1879  total_loss: 4.905  loss_ce: 0.1419  loss_mask: 0.2055  loss_dice: 0.1636  loss_ce_0: 0.08657  loss_mask_0: 0.2023  loss_dice_0: 0.1664  loss_ce_1: 0.1527  loss_mask_1: 0.2034  loss_dice_1: 0.1707  loss_ce_2: 0.1453  loss_mask_2: 0.1963  loss_dice_2: 0.1669  loss_ce_3: 0.1451  loss_mask_3: 0.205  loss_dice_3: 0.1707  loss_ce_4: 0.1501  loss_mask_4: 0.2074  loss_dice_4: 0.1654  loss_ce_5: 0.1436  loss_mask_5: 0.2069  loss_dice_5: 0.1646  loss_ce_6: 0.1424  loss_mask_6: 0.2019  loss_dice_6: 0.1591  loss_ce_7: 0.1458  loss_mask_7: 0.1991  loss_dice_7: 0.1657  loss_ce_8: 0.1424  loss_mask_8: 0.2038  loss_dice_8: 0.1617  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:25] d2.utils.events INFO:  eta: 1:40:59  iter: 1899  total_loss: 5.765  loss_ce: 0.1316  loss_mask: 0.2223  loss_dice: 0.2051  loss_ce_0: 0.08558  loss_mask_0: 0.2218  loss_dice_0: 0.1933  loss_ce_1: 0.1342  loss_mask_1: 0.2244  loss_dice_1: 0.2033  loss_ce_2: 0.1338  loss_mask_2: 0.2282  loss_dice_2: 0.2026  loss_ce_3: 0.1337  loss_mask_3: 0.2323  loss_dice_3: 0.2096  loss_ce_4: 0.1337  loss_mask_4: 0.2289  loss_dice_4: 0.2147  loss_ce_5: 0.1332  loss_mask_5: 0.2234  loss_dice_5: 0.2094  loss_ce_6: 0.1326  loss_mask_6: 0.2199  loss_dice_6: 0.2003  loss_ce_7: 0.1324  loss_mask_7: 0.2287  loss_dice_7: 0.2077  loss_ce_8: 0.1323  loss_mask_8: 0.2207  loss_dice_8: 0.1992  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:27] d2.utils.events INFO:  eta: 1:40:56  iter: 1919  total_loss: 5.24  loss_ce: 0.1285  loss_mask: 0.2597  loss_dice: 0.1386  loss_ce_0: 0.08512  loss_mask_0: 0.2428  loss_dice_0: 0.1339  loss_ce_1: 0.1262  loss_mask_1: 0.2481  loss_dice_1: 0.1397  loss_ce_2: 0.1291  loss_mask_2: 0.2572  loss_dice_2: 0.1366  loss_ce_3: 0.1285  loss_mask_3: 0.2465  loss_dice_3: 0.1402  loss_ce_4: 0.1278  loss_mask_4: 0.2515  loss_dice_4: 0.14  loss_ce_5: 0.1283  loss_mask_5: 0.2595  loss_dice_5: 0.1442  loss_ce_6: 0.1284  loss_mask_6: 0.2493  loss_dice_6: 0.1398  loss_ce_7: 0.127  loss_mask_7: 0.2561  loss_dice_7: 0.1402  loss_ce_8: 0.1289  loss_mask_8: 0.2465  loss_dice_8: 0.1403  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:30] d2.utils.events INFO:  eta: 1:40:50  iter: 1939  total_loss: 5.947  loss_ce: 0.1087  loss_mask: 0.2609  loss_dice: 0.1606  loss_ce_0: 0.08332  loss_mask_0: 0.2608  loss_dice_0: 0.1622  loss_ce_1: 0.1063  loss_mask_1: 0.2598  loss_dice_1: 0.1698  loss_ce_2: 0.1062  loss_mask_2: 0.2646  loss_dice_2: 0.1622  loss_ce_3: 0.1063  loss_mask_3: 0.2578  loss_dice_3: 0.166  loss_ce_4: 0.1066  loss_mask_4: 0.2546  loss_dice_4: 0.1637  loss_ce_5: 0.1076  loss_mask_5: 0.2633  loss_dice_5: 0.1704  loss_ce_6: 0.1074  loss_mask_6: 0.2565  loss_dice_6: 0.1645  loss_ce_7: 0.1079  loss_mask_7: 0.2524  loss_dice_7: 0.1612  loss_ce_8: 0.1079  loss_mask_8: 0.2675  loss_dice_8: 0.1581  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:32] d2.utils.events INFO:  eta: 1:40:42  iter: 1959  total_loss: 5.157  loss_ce: 0.07117  loss_mask: 0.2422  loss_dice: 0.1535  loss_ce_0: 0.07828  loss_mask_0: 0.2314  loss_dice_0: 0.1596  loss_ce_1: 0.0687  loss_mask_1: 0.2326  loss_dice_1: 0.1557  loss_ce_2: 0.06891  loss_mask_2: 0.2377  loss_dice_2: 0.1523  loss_ce_3: 0.06908  loss_mask_3: 0.2241  loss_dice_3: 0.1542  loss_ce_4: 0.06877  loss_mask_4: 0.2381  loss_dice_4: 0.1603  loss_ce_5: 0.07044  loss_mask_5: 0.2309  loss_dice_5: 0.1616  loss_ce_6: 0.07029  loss_mask_6: 0.2325  loss_dice_6: 0.1658  loss_ce_7: 0.06981  loss_mask_7: 0.2366  loss_dice_7: 0.1553  loss_ce_8: 0.07081  loss_mask_8: 0.2323  loss_dice_8: 0.1596  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:35] d2.utils.events INFO:  eta: 1:40:39  iter: 1979  total_loss: 6.353  loss_ce: 0.04357  loss_mask: 0.2281  loss_dice: 0.1785  loss_ce_0: 0.07224  loss_mask_0: 0.2301  loss_dice_0: 0.1673  loss_ce_1: 0.04208  loss_mask_1: 0.2344  loss_dice_1: 0.1743  loss_ce_2: 0.04313  loss_mask_2: 0.2394  loss_dice_2: 0.1796  loss_ce_3: 0.04297  loss_mask_3: 0.2302  loss_dice_3: 0.1749  loss_ce_4: 0.0437  loss_mask_4: 0.2292  loss_dice_4: 0.1677  loss_ce_5: 0.04314  loss_mask_5: 0.2375  loss_dice_5: 0.18  loss_ce_6: 0.04256  loss_mask_6: 0.2485  loss_dice_6: 0.177  loss_ce_7: 0.04274  loss_mask_7: 0.2436  loss_dice_7: 0.1781  loss_ce_8: 0.04347  loss_mask_8: 0.2343  loss_dice_8: 0.1736  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:37] d2.utils.events INFO:  eta: 1:40:36  iter: 1999  total_loss: 5.806  loss_ce: 0.08451  loss_mask: 0.2827  loss_dice: 0.1947  loss_ce_0: 0.06807  loss_mask_0: 0.2911  loss_dice_0: 0.1909  loss_ce_1: 0.0866  loss_mask_1: 0.2932  loss_dice_1: 0.2038  loss_ce_2: 0.08639  loss_mask_2: 0.2832  loss_dice_2: 0.1956  loss_ce_3: 0.08598  loss_mask_3: 0.2886  loss_dice_3: 0.1921  loss_ce_4: 0.09008  loss_mask_4: 0.2931  loss_dice_4: 0.2032  loss_ce_5: 0.08475  loss_mask_5: 0.2917  loss_dice_5: 0.1992  loss_ce_6: 0.08268  loss_mask_6: 0.2905  loss_dice_6: 0.1952  loss_ce_7: 0.08518  loss_mask_7: 0.2946  loss_dice_7: 0.2003  loss_ce_8: 0.08559  loss_mask_8: 0.2896  loss_dice_8: 0.1942  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:40] d2.utils.events INFO:  eta: 1:40:33  iter: 2019  total_loss: 6.277  loss_ce: 0.0751  loss_mask: 0.1533  loss_dice: 0.22  loss_ce_0: 0.06647  loss_mask_0: 0.1441  loss_dice_0: 0.2244  loss_ce_1: 0.07637  loss_mask_1: 0.1571  loss_dice_1: 0.243  loss_ce_2: 0.07608  loss_mask_2: 0.1558  loss_dice_2: 0.2258  loss_ce_3: 0.07577  loss_mask_3: 0.1579  loss_dice_3: 0.2351  loss_ce_4: 0.07605  loss_mask_4: 0.157  loss_dice_4: 0.2324  loss_ce_5: 0.07469  loss_mask_5: 0.1468  loss_dice_5: 0.2274  loss_ce_6: 0.07415  loss_mask_6: 0.1479  loss_dice_6: 0.2202  loss_ce_7: 0.07596  loss_mask_7: 0.1511  loss_dice_7: 0.2292  loss_ce_8: 0.07458  loss_mask_8: 0.1513  loss_dice_8: 0.2269  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:42] d2.utils.events INFO:  eta: 1:40:31  iter: 2039  total_loss: 5.991  loss_ce: 0.06499  loss_mask: 0.2486  loss_dice: 0.1675  loss_ce_0: 0.06386  loss_mask_0: 0.2522  loss_dice_0: 0.1553  loss_ce_1: 0.06459  loss_mask_1: 0.2551  loss_dice_1: 0.1636  loss_ce_2: 0.06463  loss_mask_2: 0.2479  loss_dice_2: 0.1613  loss_ce_3: 0.06495  loss_mask_3: 0.2579  loss_dice_3: 0.1625  loss_ce_4: 0.06476  loss_mask_4: 0.2539  loss_dice_4: 0.1615  loss_ce_5: 0.0646  loss_mask_5: 0.2373  loss_dice_5: 0.1548  loss_ce_6: 0.06463  loss_mask_6: 0.2557  loss_dice_6: 0.1604  loss_ce_7: 0.06506  loss_mask_7: 0.2445  loss_dice_7: 0.1589  loss_ce_8: 0.06466  loss_mask_8: 0.2552  loss_dice_8: 0.1622  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:45] d2.utils.events INFO:  eta: 1:40:27  iter: 2059  total_loss: 6.784  loss_ce: 0.07652  loss_mask: 0.2475  loss_dice: 0.2344  loss_ce_0: 0.06264  loss_mask_0: 0.2417  loss_dice_0: 0.225  loss_ce_1: 0.07691  loss_mask_1: 0.2677  loss_dice_1: 0.2414  loss_ce_2: 0.07693  loss_mask_2: 0.2442  loss_dice_2: 0.2468  loss_ce_3: 0.07693  loss_mask_3: 0.238  loss_dice_3: 0.2441  loss_ce_4: 0.07761  loss_mask_4: 0.2405  loss_dice_4: 0.2386  loss_ce_5: 0.07635  loss_mask_5: 0.2459  loss_dice_5: 0.2426  loss_ce_6: 0.07614  loss_mask_6: 0.2376  loss_dice_6: 0.2343  loss_ce_7: 0.07638  loss_mask_7: 0.2433  loss_dice_7: 0.23  loss_ce_8: 0.07693  loss_mask_8: 0.2481  loss_dice_8: 0.2343  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:47] d2.utils.events INFO:  eta: 1:40:23  iter: 2079  total_loss: 5.923  loss_ce: 0.1349  loss_mask: 0.2169  loss_dice: 0.1469  loss_ce_0: 0.103  loss_mask_0: 0.2024  loss_dice_0: 0.1412  loss_ce_1: 0.1335  loss_mask_1: 0.217  loss_dice_1: 0.1474  loss_ce_2: 0.1325  loss_mask_2: 0.218  loss_dice_2: 0.1447  loss_ce_3: 0.1336  loss_mask_3: 0.2142  loss_dice_3: 0.146  loss_ce_4: 0.1298  loss_mask_4: 0.2098  loss_dice_4: 0.138  loss_ce_5: 0.1348  loss_mask_5: 0.2182  loss_dice_5: 0.1449  loss_ce_6: 0.1352  loss_mask_6: 0.2176  loss_dice_6: 0.1435  loss_ce_7: 0.1362  loss_mask_7: 0.2118  loss_dice_7: 0.1416  loss_ce_8: 0.1316  loss_mask_8: 0.2202  loss_dice_8: 0.1437  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:50] d2.utils.events INFO:  eta: 1:40:22  iter: 2099  total_loss: 5.918  loss_ce: 0.1641  loss_mask: 0.1936  loss_dice: 0.1769  loss_ce_0: 0.07064  loss_mask_0: 0.1895  loss_dice_0: 0.1859  loss_ce_1: 0.1662  loss_mask_1: 0.2038  loss_dice_1: 0.1879  loss_ce_2: 0.1665  loss_mask_2: 0.1951  loss_dice_2: 0.1815  loss_ce_3: 0.1654  loss_mask_3: 0.1917  loss_dice_3: 0.1848  loss_ce_4: 0.1696  loss_mask_4: 0.1907  loss_dice_4: 0.1905  loss_ce_5: 0.1646  loss_mask_5: 0.2008  loss_dice_5: 0.1846  loss_ce_6: 0.1645  loss_mask_6: 0.2029  loss_dice_6: 0.1793  loss_ce_7: 0.163  loss_mask_7: 0.1989  loss_dice_7: 0.1892  loss_ce_8: 0.1684  loss_mask_8: 0.1948  loss_dice_8: 0.1866  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:52] d2.utils.events INFO:  eta: 1:40:18  iter: 2119  total_loss: 5.095  loss_ce: 0.109  loss_mask: 0.2312  loss_dice: 0.1459  loss_ce_0: 0.07015  loss_mask_0: 0.2305  loss_dice_0: 0.1535  loss_ce_1: 0.1157  loss_mask_1: 0.2302  loss_dice_1: 0.1487  loss_ce_2: 0.1121  loss_mask_2: 0.2275  loss_dice_2: 0.1452  loss_ce_3: 0.1129  loss_mask_3: 0.2309  loss_dice_3: 0.1563  loss_ce_4: 0.1029  loss_mask_4: 0.2342  loss_dice_4: 0.1516  loss_ce_5: 0.1103  loss_mask_5: 0.2241  loss_dice_5: 0.1494  loss_ce_6: 0.1124  loss_mask_6: 0.2259  loss_dice_6: 0.1551  loss_ce_7: 0.1135  loss_mask_7: 0.2237  loss_dice_7: 0.156  loss_ce_8: 0.1055  loss_mask_8: 0.2281  loss_dice_8: 0.1523  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:36:55] d2.utils.events INFO:  eta: 1:40:19  iter: 2139  total_loss: 5.506  loss_ce: 0.06008  loss_mask: 0.2103  loss_dice: 0.1932  loss_ce_0: 0.06774  loss_mask_0: 0.2116  loss_dice_0: 0.1903  loss_ce_1: 0.0626  loss_mask_1: 0.2169  loss_dice_1: 0.1998  loss_ce_2: 0.06055  loss_mask_2: 0.2099  loss_dice_2: 0.191  loss_ce_3: 0.06174  loss_mask_3: 0.2078  loss_dice_3: 0.1913  loss_ce_4: 0.05584  loss_mask_4: 0.2083  loss_dice_4: 0.2022  loss_ce_5: 0.06  loss_mask_5: 0.2105  loss_dice_5: 0.2017  loss_ce_6: 0.06151  loss_mask_6: 0.2096  loss_dice_6: 0.2012  loss_ce_7: 0.06108  loss_mask_7: 0.2119  loss_dice_7: 0.1933  loss_ce_8: 0.05847  loss_mask_8: 0.2167  loss_dice_8: 0.2028  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:36:57] d2.utils.events INFO:  eta: 1:40:18  iter: 2159  total_loss: 5.521  loss_ce: 0.08015  loss_mask: 0.205  loss_dice: 0.1569  loss_ce_0: 0.06375  loss_mask_0: 0.2115  loss_dice_0: 0.157  loss_ce_1: 0.07529  loss_mask_1: 0.2095  loss_dice_1: 0.1603  loss_ce_2: 0.0772  loss_mask_2: 0.2077  loss_dice_2: 0.1542  loss_ce_3: 0.0767  loss_mask_3: 0.2161  loss_dice_3: 0.1591  loss_ce_4: 0.08453  loss_mask_4: 0.2142  loss_dice_4: 0.1497  loss_ce_5: 0.07896  loss_mask_5: 0.2019  loss_dice_5: 0.1587  loss_ce_6: 0.07674  loss_mask_6: 0.2131  loss_dice_6: 0.161  loss_ce_7: 0.07969  loss_mask_7: 0.2154  loss_dice_7: 0.1506  loss_ce_8: 0.07922  loss_mask_8: 0.2129  loss_dice_8: 0.1518  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:00] d2.utils.events INFO:  eta: 1:40:18  iter: 2179  total_loss: 5.155  loss_ce: 0.1287  loss_mask: 0.2089  loss_dice: 0.1298  loss_ce_0: 0.08225  loss_mask_0: 0.2277  loss_dice_0: 0.1287  loss_ce_1: 0.1284  loss_mask_1: 0.2206  loss_dice_1: 0.1372  loss_ce_2: 0.1284  loss_mask_2: 0.2184  loss_dice_2: 0.1315  loss_ce_3: 0.1284  loss_mask_3: 0.2244  loss_dice_3: 0.1349  loss_ce_4: 0.1315  loss_mask_4: 0.2172  loss_dice_4: 0.1298  loss_ce_5: 0.1282  loss_mask_5: 0.2261  loss_dice_5: 0.1286  loss_ce_6: 0.1283  loss_mask_6: 0.2245  loss_dice_6: 0.1337  loss_ce_7: 0.128  loss_mask_7: 0.2182  loss_dice_7: 0.1345  loss_ce_8: 0.1284  loss_mask_8: 0.2204  loss_dice_8: 0.1318  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:02] d2.utils.events INFO:  eta: 1:40:16  iter: 2199  total_loss: 6.813  loss_ce: 0.1164  loss_mask: 0.2872  loss_dice: 0.1933  loss_ce_0: 0.06635  loss_mask_0: 0.2708  loss_dice_0: 0.1849  loss_ce_1: 0.1181  loss_mask_1: 0.2754  loss_dice_1: 0.1909  loss_ce_2: 0.1184  loss_mask_2: 0.2819  loss_dice_2: 0.1916  loss_ce_3: 0.1174  loss_mask_3: 0.2753  loss_dice_3: 0.1969  loss_ce_4: 0.1173  loss_mask_4: 0.2772  loss_dice_4: 0.1961  loss_ce_5: 0.1172  loss_mask_5: 0.2607  loss_dice_5: 0.1934  loss_ce_6: 0.1172  loss_mask_6: 0.2832  loss_dice_6: 0.1872  loss_ce_7: 0.1173  loss_mask_7: 0.2794  loss_dice_7: 0.1958  loss_ce_8: 0.117  loss_mask_8: 0.2811  loss_dice_8: 0.1923  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:05] d2.utils.events INFO:  eta: 1:40:16  iter: 2219  total_loss: 5.973  loss_ce: 0.102  loss_mask: 0.2341  loss_dice: 0.1684  loss_ce_0: 0.06605  loss_mask_0: 0.2449  loss_dice_0: 0.1687  loss_ce_1: 0.1012  loss_mask_1: 0.2507  loss_dice_1: 0.1749  loss_ce_2: 0.104  loss_mask_2: 0.2394  loss_dice_2: 0.1734  loss_ce_3: 0.1037  loss_mask_3: 0.2411  loss_dice_3: 0.1719  loss_ce_4: 0.1004  loss_mask_4: 0.2456  loss_dice_4: 0.1726  loss_ce_5: 0.1032  loss_mask_5: 0.2388  loss_dice_5: 0.1712  loss_ce_6: 0.1035  loss_mask_6: 0.2528  loss_dice_6: 0.1746  loss_ce_7: 0.1013  loss_mask_7: 0.2468  loss_dice_7: 0.179  loss_ce_8: 0.1021  loss_mask_8: 0.241  loss_dice_8: 0.1738  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:08] d2.utils.events INFO:  eta: 1:40:17  iter: 2239  total_loss: 5.627  loss_ce: 0.07012  loss_mask: 0.1848  loss_dice: 0.1734  loss_ce_0: 0.06366  loss_mask_0: 0.1815  loss_dice_0: 0.1792  loss_ce_1: 0.07103  loss_mask_1: 0.1911  loss_dice_1: 0.1928  loss_ce_2: 0.07025  loss_mask_2: 0.1812  loss_dice_2: 0.1868  loss_ce_3: 0.07041  loss_mask_3: 0.191  loss_dice_3: 0.1938  loss_ce_4: 0.06863  loss_mask_4: 0.1961  loss_dice_4: 0.1886  loss_ce_5: 0.07039  loss_mask_5: 0.1921  loss_dice_5: 0.1775  loss_ce_6: 0.0711  loss_mask_6: 0.1841  loss_dice_6: 0.1904  loss_ce_7: 0.07042  loss_mask_7: 0.188  loss_dice_7: 0.1862  loss_ce_8: 0.0694  loss_mask_8: 0.189  loss_dice_8: 0.185  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:10] d2.utils.events INFO:  eta: 1:40:13  iter: 2259  total_loss: 5.503  loss_ce: 0.1065  loss_mask: 0.2343  loss_dice: 0.1355  loss_ce_0: 0.06386  loss_mask_0: 0.2378  loss_dice_0: 0.1374  loss_ce_1: 0.1066  loss_mask_1: 0.2374  loss_dice_1: 0.135  loss_ce_2: 0.1074  loss_mask_2: 0.2317  loss_dice_2: 0.1344  loss_ce_3: 0.1073  loss_mask_3: 0.2339  loss_dice_3: 0.139  loss_ce_4: 0.1089  loss_mask_4: 0.2256  loss_dice_4: 0.1417  loss_ce_5: 0.1065  loss_mask_5: 0.2309  loss_dice_5: 0.14  loss_ce_6: 0.1056  loss_mask_6: 0.2247  loss_dice_6: 0.1442  loss_ce_7: 0.1066  loss_mask_7: 0.2327  loss_dice_7: 0.1375  loss_ce_8: 0.1079  loss_mask_8: 0.2293  loss_dice_8: 0.1362  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:13] d2.utils.events INFO:  eta: 1:40:06  iter: 2279  total_loss: 5.878  loss_ce: 0.1291  loss_mask: 0.2144  loss_dice: 0.1519  loss_ce_0: 0.08006  loss_mask_0: 0.2151  loss_dice_0: 0.1521  loss_ce_1: 0.1304  loss_mask_1: 0.2188  loss_dice_1: 0.1548  loss_ce_2: 0.1314  loss_mask_2: 0.2179  loss_dice_2: 0.1504  loss_ce_3: 0.1313  loss_mask_3: 0.2167  loss_dice_3: 0.1492  loss_ce_4: 0.1323  loss_mask_4: 0.2196  loss_dice_4: 0.149  loss_ce_5: 0.1307  loss_mask_5: 0.2187  loss_dice_5: 0.164  loss_ce_6: 0.1302  loss_mask_6: 0.2112  loss_dice_6: 0.1553  loss_ce_7: 0.1306  loss_mask_7: 0.2189  loss_dice_7: 0.1544  loss_ce_8: 0.1311  loss_mask_8: 0.2124  loss_dice_8: 0.1521  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:37:15] d2.utils.events INFO:  eta: 1:40:04  iter: 2299  total_loss: 5.465  loss_ce: 0.1269  loss_mask: 0.2224  loss_dice: 0.183  loss_ce_0: 0.0667  loss_mask_0: 0.2333  loss_dice_0: 0.1902  loss_ce_1: 0.1267  loss_mask_1: 0.2269  loss_dice_1: 0.1946  loss_ce_2: 0.1266  loss_mask_2: 0.2224  loss_dice_2: 0.1888  loss_ce_3: 0.1266  loss_mask_3: 0.2329  loss_dice_3: 0.1788  loss_ce_4: 0.1267  loss_mask_4: 0.2359  loss_dice_4: 0.1902  loss_ce_5: 0.1266  loss_mask_5: 0.2291  loss_dice_5: 0.1854  loss_ce_6: 0.1267  loss_mask_6: 0.2167  loss_dice_6: 0.1876  loss_ce_7: 0.1266  loss_mask_7: 0.2298  loss_dice_7: 0.1866  loss_ce_8: 0.1266  loss_mask_8: 0.2313  loss_dice_8: 0.1917  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:37:17] d2.utils.events INFO:  eta: 1:39:59  iter: 2319  total_loss: 5.754  loss_ce: 0.1297  loss_mask: 0.227  loss_dice: 0.1716  loss_ce_0: 0.0792  loss_mask_0: 0.2235  loss_dice_0: 0.168  loss_ce_1: 0.1285  loss_mask_1: 0.2285  loss_dice_1: 0.1695  loss_ce_2: 0.1298  loss_mask_2: 0.2306  loss_dice_2: 0.1694  loss_ce_3: 0.1297  loss_mask_3: 0.2381  loss_dice_3: 0.1717  loss_ce_4: 0.1285  loss_mask_4: 0.2355  loss_dice_4: 0.1723  loss_ce_5: 0.1294  loss_mask_5: 0.221  loss_dice_5: 0.1705  loss_ce_6: 0.1296  loss_mask_6: 0.235  loss_dice_6: 0.1719  loss_ce_7: 0.1285  loss_mask_7: 0.2333  loss_dice_7: 0.1704  loss_ce_8: 0.1298  loss_mask_8: 0.2228  loss_dice_8: 0.1722  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:37:20] d2.utils.events INFO:  eta: 1:39:58  iter: 2339  total_loss: 6.15  loss_ce: 0.1197  loss_mask: 0.2156  loss_dice: 0.2366  loss_ce_0: 0.06793  loss_mask_0: 0.2267  loss_dice_0: 0.2438  loss_ce_1: 0.1199  loss_mask_1: 0.2313  loss_dice_1: 0.262  loss_ce_2: 0.1207  loss_mask_2: 0.2303  loss_dice_2: 0.2392  loss_ce_3: 0.1202  loss_mask_3: 0.2295  loss_dice_3: 0.2455  loss_ce_4: 0.1205  loss_mask_4: 0.2261  loss_dice_4: 0.2431  loss_ce_5: 0.12  loss_mask_5: 0.2186  loss_dice_5: 0.2457  loss_ce_6: 0.1197  loss_mask_6: 0.2283  loss_dice_6: 0.2321  loss_ce_7: 0.1198  loss_mask_7: 0.2288  loss_dice_7: 0.2488  loss_ce_8: 0.1203  loss_mask_8: 0.2203  loss_dice_8: 0.2304  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:23] d2.utils.events INFO:  eta: 1:39:58  iter: 2359  total_loss: 5.722  loss_ce: 0.1305  loss_mask: 0.2392  loss_dice: 0.1859  loss_ce_0: 0.08643  loss_mask_0: 0.2341  loss_dice_0: 0.1828  loss_ce_1: 0.1306  loss_mask_1: 0.2361  loss_dice_1: 0.1802  loss_ce_2: 0.1306  loss_mask_2: 0.2315  loss_dice_2: 0.1808  loss_ce_3: 0.1306  loss_mask_3: 0.2425  loss_dice_3: 0.1894  loss_ce_4: 0.1307  loss_mask_4: 0.2353  loss_dice_4: 0.1788  loss_ce_5: 0.1305  loss_mask_5: 0.2272  loss_dice_5: 0.1861  loss_ce_6: 0.1305  loss_mask_6: 0.246  loss_dice_6: 0.1807  loss_ce_7: 0.1305  loss_mask_7: 0.2369  loss_dice_7: 0.18  loss_ce_8: 0.1306  loss_mask_8: 0.2348  loss_dice_8: 0.1801  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:25] d2.utils.events INFO:  eta: 1:39:53  iter: 2379  total_loss: 6.249  loss_ce: 0.1294  loss_mask: 0.1779  loss_dice: 0.2458  loss_ce_0: 0.0777  loss_mask_0: 0.1704  loss_dice_0: 0.2441  loss_ce_1: 0.1297  loss_mask_1: 0.1674  loss_dice_1: 0.2609  loss_ce_2: 0.1288  loss_mask_2: 0.1758  loss_dice_2: 0.2561  loss_ce_3: 0.1296  loss_mask_3: 0.1789  loss_dice_3: 0.259  loss_ce_4: 0.1296  loss_mask_4: 0.1672  loss_dice_4: 0.2438  loss_ce_5: 0.1296  loss_mask_5: 0.1762  loss_dice_5: 0.2441  loss_ce_6: 0.1295  loss_mask_6: 0.1735  loss_dice_6: 0.2549  loss_ce_7: 0.1295  loss_mask_7: 0.1811  loss_dice_7: 0.2528  loss_ce_8: 0.1295  loss_mask_8: 0.1751  loss_dice_8: 0.2447  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:28] d2.utils.events INFO:  eta: 1:39:48  iter: 2399  total_loss: 6.049  loss_ce: 0.1141  loss_mask: 0.1709  loss_dice: 0.2145  loss_ce_0: 0.0701  loss_mask_0: 0.1796  loss_dice_0: 0.2121  loss_ce_1: 0.1133  loss_mask_1: 0.1764  loss_dice_1: 0.2296  loss_ce_2: 0.1132  loss_mask_2: 0.1841  loss_dice_2: 0.2276  loss_ce_3: 0.1133  loss_mask_3: 0.1758  loss_dice_3: 0.2231  loss_ce_4: 0.1134  loss_mask_4: 0.1804  loss_dice_4: 0.2271  loss_ce_5: 0.1135  loss_mask_5: 0.1837  loss_dice_5: 0.2197  loss_ce_6: 0.1135  loss_mask_6: 0.1759  loss_dice_6: 0.2145  loss_ce_7: 0.1137  loss_mask_7: 0.1742  loss_dice_7: 0.2315  loss_ce_8: 0.1138  loss_mask_8: 0.1798  loss_dice_8: 0.2233  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:30] d2.utils.events INFO:  eta: 1:39:45  iter: 2419  total_loss: 6.537  loss_ce: 0.1406  loss_mask: 0.2041  loss_dice: 0.2061  loss_ce_0: 0.07788  loss_mask_0: 0.1977  loss_dice_0: 0.1994  loss_ce_1: 0.1412  loss_mask_1: 0.2041  loss_dice_1: 0.2139  loss_ce_2: 0.1416  loss_mask_2: 0.2112  loss_dice_2: 0.2076  loss_ce_3: 0.1412  loss_mask_3: 0.2094  loss_dice_3: 0.2126  loss_ce_4: 0.1413  loss_mask_4: 0.2088  loss_dice_4: 0.2067  loss_ce_5: 0.1406  loss_mask_5: 0.2101  loss_dice_5: 0.2101  loss_ce_6: 0.1406  loss_mask_6: 0.2  loss_dice_6: 0.2106  loss_ce_7: 0.1408  loss_mask_7: 0.2102  loss_dice_7: 0.1984  loss_ce_8: 0.1408  loss_mask_8: 0.2081  loss_dice_8: 0.2076  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:33] d2.utils.events INFO:  eta: 1:39:43  iter: 2439  total_loss: 5.303  loss_ce: 0.09945  loss_mask: 0.2524  loss_dice: 0.1513  loss_ce_0: 0.06629  loss_mask_0: 0.2561  loss_dice_0: 0.1452  loss_ce_1: 0.1005  loss_mask_1: 0.2555  loss_dice_1: 0.1445  loss_ce_2: 0.1008  loss_mask_2: 0.2645  loss_dice_2: 0.1456  loss_ce_3: 0.1005  loss_mask_3: 0.2509  loss_dice_3: 0.1454  loss_ce_4: 0.1003  loss_mask_4: 0.2517  loss_dice_4: 0.1421  loss_ce_5: 0.09973  loss_mask_5: 0.2557  loss_dice_5: 0.1478  loss_ce_6: 0.09932  loss_mask_6: 0.2542  loss_dice_6: 0.1482  loss_ce_7: 0.09962  loss_mask_7: 0.2647  loss_dice_7: 0.1413  loss_ce_8: 0.09985  loss_mask_8: 0.2512  loss_dice_8: 0.1436  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:35] d2.utils.events INFO:  eta: 1:39:38  iter: 2459  total_loss: 5.277  loss_ce: 0.1281  loss_mask: 0.233  loss_dice: 0.1505  loss_ce_0: 0.07711  loss_mask_0: 0.2399  loss_dice_0: 0.1512  loss_ce_1: 0.128  loss_mask_1: 0.2443  loss_dice_1: 0.1449  loss_ce_2: 0.1267  loss_mask_2: 0.2444  loss_dice_2: 0.1444  loss_ce_3: 0.1279  loss_mask_3: 0.2382  loss_dice_3: 0.1439  loss_ce_4: 0.1278  loss_mask_4: 0.2337  loss_dice_4: 0.1429  loss_ce_5: 0.1275  loss_mask_5: 0.2448  loss_dice_5: 0.1467  loss_ce_6: 0.1279  loss_mask_6: 0.246  loss_dice_6: 0.1504  loss_ce_7: 0.128  loss_mask_7: 0.2429  loss_dice_7: 0.1484  loss_ce_8: 0.128  loss_mask_8: 0.2404  loss_dice_8: 0.1394  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:37:38] d2.utils.events INFO:  eta: 1:39:34  iter: 2479  total_loss: 5.844  loss_ce: 0.1169  loss_mask: 0.2246  loss_dice: 0.1595  loss_ce_0: 0.06738  loss_mask_0: 0.2311  loss_dice_0: 0.1555  loss_ce_1: 0.1171  loss_mask_1: 0.2214  loss_dice_1: 0.1646  loss_ce_2: 0.1175  loss_mask_2: 0.2283  loss_dice_2: 0.1607  loss_ce_3: 0.1173  loss_mask_3: 0.2298  loss_dice_3: 0.1629  loss_ce_4: 0.1178  loss_mask_4: 0.2293  loss_dice_4: 0.1653  loss_ce_5: 0.1169  loss_mask_5: 0.228  loss_dice_5: 0.1677  loss_ce_6: 0.117  loss_mask_6: 0.2355  loss_dice_6: 0.1632  loss_ce_7: 0.117  loss_mask_7: 0.2318  loss_dice_7: 0.1654  loss_ce_8: 0.1172  loss_mask_8: 0.231  loss_dice_8: 0.1592  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:37:40] d2.utils.events INFO:  eta: 1:39:27  iter: 2499  total_loss: 5.672  loss_ce: 0.1528  loss_mask: 0.2497  loss_dice: 0.158  loss_ce_0: 0.08637  loss_mask_0: 0.2562  loss_dice_0: 0.1555  loss_ce_1: 0.1542  loss_mask_1: 0.2554  loss_dice_1: 0.1601  loss_ce_2: 0.154  loss_mask_2: 0.2528  loss_dice_2: 0.1543  loss_ce_3: 0.154  loss_mask_3: 0.2601  loss_dice_3: 0.152  loss_ce_4: 0.1536  loss_mask_4: 0.2485  loss_dice_4: 0.1596  loss_ce_5: 0.1532  loss_mask_5: 0.2571  loss_dice_5: 0.1617  loss_ce_6: 0.1534  loss_mask_6: 0.2607  loss_dice_6: 0.1568  loss_ce_7: 0.1533  loss_mask_7: 0.2646  loss_dice_7: 0.1594  loss_ce_8: 0.1533  loss_mask_8: 0.2532  loss_dice_8: 0.157  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:37:43] d2.utils.events INFO:  eta: 1:39:22  iter: 2519  total_loss: 5.002  loss_ce: 0.1107  loss_mask: 0.1962  loss_dice: 0.1601  loss_ce_0: 0.06697  loss_mask_0: 0.2061  loss_dice_0: 0.1661  loss_ce_1: 0.1104  loss_mask_1: 0.2022  loss_dice_1: 0.1624  loss_ce_2: 0.111  loss_mask_2: 0.2049  loss_dice_2: 0.1648  loss_ce_3: 0.1112  loss_mask_3: 0.1973  loss_dice_3: 0.1673  loss_ce_4: 0.111  loss_mask_4: 0.1922  loss_dice_4: 0.1693  loss_ce_5: 0.1109  loss_mask_5: 0.2056  loss_dice_5: 0.1616  loss_ce_6: 0.1106  loss_mask_6: 0.2039  loss_dice_6: 0.1609  loss_ce_7: 0.1105  loss_mask_7: 0.203  loss_dice_7: 0.173  loss_ce_8: 0.1106  loss_mask_8: 0.1933  loss_dice_8: 0.1616  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:45] d2.utils.events INFO:  eta: 1:39:19  iter: 2539  total_loss: 5.408  loss_ce: 0.1245  loss_mask: 0.222  loss_dice: 0.1772  loss_ce_0: 0.08136  loss_mask_0: 0.2203  loss_dice_0: 0.1771  loss_ce_1: 0.1253  loss_mask_1: 0.2301  loss_dice_1: 0.182  loss_ce_2: 0.1253  loss_mask_2: 0.2351  loss_dice_2: 0.1742  loss_ce_3: 0.1254  loss_mask_3: 0.2259  loss_dice_3: 0.1833  loss_ce_4: 0.1253  loss_mask_4: 0.2297  loss_dice_4: 0.1838  loss_ce_5: 0.1255  loss_mask_5: 0.2356  loss_dice_5: 0.1846  loss_ce_6: 0.1253  loss_mask_6: 0.2278  loss_dice_6: 0.1724  loss_ce_7: 0.1256  loss_mask_7: 0.242  loss_dice_7: 0.1872  loss_ce_8: 0.1255  loss_mask_8: 0.231  loss_dice_8: 0.1803  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:48] d2.utils.events INFO:  eta: 1:39:17  iter: 2559  total_loss: 5.52  loss_ce: 0.1326  loss_mask: 0.2153  loss_dice: 0.1654  loss_ce_0: 0.07543  loss_mask_0: 0.222  loss_dice_0: 0.1621  loss_ce_1: 0.1332  loss_mask_1: 0.2211  loss_dice_1: 0.1742  loss_ce_2: 0.1334  loss_mask_2: 0.2225  loss_dice_2: 0.1731  loss_ce_3: 0.1335  loss_mask_3: 0.2126  loss_dice_3: 0.1713  loss_ce_4: 0.1334  loss_mask_4: 0.2179  loss_dice_4: 0.1744  loss_ce_5: 0.1331  loss_mask_5: 0.2181  loss_dice_5: 0.1833  loss_ce_6: 0.1331  loss_mask_6: 0.2213  loss_dice_6: 0.1772  loss_ce_7: 0.1329  loss_mask_7: 0.2161  loss_dice_7: 0.1796  loss_ce_8: 0.1332  loss_mask_8: 0.2141  loss_dice_8: 0.1676  time: 0.1256  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:37:50] d2.utils.events INFO:  eta: 1:39:15  iter: 2579  total_loss: 5.65  loss_ce: 0.1609  loss_mask: 0.238  loss_dice: 0.1712  loss_ce_0: 0.07551  loss_mask_0: 0.2453  loss_dice_0: 0.1774  loss_ce_1: 0.1608  loss_mask_1: 0.241  loss_dice_1: 0.1771  loss_ce_2: 0.1599  loss_mask_2: 0.2429  loss_dice_2: 0.1828  loss_ce_3: 0.1596  loss_mask_3: 0.2393  loss_dice_3: 0.181  loss_ce_4: 0.16  loss_mask_4: 0.2378  loss_dice_4: 0.1853  loss_ce_5: 0.1604  loss_mask_5: 0.2377  loss_dice_5: 0.174  loss_ce_6: 0.1609  loss_mask_6: 0.2435  loss_dice_6: 0.1747  loss_ce_7: 0.161  loss_mask_7: 0.2402  loss_dice_7: 0.1767  loss_ce_8: 0.1608  loss_mask_8: 0.2506  loss_dice_8: 0.1835  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:53] d2.utils.events INFO:  eta: 1:39:12  iter: 2599  total_loss: 5.298  loss_ce: 0.1384  loss_mask: 0.2171  loss_dice: 0.133  loss_ce_0: 0.07572  loss_mask_0: 0.2223  loss_dice_0: 0.1284  loss_ce_1: 0.1404  loss_mask_1: 0.2176  loss_dice_1: 0.1378  loss_ce_2: 0.1416  loss_mask_2: 0.2129  loss_dice_2: 0.1345  loss_ce_3: 0.1422  loss_mask_3: 0.2156  loss_dice_3: 0.1383  loss_ce_4: 0.143  loss_mask_4: 0.2338  loss_dice_4: 0.1383  loss_ce_5: 0.1402  loss_mask_5: 0.218  loss_dice_5: 0.1352  loss_ce_6: 0.1405  loss_mask_6: 0.2141  loss_dice_6: 0.1384  loss_ce_7: 0.1395  loss_mask_7: 0.2228  loss_dice_7: 0.1296  loss_ce_8: 0.1406  loss_mask_8: 0.2178  loss_dice_8: 0.1323  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:55] d2.utils.events INFO:  eta: 1:39:10  iter: 2619  total_loss: 5.034  loss_ce: 0.1198  loss_mask: 0.2385  loss_dice: 0.1387  loss_ce_0: 0.07415  loss_mask_0: 0.2367  loss_dice_0: 0.135  loss_ce_1: 0.1183  loss_mask_1: 0.2392  loss_dice_1: 0.1392  loss_ce_2: 0.1183  loss_mask_2: 0.2477  loss_dice_2: 0.1363  loss_ce_3: 0.1188  loss_mask_3: 0.2267  loss_dice_3: 0.1392  loss_ce_4: 0.1179  loss_mask_4: 0.2337  loss_dice_4: 0.1414  loss_ce_5: 0.1184  loss_mask_5: 0.2457  loss_dice_5: 0.14  loss_ce_6: 0.1189  loss_mask_6: 0.2495  loss_dice_6: 0.1448  loss_ce_7: 0.1196  loss_mask_7: 0.2339  loss_dice_7: 0.1447  loss_ce_8: 0.1191  loss_mask_8: 0.2412  loss_dice_8: 0.1402  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:37:58] d2.utils.events INFO:  eta: 1:39:07  iter: 2639  total_loss: 5.65  loss_ce: 0.1306  loss_mask: 0.2129  loss_dice: 0.207  loss_ce_0: 0.07476  loss_mask_0: 0.206  loss_dice_0: 0.2001  loss_ce_1: 0.1307  loss_mask_1: 0.1974  loss_dice_1: 0.2142  loss_ce_2: 0.1307  loss_mask_2: 0.2059  loss_dice_2: 0.201  loss_ce_3: 0.1308  loss_mask_3: 0.2093  loss_dice_3: 0.2066  loss_ce_4: 0.1307  loss_mask_4: 0.2118  loss_dice_4: 0.2005  loss_ce_5: 0.1305  loss_mask_5: 0.1979  loss_dice_5: 0.2083  loss_ce_6: 0.1307  loss_mask_6: 0.1956  loss_dice_6: 0.2039  loss_ce_7: 0.1305  loss_mask_7: 0.1957  loss_dice_7: 0.2037  loss_ce_8: 0.1306  loss_mask_8: 0.2057  loss_dice_8: 0.2029  time: 0.1256  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:38:00] d2.utils.events INFO:  eta: 1:39:06  iter: 2659  total_loss: 5.684  loss_ce: 0.1396  loss_mask: 0.2569  loss_dice: 0.1751  loss_ce_0: 0.07698  loss_mask_0: 0.2508  loss_dice_0: 0.1664  loss_ce_1: 0.1402  loss_mask_1: 0.258  loss_dice_1: 0.1799  loss_ce_2: 0.1404  loss_mask_2: 0.268  loss_dice_2: 0.1794  loss_ce_3: 0.1402  loss_mask_3: 0.2545  loss_dice_3: 0.1789  loss_ce_4: 0.1404  loss_mask_4: 0.2641  loss_dice_4: 0.1787  loss_ce_5: 0.1396  loss_mask_5: 0.2513  loss_dice_5: 0.1746  loss_ce_6: 0.14  loss_mask_6: 0.2535  loss_dice_6: 0.1708  loss_ce_7: 0.1406  loss_mask_7: 0.2541  loss_dice_7: 0.1788  loss_ce_8: 0.14  loss_mask_8: 0.2525  loss_dice_8: 0.1827  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:38:03] d2.utils.events INFO:  eta: 1:39:06  iter: 2679  total_loss: 5.948  loss_ce: 0.1313  loss_mask: 0.2269  loss_dice: 0.1788  loss_ce_0: 0.07468  loss_mask_0: 0.227  loss_dice_0: 0.1795  loss_ce_1: 0.1315  loss_mask_1: 0.2395  loss_dice_1: 0.1815  loss_ce_2: 0.1315  loss_mask_2: 0.2287  loss_dice_2: 0.1768  loss_ce_3: 0.1315  loss_mask_3: 0.239  loss_dice_3: 0.1814  loss_ce_4: 0.1314  loss_mask_4: 0.2336  loss_dice_4: 0.1811  loss_ce_5: 0.1313  loss_mask_5: 0.2323  loss_dice_5: 0.1766  loss_ce_6: 0.1314  loss_mask_6: 0.2366  loss_dice_6: 0.1719  loss_ce_7: 0.1313  loss_mask_7: 0.2283  loss_dice_7: 0.1838  loss_ce_8: 0.1315  loss_mask_8: 0.2369  loss_dice_8: 0.1815  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:38:05] d2.utils.events INFO:  eta: 1:39:06  iter: 2699  total_loss: 5.459  loss_ce: 0.1279  loss_mask: 0.2399  loss_dice: 0.1719  loss_ce_0: 0.07464  loss_mask_0: 0.2292  loss_dice_0: 0.1583  loss_ce_1: 0.1281  loss_mask_1: 0.2293  loss_dice_1: 0.1704  loss_ce_2: 0.1278  loss_mask_2: 0.2555  loss_dice_2: 0.1719  loss_ce_3: 0.1279  loss_mask_3: 0.2344  loss_dice_3: 0.1765  loss_ce_4: 0.1278  loss_mask_4: 0.2365  loss_dice_4: 0.1735  loss_ce_5: 0.1279  loss_mask_5: 0.2318  loss_dice_5: 0.1652  loss_ce_6: 0.1278  loss_mask_6: 0.2333  loss_dice_6: 0.1748  loss_ce_7: 0.1277  loss_mask_7: 0.2292  loss_dice_7: 0.1734  loss_ce_8: 0.128  loss_mask_8: 0.2246  loss_dice_8: 0.1731  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:08] d2.utils.events INFO:  eta: 1:39:03  iter: 2719  total_loss: 6.017  loss_ce: 0.1187  loss_mask: 0.1815  loss_dice: 0.2514  loss_ce_0: 0.07238  loss_mask_0: 0.1831  loss_dice_0: 0.2496  loss_ce_1: 0.1197  loss_mask_1: 0.1818  loss_dice_1: 0.2677  loss_ce_2: 0.1196  loss_mask_2: 0.1889  loss_dice_2: 0.2559  loss_ce_3: 0.1197  loss_mask_3: 0.1773  loss_dice_3: 0.2628  loss_ce_4: 0.1199  loss_mask_4: 0.1735  loss_dice_4: 0.2643  loss_ce_5: 0.1194  loss_mask_5: 0.189  loss_dice_5: 0.2566  loss_ce_6: 0.1196  loss_mask_6: 0.184  loss_dice_6: 0.2645  loss_ce_7: 0.12  loss_mask_7: 0.1808  loss_dice_7: 0.2678  loss_ce_8: 0.1194  loss_mask_8: 0.1743  loss_dice_8: 0.2669  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:10] d2.utils.events INFO:  eta: 1:38:56  iter: 2739  total_loss: 5.62  loss_ce: 0.09715  loss_mask: 0.1865  loss_dice: 0.174  loss_ce_0: 0.07003  loss_mask_0: 0.1889  loss_dice_0: 0.1824  loss_ce_1: 0.09709  loss_mask_1: 0.2003  loss_dice_1: 0.1907  loss_ce_2: 0.09656  loss_mask_2: 0.1928  loss_dice_2: 0.1858  loss_ce_3: 0.0964  loss_mask_3: 0.1877  loss_dice_3: 0.1808  loss_ce_4: 0.09588  loss_mask_4: 0.197  loss_dice_4: 0.1858  loss_ce_5: 0.09654  loss_mask_5: 0.1916  loss_dice_5: 0.1931  loss_ce_6: 0.09655  loss_mask_6: 0.1914  loss_dice_6: 0.1866  loss_ce_7: 0.09676  loss_mask_7: 0.2015  loss_dice_7: 0.1839  loss_ce_8: 0.09662  loss_mask_8: 0.1945  loss_dice_8: 0.1756  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:13] d2.utils.events INFO:  eta: 1:38:52  iter: 2759  total_loss: 6.184  loss_ce: 0.1622  loss_mask: 0.2592  loss_dice: 0.1517  loss_ce_0: 0.08203  loss_mask_0: 0.261  loss_dice_0: 0.1568  loss_ce_1: 0.1609  loss_mask_1: 0.2612  loss_dice_1: 0.1625  loss_ce_2: 0.1609  loss_mask_2: 0.2522  loss_dice_2: 0.1502  loss_ce_3: 0.1609  loss_mask_3: 0.271  loss_dice_3: 0.1565  loss_ce_4: 0.1617  loss_mask_4: 0.2555  loss_dice_4: 0.164  loss_ce_5: 0.1612  loss_mask_5: 0.2642  loss_dice_5: 0.1644  loss_ce_6: 0.1613  loss_mask_6: 0.253  loss_dice_6: 0.1569  loss_ce_7: 0.1616  loss_mask_7: 0.2563  loss_dice_7: 0.1572  loss_ce_8: 0.1615  loss_mask_8: 0.2605  loss_dice_8: 0.1573  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:38:15] d2.utils.events INFO:  eta: 1:38:47  iter: 2779  total_loss: 5.744  loss_ce: 0.1229  loss_mask: 0.2379  loss_dice: 0.2003  loss_ce_0: 0.07723  loss_mask_0: 0.2373  loss_dice_0: 0.2177  loss_ce_1: 0.1228  loss_mask_1: 0.2344  loss_dice_1: 0.2068  loss_ce_2: 0.1216  loss_mask_2: 0.245  loss_dice_2: 0.2088  loss_ce_3: 0.1216  loss_mask_3: 0.243  loss_dice_3: 0.2025  loss_ce_4: 0.1223  loss_mask_4: 0.2412  loss_dice_4: 0.2091  loss_ce_5: 0.1229  loss_mask_5: 0.2372  loss_dice_5: 0.2131  loss_ce_6: 0.1222  loss_mask_6: 0.2308  loss_dice_6: 0.204  loss_ce_7: 0.1228  loss_mask_7: 0.2329  loss_dice_7: 0.2075  loss_ce_8: 0.1229  loss_mask_8: 0.2457  loss_dice_8: 0.2046  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:18] d2.utils.events INFO:  eta: 1:38:46  iter: 2799  total_loss: 5.257  loss_ce: 0.1017  loss_mask: 0.1964  loss_dice: 0.2012  loss_ce_0: 0.0739  loss_mask_0: 0.1911  loss_dice_0: 0.2069  loss_ce_1: 0.1005  loss_mask_1: 0.1916  loss_dice_1: 0.2034  loss_ce_2: 0.1005  loss_mask_2: 0.1895  loss_dice_2: 0.2005  loss_ce_3: 0.1004  loss_mask_3: 0.1908  loss_dice_3: 0.2024  loss_ce_4: 0.1007  loss_mask_4: 0.1879  loss_dice_4: 0.2054  loss_ce_5: 0.1008  loss_mask_5: 0.1843  loss_dice_5: 0.2063  loss_ce_6: 0.09992  loss_mask_6: 0.1902  loss_dice_6: 0.2058  loss_ce_7: 0.1004  loss_mask_7: 0.185  loss_dice_7: 0.1947  loss_ce_8: 0.1004  loss_mask_8: 0.1992  loss_dice_8: 0.2012  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:21] d2.utils.events INFO:  eta: 1:38:44  iter: 2819  total_loss: 5.273  loss_ce: 0.1334  loss_mask: 0.2562  loss_dice: 0.1348  loss_ce_0: 0.07393  loss_mask_0: 0.2545  loss_dice_0: 0.1391  loss_ce_1: 0.1336  loss_mask_1: 0.2569  loss_dice_1: 0.1402  loss_ce_2: 0.1319  loss_mask_2: 0.2549  loss_dice_2: 0.1455  loss_ce_3: 0.1326  loss_mask_3: 0.2546  loss_dice_3: 0.1365  loss_ce_4: 0.1332  loss_mask_4: 0.2609  loss_dice_4: 0.138  loss_ce_5: 0.1333  loss_mask_5: 0.2623  loss_dice_5: 0.1374  loss_ce_6: 0.1337  loss_mask_6: 0.2586  loss_dice_6: 0.1413  loss_ce_7: 0.1339  loss_mask_7: 0.2542  loss_dice_7: 0.1438  loss_ce_8: 0.134  loss_mask_8: 0.2559  loss_dice_8: 0.1338  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:38:23] d2.utils.events INFO:  eta: 1:38:42  iter: 2839  total_loss: 5.468  loss_ce: 0.141  loss_mask: 0.223  loss_dice: 0.1554  loss_ce_0: 0.07436  loss_mask_0: 0.2207  loss_dice_0: 0.1481  loss_ce_1: 0.1417  loss_mask_1: 0.2247  loss_dice_1: 0.1555  loss_ce_2: 0.1431  loss_mask_2: 0.2095  loss_dice_2: 0.1514  loss_ce_3: 0.1429  loss_mask_3: 0.2068  loss_dice_3: 0.1527  loss_ce_4: 0.1417  loss_mask_4: 0.2139  loss_dice_4: 0.1533  loss_ce_5: 0.1415  loss_mask_5: 0.2263  loss_dice_5: 0.1507  loss_ce_6: 0.1414  loss_mask_6: 0.2198  loss_dice_6: 0.1516  loss_ce_7: 0.1409  loss_mask_7: 0.2055  loss_dice_7: 0.1517  loss_ce_8: 0.1408  loss_mask_8: 0.2127  loss_dice_8: 0.1449  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:38:26] d2.utils.events INFO:  eta: 1:38:39  iter: 2859  total_loss: 5.01  loss_ce: 0.1299  loss_mask: 0.1778  loss_dice: 0.1491  loss_ce_0: 0.07364  loss_mask_0: 0.178  loss_dice_0: 0.1508  loss_ce_1: 0.1304  loss_mask_1: 0.1814  loss_dice_1: 0.1465  loss_ce_2: 0.1294  loss_mask_2: 0.1761  loss_dice_2: 0.1444  loss_ce_3: 0.13  loss_mask_3: 0.1727  loss_dice_3: 0.1479  loss_ce_4: 0.1303  loss_mask_4: 0.1721  loss_dice_4: 0.1424  loss_ce_5: 0.1297  loss_mask_5: 0.1762  loss_dice_5: 0.1478  loss_ce_6: 0.1302  loss_mask_6: 0.1768  loss_dice_6: 0.1444  loss_ce_7: 0.13  loss_mask_7: 0.1829  loss_dice_7: 0.1411  loss_ce_8: 0.1298  loss_mask_8: 0.1817  loss_dice_8: 0.1488  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:28] d2.utils.events INFO:  eta: 1:38:37  iter: 2879  total_loss: 5.848  loss_ce: 0.1211  loss_mask: 0.2061  loss_dice: 0.211  loss_ce_0: 0.07282  loss_mask_0: 0.2042  loss_dice_0: 0.1975  loss_ce_1: 0.1203  loss_mask_1: 0.1894  loss_dice_1: 0.2106  loss_ce_2: 0.1202  loss_mask_2: 0.2042  loss_dice_2: 0.208  loss_ce_3: 0.1206  loss_mask_3: 0.2004  loss_dice_3: 0.2049  loss_ce_4: 0.1204  loss_mask_4: 0.2018  loss_dice_4: 0.2039  loss_ce_5: 0.1207  loss_mask_5: 0.1893  loss_dice_5: 0.2022  loss_ce_6: 0.1204  loss_mask_6: 0.2013  loss_dice_6: 0.2063  loss_ce_7: 0.1207  loss_mask_7: 0.1973  loss_dice_7: 0.2098  loss_ce_8: 0.1211  loss_mask_8: 0.196  loss_dice_8: 0.2027  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:31] d2.utils.events INFO:  eta: 1:38:35  iter: 2899  total_loss: 5.414  loss_ce: 0.1477  loss_mask: 0.1855  loss_dice: 0.2  loss_ce_0: 0.07685  loss_mask_0: 0.1835  loss_dice_0: 0.1993  loss_ce_1: 0.1475  loss_mask_1: 0.1935  loss_dice_1: 0.1872  loss_ce_2: 0.1469  loss_mask_2: 0.189  loss_dice_2: 0.1982  loss_ce_3: 0.1475  loss_mask_3: 0.1821  loss_dice_3: 0.2016  loss_ce_4: 0.1474  loss_mask_4: 0.1864  loss_dice_4: 0.2076  loss_ce_5: 0.1476  loss_mask_5: 0.1809  loss_dice_5: 0.2034  loss_ce_6: 0.148  loss_mask_6: 0.1798  loss_dice_6: 0.2004  loss_ce_7: 0.1477  loss_mask_7: 0.1803  loss_dice_7: 0.1983  loss_ce_8: 0.1478  loss_mask_8: 0.1837  loss_dice_8: 0.196  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:33] d2.utils.events INFO:  eta: 1:38:33  iter: 2919  total_loss: 5.197  loss_ce: 0.1155  loss_mask: 0.2286  loss_dice: 0.1656  loss_ce_0: 0.06893  loss_mask_0: 0.2355  loss_dice_0: 0.1647  loss_ce_1: 0.1158  loss_mask_1: 0.237  loss_dice_1: 0.1671  loss_ce_2: 0.1165  loss_mask_2: 0.2344  loss_dice_2: 0.1739  loss_ce_3: 0.1159  loss_mask_3: 0.2207  loss_dice_3: 0.1693  loss_ce_4: 0.1162  loss_mask_4: 0.238  loss_dice_4: 0.1661  loss_ce_5: 0.1159  loss_mask_5: 0.2268  loss_dice_5: 0.167  loss_ce_6: 0.1157  loss_mask_6: 0.2237  loss_dice_6: 0.1763  loss_ce_7: 0.1157  loss_mask_7: 0.2179  loss_dice_7: 0.171  loss_ce_8: 0.116  loss_mask_8: 0.2303  loss_dice_8: 0.172  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:36] d2.utils.events INFO:  eta: 1:38:31  iter: 2939  total_loss: 5.314  loss_ce: 0.1279  loss_mask: 0.2054  loss_dice: 0.1865  loss_ce_0: 0.07278  loss_mask_0: 0.2167  loss_dice_0: 0.196  loss_ce_1: 0.1281  loss_mask_1: 0.2063  loss_dice_1: 0.1909  loss_ce_2: 0.128  loss_mask_2: 0.2028  loss_dice_2: 0.1899  loss_ce_3: 0.128  loss_mask_3: 0.1968  loss_dice_3: 0.1999  loss_ce_4: 0.128  loss_mask_4: 0.2015  loss_dice_4: 0.1948  loss_ce_5: 0.128  loss_mask_5: 0.208  loss_dice_5: 0.1944  loss_ce_6: 0.1279  loss_mask_6: 0.2008  loss_dice_6: 0.1904  loss_ce_7: 0.1279  loss_mask_7: 0.2018  loss_dice_7: 0.1904  loss_ce_8: 0.1279  loss_mask_8: 0.2095  loss_dice_8: 0.1965  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:38] d2.utils.events INFO:  eta: 1:38:33  iter: 2959  total_loss: 5.164  loss_ce: 0.1293  loss_mask: 0.2213  loss_dice: 0.1484  loss_ce_0: 0.07282  loss_mask_0: 0.2212  loss_dice_0: 0.1485  loss_ce_1: 0.1291  loss_mask_1: 0.2157  loss_dice_1: 0.1524  loss_ce_2: 0.129  loss_mask_2: 0.2317  loss_dice_2: 0.1518  loss_ce_3: 0.129  loss_mask_3: 0.2207  loss_dice_3: 0.1523  loss_ce_4: 0.1289  loss_mask_4: 0.2208  loss_dice_4: 0.1506  loss_ce_5: 0.1289  loss_mask_5: 0.2231  loss_dice_5: 0.15  loss_ce_6: 0.129  loss_mask_6: 0.2145  loss_dice_6: 0.1523  loss_ce_7: 0.1293  loss_mask_7: 0.2274  loss_dice_7: 0.1478  loss_ce_8: 0.1288  loss_mask_8: 0.2212  loss_dice_8: 0.1479  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:41] d2.utils.events INFO:  eta: 1:38:27  iter: 2979  total_loss: 5.71  loss_ce: 0.1369  loss_mask: 0.2143  loss_dice: 0.1645  loss_ce_0: 0.07008  loss_mask_0: 0.2221  loss_dice_0: 0.1681  loss_ce_1: 0.1373  loss_mask_1: 0.2164  loss_dice_1: 0.163  loss_ce_2: 0.138  loss_mask_2: 0.2244  loss_dice_2: 0.1676  loss_ce_3: 0.1378  loss_mask_3: 0.2276  loss_dice_3: 0.1648  loss_ce_4: 0.1379  loss_mask_4: 0.2125  loss_dice_4: 0.1624  loss_ce_5: 0.1375  loss_mask_5: 0.2045  loss_dice_5: 0.1685  loss_ce_6: 0.1375  loss_mask_6: 0.2306  loss_dice_6: 0.1676  loss_ce_7: 0.1373  loss_mask_7: 0.2272  loss_dice_7: 0.1634  loss_ce_8: 0.1373  loss_mask_8: 0.215  loss_dice_8: 0.1682  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:43] d2.utils.events INFO:  eta: 1:38:28  iter: 2999  total_loss: 5.136  loss_ce: 0.1282  loss_mask: 0.2182  loss_dice: 0.1493  loss_ce_0: 0.07523  loss_mask_0: 0.228  loss_dice_0: 0.1576  loss_ce_1: 0.1293  loss_mask_1: 0.2091  loss_dice_1: 0.1557  loss_ce_2: 0.1294  loss_mask_2: 0.2111  loss_dice_2: 0.1538  loss_ce_3: 0.1298  loss_mask_3: 0.2146  loss_dice_3: 0.1531  loss_ce_4: 0.1287  loss_mask_4: 0.211  loss_dice_4: 0.1512  loss_ce_5: 0.1285  loss_mask_5: 0.215  loss_dice_5: 0.1518  loss_ce_6: 0.1286  loss_mask_6: 0.2235  loss_dice_6: 0.1537  loss_ce_7: 0.1287  loss_mask_7: 0.2126  loss_dice_7: 0.1567  loss_ce_8: 0.1287  loss_mask_8: 0.2188  loss_dice_8: 0.1495  time: 0.1256  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:38:46] d2.utils.events INFO:  eta: 1:38:25  iter: 3019  total_loss: 4.901  loss_ce: 0.1477  loss_mask: 0.1921  loss_dice: 0.1447  loss_ce_0: 0.07218  loss_mask_0: 0.1764  loss_dice_0: 0.147  loss_ce_1: 0.1491  loss_mask_1: 0.1896  loss_dice_1: 0.1457  loss_ce_2: 0.1497  loss_mask_2: 0.1851  loss_dice_2: 0.1499  loss_ce_3: 0.1501  loss_mask_3: 0.1861  loss_dice_3: 0.148  loss_ce_4: 0.1492  loss_mask_4: 0.1827  loss_dice_4: 0.1419  loss_ce_5: 0.1487  loss_mask_5: 0.1825  loss_dice_5: 0.1365  loss_ce_6: 0.149  loss_mask_6: 0.1916  loss_dice_6: 0.1459  loss_ce_7: 0.1485  loss_mask_7: 0.1857  loss_dice_7: 0.1472  loss_ce_8: 0.1487  loss_mask_8: 0.1923  loss_dice_8: 0.1474  time: 0.1256  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:38:48] d2.utils.events INFO:  eta: 1:38:23  iter: 3039  total_loss: 5.434  loss_ce: 0.1279  loss_mask: 0.2127  loss_dice: 0.1434  loss_ce_0: 0.07166  loss_mask_0: 0.2284  loss_dice_0: 0.1486  loss_ce_1: 0.1265  loss_mask_1: 0.2193  loss_dice_1: 0.1492  loss_ce_2: 0.1263  loss_mask_2: 0.2093  loss_dice_2: 0.1467  loss_ce_3: 0.126  loss_mask_3: 0.219  loss_dice_3: 0.1505  loss_ce_4: 0.1265  loss_mask_4: 0.2336  loss_dice_4: 0.1512  loss_ce_5: 0.1271  loss_mask_5: 0.2239  loss_dice_5: 0.1496  loss_ce_6: 0.1268  loss_mask_6: 0.2243  loss_dice_6: 0.1461  loss_ce_7: 0.1271  loss_mask_7: 0.2243  loss_dice_7: 0.1555  loss_ce_8: 0.1269  loss_mask_8: 0.2311  loss_dice_8: 0.147  time: 0.1256  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:38:51] d2.utils.events INFO:  eta: 1:38:21  iter: 3059  total_loss: 4.74  loss_ce: 0.1392  loss_mask: 0.1852  loss_dice: 0.155  loss_ce_0: 0.07531  loss_mask_0: 0.1842  loss_dice_0: 0.1605  loss_ce_1: 0.1392  loss_mask_1: 0.1912  loss_dice_1: 0.1477  loss_ce_2: 0.1385  loss_mask_2: 0.184  loss_dice_2: 0.1527  loss_ce_3: 0.1393  loss_mask_3: 0.1818  loss_dice_3: 0.1495  loss_ce_4: 0.1391  loss_mask_4: 0.1916  loss_dice_4: 0.1464  loss_ce_5: 0.1392  loss_mask_5: 0.186  loss_dice_5: 0.1457  loss_ce_6: 0.1402  loss_mask_6: 0.192  loss_dice_6: 0.1493  loss_ce_7: 0.1389  loss_mask_7: 0.1863  loss_dice_7: 0.1511  loss_ce_8: 0.1398  loss_mask_8: 0.1872  loss_dice_8: 0.1485  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:53] d2.utils.events INFO:  eta: 1:38:18  iter: 3079  total_loss: 5.131  loss_ce: 0.1259  loss_mask: 0.2172  loss_dice: 0.1395  loss_ce_0: 0.0697  loss_mask_0: 0.2081  loss_dice_0: 0.1342  loss_ce_1: 0.1271  loss_mask_1: 0.2106  loss_dice_1: 0.1338  loss_ce_2: 0.1281  loss_mask_2: 0.2073  loss_dice_2: 0.1359  loss_ce_3: 0.1274  loss_mask_3: 0.2098  loss_dice_3: 0.1416  loss_ce_4: 0.1271  loss_mask_4: 0.211  loss_dice_4: 0.1334  loss_ce_5: 0.1267  loss_mask_5: 0.2029  loss_dice_5: 0.1345  loss_ce_6: 0.1257  loss_mask_6: 0.2061  loss_dice_6: 0.1373  loss_ce_7: 0.1269  loss_mask_7: 0.2092  loss_dice_7: 0.1373  loss_ce_8: 0.1262  loss_mask_8: 0.2076  loss_dice_8: 0.1354  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:56] d2.utils.events INFO:  eta: 1:38:11  iter: 3099  total_loss: 5.608  loss_ce: 0.1218  loss_mask: 0.2444  loss_dice: 0.1938  loss_ce_0: 0.06884  loss_mask_0: 0.2402  loss_dice_0: 0.1933  loss_ce_1: 0.1226  loss_mask_1: 0.2532  loss_dice_1: 0.1971  loss_ce_2: 0.1225  loss_mask_2: 0.2654  loss_dice_2: 0.1872  loss_ce_3: 0.1226  loss_mask_3: 0.2624  loss_dice_3: 0.1894  loss_ce_4: 0.1228  loss_mask_4: 0.2582  loss_dice_4: 0.1945  loss_ce_5: 0.1223  loss_mask_5: 0.2557  loss_dice_5: 0.1828  loss_ce_6: 0.1218  loss_mask_6: 0.2452  loss_dice_6: 0.191  loss_ce_7: 0.1224  loss_mask_7: 0.2507  loss_dice_7: 0.1995  loss_ce_8: 0.122  loss_mask_8: 0.2511  loss_dice_8: 0.1915  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:38:58] d2.utils.events INFO:  eta: 1:38:08  iter: 3119  total_loss: 5.074  loss_ce: 0.1276  loss_mask: 0.2458  loss_dice: 0.135  loss_ce_0: 0.07197  loss_mask_0: 0.2443  loss_dice_0: 0.1385  loss_ce_1: 0.1279  loss_mask_1: 0.2415  loss_dice_1: 0.1358  loss_ce_2: 0.1277  loss_mask_2: 0.2486  loss_dice_2: 0.1391  loss_ce_3: 0.1276  loss_mask_3: 0.2463  loss_dice_3: 0.1425  loss_ce_4: 0.128  loss_mask_4: 0.2407  loss_dice_4: 0.1411  loss_ce_5: 0.1277  loss_mask_5: 0.237  loss_dice_5: 0.1342  loss_ce_6: 0.1276  loss_mask_6: 0.2485  loss_dice_6: 0.1385  loss_ce_7: 0.128  loss_mask_7: 0.2436  loss_dice_7: 0.1397  loss_ce_8: 0.1275  loss_mask_8: 0.2419  loss_dice_8: 0.139  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:01] d2.utils.events INFO:  eta: 1:38:03  iter: 3139  total_loss: 5.258  loss_ce: 0.1244  loss_mask: 0.2017  loss_dice: 0.1653  loss_ce_0: 0.06826  loss_mask_0: 0.1978  loss_dice_0: 0.1652  loss_ce_1: 0.1251  loss_mask_1: 0.1909  loss_dice_1: 0.1596  loss_ce_2: 0.1255  loss_mask_2: 0.1961  loss_dice_2: 0.16  loss_ce_3: 0.1255  loss_mask_3: 0.2031  loss_dice_3: 0.166  loss_ce_4: 0.1259  loss_mask_4: 0.2012  loss_dice_4: 0.1618  loss_ce_5: 0.1253  loss_mask_5: 0.1992  loss_dice_5: 0.1642  loss_ce_6: 0.125  loss_mask_6: 0.2025  loss_dice_6: 0.1635  loss_ce_7: 0.1249  loss_mask_7: 0.2001  loss_dice_7: 0.1691  loss_ce_8: 0.1252  loss_mask_8: 0.2062  loss_dice_8: 0.1667  time: 0.1256  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:03] d2.utils.events INFO:  eta: 1:37:56  iter: 3159  total_loss: 5.036  loss_ce: 0.1268  loss_mask: 0.2137  loss_dice: 0.1712  loss_ce_0: 0.06851  loss_mask_0: 0.2274  loss_dice_0: 0.1677  loss_ce_1: 0.126  loss_mask_1: 0.2141  loss_dice_1: 0.1715  loss_ce_2: 0.1261  loss_mask_2: 0.2169  loss_dice_2: 0.1676  loss_ce_3: 0.1264  loss_mask_3: 0.2174  loss_dice_3: 0.1706  loss_ce_4: 0.1258  loss_mask_4: 0.2167  loss_dice_4: 0.1745  loss_ce_5: 0.1262  loss_mask_5: 0.2166  loss_dice_5: 0.1739  loss_ce_6: 0.1268  loss_mask_6: 0.2154  loss_dice_6: 0.1615  loss_ce_7: 0.1262  loss_mask_7: 0.229  loss_dice_7: 0.164  loss_ce_8: 0.1262  loss_mask_8: 0.2132  loss_dice_8: 0.1658  time: 0.1256  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:39:06] d2.utils.events INFO:  eta: 1:37:52  iter: 3179  total_loss: 5.353  loss_ce: 0.1367  loss_mask: 0.2184  loss_dice: 0.1605  loss_ce_0: 0.066  loss_mask_0: 0.2026  loss_dice_0: 0.1549  loss_ce_1: 0.09777  loss_mask_1: 0.1993  loss_dice_1: 0.1532  loss_ce_2: 0.09755  loss_mask_2: 0.2078  loss_dice_2: 0.1561  loss_ce_3: 0.09739  loss_mask_3: 0.2181  loss_dice_3: 0.1632  loss_ce_4: 0.09758  loss_mask_4: 0.2113  loss_dice_4: 0.1564  loss_ce_5: 0.09817  loss_mask_5: 0.2161  loss_dice_5: 0.1536  loss_ce_6: 0.09773  loss_mask_6: 0.2203  loss_dice_6: 0.1569  loss_ce_7: 0.1009  loss_mask_7: 0.208  loss_dice_7: 0.1602  loss_ce_8: 0.1122  loss_mask_8: 0.2134  loss_dice_8: 0.1501  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:09] d2.utils.events INFO:  eta: 1:37:51  iter: 3199  total_loss: 5.07  loss_ce: 0.1719  loss_mask: 0.2126  loss_dice: 0.168  loss_ce_0: 0.0805  loss_mask_0: 0.2266  loss_dice_0: 0.1706  loss_ce_1: 0.1655  loss_mask_1: 0.2182  loss_dice_1: 0.1639  loss_ce_2: 0.1488  loss_mask_2: 0.2206  loss_dice_2: 0.17  loss_ce_3: 0.148  loss_mask_3: 0.2159  loss_dice_3: 0.1768  loss_ce_4: 0.1448  loss_mask_4: 0.2209  loss_dice_4: 0.1713  loss_ce_5: 0.1492  loss_mask_5: 0.2275  loss_dice_5: 0.17  loss_ce_6: 0.1524  loss_mask_6: 0.2176  loss_dice_6: 0.1741  loss_ce_7: 0.1644  loss_mask_7: 0.212  loss_dice_7: 0.1688  loss_ce_8: 0.169  loss_mask_8: 0.2152  loss_dice_8: 0.1719  time: 0.1257  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:11] d2.utils.events INFO:  eta: 1:37:48  iter: 3219  total_loss: 4.703  loss_ce: 0.1246  loss_mask: 0.1924  loss_dice: 0.1377  loss_ce_0: 0.07182  loss_mask_0: 0.2011  loss_dice_0: 0.1453  loss_ce_1: 0.1276  loss_mask_1: 0.1968  loss_dice_1: 0.1464  loss_ce_2: 0.1238  loss_mask_2: 0.2011  loss_dice_2: 0.1463  loss_ce_3: 0.125  loss_mask_3: 0.1968  loss_dice_3: 0.1425  loss_ce_4: 0.1277  loss_mask_4: 0.2003  loss_dice_4: 0.1468  loss_ce_5: 0.1231  loss_mask_5: 0.1981  loss_dice_5: 0.1461  loss_ce_6: 0.1234  loss_mask_6: 0.188  loss_dice_6: 0.1394  loss_ce_7: 0.1241  loss_mask_7: 0.1927  loss_dice_7: 0.1473  loss_ce_8: 0.1248  loss_mask_8: 0.2002  loss_dice_8: 0.1496  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:39:14] d2.utils.events INFO:  eta: 1:37:49  iter: 3239  total_loss: 6.076  loss_ce: 0.1145  loss_mask: 0.182  loss_dice: 0.1755  loss_ce_0: 0.07403  loss_mask_0: 0.1739  loss_dice_0: 0.1671  loss_ce_1: 0.1099  loss_mask_1: 0.1786  loss_dice_1: 0.1739  loss_ce_2: 0.1063  loss_mask_2: 0.1801  loss_dice_2: 0.1705  loss_ce_3: 0.1062  loss_mask_3: 0.1865  loss_dice_3: 0.1773  loss_ce_4: 0.1054  loss_mask_4: 0.1876  loss_dice_4: 0.1711  loss_ce_5: 0.1048  loss_mask_5: 0.1884  loss_dice_5: 0.1776  loss_ce_6: 0.1019  loss_mask_6: 0.1901  loss_dice_6: 0.18  loss_ce_7: 0.1049  loss_mask_7: 0.176  loss_dice_7: 0.1712  loss_ce_8: 0.1122  loss_mask_8: 0.1795  loss_dice_8: 0.1708  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:39:16] d2.utils.events INFO:  eta: 1:37:49  iter: 3259  total_loss: 5.325  loss_ce: 0.1098  loss_mask: 0.1784  loss_dice: 0.1842  loss_ce_0: 0.07172  loss_mask_0: 0.1959  loss_dice_0: 0.1787  loss_ce_1: 0.1195  loss_mask_1: 0.1869  loss_dice_1: 0.187  loss_ce_2: 0.1309  loss_mask_2: 0.1851  loss_dice_2: 0.1764  loss_ce_3: 0.1313  loss_mask_3: 0.1899  loss_dice_3: 0.1781  loss_ce_4: 0.1325  loss_mask_4: 0.1898  loss_dice_4: 0.1799  loss_ce_5: 0.1305  loss_mask_5: 0.1766  loss_dice_5: 0.1916  loss_ce_6: 0.1306  loss_mask_6: 0.1851  loss_dice_6: 0.1925  loss_ce_7: 0.1193  loss_mask_7: 0.181  loss_dice_7: 0.1783  loss_ce_8: 0.1098  loss_mask_8: 0.1879  loss_dice_8: 0.1846  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:39:19] d2.utils.events INFO:  eta: 1:37:51  iter: 3279  total_loss: 5.759  loss_ce: 0.147  loss_mask: 0.2432  loss_dice: 0.159  loss_ce_0: 0.07188  loss_mask_0: 0.234  loss_dice_0: 0.1602  loss_ce_1: 0.1392  loss_mask_1: 0.2359  loss_dice_1: 0.1544  loss_ce_2: 0.1292  loss_mask_2: 0.2434  loss_dice_2: 0.1548  loss_ce_3: 0.1295  loss_mask_3: 0.2315  loss_dice_3: 0.1597  loss_ce_4: 0.1301  loss_mask_4: 0.2408  loss_dice_4: 0.1525  loss_ce_5: 0.1294  loss_mask_5: 0.2481  loss_dice_5: 0.1576  loss_ce_6: 0.13  loss_mask_6: 0.2332  loss_dice_6: 0.159  loss_ce_7: 0.1385  loss_mask_7: 0.2396  loss_dice_7: 0.1558  loss_ce_8: 0.1478  loss_mask_8: 0.2471  loss_dice_8: 0.161  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:39:21] d2.utils.events INFO:  eta: 1:37:51  iter: 3299  total_loss: 5.641  loss_ce: 0.1229  loss_mask: 0.2083  loss_dice: 0.17  loss_ce_0: 0.07037  loss_mask_0: 0.2006  loss_dice_0: 0.1633  loss_ce_1: 0.1243  loss_mask_1: 0.209  loss_dice_1: 0.1687  loss_ce_2: 0.127  loss_mask_2: 0.207  loss_dice_2: 0.1641  loss_ce_3: 0.1269  loss_mask_3: 0.2079  loss_dice_3: 0.1655  loss_ce_4: 0.1265  loss_mask_4: 0.2202  loss_dice_4: 0.1709  loss_ce_5: 0.1267  loss_mask_5: 0.2037  loss_dice_5: 0.1665  loss_ce_6: 0.1272  loss_mask_6: 0.2156  loss_dice_6: 0.1686  loss_ce_7: 0.1259  loss_mask_7: 0.2052  loss_dice_7: 0.164  loss_ce_8: 0.1226  loss_mask_8: 0.2122  loss_dice_8: 0.1619  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:24] d2.utils.events INFO:  eta: 1:37:49  iter: 3319  total_loss: 5.67  loss_ce: 0.1289  loss_mask: 0.1901  loss_dice: 0.2087  loss_ce_0: 0.07132  loss_mask_0: 0.187  loss_dice_0: 0.2103  loss_ce_1: 0.1298  loss_mask_1: 0.2008  loss_dice_1: 0.2234  loss_ce_2: 0.1297  loss_mask_2: 0.1912  loss_dice_2: 0.2103  loss_ce_3: 0.1297  loss_mask_3: 0.184  loss_dice_3: 0.2092  loss_ce_4: 0.1302  loss_mask_4: 0.1917  loss_dice_4: 0.2149  loss_ce_5: 0.1298  loss_mask_5: 0.2018  loss_dice_5: 0.2172  loss_ce_6: 0.1292  loss_mask_6: 0.1898  loss_dice_6: 0.2129  loss_ce_7: 0.1295  loss_mask_7: 0.194  loss_dice_7: 0.2078  loss_ce_8: 0.1295  loss_mask_8: 0.1819  loss_dice_8: 0.2231  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:26] d2.utils.events INFO:  eta: 1:37:47  iter: 3339  total_loss: 6.097  loss_ce: 0.1058  loss_mask: 0.2132  loss_dice: 0.1724  loss_ce_0: 0.06709  loss_mask_0: 0.2082  loss_dice_0: 0.1662  loss_ce_1: 0.1067  loss_mask_1: 0.2169  loss_dice_1: 0.1686  loss_ce_2: 0.1067  loss_mask_2: 0.2137  loss_dice_2: 0.1661  loss_ce_3: 0.1073  loss_mask_3: 0.2145  loss_dice_3: 0.1735  loss_ce_4: 0.108  loss_mask_4: 0.2095  loss_dice_4: 0.166  loss_ce_5: 0.1071  loss_mask_5: 0.2117  loss_dice_5: 0.1764  loss_ce_6: 0.1069  loss_mask_6: 0.2137  loss_dice_6: 0.1767  loss_ce_7: 0.1064  loss_mask_7: 0.2221  loss_dice_7: 0.166  loss_ce_8: 0.1055  loss_mask_8: 0.2068  loss_dice_8: 0.1708  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:29] d2.utils.events INFO:  eta: 1:37:41  iter: 3359  total_loss: 5.905  loss_ce: 0.1508  loss_mask: 0.2401  loss_dice: 0.1743  loss_ce_0: 0.07659  loss_mask_0: 0.233  loss_dice_0: 0.1785  loss_ce_1: 0.1496  loss_mask_1: 0.2373  loss_dice_1: 0.174  loss_ce_2: 0.1493  loss_mask_2: 0.2309  loss_dice_2: 0.1796  loss_ce_3: 0.1486  loss_mask_3: 0.2285  loss_dice_3: 0.1794  loss_ce_4: 0.148  loss_mask_4: 0.2273  loss_dice_4: 0.1779  loss_ce_5: 0.1484  loss_mask_5: 0.232  loss_dice_5: 0.1758  loss_ce_6: 0.1489  loss_mask_6: 0.2433  loss_dice_6: 0.1776  loss_ce_7: 0.1494  loss_mask_7: 0.2383  loss_dice_7: 0.1784  loss_ce_8: 0.1504  loss_mask_8: 0.2391  loss_dice_8: 0.1797  time: 0.1257  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:39:32] d2.utils.events INFO:  eta: 1:37:40  iter: 3379  total_loss: 4.92  loss_ce: 0.1276  loss_mask: 0.1967  loss_dice: 0.1438  loss_ce_0: 0.06724  loss_mask_0: 0.2005  loss_dice_0: 0.1439  loss_ce_1: 0.1289  loss_mask_1: 0.1942  loss_dice_1: 0.1477  loss_ce_2: 0.1299  loss_mask_2: 0.1948  loss_dice_2: 0.146  loss_ce_3: 0.1305  loss_mask_3: 0.1901  loss_dice_3: 0.1452  loss_ce_4: 0.13  loss_mask_4: 0.1872  loss_dice_4: 0.1407  loss_ce_5: 0.1305  loss_mask_5: 0.1848  loss_dice_5: 0.1438  loss_ce_6: 0.1308  loss_mask_6: 0.1941  loss_dice_6: 0.1482  loss_ce_7: 0.129  loss_mask_7: 0.178  loss_dice_7: 0.1454  loss_ce_8: 0.129  loss_mask_8: 0.1823  loss_dice_8: 0.1479  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:39:34] d2.utils.events INFO:  eta: 1:37:38  iter: 3399  total_loss: 5.223  loss_ce: 0.1296  loss_mask: 0.2274  loss_dice: 0.1533  loss_ce_0: 0.07133  loss_mask_0: 0.2225  loss_dice_0: 0.1568  loss_ce_1: 0.1298  loss_mask_1: 0.2297  loss_dice_1: 0.1531  loss_ce_2: 0.1298  loss_mask_2: 0.2253  loss_dice_2: 0.1536  loss_ce_3: 0.1295  loss_mask_3: 0.2224  loss_dice_3: 0.1452  loss_ce_4: 0.1295  loss_mask_4: 0.2216  loss_dice_4: 0.1516  loss_ce_5: 0.1296  loss_mask_5: 0.2251  loss_dice_5: 0.162  loss_ce_6: 0.1297  loss_mask_6: 0.2176  loss_dice_6: 0.156  loss_ce_7: 0.1295  loss_mask_7: 0.2281  loss_dice_7: 0.151  loss_ce_8: 0.1296  loss_mask_8: 0.217  loss_dice_8: 0.1514  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:37] d2.utils.events INFO:  eta: 1:37:37  iter: 3419  total_loss: 5.043  loss_ce: 0.105  loss_mask: 0.1928  loss_dice: 0.1572  loss_ce_0: 0.06568  loss_mask_0: 0.186  loss_dice_0: 0.1525  loss_ce_1: 0.1034  loss_mask_1: 0.1941  loss_dice_1: 0.1573  loss_ce_2: 0.1022  loss_mask_2: 0.2098  loss_dice_2: 0.1539  loss_ce_3: 0.1016  loss_mask_3: 0.1971  loss_dice_3: 0.1623  loss_ce_4: 0.1012  loss_mask_4: 0.2043  loss_dice_4: 0.1568  loss_ce_5: 0.1015  loss_mask_5: 0.1885  loss_dice_5: 0.1558  loss_ce_6: 0.102  loss_mask_6: 0.1959  loss_dice_6: 0.1545  loss_ce_7: 0.1029  loss_mask_7: 0.1917  loss_dice_7: 0.1601  loss_ce_8: 0.1033  loss_mask_8: 0.1954  loss_dice_8: 0.156  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:39] d2.utils.events INFO:  eta: 1:37:38  iter: 3439  total_loss: 5.524  loss_ce: 0.1698  loss_mask: 0.2204  loss_dice: 0.1635  loss_ce_0: 0.0796  loss_mask_0: 0.2158  loss_dice_0: 0.1635  loss_ce_1: 0.1694  loss_mask_1: 0.2222  loss_dice_1: 0.1687  loss_ce_2: 0.1693  loss_mask_2: 0.2187  loss_dice_2: 0.1662  loss_ce_3: 0.1689  loss_mask_3: 0.2064  loss_dice_3: 0.1638  loss_ce_4: 0.169  loss_mask_4: 0.2207  loss_dice_4: 0.1628  loss_ce_5: 0.1693  loss_mask_5: 0.2195  loss_dice_5: 0.1762  loss_ce_6: 0.1695  loss_mask_6: 0.2209  loss_dice_6: 0.1686  loss_ce_7: 0.1695  loss_mask_7: 0.2111  loss_dice_7: 0.1701  loss_ce_8: 0.1704  loss_mask_8: 0.2175  loss_dice_8: 0.1675  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:42] d2.utils.events INFO:  eta: 1:37:37  iter: 3459  total_loss: 5.549  loss_ce: 0.1281  loss_mask: 0.2398  loss_dice: 0.1428  loss_ce_0: 0.07115  loss_mask_0: 0.2431  loss_dice_0: 0.1389  loss_ce_1: 0.13  loss_mask_1: 0.2458  loss_dice_1: 0.1408  loss_ce_2: 0.1302  loss_mask_2: 0.2283  loss_dice_2: 0.1421  loss_ce_3: 0.1307  loss_mask_3: 0.2393  loss_dice_3: 0.1403  loss_ce_4: 0.1312  loss_mask_4: 0.2405  loss_dice_4: 0.1406  loss_ce_5: 0.131  loss_mask_5: 0.2412  loss_dice_5: 0.1437  loss_ce_6: 0.1307  loss_mask_6: 0.2483  loss_dice_6: 0.138  loss_ce_7: 0.13  loss_mask_7: 0.2396  loss_dice_7: 0.1443  loss_ce_8: 0.1287  loss_mask_8: 0.2373  loss_dice_8: 0.1451  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:44] d2.utils.events INFO:  eta: 1:37:32  iter: 3479  total_loss: 5.365  loss_ce: 0.1275  loss_mask: 0.2397  loss_dice: 0.1878  loss_ce_0: 0.07583  loss_mask_0: 0.2292  loss_dice_0: 0.1811  loss_ce_1: 0.1259  loss_mask_1: 0.2447  loss_dice_1: 0.1909  loss_ce_2: 0.1247  loss_mask_2: 0.24  loss_dice_2: 0.1922  loss_ce_3: 0.1248  loss_mask_3: 0.2446  loss_dice_3: 0.1856  loss_ce_4: 0.125  loss_mask_4: 0.2291  loss_dice_4: 0.1826  loss_ce_5: 0.1247  loss_mask_5: 0.2361  loss_dice_5: 0.1865  loss_ce_6: 0.1245  loss_mask_6: 0.2343  loss_dice_6: 0.1816  loss_ce_7: 0.1248  loss_mask_7: 0.2261  loss_dice_7: 0.1793  loss_ce_8: 0.1256  loss_mask_8: 0.2322  loss_dice_8: 0.1843  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:47] d2.utils.events INFO:  eta: 1:37:30  iter: 3499  total_loss: 5.26  loss_ce: 0.08979  loss_mask: 0.2039  loss_dice: 0.1558  loss_ce_0: 0.07057  loss_mask_0: 0.2027  loss_dice_0: 0.1585  loss_ce_1: 0.08521  loss_mask_1: 0.1951  loss_dice_1: 0.159  loss_ce_2: 0.08278  loss_mask_2: 0.1989  loss_dice_2: 0.1542  loss_ce_3: 0.08139  loss_mask_3: 0.2039  loss_dice_3: 0.1612  loss_ce_4: 0.08089  loss_mask_4: 0.2048  loss_dice_4: 0.16  loss_ce_5: 0.081  loss_mask_5: 0.1905  loss_dice_5: 0.1587  loss_ce_6: 0.08104  loss_mask_6: 0.194  loss_dice_6: 0.1656  loss_ce_7: 0.08379  loss_mask_7: 0.202  loss_dice_7: 0.1536  loss_ce_8: 0.08517  loss_mask_8: 0.1948  loss_dice_8: 0.1527  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:49] d2.utils.events INFO:  eta: 1:37:29  iter: 3519  total_loss: 5.465  loss_ce: 0.1569  loss_mask: 0.2127  loss_dice: 0.1499  loss_ce_0: 0.07306  loss_mask_0: 0.2194  loss_dice_0: 0.1465  loss_ce_1: 0.1548  loss_mask_1: 0.2207  loss_dice_1: 0.154  loss_ce_2: 0.1547  loss_mask_2: 0.2109  loss_dice_2: 0.1542  loss_ce_3: 0.1538  loss_mask_3: 0.2181  loss_dice_3: 0.1558  loss_ce_4: 0.153  loss_mask_4: 0.2204  loss_dice_4: 0.1542  loss_ce_5: 0.1537  loss_mask_5: 0.2176  loss_dice_5: 0.1572  loss_ce_6: 0.154  loss_mask_6: 0.2145  loss_dice_6: 0.158  loss_ce_7: 0.1548  loss_mask_7: 0.2174  loss_dice_7: 0.1525  loss_ce_8: 0.1562  loss_mask_8: 0.2068  loss_dice_8: 0.153  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:52] d2.utils.events INFO:  eta: 1:37:29  iter: 3539  total_loss: 5.653  loss_ce: 0.1242  loss_mask: 0.2407  loss_dice: 0.1827  loss_ce_0: 0.06945  loss_mask_0: 0.2294  loss_dice_0: 0.1683  loss_ce_1: 0.1282  loss_mask_1: 0.2259  loss_dice_1: 0.1753  loss_ce_2: 0.1269  loss_mask_2: 0.2268  loss_dice_2: 0.1727  loss_ce_3: 0.1276  loss_mask_3: 0.2357  loss_dice_3: 0.1774  loss_ce_4: 0.1282  loss_mask_4: 0.2256  loss_dice_4: 0.1663  loss_ce_5: 0.1272  loss_mask_5: 0.2344  loss_dice_5: 0.1673  loss_ce_6: 0.1272  loss_mask_6: 0.2367  loss_dice_6: 0.1791  loss_ce_7: 0.1275  loss_mask_7: 0.2283  loss_dice_7: 0.1773  loss_ce_8: 0.1274  loss_mask_8: 0.2252  loss_dice_8: 0.1781  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:54] d2.utils.events INFO:  eta: 1:37:24  iter: 3559  total_loss: 5.019  loss_ce: 0.1068  loss_mask: 0.2084  loss_dice: 0.1418  loss_ce_0: 0.06727  loss_mask_0: 0.2095  loss_dice_0: 0.1358  loss_ce_1: 0.1065  loss_mask_1: 0.2134  loss_dice_1: 0.1426  loss_ce_2: 0.106  loss_mask_2: 0.2101  loss_dice_2: 0.146  loss_ce_3: 0.1062  loss_mask_3: 0.2108  loss_dice_3: 0.1439  loss_ce_4: 0.1055  loss_mask_4: 0.2025  loss_dice_4: 0.1371  loss_ce_5: 0.1052  loss_mask_5: 0.2091  loss_dice_5: 0.1416  loss_ce_6: 0.1044  loss_mask_6: 0.2004  loss_dice_6: 0.144  loss_ce_7: 0.1068  loss_mask_7: 0.2216  loss_dice_7: 0.1459  loss_ce_8: 0.1064  loss_mask_8: 0.2124  loss_dice_8: 0.1384  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:39:57] d2.utils.events INFO:  eta: 1:37:24  iter: 3579  total_loss: 5.782  loss_ce: 0.136  loss_mask: 0.2417  loss_dice: 0.1801  loss_ce_0: 0.07078  loss_mask_0: 0.2254  loss_dice_0: 0.1767  loss_ce_1: 0.1369  loss_mask_1: 0.251  loss_dice_1: 0.1803  loss_ce_2: 0.1373  loss_mask_2: 0.2377  loss_dice_2: 0.1765  loss_ce_3: 0.1375  loss_mask_3: 0.2469  loss_dice_3: 0.1719  loss_ce_4: 0.1375  loss_mask_4: 0.2454  loss_dice_4: 0.1759  loss_ce_5: 0.1377  loss_mask_5: 0.2471  loss_dice_5: 0.1794  loss_ce_6: 0.138  loss_mask_6: 0.2439  loss_dice_6: 0.1721  loss_ce_7: 0.1369  loss_mask_7: 0.2477  loss_dice_7: 0.1815  loss_ce_8: 0.1368  loss_mask_8: 0.2505  loss_dice_8: 0.1786  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:39:59] d2.utils.events INFO:  eta: 1:37:24  iter: 3599  total_loss: 5.374  loss_ce: 0.1401  loss_mask: 0.2114  loss_dice: 0.1354  loss_ce_0: 0.07778  loss_mask_0: 0.2186  loss_dice_0: 0.1317  loss_ce_1: 0.137  loss_mask_1: 0.2168  loss_dice_1: 0.1367  loss_ce_2: 0.1356  loss_mask_2: 0.2159  loss_dice_2: 0.1377  loss_ce_3: 0.1344  loss_mask_3: 0.2131  loss_dice_3: 0.1358  loss_ce_4: 0.1332  loss_mask_4: 0.2102  loss_dice_4: 0.1374  loss_ce_5: 0.1341  loss_mask_5: 0.2242  loss_dice_5: 0.1409  loss_ce_6: 0.132  loss_mask_6: 0.2119  loss_dice_6: 0.1325  loss_ce_7: 0.1363  loss_mask_7: 0.2145  loss_dice_7: 0.1375  loss_ce_8: 0.1374  loss_mask_8: 0.2081  loss_dice_8: 0.1368  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:02] d2.utils.events INFO:  eta: 1:37:22  iter: 3619  total_loss: 5.241  loss_ce: 0.1324  loss_mask: 0.2249  loss_dice: 0.1488  loss_ce_0: 0.06618  loss_mask_0: 0.2279  loss_dice_0: 0.1533  loss_ce_1: 0.1322  loss_mask_1: 0.2223  loss_dice_1: 0.1539  loss_ce_2: 0.1311  loss_mask_2: 0.2355  loss_dice_2: 0.1532  loss_ce_3: 0.1303  loss_mask_3: 0.229  loss_dice_3: 0.1511  loss_ce_4: 0.1302  loss_mask_4: 0.2322  loss_dice_4: 0.1552  loss_ce_5: 0.1303  loss_mask_5: 0.2413  loss_dice_5: 0.1532  loss_ce_6: 0.1304  loss_mask_6: 0.2264  loss_dice_6: 0.1509  loss_ce_7: 0.1319  loss_mask_7: 0.2327  loss_dice_7: 0.1535  loss_ce_8: 0.1323  loss_mask_8: 0.2313  loss_dice_8: 0.1533  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:40:04] d2.utils.events INFO:  eta: 1:37:18  iter: 3639  total_loss: 5.596  loss_ce: 0.1339  loss_mask: 0.2082  loss_dice: 0.1854  loss_ce_0: 0.07626  loss_mask_0: 0.2027  loss_dice_0: 0.1702  loss_ce_1: 0.1332  loss_mask_1: 0.2079  loss_dice_1: 0.1868  loss_ce_2: 0.1326  loss_mask_2: 0.2177  loss_dice_2: 0.1753  loss_ce_3: 0.1328  loss_mask_3: 0.2125  loss_dice_3: 0.179  loss_ce_4: 0.1331  loss_mask_4: 0.211  loss_dice_4: 0.1786  loss_ce_5: 0.1328  loss_mask_5: 0.2062  loss_dice_5: 0.187  loss_ce_6: 0.1331  loss_mask_6: 0.2006  loss_dice_6: 0.1805  loss_ce_7: 0.133  loss_mask_7: 0.2103  loss_dice_7: 0.182  loss_ce_8: 0.1331  loss_mask_8: 0.2105  loss_dice_8: 0.1881  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:07] d2.utils.events INFO:  eta: 1:37:11  iter: 3659  total_loss: 5.233  loss_ce: 0.1154  loss_mask: 0.2336  loss_dice: 0.1775  loss_ce_0: 0.06525  loss_mask_0: 0.2254  loss_dice_0: 0.1705  loss_ce_1: 0.1143  loss_mask_1: 0.2374  loss_dice_1: 0.1869  loss_ce_2: 0.1143  loss_mask_2: 0.2344  loss_dice_2: 0.1801  loss_ce_3: 0.1139  loss_mask_3: 0.2276  loss_dice_3: 0.1803  loss_ce_4: 0.1137  loss_mask_4: 0.2274  loss_dice_4: 0.1825  loss_ce_5: 0.1136  loss_mask_5: 0.229  loss_dice_5: 0.1779  loss_ce_6: 0.1137  loss_mask_6: 0.2321  loss_dice_6: 0.1808  loss_ce_7: 0.1144  loss_mask_7: 0.2257  loss_dice_7: 0.1827  loss_ce_8: 0.1149  loss_mask_8: 0.2243  loss_dice_8: 0.1832  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:09] d2.utils.events INFO:  eta: 1:37:07  iter: 3679  total_loss: 5.54  loss_ce: 0.1315  loss_mask: 0.2152  loss_dice: 0.1564  loss_ce_0: 0.07108  loss_mask_0: 0.2106  loss_dice_0: 0.1528  loss_ce_1: 0.1321  loss_mask_1: 0.2115  loss_dice_1: 0.1557  loss_ce_2: 0.1323  loss_mask_2: 0.2156  loss_dice_2: 0.1516  loss_ce_3: 0.1325  loss_mask_3: 0.2052  loss_dice_3: 0.1611  loss_ce_4: 0.1325  loss_mask_4: 0.2127  loss_dice_4: 0.158  loss_ce_5: 0.1326  loss_mask_5: 0.2162  loss_dice_5: 0.1579  loss_ce_6: 0.1326  loss_mask_6: 0.2129  loss_dice_6: 0.1605  loss_ce_7: 0.1322  loss_mask_7: 0.2156  loss_dice_7: 0.1545  loss_ce_8: 0.132  loss_mask_8: 0.2048  loss_dice_8: 0.1599  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:40:12] d2.utils.events INFO:  eta: 1:37:05  iter: 3699  total_loss: 5.839  loss_ce: 0.09552  loss_mask: 0.2218  loss_dice: 0.2373  loss_ce_0: 0.06232  loss_mask_0: 0.2155  loss_dice_0: 0.2319  loss_ce_1: 0.09434  loss_mask_1: 0.2196  loss_dice_1: 0.2433  loss_ce_2: 0.09394  loss_mask_2: 0.2213  loss_dice_2: 0.2349  loss_ce_3: 0.09352  loss_mask_3: 0.2216  loss_dice_3: 0.248  loss_ce_4: 0.09333  loss_mask_4: 0.2214  loss_dice_4: 0.2591  loss_ce_5: 0.09352  loss_mask_5: 0.2199  loss_dice_5: 0.2578  loss_ce_6: 0.09318  loss_mask_6: 0.2233  loss_dice_6: 0.2479  loss_ce_7: 0.09392  loss_mask_7: 0.2144  loss_dice_7: 0.252  loss_ce_8: 0.09421  loss_mask_8: 0.2226  loss_dice_8: 0.2491  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:15] d2.utils.events INFO:  eta: 1:37:02  iter: 3719  total_loss: 5.577  loss_ce: 0.1361  loss_mask: 0.2243  loss_dice: 0.1494  loss_ce_0: 0.07096  loss_mask_0: 0.2161  loss_dice_0: 0.1538  loss_ce_1: 0.1367  loss_mask_1: 0.2165  loss_dice_1: 0.1538  loss_ce_2: 0.1366  loss_mask_2: 0.2098  loss_dice_2: 0.1619  loss_ce_3: 0.1367  loss_mask_3: 0.2147  loss_dice_3: 0.1583  loss_ce_4: 0.1369  loss_mask_4: 0.2201  loss_dice_4: 0.1504  loss_ce_5: 0.1367  loss_mask_5: 0.2228  loss_dice_5: 0.1539  loss_ce_6: 0.1369  loss_mask_6: 0.2169  loss_dice_6: 0.1551  loss_ce_7: 0.1365  loss_mask_7: 0.2194  loss_dice_7: 0.1534  loss_ce_8: 0.1366  loss_mask_8: 0.2268  loss_dice_8: 0.164  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:17] d2.utils.events INFO:  eta: 1:37:02  iter: 3739  total_loss: 5.542  loss_ce: 0.08983  loss_mask: 0.2134  loss_dice: 0.1894  loss_ce_0: 0.05818  loss_mask_0: 0.2198  loss_dice_0: 0.1858  loss_ce_1: 0.09096  loss_mask_1: 0.2314  loss_dice_1: 0.1892  loss_ce_2: 0.09114  loss_mask_2: 0.2137  loss_dice_2: 0.1819  loss_ce_3: 0.09132  loss_mask_3: 0.2261  loss_dice_3: 0.1884  loss_ce_4: 0.09148  loss_mask_4: 0.2242  loss_dice_4: 0.184  loss_ce_5: 0.09111  loss_mask_5: 0.218  loss_dice_5: 0.1858  loss_ce_6: 0.09101  loss_mask_6: 0.2252  loss_dice_6: 0.1948  loss_ce_7: 0.09097  loss_mask_7: 0.2265  loss_dice_7: 0.1877  loss_ce_8: 0.09021  loss_mask_8: 0.2143  loss_dice_8: 0.1907  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:20] d2.utils.events INFO:  eta: 1:37:01  iter: 3759  total_loss: 5.149  loss_ce: 0.177  loss_mask: 0.2224  loss_dice: 0.1623  loss_ce_0: 0.0864  loss_mask_0: 0.2166  loss_dice_0: 0.163  loss_ce_1: 0.1763  loss_mask_1: 0.2164  loss_dice_1: 0.1642  loss_ce_2: 0.1761  loss_mask_2: 0.2086  loss_dice_2: 0.162  loss_ce_3: 0.1755  loss_mask_3: 0.2275  loss_dice_3: 0.1651  loss_ce_4: 0.1755  loss_mask_4: 0.2281  loss_dice_4: 0.1656  loss_ce_5: 0.1759  loss_mask_5: 0.2272  loss_dice_5: 0.1648  loss_ce_6: 0.1761  loss_mask_6: 0.2275  loss_dice_6: 0.1712  loss_ce_7: 0.176  loss_mask_7: 0.2131  loss_dice_7: 0.1668  loss_ce_8: 0.1766  loss_mask_8: 0.2137  loss_dice_8: 0.1702  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:22] d2.utils.events INFO:  eta: 1:37:01  iter: 3779  total_loss: 5.155  loss_ce: 0.1393  loss_mask: 0.2095  loss_dice: 0.1536  loss_ce_0: 0.08278  loss_mask_0: 0.211  loss_dice_0: 0.1489  loss_ce_1: 0.1374  loss_mask_1: 0.2097  loss_dice_1: 0.1537  loss_ce_2: 0.1377  loss_mask_2: 0.2029  loss_dice_2: 0.1602  loss_ce_3: 0.1365  loss_mask_3: 0.2169  loss_dice_3: 0.1507  loss_ce_4: 0.1366  loss_mask_4: 0.2055  loss_dice_4: 0.1552  loss_ce_5: 0.1372  loss_mask_5: 0.2075  loss_dice_5: 0.1506  loss_ce_6: 0.1368  loss_mask_6: 0.2117  loss_dice_6: 0.156  loss_ce_7: 0.1375  loss_mask_7: 0.2056  loss_dice_7: 0.1486  loss_ce_8: 0.1375  loss_mask_8: 0.2033  loss_dice_8: 0.1572  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:25] d2.utils.events INFO:  eta: 1:36:59  iter: 3799  total_loss: 4.748  loss_ce: 0.1085  loss_mask: 0.2034  loss_dice: 0.1497  loss_ce_0: 0.07519  loss_mask_0: 0.2148  loss_dice_0: 0.156  loss_ce_1: 0.1037  loss_mask_1: 0.2015  loss_dice_1: 0.1451  loss_ce_2: 0.1015  loss_mask_2: 0.2165  loss_dice_2: 0.1544  loss_ce_3: 0.1001  loss_mask_3: 0.2071  loss_dice_3: 0.1474  loss_ce_4: 0.09877  loss_mask_4: 0.2162  loss_dice_4: 0.1552  loss_ce_5: 0.1004  loss_mask_5: 0.1996  loss_dice_5: 0.1513  loss_ce_6: 0.1002  loss_mask_6: 0.2149  loss_dice_6: 0.1501  loss_ce_7: 0.1032  loss_mask_7: 0.2046  loss_dice_7: 0.1457  loss_ce_8: 0.1033  loss_mask_8: 0.2029  loss_dice_8: 0.1543  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:27] d2.utils.events INFO:  eta: 1:36:55  iter: 3819  total_loss: 5.459  loss_ce: 0.1741  loss_mask: 0.2071  loss_dice: 0.133  loss_ce_0: 0.06931  loss_mask_0: 0.212  loss_dice_0: 0.1333  loss_ce_1: 0.1764  loss_mask_1: 0.2047  loss_dice_1: 0.133  loss_ce_2: 0.1778  loss_mask_2: 0.2065  loss_dice_2: 0.1358  loss_ce_3: 0.1787  loss_mask_3: 0.2026  loss_dice_3: 0.1343  loss_ce_4: 0.1791  loss_mask_4: 0.2024  loss_dice_4: 0.136  loss_ce_5: 0.1785  loss_mask_5: 0.2034  loss_dice_5: 0.1366  loss_ce_6: 0.1795  loss_mask_6: 0.2158  loss_dice_6: 0.1255  loss_ce_7: 0.1774  loss_mask_7: 0.1976  loss_dice_7: 0.1342  loss_ce_8: 0.178  loss_mask_8: 0.1944  loss_dice_8: 0.1353  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:30] d2.utils.events INFO:  eta: 1:36:53  iter: 3839  total_loss: 5.02  loss_ce: 0.1291  loss_mask: 0.1972  loss_dice: 0.13  loss_ce_0: 0.0704  loss_mask_0: 0.2004  loss_dice_0: 0.1344  loss_ce_1: 0.1289  loss_mask_1: 0.2057  loss_dice_1: 0.1346  loss_ce_2: 0.1289  loss_mask_2: 0.1945  loss_dice_2: 0.1344  loss_ce_3: 0.1289  loss_mask_3: 0.19  loss_dice_3: 0.1302  loss_ce_4: 0.1288  loss_mask_4: 0.2038  loss_dice_4: 0.1368  loss_ce_5: 0.1289  loss_mask_5: 0.202  loss_dice_5: 0.1283  loss_ce_6: 0.129  loss_mask_6: 0.209  loss_dice_6: 0.1295  loss_ce_7: 0.129  loss_mask_7: 0.1968  loss_dice_7: 0.1284  loss_ce_8: 0.1291  loss_mask_8: 0.1958  loss_dice_8: 0.1308  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:32] d2.utils.events INFO:  eta: 1:36:51  iter: 3859  total_loss: 4.591  loss_ce: 0.1311  loss_mask: 0.1946  loss_dice: 0.1256  loss_ce_0: 0.07037  loss_mask_0: 0.1954  loss_dice_0: 0.1334  loss_ce_1: 0.1316  loss_mask_1: 0.2025  loss_dice_1: 0.1366  loss_ce_2: 0.1309  loss_mask_2: 0.1962  loss_dice_2: 0.1293  loss_ce_3: 0.1304  loss_mask_3: 0.2009  loss_dice_3: 0.1337  loss_ce_4: 0.1294  loss_mask_4: 0.195  loss_dice_4: 0.1315  loss_ce_5: 0.1305  loss_mask_5: 0.1938  loss_dice_5: 0.1318  loss_ce_6: 0.1305  loss_mask_6: 0.1916  loss_dice_6: 0.1371  loss_ce_7: 0.1317  loss_mask_7: 0.1987  loss_dice_7: 0.1326  loss_ce_8: 0.1317  loss_mask_8: 0.2031  loss_dice_8: 0.1333  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:35] d2.utils.events INFO:  eta: 1:36:48  iter: 3879  total_loss: 5.389  loss_ce: 0.1424  loss_mask: 0.1694  loss_dice: 0.1485  loss_ce_0: 0.07157  loss_mask_0: 0.18  loss_dice_0: 0.1484  loss_ce_1: 0.1449  loss_mask_1: 0.1865  loss_dice_1: 0.1431  loss_ce_2: 0.1459  loss_mask_2: 0.1773  loss_dice_2: 0.1503  loss_ce_3: 0.1464  loss_mask_3: 0.1769  loss_dice_3: 0.1492  loss_ce_4: 0.1465  loss_mask_4: 0.185  loss_dice_4: 0.152  loss_ce_5: 0.1465  loss_mask_5: 0.1849  loss_dice_5: 0.1509  loss_ce_6: 0.1467  loss_mask_6: 0.1893  loss_dice_6: 0.1528  loss_ce_7: 0.1452  loss_mask_7: 0.187  loss_dice_7: 0.1564  loss_ce_8: 0.145  loss_mask_8: 0.1801  loss_dice_8: 0.1581  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:37] d2.utils.events INFO:  eta: 1:36:47  iter: 3899  total_loss: 5.539  loss_ce: 0.126  loss_mask: 0.2579  loss_dice: 0.141  loss_ce_0: 0.07028  loss_mask_0: 0.2489  loss_dice_0: 0.1446  loss_ce_1: 0.1256  loss_mask_1: 0.24  loss_dice_1: 0.1473  loss_ce_2: 0.1253  loss_mask_2: 0.2456  loss_dice_2: 0.1464  loss_ce_3: 0.1253  loss_mask_3: 0.2574  loss_dice_3: 0.146  loss_ce_4: 0.1251  loss_mask_4: 0.2521  loss_dice_4: 0.1434  loss_ce_5: 0.1252  loss_mask_5: 0.2543  loss_dice_5: 0.1458  loss_ce_6: 0.1253  loss_mask_6: 0.249  loss_dice_6: 0.1389  loss_ce_7: 0.1257  loss_mask_7: 0.2484  loss_dice_7: 0.1474  loss_ce_8: 0.1256  loss_mask_8: 0.2483  loss_dice_8: 0.1416  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:40] d2.utils.events INFO:  eta: 1:36:44  iter: 3919  total_loss: 5.499  loss_ce: 0.1473  loss_mask: 0.1842  loss_dice: 0.2018  loss_ce_0: 0.07067  loss_mask_0: 0.1812  loss_dice_0: 0.1967  loss_ce_1: 0.1507  loss_mask_1: 0.1781  loss_dice_1: 0.2036  loss_ce_2: 0.1523  loss_mask_2: 0.1741  loss_dice_2: 0.2023  loss_ce_3: 0.1528  loss_mask_3: 0.1797  loss_dice_3: 0.2102  loss_ce_4: 0.1541  loss_mask_4: 0.1742  loss_dice_4: 0.1954  loss_ce_5: 0.1531  loss_mask_5: 0.1792  loss_dice_5: 0.2043  loss_ce_6: 0.1532  loss_mask_6: 0.1832  loss_dice_6: 0.2055  loss_ce_7: 0.1507  loss_mask_7: 0.179  loss_dice_7: 0.2032  loss_ce_8: 0.1502  loss_mask_8: 0.1871  loss_dice_8: 0.2065  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:42] d2.utils.events INFO:  eta: 1:36:44  iter: 3939  total_loss: 4.789  loss_ce: 0.1478  loss_mask: 0.176  loss_dice: 0.1667  loss_ce_0: 0.07195  loss_mask_0: 0.1933  loss_dice_0: 0.1641  loss_ce_1: 0.1478  loss_mask_1: 0.1908  loss_dice_1: 0.1692  loss_ce_2: 0.1481  loss_mask_2: 0.1708  loss_dice_2: 0.1639  loss_ce_3: 0.1484  loss_mask_3: 0.2052  loss_dice_3: 0.1616  loss_ce_4: 0.1487  loss_mask_4: 0.1966  loss_dice_4: 0.1688  loss_ce_5: 0.1482  loss_mask_5: 0.1728  loss_dice_5: 0.1632  loss_ce_6: 0.1485  loss_mask_6: 0.181  loss_dice_6: 0.1619  loss_ce_7: 0.1482  loss_mask_7: 0.1838  loss_dice_7: 0.1681  loss_ce_8: 0.1487  loss_mask_8: 0.1872  loss_dice_8: 0.1698  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:40:45] d2.utils.events INFO:  eta: 1:36:42  iter: 3959  total_loss: 5.396  loss_ce: 0.1272  loss_mask: 0.2222  loss_dice: 0.1569  loss_ce_0: 0.06966  loss_mask_0: 0.2206  loss_dice_0: 0.1555  loss_ce_1: 0.1275  loss_mask_1: 0.2278  loss_dice_1: 0.1585  loss_ce_2: 0.1275  loss_mask_2: 0.225  loss_dice_2: 0.1549  loss_ce_3: 0.128  loss_mask_3: 0.2231  loss_dice_3: 0.1551  loss_ce_4: 0.1276  loss_mask_4: 0.2331  loss_dice_4: 0.1589  loss_ce_5: 0.1277  loss_mask_5: 0.2298  loss_dice_5: 0.1537  loss_ce_6: 0.1278  loss_mask_6: 0.2278  loss_dice_6: 0.1589  loss_ce_7: 0.1273  loss_mask_7: 0.2268  loss_dice_7: 0.1521  loss_ce_8: 0.1272  loss_mask_8: 0.22  loss_dice_8: 0.1556  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:40:47] d2.utils.events INFO:  eta: 1:36:39  iter: 3979  total_loss: 5.022  loss_ce: 0.12  loss_mask: 0.2106  loss_dice: 0.1294  loss_ce_0: 0.06982  loss_mask_0: 0.2178  loss_dice_0: 0.1333  loss_ce_1: 0.1173  loss_mask_1: 0.2231  loss_dice_1: 0.131  loss_ce_2: 0.1167  loss_mask_2: 0.2217  loss_dice_2: 0.1307  loss_ce_3: 0.1159  loss_mask_3: 0.2098  loss_dice_3: 0.1397  loss_ce_4: 0.1152  loss_mask_4: 0.207  loss_dice_4: 0.1367  loss_ce_5: 0.1161  loss_mask_5: 0.2155  loss_dice_5: 0.1325  loss_ce_6: 0.1156  loss_mask_6: 0.2259  loss_dice_6: 0.1305  loss_ce_7: 0.1172  loss_mask_7: 0.2228  loss_dice_7: 0.128  loss_ce_8: 0.1181  loss_mask_8: 0.2104  loss_dice_8: 0.1268  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:50] d2.utils.events INFO:  eta: 1:36:37  iter: 3999  total_loss: 5.606  loss_ce: 0.1315  loss_mask: 0.2271  loss_dice: 0.1479  loss_ce_0: 0.0699  loss_mask_0: 0.2453  loss_dice_0: 0.1489  loss_ce_1: 0.1312  loss_mask_1: 0.2335  loss_dice_1: 0.1455  loss_ce_2: 0.1307  loss_mask_2: 0.225  loss_dice_2: 0.1514  loss_ce_3: 0.1308  loss_mask_3: 0.2469  loss_dice_3: 0.1562  loss_ce_4: 0.1307  loss_mask_4: 0.2348  loss_dice_4: 0.1546  loss_ce_5: 0.1308  loss_mask_5: 0.2303  loss_dice_5: 0.1447  loss_ce_6: 0.131  loss_mask_6: 0.24  loss_dice_6: 0.1471  loss_ce_7: 0.1312  loss_mask_7: 0.2327  loss_dice_7: 0.152  loss_ce_8: 0.1313  loss_mask_8: 0.2345  loss_dice_8: 0.1491  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:53] d2.utils.events INFO:  eta: 1:36:35  iter: 4019  total_loss: 6.371  loss_ce: 0.1022  loss_mask: 0.2299  loss_dice: 0.1976  loss_ce_0: 0.06601  loss_mask_0: 0.2366  loss_dice_0: 0.1927  loss_ce_1: 0.1011  loss_mask_1: 0.2297  loss_dice_1: 0.204  loss_ce_2: 0.101  loss_mask_2: 0.2313  loss_dice_2: 0.2105  loss_ce_3: 0.1004  loss_mask_3: 0.2404  loss_dice_3: 0.2074  loss_ce_4: 0.1  loss_mask_4: 0.2396  loss_dice_4: 0.1982  loss_ce_5: 0.1006  loss_mask_5: 0.2342  loss_dice_5: 0.2035  loss_ce_6: 0.09983  loss_mask_6: 0.2322  loss_dice_6: 0.2032  loss_ce_7: 0.1009  loss_mask_7: 0.2304  loss_dice_7: 0.2018  loss_ce_8: 0.101  loss_mask_8: 0.231  loss_dice_8: 0.1991  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:55] d2.utils.events INFO:  eta: 1:36:33  iter: 4039  total_loss: 5.537  loss_ce: 0.09129  loss_mask: 0.1752  loss_dice: 0.1857  loss_ce_0: 0.06354  loss_mask_0: 0.1795  loss_dice_0: 0.1782  loss_ce_1: 0.0898  loss_mask_1: 0.1787  loss_dice_1: 0.1831  loss_ce_2: 0.08962  loss_mask_2: 0.1894  loss_dice_2: 0.1889  loss_ce_3: 0.08895  loss_mask_3: 0.1846  loss_dice_3: 0.1813  loss_ce_4: 0.08915  loss_mask_4: 0.1715  loss_dice_4: 0.1823  loss_ce_5: 0.0894  loss_mask_5: 0.1907  loss_dice_5: 0.1839  loss_ce_6: 0.08875  loss_mask_6: 0.1895  loss_dice_6: 0.1826  loss_ce_7: 0.08954  loss_mask_7: 0.1766  loss_dice_7: 0.1809  loss_ce_8: 0.08996  loss_mask_8: 0.1837  loss_dice_8: 0.1855  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:40:58] d2.utils.events INFO:  eta: 1:36:30  iter: 4059  total_loss: 5.249  loss_ce: 0.08654  loss_mask: 0.2054  loss_dice: 0.169  loss_ce_0: 0.0612  loss_mask_0: 0.2045  loss_dice_0: 0.157  loss_ce_1: 0.08544  loss_mask_1: 0.1934  loss_dice_1: 0.1618  loss_ce_2: 0.08531  loss_mask_2: 0.2103  loss_dice_2: 0.1681  loss_ce_3: 0.0848  loss_mask_3: 0.2049  loss_dice_3: 0.1526  loss_ce_4: 0.08515  loss_mask_4: 0.1984  loss_dice_4: 0.1553  loss_ce_5: 0.08532  loss_mask_5: 0.2057  loss_dice_5: 0.1595  loss_ce_6: 0.08457  loss_mask_6: 0.2011  loss_dice_6: 0.1577  loss_ce_7: 0.08506  loss_mask_7: 0.1992  loss_dice_7: 0.1515  loss_ce_8: 0.0855  loss_mask_8: 0.2015  loss_dice_8: 0.1535  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:00] d2.utils.events INFO:  eta: 1:36:31  iter: 4079  total_loss: 5.265  loss_ce: 0.08695  loss_mask: 0.2003  loss_dice: 0.1495  loss_ce_0: 0.0581  loss_mask_0: 0.2028  loss_dice_0: 0.1417  loss_ce_1: 0.08719  loss_mask_1: 0.2055  loss_dice_1: 0.1441  loss_ce_2: 0.0879  loss_mask_2: 0.1973  loss_dice_2: 0.1483  loss_ce_3: 0.08775  loss_mask_3: 0.2005  loss_dice_3: 0.1482  loss_ce_4: 0.08798  loss_mask_4: 0.2037  loss_dice_4: 0.1411  loss_ce_5: 0.08741  loss_mask_5: 0.1939  loss_dice_5: 0.1467  loss_ce_6: 0.08748  loss_mask_6: 0.202  loss_dice_6: 0.1468  loss_ce_7: 0.08721  loss_mask_7: 0.2009  loss_dice_7: 0.1434  loss_ce_8: 0.08705  loss_mask_8: 0.1931  loss_dice_8: 0.1419  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:03] d2.utils.events INFO:  eta: 1:36:27  iter: 4099  total_loss: 5.319  loss_ce: 0.09245  loss_mask: 0.2128  loss_dice: 0.1743  loss_ce_0: 0.05746  loss_mask_0: 0.2126  loss_dice_0: 0.1687  loss_ce_1: 0.09356  loss_mask_1: 0.2091  loss_dice_1: 0.1706  loss_ce_2: 0.09392  loss_mask_2: 0.2125  loss_dice_2: 0.1775  loss_ce_3: 0.09409  loss_mask_3: 0.2121  loss_dice_3: 0.1728  loss_ce_4: 0.09429  loss_mask_4: 0.2113  loss_dice_4: 0.1676  loss_ce_5: 0.09355  loss_mask_5: 0.2175  loss_dice_5: 0.1844  loss_ce_6: 0.09402  loss_mask_6: 0.2077  loss_dice_6: 0.173  loss_ce_7: 0.09352  loss_mask_7: 0.2102  loss_dice_7: 0.178  loss_ce_8: 0.09302  loss_mask_8: 0.2167  loss_dice_8: 0.1769  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:05] d2.utils.events INFO:  eta: 1:36:23  iter: 4119  total_loss: 5.861  loss_ce: 0.09931  loss_mask: 0.1951  loss_dice: 0.1816  loss_ce_0: 0.05758  loss_mask_0: 0.1962  loss_dice_0: 0.1818  loss_ce_1: 0.1003  loss_mask_1: 0.1886  loss_dice_1: 0.1846  loss_ce_2: 0.1012  loss_mask_2: 0.1887  loss_dice_2: 0.1828  loss_ce_3: 0.1012  loss_mask_3: 0.2065  loss_dice_3: 0.1827  loss_ce_4: 0.1012  loss_mask_4: 0.1955  loss_dice_4: 0.1727  loss_ce_5: 0.1007  loss_mask_5: 0.203  loss_dice_5: 0.1868  loss_ce_6: 0.1011  loss_mask_6: 0.1949  loss_dice_6: 0.1902  loss_ce_7: 0.1005  loss_mask_7: 0.2017  loss_dice_7: 0.1838  loss_ce_8: 0.1001  loss_mask_8: 0.1989  loss_dice_8: 0.1767  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:08] d2.utils.events INFO:  eta: 1:36:22  iter: 4139  total_loss: 5.744  loss_ce: 0.1279  loss_mask: 0.1892  loss_dice: 0.1924  loss_ce_0: 0.06967  loss_mask_0: 0.1951  loss_dice_0: 0.1887  loss_ce_1: 0.1281  loss_mask_1: 0.1794  loss_dice_1: 0.1911  loss_ce_2: 0.1277  loss_mask_2: 0.19  loss_dice_2: 0.1871  loss_ce_3: 0.1276  loss_mask_3: 0.2034  loss_dice_3: 0.1909  loss_ce_4: 0.1278  loss_mask_4: 0.1861  loss_dice_4: 0.1784  loss_ce_5: 0.1277  loss_mask_5: 0.1999  loss_dice_5: 0.1857  loss_ce_6: 0.1275  loss_mask_6: 0.1933  loss_dice_6: 0.1852  loss_ce_7: 0.1277  loss_mask_7: 0.1849  loss_dice_7: 0.1795  loss_ce_8: 0.1277  loss_mask_8: 0.2109  loss_dice_8: 0.1834  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:10] d2.utils.events INFO:  eta: 1:36:21  iter: 4159  total_loss: 4.684  loss_ce: 0.1375  loss_mask: 0.2054  loss_dice: 0.1496  loss_ce_0: 0.07889  loss_mask_0: 0.2085  loss_dice_0: 0.1458  loss_ce_1: 0.1364  loss_mask_1: 0.209  loss_dice_1: 0.1542  loss_ce_2: 0.1357  loss_mask_2: 0.2061  loss_dice_2: 0.1488  loss_ce_3: 0.1353  loss_mask_3: 0.2042  loss_dice_3: 0.1506  loss_ce_4: 0.1348  loss_mask_4: 0.2093  loss_dice_4: 0.1517  loss_ce_5: 0.1359  loss_mask_5: 0.2089  loss_dice_5: 0.1452  loss_ce_6: 0.1352  loss_mask_6: 0.2079  loss_dice_6: 0.1489  loss_ce_7: 0.1363  loss_mask_7: 0.2154  loss_dice_7: 0.1483  loss_ce_8: 0.1362  loss_mask_8: 0.2082  loss_dice_8: 0.1513  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:13] d2.utils.events INFO:  eta: 1:36:19  iter: 4179  total_loss: 5.07  loss_ce: 0.1232  loss_mask: 0.2293  loss_dice: 0.1461  loss_ce_0: 0.06076  loss_mask_0: 0.2356  loss_dice_0: 0.1461  loss_ce_1: 0.1247  loss_mask_1: 0.2325  loss_dice_1: 0.1392  loss_ce_2: 0.1255  loss_mask_2: 0.2392  loss_dice_2: 0.1524  loss_ce_3: 0.126  loss_mask_3: 0.2225  loss_dice_3: 0.1419  loss_ce_4: 0.1266  loss_mask_4: 0.2285  loss_dice_4: 0.1439  loss_ce_5: 0.1253  loss_mask_5: 0.229  loss_dice_5: 0.1463  loss_ce_6: 0.1263  loss_mask_6: 0.2311  loss_dice_6: 0.1458  loss_ce_7: 0.1248  loss_mask_7: 0.2341  loss_dice_7: 0.1484  loss_ce_8: 0.1248  loss_mask_8: 0.223  loss_dice_8: 0.1441  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:15] d2.utils.events INFO:  eta: 1:36:16  iter: 4199  total_loss: 5.379  loss_ce: 0.1147  loss_mask: 0.1772  loss_dice: 0.1844  loss_ce_0: 0.06076  loss_mask_0: 0.1589  loss_dice_0: 0.1898  loss_ce_1: 0.1141  loss_mask_1: 0.1677  loss_dice_1: 0.1916  loss_ce_2: 0.1144  loss_mask_2: 0.1753  loss_dice_2: 0.1809  loss_ce_3: 0.1144  loss_mask_3: 0.167  loss_dice_3: 0.1866  loss_ce_4: 0.1142  loss_mask_4: 0.1612  loss_dice_4: 0.1877  loss_ce_5: 0.1143  loss_mask_5: 0.1612  loss_dice_5: 0.1883  loss_ce_6: 0.1142  loss_mask_6: 0.1685  loss_dice_6: 0.1892  loss_ce_7: 0.1144  loss_mask_7: 0.1629  loss_dice_7: 0.1827  loss_ce_8: 0.1148  loss_mask_8: 0.1707  loss_dice_8: 0.1832  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:18] d2.utils.events INFO:  eta: 1:36:14  iter: 4219  total_loss: 4.5  loss_ce: 0.1352  loss_mask: 0.201  loss_dice: 0.1222  loss_ce_0: 0.07613  loss_mask_0: 0.1976  loss_dice_0: 0.1203  loss_ce_1: 0.1343  loss_mask_1: 0.2026  loss_dice_1: 0.1197  loss_ce_2: 0.1338  loss_mask_2: 0.2043  loss_dice_2: 0.1232  loss_ce_3: 0.1336  loss_mask_3: 0.1974  loss_dice_3: 0.1196  loss_ce_4: 0.1333  loss_mask_4: 0.2019  loss_dice_4: 0.1167  loss_ce_5: 0.1338  loss_mask_5: 0.2044  loss_dice_5: 0.1223  loss_ce_6: 0.1338  loss_mask_6: 0.1995  loss_dice_6: 0.1156  loss_ce_7: 0.1343  loss_mask_7: 0.1934  loss_dice_7: 0.1211  loss_ce_8: 0.1344  loss_mask_8: 0.2004  loss_dice_8: 0.1155  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:20] d2.utils.events INFO:  eta: 1:36:11  iter: 4239  total_loss: 5.148  loss_ce: 0.1279  loss_mask: 0.1928  loss_dice: 0.1662  loss_ce_0: 0.0633  loss_mask_0: 0.1949  loss_dice_0: 0.1717  loss_ce_1: 0.1279  loss_mask_1: 0.1856  loss_dice_1: 0.1632  loss_ce_2: 0.1283  loss_mask_2: 0.1886  loss_dice_2: 0.1604  loss_ce_3: 0.1285  loss_mask_3: 0.1871  loss_dice_3: 0.1715  loss_ce_4: 0.1287  loss_mask_4: 0.1887  loss_dice_4: 0.1559  loss_ce_5: 0.1283  loss_mask_5: 0.1929  loss_dice_5: 0.1648  loss_ce_6: 0.1282  loss_mask_6: 0.1795  loss_dice_6: 0.1621  loss_ce_7: 0.1282  loss_mask_7: 0.1862  loss_dice_7: 0.1609  loss_ce_8: 0.1279  loss_mask_8: 0.1907  loss_dice_8: 0.1654  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:23] d2.utils.events INFO:  eta: 1:36:07  iter: 4259  total_loss: 4.859  loss_ce: 0.116  loss_mask: 0.2379  loss_dice: 0.1242  loss_ce_0: 0.06263  loss_mask_0: 0.2379  loss_dice_0: 0.123  loss_ce_1: 0.1151  loss_mask_1: 0.2359  loss_dice_1: 0.1243  loss_ce_2: 0.1151  loss_mask_2: 0.2362  loss_dice_2: 0.1238  loss_ce_3: 0.1151  loss_mask_3: 0.238  loss_dice_3: 0.1292  loss_ce_4: 0.1149  loss_mask_4: 0.235  loss_dice_4: 0.1201  loss_ce_5: 0.1152  loss_mask_5: 0.2333  loss_dice_5: 0.1252  loss_ce_6: 0.115  loss_mask_6: 0.2255  loss_dice_6: 0.1195  loss_ce_7: 0.1154  loss_mask_7: 0.2464  loss_dice_7: 0.125  loss_ce_8: 0.1157  loss_mask_8: 0.2299  loss_dice_8: 0.117  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:25] d2.utils.events INFO:  eta: 1:36:04  iter: 4279  total_loss: 5.318  loss_ce: 0.09543  loss_mask: 0.2265  loss_dice: 0.1603  loss_ce_0: 0.06062  loss_mask_0: 0.2304  loss_dice_0: 0.1581  loss_ce_1: 0.0932  loss_mask_1: 0.2136  loss_dice_1: 0.1525  loss_ce_2: 0.09274  loss_mask_2: 0.2258  loss_dice_2: 0.1659  loss_ce_3: 0.09209  loss_mask_3: 0.2251  loss_dice_3: 0.1583  loss_ce_4: 0.09176  loss_mask_4: 0.2286  loss_dice_4: 0.1571  loss_ce_5: 0.09258  loss_mask_5: 0.2271  loss_dice_5: 0.1591  loss_ce_6: 0.09202  loss_mask_6: 0.2178  loss_dice_6: 0.1614  loss_ce_7: 0.09366  loss_mask_7: 0.2228  loss_dice_7: 0.157  loss_ce_8: 0.09398  loss_mask_8: 0.2274  loss_dice_8: 0.1651  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:28] d2.utils.events INFO:  eta: 1:36:02  iter: 4299  total_loss: 5.272  loss_ce: 0.1666  loss_mask: 0.1562  loss_dice: 0.182  loss_ce_0: 0.0788  loss_mask_0: 0.148  loss_dice_0: 0.1675  loss_ce_1: 0.1667  loss_mask_1: 0.1601  loss_dice_1: 0.1633  loss_ce_2: 0.1656  loss_mask_2: 0.1463  loss_dice_2: 0.1747  loss_ce_3: 0.1672  loss_mask_3: 0.1506  loss_dice_3: 0.167  loss_ce_4: 0.1671  loss_mask_4: 0.1529  loss_dice_4: 0.1748  loss_ce_5: 0.1658  loss_mask_5: 0.1539  loss_dice_5: 0.176  loss_ce_6: 0.1676  loss_mask_6: 0.1527  loss_dice_6: 0.1717  loss_ce_7: 0.1674  loss_mask_7: 0.1469  loss_dice_7: 0.1667  loss_ce_8: 0.1664  loss_mask_8: 0.1546  loss_dice_8: 0.1681  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:30] d2.utils.events INFO:  eta: 1:36:01  iter: 4319  total_loss: 4.921  loss_ce: 0.1004  loss_mask: 0.2171  loss_dice: 0.1615  loss_ce_0: 0.05928  loss_mask_0: 0.2189  loss_dice_0: 0.1598  loss_ce_1: 0.1011  loss_mask_1: 0.2135  loss_dice_1: 0.1594  loss_ce_2: 0.101  loss_mask_2: 0.2202  loss_dice_2: 0.1556  loss_ce_3: 0.1011  loss_mask_3: 0.2168  loss_dice_3: 0.1511  loss_ce_4: 0.1015  loss_mask_4: 0.2244  loss_dice_4: 0.1618  loss_ce_5: 0.1009  loss_mask_5: 0.222  loss_dice_5: 0.164  loss_ce_6: 0.1008  loss_mask_6: 0.2062  loss_dice_6: 0.1537  loss_ce_7: 0.1008  loss_mask_7: 0.2236  loss_dice_7: 0.1596  loss_ce_8: 0.1004  loss_mask_8: 0.2214  loss_dice_8: 0.1647  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:33] d2.utils.events INFO:  eta: 1:35:59  iter: 4339  total_loss: 5.289  loss_ce: 0.09536  loss_mask: 0.2145  loss_dice: 0.1751  loss_ce_0: 0.05821  loss_mask_0: 0.2222  loss_dice_0: 0.1811  loss_ce_1: 0.09503  loss_mask_1: 0.223  loss_dice_1: 0.1762  loss_ce_2: 0.09522  loss_mask_2: 0.2164  loss_dice_2: 0.1772  loss_ce_3: 0.09499  loss_mask_3: 0.214  loss_dice_3: 0.1772  loss_ce_4: 0.09548  loss_mask_4: 0.2233  loss_dice_4: 0.1704  loss_ce_5: 0.09509  loss_mask_5: 0.2037  loss_dice_5: 0.1749  loss_ce_6: 0.09488  loss_mask_6: 0.2215  loss_dice_6: 0.176  loss_ce_7: 0.09521  loss_mask_7: 0.2124  loss_dice_7: 0.1752  loss_ce_8: 0.09517  loss_mask_8: 0.2234  loss_dice_8: 0.1793  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:35] d2.utils.events INFO:  eta: 1:35:57  iter: 4359  total_loss: 5.188  loss_ce: 0.08497  loss_mask: 0.2572  loss_dice: 0.1506  loss_ce_0: 0.05618  loss_mask_0: 0.2438  loss_dice_0: 0.1495  loss_ce_1: 0.08401  loss_mask_1: 0.2481  loss_dice_1: 0.1531  loss_ce_2: 0.08414  loss_mask_2: 0.2551  loss_dice_2: 0.1555  loss_ce_3: 0.08377  loss_mask_3: 0.2547  loss_dice_3: 0.1543  loss_ce_4: 0.08396  loss_mask_4: 0.2559  loss_dice_4: 0.1553  loss_ce_5: 0.0839  loss_mask_5: 0.2507  loss_dice_5: 0.1506  loss_ce_6: 0.08374  loss_mask_6: 0.252  loss_dice_6: 0.1564  loss_ce_7: 0.08397  loss_mask_7: 0.2617  loss_dice_7: 0.1581  loss_ce_8: 0.08434  loss_mask_8: 0.2555  loss_dice_8: 0.1535  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:38] d2.utils.events INFO:  eta: 1:35:54  iter: 4379  total_loss: 5.338  loss_ce: 0.2018  loss_mask: 0.1977  loss_dice: 0.1546  loss_ce_0: 0.08471  loss_mask_0: 0.1989  loss_dice_0: 0.1545  loss_ce_1: 0.2023  loss_mask_1: 0.202  loss_dice_1: 0.1598  loss_ce_2: 0.2037  loss_mask_2: 0.1916  loss_dice_2: 0.1584  loss_ce_3: 0.2048  loss_mask_3: 0.1947  loss_dice_3: 0.1619  loss_ce_4: 0.204  loss_mask_4: 0.1959  loss_dice_4: 0.1576  loss_ce_5: 0.2042  loss_mask_5: 0.2056  loss_dice_5: 0.1567  loss_ce_6: 0.2052  loss_mask_6: 0.1997  loss_dice_6: 0.1612  loss_ce_7: 0.2033  loss_mask_7: 0.2004  loss_dice_7: 0.1654  loss_ce_8: 0.2031  loss_mask_8: 0.1982  loss_dice_8: 0.1641  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:41] d2.utils.events INFO:  eta: 1:35:51  iter: 4399  total_loss: 5.245  loss_ce: 0.1656  loss_mask: 0.2103  loss_dice: 0.1691  loss_ce_0: 0.08529  loss_mask_0: 0.2116  loss_dice_0: 0.1762  loss_ce_1: 0.1632  loss_mask_1: 0.2126  loss_dice_1: 0.1692  loss_ce_2: 0.163  loss_mask_2: 0.2077  loss_dice_2: 0.1658  loss_ce_3: 0.1621  loss_mask_3: 0.2118  loss_dice_3: 0.1715  loss_ce_4: 0.1616  loss_mask_4: 0.2062  loss_dice_4: 0.165  loss_ce_5: 0.1632  loss_mask_5: 0.2151  loss_dice_5: 0.1814  loss_ce_6: 0.1629  loss_mask_6: 0.2155  loss_dice_6: 0.1712  loss_ce_7: 0.1634  loss_mask_7: 0.2057  loss_dice_7: 0.1704  loss_ce_8: 0.1641  loss_mask_8: 0.2147  loss_dice_8: 0.1711  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:43] d2.utils.events INFO:  eta: 1:35:48  iter: 4419  total_loss: 5.031  loss_ce: 0.1272  loss_mask: 0.1954  loss_dice: 0.1509  loss_ce_0: 0.06912  loss_mask_0: 0.1929  loss_dice_0: 0.1513  loss_ce_1: 0.1276  loss_mask_1: 0.1951  loss_dice_1: 0.151  loss_ce_2: 0.1271  loss_mask_2: 0.1982  loss_dice_2: 0.1506  loss_ce_3: 0.127  loss_mask_3: 0.2055  loss_dice_3: 0.1498  loss_ce_4: 0.1278  loss_mask_4: 0.1987  loss_dice_4: 0.148  loss_ce_5: 0.127  loss_mask_5: 0.196  loss_dice_5: 0.1527  loss_ce_6: 0.127  loss_mask_6: 0.2  loss_dice_6: 0.1463  loss_ce_7: 0.127  loss_mask_7: 0.1963  loss_dice_7: 0.1483  loss_ce_8: 0.1271  loss_mask_8: 0.197  loss_dice_8: 0.1518  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:46] d2.utils.events INFO:  eta: 1:35:44  iter: 4439  total_loss: 5.08  loss_ce: 0.1303  loss_mask: 0.1948  loss_dice: 0.1623  loss_ce_0: 0.06858  loss_mask_0: 0.1969  loss_dice_0: 0.1639  loss_ce_1: 0.1302  loss_mask_1: 0.1888  loss_dice_1: 0.1647  loss_ce_2: 0.1299  loss_mask_2: 0.1954  loss_dice_2: 0.1608  loss_ce_3: 0.1283  loss_mask_3: 0.1941  loss_dice_3: 0.1632  loss_ce_4: 0.1271  loss_mask_4: 0.1974  loss_dice_4: 0.1583  loss_ce_5: 0.1295  loss_mask_5: 0.194  loss_dice_5: 0.1579  loss_ce_6: 0.129  loss_mask_6: 0.1955  loss_dice_6: 0.1701  loss_ce_7: 0.1303  loss_mask_7: 0.1897  loss_dice_7: 0.1623  loss_ce_8: 0.1301  loss_mask_8: 0.186  loss_dice_8: 0.1518  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:48] d2.utils.events INFO:  eta: 1:35:43  iter: 4459  total_loss: 5.044  loss_ce: 0.1288  loss_mask: 0.1855  loss_dice: 0.1551  loss_ce_0: 0.06018  loss_mask_0: 0.1793  loss_dice_0: 0.1478  loss_ce_1: 0.1291  loss_mask_1: 0.1979  loss_dice_1: 0.1574  loss_ce_2: 0.1291  loss_mask_2: 0.1806  loss_dice_2: 0.1529  loss_ce_3: 0.1292  loss_mask_3: 0.1969  loss_dice_3: 0.1533  loss_ce_4: 0.1294  loss_mask_4: 0.1877  loss_dice_4: 0.1459  loss_ce_5: 0.129  loss_mask_5: 0.1882  loss_dice_5: 0.152  loss_ce_6: 0.1292  loss_mask_6: 0.19  loss_dice_6: 0.156  loss_ce_7: 0.1291  loss_mask_7: 0.1997  loss_dice_7: 0.1509  loss_ce_8: 0.129  loss_mask_8: 0.1929  loss_dice_8: 0.1529  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:51] d2.utils.events INFO:  eta: 1:35:41  iter: 4479  total_loss: 5.311  loss_ce: 0.104  loss_mask: 0.1993  loss_dice: 0.1996  loss_ce_0: 0.05926  loss_mask_0: 0.1949  loss_dice_0: 0.196  loss_ce_1: 0.1013  loss_mask_1: 0.1891  loss_dice_1: 0.1891  loss_ce_2: 0.1012  loss_mask_2: 0.1883  loss_dice_2: 0.1994  loss_ce_3: 0.1001  loss_mask_3: 0.1872  loss_dice_3: 0.1962  loss_ce_4: 0.09824  loss_mask_4: 0.1897  loss_dice_4: 0.1929  loss_ce_5: 0.1006  loss_mask_5: 0.1966  loss_dice_5: 0.2017  loss_ce_6: 0.1008  loss_mask_6: 0.1886  loss_dice_6: 0.1954  loss_ce_7: 0.1017  loss_mask_7: 0.1928  loss_dice_7: 0.1916  loss_ce_8: 0.1027  loss_mask_8: 0.1899  loss_dice_8: 0.1904  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:53] d2.utils.events INFO:  eta: 1:35:38  iter: 4499  total_loss: 5.054  loss_ce: 0.129  loss_mask: 0.232  loss_dice: 0.1245  loss_ce_0: 0.06878  loss_mask_0: 0.2321  loss_dice_0: 0.1272  loss_ce_1: 0.129  loss_mask_1: 0.2365  loss_dice_1: 0.1291  loss_ce_2: 0.1291  loss_mask_2: 0.2321  loss_dice_2: 0.1256  loss_ce_3: 0.1291  loss_mask_3: 0.2408  loss_dice_3: 0.1256  loss_ce_4: 0.1291  loss_mask_4: 0.2358  loss_dice_4: 0.1278  loss_ce_5: 0.129  loss_mask_5: 0.234  loss_dice_5: 0.1274  loss_ce_6: 0.1291  loss_mask_6: 0.2271  loss_dice_6: 0.1266  loss_ce_7: 0.129  loss_mask_7: 0.2349  loss_dice_7: 0.1256  loss_ce_8: 0.1291  loss_mask_8: 0.2434  loss_dice_8: 0.1329  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:41:56] d2.utils.events INFO:  eta: 1:35:36  iter: 4519  total_loss: 5.004  loss_ce: 0.1286  loss_mask: 0.2283  loss_dice: 0.1262  loss_ce_0: 0.0685  loss_mask_0: 0.2251  loss_dice_0: 0.1258  loss_ce_1: 0.1278  loss_mask_1: 0.2352  loss_dice_1: 0.1237  loss_ce_2: 0.1278  loss_mask_2: 0.2327  loss_dice_2: 0.129  loss_ce_3: 0.1286  loss_mask_3: 0.2322  loss_dice_3: 0.1254  loss_ce_4: 0.1291  loss_mask_4: 0.2378  loss_dice_4: 0.1202  loss_ce_5: 0.128  loss_mask_5: 0.2326  loss_dice_5: 0.1265  loss_ce_6: 0.1278  loss_mask_6: 0.2424  loss_dice_6: 0.1215  loss_ce_7: 0.1277  loss_mask_7: 0.2505  loss_dice_7: 0.1281  loss_ce_8: 0.1278  loss_mask_8: 0.2318  loss_dice_8: 0.1253  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:41:58] d2.utils.events INFO:  eta: 1:35:33  iter: 4539  total_loss: 5.07  loss_ce: 0.127  loss_mask: 0.211  loss_dice: 0.1644  loss_ce_0: 0.06269  loss_mask_0: 0.2131  loss_dice_0: 0.1646  loss_ce_1: 0.1269  loss_mask_1: 0.2129  loss_dice_1: 0.1602  loss_ce_2: 0.1269  loss_mask_2: 0.1975  loss_dice_2: 0.1591  loss_ce_3: 0.1269  loss_mask_3: 0.2097  loss_dice_3: 0.1699  loss_ce_4: 0.1271  loss_mask_4: 0.2095  loss_dice_4: 0.1672  loss_ce_5: 0.1268  loss_mask_5: 0.2033  loss_dice_5: 0.1661  loss_ce_6: 0.127  loss_mask_6: 0.2107  loss_dice_6: 0.1615  loss_ce_7: 0.1269  loss_mask_7: 0.2013  loss_dice_7: 0.1638  loss_ce_8: 0.127  loss_mask_8: 0.2033  loss_dice_8: 0.1607  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:01] d2.utils.events INFO:  eta: 1:35:31  iter: 4559  total_loss: 4.987  loss_ce: 0.1385  loss_mask: 0.2228  loss_dice: 0.1314  loss_ce_0: 0.07445  loss_mask_0: 0.2187  loss_dice_0: 0.1307  loss_ce_1: 0.14  loss_mask_1: 0.2232  loss_dice_1: 0.1363  loss_ce_2: 0.1397  loss_mask_2: 0.2243  loss_dice_2: 0.1333  loss_ce_3: 0.1399  loss_mask_3: 0.2327  loss_dice_3: 0.1323  loss_ce_4: 0.1412  loss_mask_4: 0.2217  loss_dice_4: 0.1303  loss_ce_5: 0.1399  loss_mask_5: 0.2154  loss_dice_5: 0.1292  loss_ce_6: 0.1398  loss_mask_6: 0.2161  loss_dice_6: 0.1302  loss_ce_7: 0.1396  loss_mask_7: 0.2334  loss_dice_7: 0.1338  loss_ce_8: 0.1389  loss_mask_8: 0.2196  loss_dice_8: 0.135  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:42:03] d2.utils.events INFO:  eta: 1:35:28  iter: 4579  total_loss: 4.855  loss_ce: 0.134  loss_mask: 0.1946  loss_dice: 0.1493  loss_ce_0: 0.06815  loss_mask_0: 0.1954  loss_dice_0: 0.1548  loss_ce_1: 0.1342  loss_mask_1: 0.2001  loss_dice_1: 0.1448  loss_ce_2: 0.134  loss_mask_2: 0.2006  loss_dice_2: 0.1481  loss_ce_3: 0.134  loss_mask_3: 0.1904  loss_dice_3: 0.1461  loss_ce_4: 0.1337  loss_mask_4: 0.1994  loss_dice_4: 0.1551  loss_ce_5: 0.1342  loss_mask_5: 0.1965  loss_dice_5: 0.1546  loss_ce_6: 0.134  loss_mask_6: 0.1968  loss_dice_6: 0.1492  loss_ce_7: 0.1342  loss_mask_7: 0.1976  loss_dice_7: 0.15  loss_ce_8: 0.1341  loss_mask_8: 0.2006  loss_dice_8: 0.1571  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:06] d2.utils.events INFO:  eta: 1:35:24  iter: 4599  total_loss: 5.283  loss_ce: 0.1124  loss_mask: 0.1963  loss_dice: 0.1615  loss_ce_0: 0.0683  loss_mask_0: 0.1854  loss_dice_0: 0.1649  loss_ce_1: 0.1104  loss_mask_1: 0.1885  loss_dice_1: 0.1571  loss_ce_2: 0.1097  loss_mask_2: 0.2017  loss_dice_2: 0.1712  loss_ce_3: 0.1091  loss_mask_3: 0.1853  loss_dice_3: 0.1698  loss_ce_4: 0.1082  loss_mask_4: 0.2009  loss_dice_4: 0.1683  loss_ce_5: 0.1096  loss_mask_5: 0.1936  loss_dice_5: 0.1785  loss_ce_6: 0.1093  loss_mask_6: 0.1906  loss_dice_6: 0.1708  loss_ce_7: 0.1105  loss_mask_7: 0.19  loss_dice_7: 0.1721  loss_ce_8: 0.1107  loss_mask_8: 0.1972  loss_dice_8: 0.1694  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:08] d2.utils.events INFO:  eta: 1:35:21  iter: 4619  total_loss: 5.432  loss_ce: 0.1281  loss_mask: 0.2317  loss_dice: 0.1759  loss_ce_0: 0.06791  loss_mask_0: 0.2215  loss_dice_0: 0.1754  loss_ce_1: 0.1283  loss_mask_1: 0.2226  loss_dice_1: 0.179  loss_ce_2: 0.1281  loss_mask_2: 0.2357  loss_dice_2: 0.1784  loss_ce_3: 0.1281  loss_mask_3: 0.2268  loss_dice_3: 0.1922  loss_ce_4: 0.1282  loss_mask_4: 0.2325  loss_dice_4: 0.1746  loss_ce_5: 0.1282  loss_mask_5: 0.2213  loss_dice_5: 0.1779  loss_ce_6: 0.1282  loss_mask_6: 0.2162  loss_dice_6: 0.1753  loss_ce_7: 0.1282  loss_mask_7: 0.2182  loss_dice_7: 0.1781  loss_ce_8: 0.1281  loss_mask_8: 0.2178  loss_dice_8: 0.1811  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:11] d2.utils.events INFO:  eta: 1:35:18  iter: 4639  total_loss: 5.377  loss_ce: 0.1378  loss_mask: 0.2281  loss_dice: 0.1762  loss_ce_0: 0.06847  loss_mask_0: 0.2312  loss_dice_0: 0.1726  loss_ce_1: 0.1372  loss_mask_1: 0.2311  loss_dice_1: 0.168  loss_ce_2: 0.1371  loss_mask_2: 0.225  loss_dice_2: 0.1769  loss_ce_3: 0.1366  loss_mask_3: 0.2209  loss_dice_3: 0.1648  loss_ce_4: 0.1362  loss_mask_4: 0.2296  loss_dice_4: 0.1714  loss_ce_5: 0.1368  loss_mask_5: 0.2349  loss_dice_5: 0.1677  loss_ce_6: 0.137  loss_mask_6: 0.238  loss_dice_6: 0.1683  loss_ce_7: 0.1373  loss_mask_7: 0.2314  loss_dice_7: 0.1705  loss_ce_8: 0.1377  loss_mask_8: 0.2395  loss_dice_8: 0.1687  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:42:13] d2.utils.events INFO:  eta: 1:35:17  iter: 4659  total_loss: 5.244  loss_ce: 0.123  loss_mask: 0.1563  loss_dice: 0.1677  loss_ce_0: 0.06741  loss_mask_0: 0.1635  loss_dice_0: 0.1765  loss_ce_1: 0.1217  loss_mask_1: 0.1733  loss_dice_1: 0.1835  loss_ce_2: 0.1218  loss_mask_2: 0.1734  loss_dice_2: 0.1749  loss_ce_3: 0.1202  loss_mask_3: 0.1595  loss_dice_3: 0.1746  loss_ce_4: 0.1195  loss_mask_4: 0.1623  loss_dice_4: 0.1765  loss_ce_5: 0.1211  loss_mask_5: 0.1627  loss_dice_5: 0.1692  loss_ce_6: 0.1207  loss_mask_6: 0.1659  loss_dice_6: 0.1729  loss_ce_7: 0.1219  loss_mask_7: 0.1634  loss_dice_7: 0.177  loss_ce_8: 0.1224  loss_mask_8: 0.1595  loss_dice_8: 0.1704  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:42:16] d2.utils.events INFO:  eta: 1:35:15  iter: 4679  total_loss: 5.098  loss_ce: 0.1158  loss_mask: 0.2234  loss_dice: 0.1425  loss_ce_0: 0.0664  loss_mask_0: 0.2283  loss_dice_0: 0.1428  loss_ce_1: 0.1146  loss_mask_1: 0.2105  loss_dice_1: 0.1384  loss_ce_2: 0.1142  loss_mask_2: 0.2158  loss_dice_2: 0.142  loss_ce_3: 0.1139  loss_mask_3: 0.2305  loss_dice_3: 0.142  loss_ce_4: 0.113  loss_mask_4: 0.2165  loss_dice_4: 0.1412  loss_ce_5: 0.1139  loss_mask_5: 0.2152  loss_dice_5: 0.1426  loss_ce_6: 0.1137  loss_mask_6: 0.2242  loss_dice_6: 0.1411  loss_ce_7: 0.1147  loss_mask_7: 0.2233  loss_dice_7: 0.1405  loss_ce_8: 0.1148  loss_mask_8: 0.2269  loss_dice_8: 0.1413  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:42:18] d2.utils.events INFO:  eta: 1:35:11  iter: 4699  total_loss: 5.443  loss_ce: 0.1111  loss_mask: 0.2189  loss_dice: 0.1772  loss_ce_0: 0.06486  loss_mask_0: 0.2304  loss_dice_0: 0.1757  loss_ce_1: 0.1105  loss_mask_1: 0.2351  loss_dice_1: 0.1743  loss_ce_2: 0.1105  loss_mask_2: 0.2344  loss_dice_2: 0.1775  loss_ce_3: 0.1104  loss_mask_3: 0.2333  loss_dice_3: 0.1784  loss_ce_4: 0.11  loss_mask_4: 0.2353  loss_dice_4: 0.1811  loss_ce_5: 0.1102  loss_mask_5: 0.2289  loss_dice_5: 0.1722  loss_ce_6: 0.1099  loss_mask_6: 0.2225  loss_dice_6: 0.1736  loss_ce_7: 0.1104  loss_mask_7: 0.2342  loss_dice_7: 0.1786  loss_ce_8: 0.1104  loss_mask_8: 0.2382  loss_dice_8: 0.1734  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:21] d2.utils.events INFO:  eta: 1:35:09  iter: 4719  total_loss: 5.212  loss_ce: 0.1466  loss_mask: 0.2358  loss_dice: 0.1336  loss_ce_0: 0.07253  loss_mask_0: 0.2407  loss_dice_0: 0.1361  loss_ce_1: 0.1464  loss_mask_1: 0.2376  loss_dice_1: 0.1361  loss_ce_2: 0.1461  loss_mask_2: 0.2455  loss_dice_2: 0.1392  loss_ce_3: 0.1461  loss_mask_3: 0.2421  loss_dice_3: 0.1389  loss_ce_4: 0.1458  loss_mask_4: 0.2471  loss_dice_4: 0.1395  loss_ce_5: 0.1461  loss_mask_5: 0.2521  loss_dice_5: 0.1314  loss_ce_6: 0.1467  loss_mask_6: 0.2418  loss_dice_6: 0.1354  loss_ce_7: 0.1465  loss_mask_7: 0.24  loss_dice_7: 0.1328  loss_ce_8: 0.1466  loss_mask_8: 0.2415  loss_dice_8: 0.1321  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:24] d2.utils.events INFO:  eta: 1:35:08  iter: 4739  total_loss: 5.29  loss_ce: 0.1263  loss_mask: 0.2056  loss_dice: 0.1991  loss_ce_0: 0.06442  loss_mask_0: 0.2041  loss_dice_0: 0.1918  loss_ce_1: 0.1273  loss_mask_1: 0.2146  loss_dice_1: 0.2045  loss_ce_2: 0.1279  loss_mask_2: 0.203  loss_dice_2: 0.1996  loss_ce_3: 0.1285  loss_mask_3: 0.2167  loss_dice_3: 0.2032  loss_ce_4: 0.1285  loss_mask_4: 0.193  loss_dice_4: 0.1952  loss_ce_5: 0.1281  loss_mask_5: 0.2039  loss_dice_5: 0.1989  loss_ce_6: 0.1281  loss_mask_6: 0.2112  loss_dice_6: 0.1983  loss_ce_7: 0.1275  loss_mask_7: 0.2177  loss_dice_7: 0.2017  loss_ce_8: 0.1276  loss_mask_8: 0.2224  loss_dice_8: 0.1992  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:42:26] d2.utils.events INFO:  eta: 1:35:05  iter: 4759  total_loss: 5.731  loss_ce: 0.1313  loss_mask: 0.2059  loss_dice: 0.1897  loss_ce_0: 0.06759  loss_mask_0: 0.1923  loss_dice_0: 0.1759  loss_ce_1: 0.1317  loss_mask_1: 0.1924  loss_dice_1: 0.1849  loss_ce_2: 0.1317  loss_mask_2: 0.1829  loss_dice_2: 0.1855  loss_ce_3: 0.1319  loss_mask_3: 0.1924  loss_dice_3: 0.1826  loss_ce_4: 0.1321  loss_mask_4: 0.2004  loss_dice_4: 0.1835  loss_ce_5: 0.1318  loss_mask_5: 0.2021  loss_dice_5: 0.1864  loss_ce_6: 0.1319  loss_mask_6: 0.1928  loss_dice_6: 0.1772  loss_ce_7: 0.1316  loss_mask_7: 0.1977  loss_dice_7: 0.1846  loss_ce_8: 0.1317  loss_mask_8: 0.195  loss_dice_8: 0.1824  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:29] d2.utils.events INFO:  eta: 1:35:01  iter: 4779  total_loss: 5.325  loss_ce: 0.1117  loss_mask: 0.2106  loss_dice: 0.1491  loss_ce_0: 0.06767  loss_mask_0: 0.205  loss_dice_0: 0.1434  loss_ce_1: 0.1111  loss_mask_1: 0.215  loss_dice_1: 0.1451  loss_ce_2: 0.1104  loss_mask_2: 0.2095  loss_dice_2: 0.1362  loss_ce_3: 0.1098  loss_mask_3: 0.2165  loss_dice_3: 0.1427  loss_ce_4: 0.109  loss_mask_4: 0.2122  loss_dice_4: 0.144  loss_ce_5: 0.1102  loss_mask_5: 0.2135  loss_dice_5: 0.1413  loss_ce_6: 0.1098  loss_mask_6: 0.2088  loss_dice_6: 0.1466  loss_ce_7: 0.1109  loss_mask_7: 0.2062  loss_dice_7: 0.1431  loss_ce_8: 0.1104  loss_mask_8: 0.2006  loss_dice_8: 0.1375  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:31] d2.utils.events INFO:  eta: 1:34:58  iter: 4799  total_loss: 4.949  loss_ce: 0.1288  loss_mask: 0.2268  loss_dice: 0.12  loss_ce_0: 0.0676  loss_mask_0: 0.2245  loss_dice_0: 0.1222  loss_ce_1: 0.1288  loss_mask_1: 0.2345  loss_dice_1: 0.1218  loss_ce_2: 0.1288  loss_mask_2: 0.2301  loss_dice_2: 0.1199  loss_ce_3: 0.1288  loss_mask_3: 0.2171  loss_dice_3: 0.1196  loss_ce_4: 0.1289  loss_mask_4: 0.2301  loss_dice_4: 0.1208  loss_ce_5: 0.1288  loss_mask_5: 0.2318  loss_dice_5: 0.1243  loss_ce_6: 0.1288  loss_mask_6: 0.2204  loss_dice_6: 0.1206  loss_ce_7: 0.1287  loss_mask_7: 0.2449  loss_dice_7: 0.127  loss_ce_8: 0.1288  loss_mask_8: 0.2164  loss_dice_8: 0.1234  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:34] d2.utils.events INFO:  eta: 1:34:55  iter: 4819  total_loss: 5.372  loss_ce: 0.1322  loss_mask: 0.1942  loss_dice: 0.2171  loss_ce_0: 0.06854  loss_mask_0: 0.1845  loss_dice_0: 0.1981  loss_ce_1: 0.1317  loss_mask_1: 0.1886  loss_dice_1: 0.2005  loss_ce_2: 0.1316  loss_mask_2: 0.1979  loss_dice_2: 0.1989  loss_ce_3: 0.1311  loss_mask_3: 0.182  loss_dice_3: 0.2013  loss_ce_4: 0.131  loss_mask_4: 0.182  loss_dice_4: 0.2037  loss_ce_5: 0.1312  loss_mask_5: 0.1912  loss_dice_5: 0.2023  loss_ce_6: 0.1313  loss_mask_6: 0.1913  loss_dice_6: 0.2095  loss_ce_7: 0.1314  loss_mask_7: 0.1994  loss_dice_7: 0.2006  loss_ce_8: 0.1319  loss_mask_8: 0.1911  loss_dice_8: 0.2075  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:42:36] d2.utils.events INFO:  eta: 1:34:55  iter: 4839  total_loss: 5.274  loss_ce: 0.1383  loss_mask: 0.152  loss_dice: 0.1964  loss_ce_0: 0.06793  loss_mask_0: 0.1507  loss_dice_0: 0.1929  loss_ce_1: 0.14  loss_mask_1: 0.1649  loss_dice_1: 0.211  loss_ce_2: 0.1405  loss_mask_2: 0.1487  loss_dice_2: 0.1933  loss_ce_3: 0.1415  loss_mask_3: 0.155  loss_dice_3: 0.1859  loss_ce_4: 0.1421  loss_mask_4: 0.1529  loss_dice_4: 0.2101  loss_ce_5: 0.1409  loss_mask_5: 0.1537  loss_dice_5: 0.2044  loss_ce_6: 0.1413  loss_mask_6: 0.1499  loss_dice_6: 0.1987  loss_ce_7: 0.1401  loss_mask_7: 0.1569  loss_dice_7: 0.2112  loss_ce_8: 0.1398  loss_mask_8: 0.156  loss_dice_8: 0.1947  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:39] d2.utils.events INFO:  eta: 1:34:53  iter: 4859  total_loss: 5.046  loss_ce: 0.1147  loss_mask: 0.2119  loss_dice: 0.1595  loss_ce_0: 0.06598  loss_mask_0: 0.2039  loss_dice_0: 0.1589  loss_ce_1: 0.1141  loss_mask_1: 0.2072  loss_dice_1: 0.1641  loss_ce_2: 0.1139  loss_mask_2: 0.2042  loss_dice_2: 0.1599  loss_ce_3: 0.1134  loss_mask_3: 0.1934  loss_dice_3: 0.1562  loss_ce_4: 0.1133  loss_mask_4: 0.197  loss_dice_4: 0.1533  loss_ce_5: 0.1137  loss_mask_5: 0.2061  loss_dice_5: 0.1653  loss_ce_6: 0.1134  loss_mask_6: 0.2033  loss_dice_6: 0.1574  loss_ce_7: 0.1142  loss_mask_7: 0.2103  loss_dice_7: 0.1679  loss_ce_8: 0.114  loss_mask_8: 0.2087  loss_dice_8: 0.1618  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:41] d2.utils.events INFO:  eta: 1:34:51  iter: 4879  total_loss: 5.727  loss_ce: 0.1038  loss_mask: 0.188  loss_dice: 0.231  loss_ce_0: 0.06339  loss_mask_0: 0.1934  loss_dice_0: 0.2206  loss_ce_1: 0.103  loss_mask_1: 0.1901  loss_dice_1: 0.2506  loss_ce_2: 0.1031  loss_mask_2: 0.1839  loss_dice_2: 0.2366  loss_ce_3: 0.1027  loss_mask_3: 0.2058  loss_dice_3: 0.2418  loss_ce_4: 0.1025  loss_mask_4: 0.1885  loss_dice_4: 0.2322  loss_ce_5: 0.1029  loss_mask_5: 0.1923  loss_dice_5: 0.229  loss_ce_6: 0.1026  loss_mask_6: 0.1921  loss_dice_6: 0.2309  loss_ce_7: 0.1029  loss_mask_7: 0.2016  loss_dice_7: 0.2401  loss_ce_8: 0.1028  loss_mask_8: 0.1913  loss_dice_8: 0.2352  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:44] d2.utils.events INFO:  eta: 1:34:48  iter: 4899  total_loss: 4.949  loss_ce: 0.1467  loss_mask: 0.1989  loss_dice: 0.1613  loss_ce_0: 0.07273  loss_mask_0: 0.1875  loss_dice_0: 0.1665  loss_ce_1: 0.1458  loss_mask_1: 0.1942  loss_dice_1: 0.1629  loss_ce_2: 0.1457  loss_mask_2: 0.2015  loss_dice_2: 0.1713  loss_ce_3: 0.1455  loss_mask_3: 0.1804  loss_dice_3: 0.17  loss_ce_4: 0.1453  loss_mask_4: 0.1978  loss_dice_4: 0.1705  loss_ce_5: 0.1459  loss_mask_5: 0.2046  loss_dice_5: 0.1654  loss_ce_6: 0.1461  loss_mask_6: 0.2006  loss_dice_6: 0.166  loss_ce_7: 0.146  loss_mask_7: 0.1998  loss_dice_7: 0.1696  loss_ce_8: 0.1465  loss_mask_8: 0.2007  loss_dice_8: 0.1658  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:46] d2.utils.events INFO:  eta: 1:34:45  iter: 4919  total_loss: 4.924  loss_ce: 0.1173  loss_mask: 0.2372  loss_dice: 0.1245  loss_ce_0: 0.06242  loss_mask_0: 0.2388  loss_dice_0: 0.1214  loss_ce_1: 0.118  loss_mask_1: 0.2382  loss_dice_1: 0.1256  loss_ce_2: 0.1185  loss_mask_2: 0.2334  loss_dice_2: 0.1277  loss_ce_3: 0.1189  loss_mask_3: 0.2409  loss_dice_3: 0.1257  loss_ce_4: 0.1194  loss_mask_4: 0.2327  loss_dice_4: 0.1265  loss_ce_5: 0.1185  loss_mask_5: 0.2444  loss_dice_5: 0.1292  loss_ce_6: 0.1185  loss_mask_6: 0.2448  loss_dice_6: 0.1296  loss_ce_7: 0.1182  loss_mask_7: 0.24  loss_dice_7: 0.1307  loss_ce_8: 0.118  loss_mask_8: 0.2408  loss_dice_8: 0.1307  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:49] d2.utils.events INFO:  eta: 1:34:43  iter: 4939  total_loss: 5.046  loss_ce: 0.1346  loss_mask: 0.2226  loss_dice: 0.1333  loss_ce_0: 0.07104  loss_mask_0: 0.2137  loss_dice_0: 0.1361  loss_ce_1: 0.135  loss_mask_1: 0.219  loss_dice_1: 0.1392  loss_ce_2: 0.135  loss_mask_2: 0.2175  loss_dice_2: 0.1469  loss_ce_3: 0.1346  loss_mask_3: 0.2235  loss_dice_3: 0.1426  loss_ce_4: 0.1343  loss_mask_4: 0.2219  loss_dice_4: 0.1429  loss_ce_5: 0.1351  loss_mask_5: 0.2237  loss_dice_5: 0.1401  loss_ce_6: 0.1349  loss_mask_6: 0.2157  loss_dice_6: 0.136  loss_ce_7: 0.1351  loss_mask_7: 0.2228  loss_dice_7: 0.1355  loss_ce_8: 0.1351  loss_mask_8: 0.2205  loss_dice_8: 0.1361  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:42:51] d2.utils.events INFO:  eta: 1:34:40  iter: 4959  total_loss: 4.986  loss_ce: 0.09868  loss_mask: 0.2122  loss_dice: 0.1367  loss_ce_0: 0.06695  loss_mask_0: 0.2254  loss_dice_0: 0.1338  loss_ce_1: 0.0965  loss_mask_1: 0.2087  loss_dice_1: 0.132  loss_ce_2: 0.09559  loss_mask_2: 0.2153  loss_dice_2: 0.1372  loss_ce_3: 0.0941  loss_mask_3: 0.213  loss_dice_3: 0.1333  loss_ce_4: 0.09249  loss_mask_4: 0.2151  loss_dice_4: 0.1367  loss_ce_5: 0.09486  loss_mask_5: 0.2082  loss_dice_5: 0.1359  loss_ce_6: 0.09457  loss_mask_6: 0.2144  loss_dice_6: 0.1369  loss_ce_7: 0.09639  loss_mask_7: 0.2063  loss_dice_7: 0.1324  loss_ce_8: 0.09621  loss_mask_8: 0.2129  loss_dice_8: 0.1394  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 1071M
[04/13 14:42:54] d2.utils.events INFO:  eta: 1:34:40  iter: 4979  total_loss: 5.349  loss_ce: 0.1648  loss_mask: 0.2229  loss_dice: 0.1422  loss_ce_0: 0.07275  loss_mask_0: 0.2163  loss_dice_0: 0.1347  loss_ce_1: 0.1632  loss_mask_1: 0.2268  loss_dice_1: 0.1496  loss_ce_2: 0.1634  loss_mask_2: 0.2166  loss_dice_2: 0.1359  loss_ce_3: 0.1629  loss_mask_3: 0.2152  loss_dice_3: 0.1365  loss_ce_4: 0.1614  loss_mask_4: 0.2147  loss_dice_4: 0.1404  loss_ce_5: 0.1633  loss_mask_5: 0.205  loss_dice_5: 0.1394  loss_ce_6: 0.1637  loss_mask_6: 0.211  loss_dice_6: 0.1426  loss_ce_7: 0.1649  loss_mask_7: 0.222  loss_dice_7: 0.1389  loss_ce_8: 0.1656  loss_mask_8: 0.2174  loss_dice_8: 0.1441  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 1071M
[04/13 14:42:56] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0004999.pth
[04/13 14:42:57] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 14:42:57] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/13 14:42:57] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 14:42:57] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 14:42:57] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 14:42:57] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 14:43:00] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0535 s/iter. Eval: 0.2410 s/iter. Total: 0.2952 s/iter. ETA=0:04:09
[04/13 14:43:05] d2.evaluation.evaluator INFO: Inference done 28/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2420 s/iter. Total: 0.2958 s/iter. ETA=0:04:04
[04/13 14:43:10] d2.evaluation.evaluator INFO: Inference done 45/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2431 s/iter. Total: 0.2969 s/iter. ETA=0:04:00
[04/13 14:43:15] d2.evaluation.evaluator INFO: Inference done 62/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2427 s/iter. Total: 0.2966 s/iter. ETA=0:03:55
[04/13 14:43:20] d2.evaluation.evaluator INFO: Inference done 79/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2427 s/iter. Total: 0.2966 s/iter. ETA=0:03:50
[04/13 14:43:25] d2.evaluation.evaluator INFO: Inference done 96/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2428 s/iter. Total: 0.2967 s/iter. ETA=0:03:45
[04/13 14:43:30] d2.evaluation.evaluator INFO: Inference done 113/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2429 s/iter. Total: 0.2968 s/iter. ETA=0:03:40
[04/13 14:43:36] d2.evaluation.evaluator INFO: Inference done 130/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2428 s/iter. Total: 0.2968 s/iter. ETA=0:03:35
[04/13 14:43:41] d2.evaluation.evaluator INFO: Inference done 147/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2429 s/iter. Total: 0.2968 s/iter. ETA=0:03:30
[04/13 14:43:46] d2.evaluation.evaluator INFO: Inference done 164/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2429 s/iter. Total: 0.2968 s/iter. ETA=0:03:25
[04/13 14:43:51] d2.evaluation.evaluator INFO: Inference done 181/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2429 s/iter. Total: 0.2968 s/iter. ETA=0:03:20
[04/13 14:43:56] d2.evaluation.evaluator INFO: Inference done 198/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2429 s/iter. Total: 0.2968 s/iter. ETA=0:03:15
[04/13 14:44:01] d2.evaluation.evaluator INFO: Inference done 215/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2429 s/iter. Total: 0.2967 s/iter. ETA=0:03:10
[04/13 14:44:06] d2.evaluation.evaluator INFO: Inference done 232/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2429 s/iter. Total: 0.2967 s/iter. ETA=0:03:05
[04/13 14:44:11] d2.evaluation.evaluator INFO: Inference done 249/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2429 s/iter. Total: 0.2967 s/iter. ETA=0:03:00
[04/13 14:44:16] d2.evaluation.evaluator INFO: Inference done 266/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2431 s/iter. Total: 0.2969 s/iter. ETA=0:02:55
[04/13 14:44:21] d2.evaluation.evaluator INFO: Inference done 283/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2431 s/iter. Total: 0.2970 s/iter. ETA=0:02:50
[04/13 14:44:26] d2.evaluation.evaluator INFO: Inference done 300/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2431 s/iter. Total: 0.2970 s/iter. ETA=0:02:45
[04/13 14:44:31] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2432 s/iter. Total: 0.2971 s/iter. ETA=0:02:40
[04/13 14:44:36] d2.evaluation.evaluator INFO: Inference done 334/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2431 s/iter. Total: 0.2970 s/iter. ETA=0:02:35
[04/13 14:44:41] d2.evaluation.evaluator INFO: Inference done 351/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2431 s/iter. Total: 0.2970 s/iter. ETA=0:02:29
[04/13 14:44:46] d2.evaluation.evaluator INFO: Inference done 368/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2431 s/iter. Total: 0.2970 s/iter. ETA=0:02:24
[04/13 14:44:51] d2.evaluation.evaluator INFO: Inference done 385/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2431 s/iter. Total: 0.2970 s/iter. ETA=0:02:19
[04/13 14:44:56] d2.evaluation.evaluator INFO: Inference done 402/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2431 s/iter. Total: 0.2969 s/iter. ETA=0:02:14
[04/13 14:45:01] d2.evaluation.evaluator INFO: Inference done 419/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2430 s/iter. Total: 0.2969 s/iter. ETA=0:02:09
[04/13 14:45:06] d2.evaluation.evaluator INFO: Inference done 436/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2431 s/iter. Total: 0.2969 s/iter. ETA=0:02:04
[04/13 14:45:11] d2.evaluation.evaluator INFO: Inference done 453/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2431 s/iter. Total: 0.2969 s/iter. ETA=0:01:59
[04/13 14:45:16] d2.evaluation.evaluator INFO: Inference done 470/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2431 s/iter. Total: 0.2969 s/iter. ETA=0:01:54
[04/13 14:45:22] d2.evaluation.evaluator INFO: Inference done 487/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2432 s/iter. Total: 0.2970 s/iter. ETA=0:01:49
[04/13 14:45:27] d2.evaluation.evaluator INFO: Inference done 504/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2432 s/iter. Total: 0.2971 s/iter. ETA=0:01:44
[04/13 14:45:32] d2.evaluation.evaluator INFO: Inference done 521/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2432 s/iter. Total: 0.2971 s/iter. ETA=0:01:39
[04/13 14:45:37] d2.evaluation.evaluator INFO: Inference done 538/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2433 s/iter. Total: 0.2971 s/iter. ETA=0:01:34
[04/13 14:45:42] d2.evaluation.evaluator INFO: Inference done 555/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2433 s/iter. Total: 0.2971 s/iter. ETA=0:01:29
[04/13 14:45:47] d2.evaluation.evaluator INFO: Inference done 572/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2435 s/iter. Total: 0.2973 s/iter. ETA=0:01:24
[04/13 14:45:52] d2.evaluation.evaluator INFO: Inference done 589/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2435 s/iter. Total: 0.2974 s/iter. ETA=0:01:19
[04/13 14:45:57] d2.evaluation.evaluator INFO: Inference done 606/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2436 s/iter. Total: 0.2975 s/iter. ETA=0:01:14
[04/13 14:46:02] d2.evaluation.evaluator INFO: Inference done 623/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2437 s/iter. Total: 0.2976 s/iter. ETA=0:01:09
[04/13 14:46:07] d2.evaluation.evaluator INFO: Inference done 640/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2438 s/iter. Total: 0.2976 s/iter. ETA=0:01:04
[04/13 14:46:13] d2.evaluation.evaluator INFO: Inference done 657/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2439 s/iter. Total: 0.2977 s/iter. ETA=0:00:59
[04/13 14:46:18] d2.evaluation.evaluator INFO: Inference done 674/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2439 s/iter. Total: 0.2978 s/iter. ETA=0:00:54
[04/13 14:46:23] d2.evaluation.evaluator INFO: Inference done 691/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2440 s/iter. Total: 0.2978 s/iter. ETA=0:00:49
[04/13 14:46:28] d2.evaluation.evaluator INFO: Inference done 708/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2441 s/iter. Total: 0.2979 s/iter. ETA=0:00:44
[04/13 14:46:33] d2.evaluation.evaluator INFO: Inference done 725/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2442 s/iter. Total: 0.2981 s/iter. ETA=0:00:39
[04/13 14:46:38] d2.evaluation.evaluator INFO: Inference done 742/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2443 s/iter. Total: 0.2981 s/iter. ETA=0:00:33
[04/13 14:46:43] d2.evaluation.evaluator INFO: Inference done 759/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2443 s/iter. Total: 0.2982 s/iter. ETA=0:00:28
[04/13 14:46:48] d2.evaluation.evaluator INFO: Inference done 776/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2444 s/iter. Total: 0.2983 s/iter. ETA=0:00:23
[04/13 14:46:53] d2.evaluation.evaluator INFO: Inference done 793/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2445 s/iter. Total: 0.2983 s/iter. ETA=0:00:18
[04/13 14:46:59] d2.evaluation.evaluator INFO: Inference done 810/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2445 s/iter. Total: 0.2984 s/iter. ETA=0:00:13
[04/13 14:47:04] d2.evaluation.evaluator INFO: Inference done 827/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2446 s/iter. Total: 0.2985 s/iter. ETA=0:00:08
[04/13 14:47:09] d2.evaluation.evaluator INFO: Inference done 844/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2446 s/iter. Total: 0.2985 s/iter. ETA=0:00:03
[04/13 14:47:13] d2.evaluation.evaluator INFO: Total inference time: 0:04:14.157147 (0.298657 s / iter per device, on 1 devices)
[04/13 14:47:13] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:45 (0.052970 s / iter per device, on 1 devices)
[04/13 14:47:14] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 14:47:14] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 14:47:15] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 14:47:15] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 14:47:15] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.29 seconds.
[04/13 14:47:15] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 14:47:15] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.06 seconds.
[04/13 14:47:15] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 14:47:15] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 14:47:15] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 14:47:19] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 14:47:21] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 2.02 seconds.
[04/13 14:47:21] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 14:47:21] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.06 seconds.
[04/13 14:47:21] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 61.596 | 61.596 | 61.596 |  nan  |  nan  | 61.596 |
[04/13 14:47:21] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 14:47:21] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 70.739 | defect     | 52.453 |
[04/13 14:47:21] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 14:47:21] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 14:47:21] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 14:47:21] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 14:47:21] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 14:47:21] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 14:47:21] d2.evaluation.testing INFO: copypaste: 61.5961,61.5961,61.5961,nan,nan,61.5961
[04/13 14:47:21] d2.utils.events INFO:  eta: 1:34:37  iter: 4999  total_loss: 4.839  loss_ce: 0.1097  loss_mask: 0.2223  loss_dice: 0.1282  loss_ce_0: 0.06275  loss_mask_0: 0.2263  loss_dice_0: 0.1287  loss_ce_1: 0.1115  loss_mask_1: 0.223  loss_dice_1: 0.1285  loss_ce_2: 0.1121  loss_mask_2: 0.2344  loss_dice_2: 0.1242  loss_ce_3: 0.1129  loss_mask_3: 0.2308  loss_dice_3: 0.1267  loss_ce_4: 0.1148  loss_mask_4: 0.2222  loss_dice_4: 0.1292  loss_ce_5: 0.1124  loss_mask_5: 0.2176  loss_dice_5: 0.1273  loss_ce_6: 0.1126  loss_mask_6: 0.2325  loss_dice_6: 0.1281  loss_ce_7: 0.1105  loss_mask_7: 0.2232  loss_dice_7: 0.1286  loss_ce_8: 0.1102  loss_mask_8: 0.2255  loss_dice_8: 0.1278  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:24] d2.utils.events INFO:  eta: 1:34:33  iter: 5019  total_loss: 5.208  loss_ce: 0.1557  loss_mask: 0.2158  loss_dice: 0.1543  loss_ce_0: 0.07497  loss_mask_0: 0.2166  loss_dice_0: 0.1582  loss_ce_1: 0.1556  loss_mask_1: 0.2295  loss_dice_1: 0.1483  loss_ce_2: 0.1555  loss_mask_2: 0.2279  loss_dice_2: 0.1569  loss_ce_3: 0.1554  loss_mask_3: 0.2183  loss_dice_3: 0.1502  loss_ce_4: 0.1549  loss_mask_4: 0.2234  loss_dice_4: 0.1514  loss_ce_5: 0.1556  loss_mask_5: 0.2234  loss_dice_5: 0.1482  loss_ce_6: 0.1553  loss_mask_6: 0.2208  loss_dice_6: 0.1594  loss_ce_7: 0.1558  loss_mask_7: 0.2184  loss_dice_7: 0.1541  loss_ce_8: 0.1557  loss_mask_8: 0.214  loss_dice_8: 0.1502  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:26] d2.utils.events INFO:  eta: 1:34:29  iter: 5039  total_loss: 5.252  loss_ce: 0.1323  loss_mask: 0.2209  loss_dice: 0.1556  loss_ce_0: 0.06762  loss_mask_0: 0.2125  loss_dice_0: 0.1545  loss_ce_1: 0.1317  loss_mask_1: 0.2217  loss_dice_1: 0.1549  loss_ce_2: 0.1318  loss_mask_2: 0.215  loss_dice_2: 0.1557  loss_ce_3: 0.132  loss_mask_3: 0.2068  loss_dice_3: 0.1531  loss_ce_4: 0.1321  loss_mask_4: 0.2136  loss_dice_4: 0.1561  loss_ce_5: 0.1319  loss_mask_5: 0.2178  loss_dice_5: 0.1525  loss_ce_6: 0.1319  loss_mask_6: 0.2167  loss_dice_6: 0.1563  loss_ce_7: 0.1317  loss_mask_7: 0.2209  loss_dice_7: 0.1574  loss_ce_8: 0.1317  loss_mask_8: 0.2284  loss_dice_8: 0.1524  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:29] d2.utils.events INFO:  eta: 1:34:26  iter: 5059  total_loss: 4.828  loss_ce: 0.1223  loss_mask: 0.1996  loss_dice: 0.1226  loss_ce_0: 0.06919  loss_mask_0: 0.1958  loss_dice_0: 0.1249  loss_ce_1: 0.1211  loss_mask_1: 0.2011  loss_dice_1: 0.1267  loss_ce_2: 0.1207  loss_mask_2: 0.2025  loss_dice_2: 0.123  loss_ce_3: 0.1202  loss_mask_3: 0.1996  loss_dice_3: 0.1277  loss_ce_4: 0.1197  loss_mask_4: 0.1871  loss_dice_4: 0.1221  loss_ce_5: 0.1205  loss_mask_5: 0.2046  loss_dice_5: 0.1296  loss_ce_6: 0.1201  loss_mask_6: 0.2065  loss_dice_6: 0.1238  loss_ce_7: 0.1212  loss_mask_7: 0.1923  loss_dice_7: 0.1233  loss_ce_8: 0.121  loss_mask_8: 0.1946  loss_dice_8: 0.1251  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:31] d2.utils.events INFO:  eta: 1:34:22  iter: 5079  total_loss: 5.497  loss_ce: 0.1055  loss_mask: 0.2567  loss_dice: 0.1427  loss_ce_0: 0.06677  loss_mask_0: 0.2476  loss_dice_0: 0.1448  loss_ce_1: 0.1045  loss_mask_1: 0.2586  loss_dice_1: 0.1422  loss_ce_2: 0.1041  loss_mask_2: 0.241  loss_dice_2: 0.1456  loss_ce_3: 0.1035  loss_mask_3: 0.2566  loss_dice_3: 0.1415  loss_ce_4: 0.1031  loss_mask_4: 0.2604  loss_dice_4: 0.14  loss_ce_5: 0.1039  loss_mask_5: 0.2395  loss_dice_5: 0.1471  loss_ce_6: 0.1029  loss_mask_6: 0.2485  loss_dice_6: 0.1433  loss_ce_7: 0.1041  loss_mask_7: 0.258  loss_dice_7: 0.1444  loss_ce_8: 0.1041  loss_mask_8: 0.2554  loss_dice_8: 0.1452  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:47:34] d2.utils.events INFO:  eta: 1:34:21  iter: 5099  total_loss: 4.842  loss_ce: 0.1642  loss_mask: 0.2079  loss_dice: 0.1266  loss_ce_0: 0.07042  loss_mask_0: 0.2152  loss_dice_0: 0.1315  loss_ce_1: 0.1664  loss_mask_1: 0.2118  loss_dice_1: 0.1342  loss_ce_2: 0.1664  loss_mask_2: 0.2076  loss_dice_2: 0.1309  loss_ce_3: 0.1672  loss_mask_3: 0.2213  loss_dice_3: 0.1344  loss_ce_4: 0.1677  loss_mask_4: 0.2263  loss_dice_4: 0.136  loss_ce_5: 0.1665  loss_mask_5: 0.2149  loss_dice_5: 0.1315  loss_ce_6: 0.168  loss_mask_6: 0.217  loss_dice_6: 0.1272  loss_ce_7: 0.1666  loss_mask_7: 0.211  loss_dice_7: 0.1329  loss_ce_8: 0.1661  loss_mask_8: 0.2079  loss_dice_8: 0.1315  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:36] d2.utils.events INFO:  eta: 1:34:21  iter: 5119  total_loss: 4.969  loss_ce: 0.109  loss_mask: 0.1851  loss_dice: 0.1369  loss_ce_0: 0.06305  loss_mask_0: 0.1929  loss_dice_0: 0.1398  loss_ce_1: 0.1095  loss_mask_1: 0.1967  loss_dice_1: 0.1427  loss_ce_2: 0.11  loss_mask_2: 0.201  loss_dice_2: 0.1478  loss_ce_3: 0.1101  loss_mask_3: 0.1872  loss_dice_3: 0.1388  loss_ce_4: 0.1103  loss_mask_4: 0.1909  loss_dice_4: 0.1476  loss_ce_5: 0.1097  loss_mask_5: 0.1989  loss_dice_5: 0.1458  loss_ce_6: 0.1098  loss_mask_6: 0.1911  loss_dice_6: 0.1454  loss_ce_7: 0.1095  loss_mask_7: 0.1996  loss_dice_7: 0.1497  loss_ce_8: 0.109  loss_mask_8: 0.1936  loss_dice_8: 0.1426  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:47:39] d2.utils.events INFO:  eta: 1:34:19  iter: 5139  total_loss: 5.225  loss_ce: 0.1194  loss_mask: 0.2151  loss_dice: 0.1564  loss_ce_0: 0.06327  loss_mask_0: 0.2113  loss_dice_0: 0.1529  loss_ce_1: 0.1203  loss_mask_1: 0.2101  loss_dice_1: 0.1566  loss_ce_2: 0.1209  loss_mask_2: 0.2092  loss_dice_2: 0.1535  loss_ce_3: 0.1215  loss_mask_3: 0.2103  loss_dice_3: 0.1551  loss_ce_4: 0.1217  loss_mask_4: 0.2198  loss_dice_4: 0.1499  loss_ce_5: 0.1208  loss_mask_5: 0.2191  loss_dice_5: 0.1487  loss_ce_6: 0.1212  loss_mask_6: 0.2188  loss_dice_6: 0.1571  loss_ce_7: 0.1204  loss_mask_7: 0.2129  loss_dice_7: 0.1479  loss_ce_8: 0.1199  loss_mask_8: 0.2148  loss_dice_8: 0.1507  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:41] d2.utils.events INFO:  eta: 1:34:15  iter: 5159  total_loss: 5.346  loss_ce: 0.1112  loss_mask: 0.2441  loss_dice: 0.1622  loss_ce_0: 0.06216  loss_mask_0: 0.2409  loss_dice_0: 0.1565  loss_ce_1: 0.1109  loss_mask_1: 0.2422  loss_dice_1: 0.1562  loss_ce_2: 0.112  loss_mask_2: 0.233  loss_dice_2: 0.1618  loss_ce_3: 0.1113  loss_mask_3: 0.2355  loss_dice_3: 0.1646  loss_ce_4: 0.1112  loss_mask_4: 0.2456  loss_dice_4: 0.1631  loss_ce_5: 0.1116  loss_mask_5: 0.2462  loss_dice_5: 0.1578  loss_ce_6: 0.1115  loss_mask_6: 0.243  loss_dice_6: 0.1546  loss_ce_7: 0.1112  loss_mask_7: 0.2353  loss_dice_7: 0.1689  loss_ce_8: 0.1116  loss_mask_8: 0.2392  loss_dice_8: 0.154  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:47:44] d2.utils.events INFO:  eta: 1:34:13  iter: 5179  total_loss: 5.166  loss_ce: 0.1281  loss_mask: 0.1931  loss_dice: 0.1496  loss_ce_0: 0.0675  loss_mask_0: 0.1988  loss_dice_0: 0.1513  loss_ce_1: 0.128  loss_mask_1: 0.2026  loss_dice_1: 0.1484  loss_ce_2: 0.1281  loss_mask_2: 0.1939  loss_dice_2: 0.1492  loss_ce_3: 0.128  loss_mask_3: 0.1966  loss_dice_3: 0.1538  loss_ce_4: 0.1281  loss_mask_4: 0.2005  loss_dice_4: 0.1611  loss_ce_5: 0.1281  loss_mask_5: 0.1933  loss_dice_5: 0.151  loss_ce_6: 0.128  loss_mask_6: 0.1982  loss_dice_6: 0.1515  loss_ce_7: 0.1281  loss_mask_7: 0.2045  loss_dice_7: 0.1604  loss_ce_8: 0.1281  loss_mask_8: 0.1965  loss_dice_8: 0.1549  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:47] d2.utils.events INFO:  eta: 1:34:11  iter: 5199  total_loss: 5.678  loss_ce: 0.1134  loss_mask: 0.1569  loss_dice: 0.1923  loss_ce_0: 0.0613  loss_mask_0: 0.166  loss_dice_0: 0.1954  loss_ce_1: 0.1139  loss_mask_1: 0.1647  loss_dice_1: 0.2009  loss_ce_2: 0.1142  loss_mask_2: 0.1459  loss_dice_2: 0.1932  loss_ce_3: 0.1142  loss_mask_3: 0.1565  loss_dice_3: 0.1867  loss_ce_4: 0.1145  loss_mask_4: 0.1614  loss_dice_4: 0.1889  loss_ce_5: 0.1136  loss_mask_5: 0.1604  loss_dice_5: 0.1864  loss_ce_6: 0.1138  loss_mask_6: 0.1627  loss_dice_6: 0.1961  loss_ce_7: 0.114  loss_mask_7: 0.1545  loss_dice_7: 0.2  loss_ce_8: 0.1136  loss_mask_8: 0.1562  loss_dice_8: 0.1989  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:49] d2.utils.events INFO:  eta: 1:34:10  iter: 5219  total_loss: 5.518  loss_ce: 0.1146  loss_mask: 0.2163  loss_dice: 0.1913  loss_ce_0: 0.06114  loss_mask_0: 0.2371  loss_dice_0: 0.1836  loss_ce_1: 0.115  loss_mask_1: 0.2326  loss_dice_1: 0.1847  loss_ce_2: 0.1153  loss_mask_2: 0.2312  loss_dice_2: 0.1752  loss_ce_3: 0.1156  loss_mask_3: 0.228  loss_dice_3: 0.1901  loss_ce_4: 0.1157  loss_mask_4: 0.2207  loss_dice_4: 0.171  loss_ce_5: 0.1151  loss_mask_5: 0.2333  loss_dice_5: 0.1792  loss_ce_6: 0.1152  loss_mask_6: 0.2337  loss_dice_6: 0.1808  loss_ce_7: 0.1151  loss_mask_7: 0.2347  loss_dice_7: 0.1793  loss_ce_8: 0.1151  loss_mask_8: 0.2423  loss_dice_8: 0.184  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:52] d2.utils.events INFO:  eta: 1:34:07  iter: 5239  total_loss: 5.008  loss_ce: 0.1302  loss_mask: 0.21  loss_dice: 0.1215  loss_ce_0: 0.07185  loss_mask_0: 0.2152  loss_dice_0: 0.1278  loss_ce_1: 0.1302  loss_mask_1: 0.2088  loss_dice_1: 0.1255  loss_ce_2: 0.1298  loss_mask_2: 0.2066  loss_dice_2: 0.1236  loss_ce_3: 0.1296  loss_mask_3: 0.2181  loss_dice_3: 0.1299  loss_ce_4: 0.1296  loss_mask_4: 0.2184  loss_dice_4: 0.1271  loss_ce_5: 0.1298  loss_mask_5: 0.2168  loss_dice_5: 0.1255  loss_ce_6: 0.1297  loss_mask_6: 0.2132  loss_dice_6: 0.1238  loss_ce_7: 0.1301  loss_mask_7: 0.2179  loss_dice_7: 0.1285  loss_ce_8: 0.1301  loss_mask_8: 0.2167  loss_dice_8: 0.1213  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:54] d2.utils.events INFO:  eta: 1:34:04  iter: 5259  total_loss: 4.806  loss_ce: 0.123  loss_mask: 0.2152  loss_dice: 0.1092  loss_ce_0: 0.06862  loss_mask_0: 0.2222  loss_dice_0: 0.115  loss_ce_1: 0.1226  loss_mask_1: 0.2185  loss_dice_1: 0.114  loss_ce_2: 0.1217  loss_mask_2: 0.2109  loss_dice_2: 0.1099  loss_ce_3: 0.1213  loss_mask_3: 0.2151  loss_dice_3: 0.1127  loss_ce_4: 0.1207  loss_mask_4: 0.2196  loss_dice_4: 0.1132  loss_ce_5: 0.1215  loss_mask_5: 0.2124  loss_dice_5: 0.1125  loss_ce_6: 0.1216  loss_mask_6: 0.22  loss_dice_6: 0.1075  loss_ce_7: 0.1223  loss_mask_7: 0.2205  loss_dice_7: 0.1126  loss_ce_8: 0.1219  loss_mask_8: 0.2031  loss_dice_8: 0.1102  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:47:57] d2.utils.events INFO:  eta: 1:34:00  iter: 5279  total_loss: 5.387  loss_ce: 0.101  loss_mask: 0.22  loss_dice: 0.1436  loss_ce_0: 0.06656  loss_mask_0: 0.2196  loss_dice_0: 0.1421  loss_ce_1: 0.09998  loss_mask_1: 0.2219  loss_dice_1: 0.1399  loss_ce_2: 0.09889  loss_mask_2: 0.2237  loss_dice_2: 0.1417  loss_ce_3: 0.09814  loss_mask_3: 0.2325  loss_dice_3: 0.1414  loss_ce_4: 0.09684  loss_mask_4: 0.2254  loss_dice_4: 0.1438  loss_ce_5: 0.09855  loss_mask_5: 0.2228  loss_dice_5: 0.1438  loss_ce_6: 0.09825  loss_mask_6: 0.2263  loss_dice_6: 0.1401  loss_ce_7: 0.09975  loss_mask_7: 0.2262  loss_dice_7: 0.1411  loss_ce_8: 0.09923  loss_mask_8: 0.2273  loss_dice_8: 0.1449  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:47:59] d2.utils.events INFO:  eta: 1:34:00  iter: 5299  total_loss: 5.853  loss_ce: 0.1326  loss_mask: 0.2293  loss_dice: 0.1455  loss_ce_0: 0.06696  loss_mask_0: 0.2309  loss_dice_0: 0.1398  loss_ce_1: 0.1329  loss_mask_1: 0.2173  loss_dice_1: 0.1418  loss_ce_2: 0.1331  loss_mask_2: 0.2262  loss_dice_2: 0.1505  loss_ce_3: 0.1335  loss_mask_3: 0.2317  loss_dice_3: 0.1493  loss_ce_4: 0.1338  loss_mask_4: 0.2251  loss_dice_4: 0.1403  loss_ce_5: 0.1332  loss_mask_5: 0.2283  loss_dice_5: 0.1412  loss_ce_6: 0.1336  loss_mask_6: 0.2328  loss_dice_6: 0.1424  loss_ce_7: 0.1329  loss_mask_7: 0.2289  loss_dice_7: 0.1429  loss_ce_8: 0.1331  loss_mask_8: 0.2246  loss_dice_8: 0.1485  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:02] d2.utils.events INFO:  eta: 1:33:54  iter: 5319  total_loss: 5.954  loss_ce: 0.141  loss_mask: 0.2189  loss_dice: 0.1622  loss_ce_0: 0.07205  loss_mask_0: 0.2325  loss_dice_0: 0.1613  loss_ce_1: 0.1382  loss_mask_1: 0.2333  loss_dice_1: 0.1644  loss_ce_2: 0.1383  loss_mask_2: 0.2366  loss_dice_2: 0.1598  loss_ce_3: 0.1371  loss_mask_3: 0.2423  loss_dice_3: 0.1569  loss_ce_4: 0.1349  loss_mask_4: 0.2395  loss_dice_4: 0.1568  loss_ce_5: 0.1375  loss_mask_5: 0.2367  loss_dice_5: 0.1565  loss_ce_6: 0.1376  loss_mask_6: 0.2282  loss_dice_6: 0.1623  loss_ce_7: 0.1396  loss_mask_7: 0.2462  loss_dice_7: 0.1657  loss_ce_8: 0.1401  loss_mask_8: 0.2331  loss_dice_8: 0.1671  time: 0.1258  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 14:48:04] d2.utils.events INFO:  eta: 1:33:55  iter: 5339  total_loss: 5.23  loss_ce: 0.1287  loss_mask: 0.2343  loss_dice: 0.1669  loss_ce_0: 0.06396  loss_mask_0: 0.2227  loss_dice_0: 0.1541  loss_ce_1: 0.13  loss_mask_1: 0.2309  loss_dice_1: 0.1631  loss_ce_2: 0.13  loss_mask_2: 0.2252  loss_dice_2: 0.164  loss_ce_3: 0.1308  loss_mask_3: 0.2206  loss_dice_3: 0.1656  loss_ce_4: 0.1328  loss_mask_4: 0.2322  loss_dice_4: 0.1653  loss_ce_5: 0.1303  loss_mask_5: 0.2303  loss_dice_5: 0.1653  loss_ce_6: 0.1308  loss_mask_6: 0.2259  loss_dice_6: 0.1625  loss_ce_7: 0.1291  loss_mask_7: 0.2279  loss_dice_7: 0.1653  loss_ce_8: 0.1295  loss_mask_8: 0.2181  loss_dice_8: 0.1553  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:07] d2.utils.events INFO:  eta: 1:33:54  iter: 5359  total_loss: 5.806  loss_ce: 0.1292  loss_mask: 0.212  loss_dice: 0.1562  loss_ce_0: 0.06735  loss_mask_0: 0.2029  loss_dice_0: 0.1493  loss_ce_1: 0.1294  loss_mask_1: 0.2091  loss_dice_1: 0.1511  loss_ce_2: 0.1295  loss_mask_2: 0.2099  loss_dice_2: 0.1525  loss_ce_3: 0.1295  loss_mask_3: 0.2158  loss_dice_3: 0.1488  loss_ce_4: 0.1298  loss_mask_4: 0.2081  loss_dice_4: 0.1459  loss_ce_5: 0.1295  loss_mask_5: 0.2002  loss_dice_5: 0.1493  loss_ce_6: 0.1296  loss_mask_6: 0.2127  loss_dice_6: 0.1477  loss_ce_7: 0.1293  loss_mask_7: 0.2083  loss_dice_7: 0.1519  loss_ce_8: 0.1294  loss_mask_8: 0.2102  loss_dice_8: 0.1543  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:09] d2.utils.events INFO:  eta: 1:33:53  iter: 5379  total_loss: 4.678  loss_ce: 0.1155  loss_mask: 0.1947  loss_dice: 0.1407  loss_ce_0: 0.06297  loss_mask_0: 0.2  loss_dice_0: 0.1466  loss_ce_1: 0.1147  loss_mask_1: 0.193  loss_dice_1: 0.1455  loss_ce_2: 0.1144  loss_mask_2: 0.1975  loss_dice_2: 0.1474  loss_ce_3: 0.1142  loss_mask_3: 0.1975  loss_dice_3: 0.1423  loss_ce_4: 0.1138  loss_mask_4: 0.1863  loss_dice_4: 0.1388  loss_ce_5: 0.1146  loss_mask_5: 0.1932  loss_dice_5: 0.1451  loss_ce_6: 0.1147  loss_mask_6: 0.194  loss_dice_6: 0.1379  loss_ce_7: 0.1146  loss_mask_7: 0.1939  loss_dice_7: 0.144  loss_ce_8: 0.1151  loss_mask_8: 0.1913  loss_dice_8: 0.1428  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:12] d2.utils.events INFO:  eta: 1:33:50  iter: 5399  total_loss: 5.832  loss_ce: 0.1347  loss_mask: 0.1665  loss_dice: 0.1713  loss_ce_0: 0.07081  loss_mask_0: 0.1704  loss_dice_0: 0.1707  loss_ce_1: 0.1348  loss_mask_1: 0.164  loss_dice_1: 0.1731  loss_ce_2: 0.1355  loss_mask_2: 0.168  loss_dice_2: 0.1716  loss_ce_3: 0.1357  loss_mask_3: 0.156  loss_dice_3: 0.17  loss_ce_4: 0.1355  loss_mask_4: 0.1696  loss_dice_4: 0.1602  loss_ce_5: 0.1355  loss_mask_5: 0.1735  loss_dice_5: 0.1723  loss_ce_6: 0.1357  loss_mask_6: 0.1702  loss_dice_6: 0.1702  loss_ce_7: 0.1347  loss_mask_7: 0.1693  loss_dice_7: 0.1704  loss_ce_8: 0.1351  loss_mask_8: 0.1644  loss_dice_8: 0.181  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:14] d2.utils.events INFO:  eta: 1:33:48  iter: 5419  total_loss: 5.027  loss_ce: 0.1122  loss_mask: 0.2174  loss_dice: 0.1605  loss_ce_0: 0.06656  loss_mask_0: 0.214  loss_dice_0: 0.1692  loss_ce_1: 0.1103  loss_mask_1: 0.2133  loss_dice_1: 0.1698  loss_ce_2: 0.1098  loss_mask_2: 0.2149  loss_dice_2: 0.1616  loss_ce_3: 0.1092  loss_mask_3: 0.2053  loss_dice_3: 0.1685  loss_ce_4: 0.1078  loss_mask_4: 0.2066  loss_dice_4: 0.1632  loss_ce_5: 0.1094  loss_mask_5: 0.2067  loss_dice_5: 0.1688  loss_ce_6: 0.1088  loss_mask_6: 0.2145  loss_dice_6: 0.1624  loss_ce_7: 0.1105  loss_mask_7: 0.2246  loss_dice_7: 0.1624  loss_ce_8: 0.111  loss_mask_8: 0.2165  loss_dice_8: 0.1588  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:17] d2.utils.events INFO:  eta: 1:33:44  iter: 5439  total_loss: 5.848  loss_ce: 0.1529  loss_mask: 0.1535  loss_dice: 0.1642  loss_ce_0: 0.06927  loss_mask_0: 0.1495  loss_dice_0: 0.1708  loss_ce_1: 0.1535  loss_mask_1: 0.1553  loss_dice_1: 0.1703  loss_ce_2: 0.1542  loss_mask_2: 0.1499  loss_dice_2: 0.1744  loss_ce_3: 0.1548  loss_mask_3: 0.1459  loss_dice_3: 0.1688  loss_ce_4: 0.1552  loss_mask_4: 0.1454  loss_dice_4: 0.1655  loss_ce_5: 0.1545  loss_mask_5: 0.1484  loss_dice_5: 0.1676  loss_ce_6: 0.1554  loss_mask_6: 0.1491  loss_dice_6: 0.1707  loss_ce_7: 0.154  loss_mask_7: 0.149  loss_dice_7: 0.1684  loss_ce_8: 0.1541  loss_mask_8: 0.1574  loss_dice_8: 0.1733  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:20] d2.utils.events INFO:  eta: 1:33:40  iter: 5459  total_loss: 4.888  loss_ce: 0.1307  loss_mask: 0.2118  loss_dice: 0.1216  loss_ce_0: 0.06833  loss_mask_0: 0.2124  loss_dice_0: 0.1206  loss_ce_1: 0.1299  loss_mask_1: 0.2114  loss_dice_1: 0.117  loss_ce_2: 0.1299  loss_mask_2: 0.2162  loss_dice_2: 0.1205  loss_ce_3: 0.1296  loss_mask_3: 0.2054  loss_dice_3: 0.1175  loss_ce_4: 0.1291  loss_mask_4: 0.2117  loss_dice_4: 0.1191  loss_ce_5: 0.1297  loss_mask_5: 0.2139  loss_dice_5: 0.1187  loss_ce_6: 0.1299  loss_mask_6: 0.2038  loss_dice_6: 0.1183  loss_ce_7: 0.1301  loss_mask_7: 0.2102  loss_dice_7: 0.1196  loss_ce_8: 0.1308  loss_mask_8: 0.2083  loss_dice_8: 0.1205  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:22] d2.utils.events INFO:  eta: 1:33:40  iter: 5479  total_loss: 5.137  loss_ce: 0.137  loss_mask: 0.201  loss_dice: 0.1417  loss_ce_0: 0.06725  loss_mask_0: 0.195  loss_dice_0: 0.1418  loss_ce_1: 0.1373  loss_mask_1: 0.1939  loss_dice_1: 0.1446  loss_ce_2: 0.1377  loss_mask_2: 0.1862  loss_dice_2: 0.142  loss_ce_3: 0.1379  loss_mask_3: 0.1983  loss_dice_3: 0.1393  loss_ce_4: 0.1392  loss_mask_4: 0.1948  loss_dice_4: 0.1393  loss_ce_5: 0.1377  loss_mask_5: 0.1963  loss_dice_5: 0.1404  loss_ce_6: 0.138  loss_mask_6: 0.1875  loss_dice_6: 0.144  loss_ce_7: 0.1377  loss_mask_7: 0.2007  loss_dice_7: 0.1401  loss_ce_8: 0.1373  loss_mask_8: 0.1983  loss_dice_8: 0.1416  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:25] d2.utils.events INFO:  eta: 1:33:36  iter: 5499  total_loss: 5.416  loss_ce: 0.1289  loss_mask: 0.1913  loss_dice: 0.1718  loss_ce_0: 0.06692  loss_mask_0: 0.1807  loss_dice_0: 0.1697  loss_ce_1: 0.1283  loss_mask_1: 0.1812  loss_dice_1: 0.1667  loss_ce_2: 0.1285  loss_mask_2: 0.1859  loss_dice_2: 0.1683  loss_ce_3: 0.1285  loss_mask_3: 0.1739  loss_dice_3: 0.1626  loss_ce_4: 0.1285  loss_mask_4: 0.1885  loss_dice_4: 0.1706  loss_ce_5: 0.1285  loss_mask_5: 0.1803  loss_dice_5: 0.1664  loss_ce_6: 0.1285  loss_mask_6: 0.2018  loss_dice_6: 0.1754  loss_ce_7: 0.1288  loss_mask_7: 0.1773  loss_dice_7: 0.1661  loss_ce_8: 0.1287  loss_mask_8: 0.1888  loss_dice_8: 0.1668  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:27] d2.utils.events INFO:  eta: 1:33:32  iter: 5519  total_loss: 4.876  loss_ce: 0.1301  loss_mask: 0.2196  loss_dice: 0.1198  loss_ce_0: 0.06721  loss_mask_0: 0.2159  loss_dice_0: 0.118  loss_ce_1: 0.1304  loss_mask_1: 0.2142  loss_dice_1: 0.1234  loss_ce_2: 0.1303  loss_mask_2: 0.2254  loss_dice_2: 0.1197  loss_ce_3: 0.1305  loss_mask_3: 0.2167  loss_dice_3: 0.12  loss_ce_4: 0.1304  loss_mask_4: 0.2139  loss_dice_4: 0.1211  loss_ce_5: 0.1303  loss_mask_5: 0.2131  loss_dice_5: 0.1229  loss_ce_6: 0.1305  loss_mask_6: 0.212  loss_dice_6: 0.118  loss_ce_7: 0.1301  loss_mask_7: 0.2148  loss_dice_7: 0.1179  loss_ce_8: 0.1302  loss_mask_8: 0.215  loss_dice_8: 0.1165  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:30] d2.utils.events INFO:  eta: 1:33:29  iter: 5539  total_loss: 5.133  loss_ce: 0.1306  loss_mask: 0.2016  loss_dice: 0.1883  loss_ce_0: 0.06708  loss_mask_0: 0.2055  loss_dice_0: 0.1822  loss_ce_1: 0.1301  loss_mask_1: 0.2002  loss_dice_1: 0.1782  loss_ce_2: 0.1311  loss_mask_2: 0.2004  loss_dice_2: 0.1801  loss_ce_3: 0.1311  loss_mask_3: 0.2059  loss_dice_3: 0.1804  loss_ce_4: 0.1305  loss_mask_4: 0.199  loss_dice_4: 0.1853  loss_ce_5: 0.131  loss_mask_5: 0.2001  loss_dice_5: 0.1802  loss_ce_6: 0.1307  loss_mask_6: 0.1971  loss_dice_6: 0.1824  loss_ce_7: 0.1302  loss_mask_7: 0.1969  loss_dice_7: 0.1822  loss_ce_8: 0.1307  loss_mask_8: 0.1975  loss_dice_8: 0.1902  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:32] d2.utils.events INFO:  eta: 1:33:28  iter: 5559  total_loss: 4.941  loss_ce: 0.1274  loss_mask: 0.2062  loss_dice: 0.1253  loss_ce_0: 0.0671  loss_mask_0: 0.2147  loss_dice_0: 0.1253  loss_ce_1: 0.1274  loss_mask_1: 0.2063  loss_dice_1: 0.1278  loss_ce_2: 0.1273  loss_mask_2: 0.2052  loss_dice_2: 0.1255  loss_ce_3: 0.1273  loss_mask_3: 0.2076  loss_dice_3: 0.1303  loss_ce_4: 0.1268  loss_mask_4: 0.2135  loss_dice_4: 0.127  loss_ce_5: 0.1272  loss_mask_5: 0.2083  loss_dice_5: 0.1323  loss_ce_6: 0.1275  loss_mask_6: 0.2085  loss_dice_6: 0.1328  loss_ce_7: 0.1272  loss_mask_7: 0.2123  loss_dice_7: 0.1299  loss_ce_8: 0.1274  loss_mask_8: 0.2035  loss_dice_8: 0.1251  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:35] d2.utils.events INFO:  eta: 1:33:28  iter: 5579  total_loss: 4.996  loss_ce: 0.11  loss_mask: 0.1691  loss_dice: 0.1547  loss_ce_0: 0.06505  loss_mask_0: 0.1752  loss_dice_0: 0.157  loss_ce_1: 0.1092  loss_mask_1: 0.1659  loss_dice_1: 0.149  loss_ce_2: 0.1089  loss_mask_2: 0.1723  loss_dice_2: 0.1616  loss_ce_3: 0.1083  loss_mask_3: 0.1686  loss_dice_3: 0.1493  loss_ce_4: 0.107  loss_mask_4: 0.168  loss_dice_4: 0.148  loss_ce_5: 0.1083  loss_mask_5: 0.1604  loss_dice_5: 0.1553  loss_ce_6: 0.1085  loss_mask_6: 0.1717  loss_dice_6: 0.1486  loss_ce_7: 0.1091  loss_mask_7: 0.1627  loss_dice_7: 0.1553  loss_ce_8: 0.1089  loss_mask_8: 0.1675  loss_dice_8: 0.1479  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:37] d2.utils.events INFO:  eta: 1:33:26  iter: 5599  total_loss: 5.712  loss_ce: 0.1642  loss_mask: 0.2377  loss_dice: 0.166  loss_ce_0: 0.0734  loss_mask_0: 0.2431  loss_dice_0: 0.1639  loss_ce_1: 0.1643  loss_mask_1: 0.241  loss_dice_1: 0.1655  loss_ce_2: 0.1645  loss_mask_2: 0.2352  loss_dice_2: 0.165  loss_ce_3: 0.1649  loss_mask_3: 0.2512  loss_dice_3: 0.163  loss_ce_4: 0.1647  loss_mask_4: 0.2393  loss_dice_4: 0.1662  loss_ce_5: 0.1649  loss_mask_5: 0.2475  loss_dice_5: 0.1685  loss_ce_6: 0.1656  loss_mask_6: 0.2463  loss_dice_6: 0.1621  loss_ce_7: 0.165  loss_mask_7: 0.2397  loss_dice_7: 0.1588  loss_ce_8: 0.1656  loss_mask_8: 0.2486  loss_dice_8: 0.1664  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:40] d2.utils.events INFO:  eta: 1:33:25  iter: 5619  total_loss: 4.893  loss_ce: 0.1277  loss_mask: 0.2334  loss_dice: 0.1411  loss_ce_0: 0.06733  loss_mask_0: 0.2268  loss_dice_0: 0.1416  loss_ce_1: 0.128  loss_mask_1: 0.2204  loss_dice_1: 0.1393  loss_ce_2: 0.1279  loss_mask_2: 0.2338  loss_dice_2: 0.1501  loss_ce_3: 0.1279  loss_mask_3: 0.2097  loss_dice_3: 0.1389  loss_ce_4: 0.128  loss_mask_4: 0.2255  loss_dice_4: 0.1424  loss_ce_5: 0.1279  loss_mask_5: 0.2232  loss_dice_5: 0.1416  loss_ce_6: 0.1279  loss_mask_6: 0.2243  loss_dice_6: 0.1435  loss_ce_7: 0.1279  loss_mask_7: 0.2227  loss_dice_7: 0.1423  loss_ce_8: 0.1278  loss_mask_8: 0.2223  loss_dice_8: 0.1428  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:42] d2.utils.events INFO:  eta: 1:33:23  iter: 5639  total_loss: 5.011  loss_ce: 0.1339  loss_mask: 0.2244  loss_dice: 0.1425  loss_ce_0: 0.07111  loss_mask_0: 0.2159  loss_dice_0: 0.1481  loss_ce_1: 0.1333  loss_mask_1: 0.2175  loss_dice_1: 0.1367  loss_ce_2: 0.1331  loss_mask_2: 0.2208  loss_dice_2: 0.134  loss_ce_3: 0.1328  loss_mask_3: 0.2246  loss_dice_3: 0.1381  loss_ce_4: 0.1311  loss_mask_4: 0.2107  loss_dice_4: 0.1392  loss_ce_5: 0.1328  loss_mask_5: 0.2243  loss_dice_5: 0.1428  loss_ce_6: 0.1328  loss_mask_6: 0.2374  loss_dice_6: 0.1406  loss_ce_7: 0.1336  loss_mask_7: 0.2206  loss_dice_7: 0.1399  loss_ce_8: 0.1335  loss_mask_8: 0.232  loss_dice_8: 0.1354  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:45] d2.utils.events INFO:  eta: 1:33:18  iter: 5659  total_loss: 4.961  loss_ce: 0.12  loss_mask: 0.2228  loss_dice: 0.1436  loss_ce_0: 0.06346  loss_mask_0: 0.2151  loss_dice_0: 0.1409  loss_ce_1: 0.1203  loss_mask_1: 0.2176  loss_dice_1: 0.147  loss_ce_2: 0.1198  loss_mask_2: 0.2196  loss_dice_2: 0.1424  loss_ce_3: 0.1196  loss_mask_3: 0.2202  loss_dice_3: 0.1448  loss_ce_4: 0.1207  loss_mask_4: 0.2172  loss_dice_4: 0.1487  loss_ce_5: 0.1198  loss_mask_5: 0.2284  loss_dice_5: 0.1452  loss_ce_6: 0.1202  loss_mask_6: 0.2145  loss_dice_6: 0.142  loss_ce_7: 0.1203  loss_mask_7: 0.2216  loss_dice_7: 0.149  loss_ce_8: 0.1201  loss_mask_8: 0.2256  loss_dice_8: 0.138  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:47] d2.utils.events INFO:  eta: 1:33:16  iter: 5679  total_loss: 5.961  loss_ce: 0.1444  loss_mask: 0.1638  loss_dice: 0.2121  loss_ce_0: 0.07198  loss_mask_0: 0.1663  loss_dice_0: 0.2074  loss_ce_1: 0.1447  loss_mask_1: 0.1605  loss_dice_1: 0.2218  loss_ce_2: 0.145  loss_mask_2: 0.1598  loss_dice_2: 0.2184  loss_ce_3: 0.1451  loss_mask_3: 0.1585  loss_dice_3: 0.2208  loss_ce_4: 0.1458  loss_mask_4: 0.1567  loss_dice_4: 0.2199  loss_ce_5: 0.145  loss_mask_5: 0.1561  loss_dice_5: 0.2212  loss_ce_6: 0.1449  loss_mask_6: 0.1619  loss_dice_6: 0.2285  loss_ce_7: 0.1449  loss_mask_7: 0.1611  loss_dice_7: 0.2328  loss_ce_8: 0.145  loss_mask_8: 0.1646  loss_dice_8: 0.222  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:48:50] d2.utils.events INFO:  eta: 1:33:12  iter: 5699  total_loss: 4.663  loss_ce: 0.1309  loss_mask: 0.2081  loss_dice: 0.1351  loss_ce_0: 0.06997  loss_mask_0: 0.2036  loss_dice_0: 0.1394  loss_ce_1: 0.1312  loss_mask_1: 0.2061  loss_dice_1: 0.1362  loss_ce_2: 0.1312  loss_mask_2: 0.205  loss_dice_2: 0.1346  loss_ce_3: 0.1314  loss_mask_3: 0.2063  loss_dice_3: 0.1379  loss_ce_4: 0.1316  loss_mask_4: 0.2153  loss_dice_4: 0.1381  loss_ce_5: 0.1313  loss_mask_5: 0.2129  loss_dice_5: 0.1378  loss_ce_6: 0.1313  loss_mask_6: 0.2129  loss_dice_6: 0.1308  loss_ce_7: 0.1313  loss_mask_7: 0.2071  loss_dice_7: 0.1324  loss_ce_8: 0.1312  loss_mask_8: 0.2131  loss_dice_8: 0.1341  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:52] d2.utils.events INFO:  eta: 1:33:09  iter: 5719  total_loss: 5.106  loss_ce: 0.1254  loss_mask: 0.2214  loss_dice: 0.1453  loss_ce_0: 0.06768  loss_mask_0: 0.2175  loss_dice_0: 0.142  loss_ce_1: 0.1244  loss_mask_1: 0.2238  loss_dice_1: 0.1477  loss_ce_2: 0.1242  loss_mask_2: 0.2223  loss_dice_2: 0.1489  loss_ce_3: 0.1238  loss_mask_3: 0.2235  loss_dice_3: 0.148  loss_ce_4: 0.1229  loss_mask_4: 0.2196  loss_dice_4: 0.1472  loss_ce_5: 0.1238  loss_mask_5: 0.2099  loss_dice_5: 0.1409  loss_ce_6: 0.1237  loss_mask_6: 0.2204  loss_dice_6: 0.1451  loss_ce_7: 0.1241  loss_mask_7: 0.2131  loss_dice_7: 0.1494  loss_ce_8: 0.1245  loss_mask_8: 0.2206  loss_dice_8: 0.148  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:55] d2.utils.events INFO:  eta: 1:33:07  iter: 5739  total_loss: 5.173  loss_ce: 0.1292  loss_mask: 0.1955  loss_dice: 0.1418  loss_ce_0: 0.06623  loss_mask_0: 0.19  loss_dice_0: 0.146  loss_ce_1: 0.1291  loss_mask_1: 0.1921  loss_dice_1: 0.1419  loss_ce_2: 0.1293  loss_mask_2: 0.1826  loss_dice_2: 0.1385  loss_ce_3: 0.1292  loss_mask_3: 0.1932  loss_dice_3: 0.1399  loss_ce_4: 0.1285  loss_mask_4: 0.1859  loss_dice_4: 0.1398  loss_ce_5: 0.1291  loss_mask_5: 0.2004  loss_dice_5: 0.142  loss_ce_6: 0.1296  loss_mask_6: 0.1892  loss_dice_6: 0.1416  loss_ce_7: 0.1292  loss_mask_7: 0.1865  loss_dice_7: 0.1402  loss_ce_8: 0.1295  loss_mask_8: 0.1901  loss_dice_8: 0.1435  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:48:57] d2.utils.events INFO:  eta: 1:33:05  iter: 5759  total_loss: 5.737  loss_ce: 0.1346  loss_mask: 0.2083  loss_dice: 0.1721  loss_ce_0: 0.0685  loss_mask_0: 0.208  loss_dice_0: 0.1742  loss_ce_1: 0.1353  loss_mask_1: 0.2088  loss_dice_1: 0.1698  loss_ce_2: 0.1353  loss_mask_2: 0.206  loss_dice_2: 0.1773  loss_ce_3: 0.1356  loss_mask_3: 0.2114  loss_dice_3: 0.1762  loss_ce_4: 0.1365  loss_mask_4: 0.2199  loss_dice_4: 0.1717  loss_ce_5: 0.1354  loss_mask_5: 0.211  loss_dice_5: 0.1688  loss_ce_6: 0.1352  loss_mask_6: 0.2173  loss_dice_6: 0.1685  loss_ce_7: 0.1354  loss_mask_7: 0.2144  loss_dice_7: 0.1705  loss_ce_8: 0.1349  loss_mask_8: 0.2113  loss_dice_8: 0.1692  time: 0.1258  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 14:49:00] d2.utils.events INFO:  eta: 1:33:04  iter: 5779  total_loss: 5.298  loss_ce: 0.1276  loss_mask: 0.212  loss_dice: 0.1443  loss_ce_0: 0.06667  loss_mask_0: 0.2092  loss_dice_0: 0.1481  loss_ce_1: 0.1286  loss_mask_1: 0.1962  loss_dice_1: 0.1481  loss_ce_2: 0.1276  loss_mask_2: 0.209  loss_dice_2: 0.1469  loss_ce_3: 0.1277  loss_mask_3: 0.196  loss_dice_3: 0.1506  loss_ce_4: 0.1298  loss_mask_4: 0.2017  loss_dice_4: 0.1483  loss_ce_5: 0.1276  loss_mask_5: 0.1921  loss_dice_5: 0.1523  loss_ce_6: 0.1277  loss_mask_6: 0.199  loss_dice_6: 0.1441  loss_ce_7: 0.1284  loss_mask_7: 0.1828  loss_dice_7: 0.1485  loss_ce_8: 0.1277  loss_mask_8: 0.1973  loss_dice_8: 0.1491  time: 0.1258  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 14:49:03] d2.utils.events INFO:  eta: 1:33:00  iter: 5799  total_loss: 4.924  loss_ce: 0.1283  loss_mask: 0.2341  loss_dice: 0.1235  loss_ce_0: 0.0666  loss_mask_0: 0.2353  loss_dice_0: 0.1195  loss_ce_1: 0.1282  loss_mask_1: 0.2373  loss_dice_1: 0.1226  loss_ce_2: 0.1283  loss_mask_2: 0.231  loss_dice_2: 0.1235  loss_ce_3: 0.1283  loss_mask_3: 0.2351  loss_dice_3: 0.1237  loss_ce_4: 0.1281  loss_mask_4: 0.227  loss_dice_4: 0.1198  loss_ce_5: 0.1283  loss_mask_5: 0.227  loss_dice_5: 0.125  loss_ce_6: 0.1283  loss_mask_6: 0.2262  loss_dice_6: 0.1197  loss_ce_7: 0.1281  loss_mask_7: 0.2321  loss_dice_7: 0.1247  loss_ce_8: 0.1283  loss_mask_8: 0.2335  loss_dice_8: 0.1242  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:05] d2.utils.events INFO:  eta: 1:33:02  iter: 5819  total_loss: 4.884  loss_ce: 0.1342  loss_mask: 0.2093  loss_dice: 0.1386  loss_ce_0: 0.06745  loss_mask_0: 0.2042  loss_dice_0: 0.1365  loss_ce_1: 0.1323  loss_mask_1: 0.2046  loss_dice_1: 0.139  loss_ce_2: 0.1348  loss_mask_2: 0.2013  loss_dice_2: 0.1404  loss_ce_3: 0.1351  loss_mask_3: 0.2014  loss_dice_3: 0.1356  loss_ce_4: 0.1325  loss_mask_4: 0.2035  loss_dice_4: 0.1351  loss_ce_5: 0.135  loss_mask_5: 0.2014  loss_dice_5: 0.1487  loss_ce_6: 0.135  loss_mask_6: 0.2066  loss_dice_6: 0.1367  loss_ce_7: 0.1326  loss_mask_7: 0.1998  loss_dice_7: 0.1366  loss_ce_8: 0.1346  loss_mask_8: 0.1974  loss_dice_8: 0.1293  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:49:08] d2.utils.events INFO:  eta: 1:32:59  iter: 5839  total_loss: 5.158  loss_ce: 0.1306  loss_mask: 0.1974  loss_dice: 0.1957  loss_ce_0: 0.07004  loss_mask_0: 0.205  loss_dice_0: 0.1824  loss_ce_1: 0.1294  loss_mask_1: 0.2073  loss_dice_1: 0.1861  loss_ce_2: 0.1294  loss_mask_2: 0.204  loss_dice_2: 0.1897  loss_ce_3: 0.1294  loss_mask_3: 0.2099  loss_dice_3: 0.1947  loss_ce_4: 0.1304  loss_mask_4: 0.2133  loss_dice_4: 0.1841  loss_ce_5: 0.1294  loss_mask_5: 0.2075  loss_dice_5: 0.188  loss_ce_6: 0.1294  loss_mask_6: 0.2056  loss_dice_6: 0.1843  loss_ce_7: 0.1297  loss_mask_7: 0.2026  loss_dice_7: 0.1922  loss_ce_8: 0.1299  loss_mask_8: 0.2076  loss_dice_8: 0.1842  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:10] d2.utils.events INFO:  eta: 1:32:57  iter: 5859  total_loss: 4.937  loss_ce: 0.1217  loss_mask: 0.2394  loss_dice: 0.142  loss_ce_0: 0.06713  loss_mask_0: 0.246  loss_dice_0: 0.1427  loss_ce_1: 0.1193  loss_mask_1: 0.2459  loss_dice_1: 0.1448  loss_ce_2: 0.1197  loss_mask_2: 0.2378  loss_dice_2: 0.1405  loss_ce_3: 0.1194  loss_mask_3: 0.2321  loss_dice_3: 0.1447  loss_ce_4: 0.1148  loss_mask_4: 0.2463  loss_dice_4: 0.1411  loss_ce_5: 0.1188  loss_mask_5: 0.2481  loss_dice_5: 0.1424  loss_ce_6: 0.1192  loss_mask_6: 0.2361  loss_dice_6: 0.1408  loss_ce_7: 0.118  loss_mask_7: 0.2404  loss_dice_7: 0.1428  loss_ce_8: 0.1207  loss_mask_8: 0.2362  loss_dice_8: 0.1462  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:49:13] d2.utils.events INFO:  eta: 1:32:56  iter: 5879  total_loss: 5.536  loss_ce: 0.1059  loss_mask: 0.2452  loss_dice: 0.1577  loss_ce_0: 0.06497  loss_mask_0: 0.2492  loss_dice_0: 0.1583  loss_ce_1: 0.105  loss_mask_1: 0.2352  loss_dice_1: 0.1572  loss_ce_2: 0.1053  loss_mask_2: 0.2428  loss_dice_2: 0.1628  loss_ce_3: 0.1054  loss_mask_3: 0.2337  loss_dice_3: 0.1544  loss_ce_4: 0.1048  loss_mask_4: 0.237  loss_dice_4: 0.1575  loss_ce_5: 0.1051  loss_mask_5: 0.2338  loss_dice_5: 0.1544  loss_ce_6: 0.1048  loss_mask_6: 0.2417  loss_dice_6: 0.1561  loss_ce_7: 0.1044  loss_mask_7: 0.2409  loss_dice_7: 0.1618  loss_ce_8: 0.1053  loss_mask_8: 0.2409  loss_dice_8: 0.1593  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:49:15] d2.utils.events INFO:  eta: 1:32:56  iter: 5899  total_loss: 5.642  loss_ce: 0.09889  loss_mask: 0.2253  loss_dice: 0.1704  loss_ce_0: 0.06241  loss_mask_0: 0.2239  loss_dice_0: 0.1685  loss_ce_1: 0.09801  loss_mask_1: 0.2175  loss_dice_1: 0.1712  loss_ce_2: 0.09867  loss_mask_2: 0.2188  loss_dice_2: 0.1754  loss_ce_3: 0.09859  loss_mask_3: 0.2159  loss_dice_3: 0.1731  loss_ce_4: 0.09852  loss_mask_4: 0.2155  loss_dice_4: 0.1822  loss_ce_5: 0.09829  loss_mask_5: 0.2248  loss_dice_5: 0.1701  loss_ce_6: 0.09819  loss_mask_6: 0.2268  loss_dice_6: 0.1728  loss_ce_7: 0.09779  loss_mask_7: 0.228  loss_dice_7: 0.1674  loss_ce_8: 0.09841  loss_mask_8: 0.2185  loss_dice_8: 0.1718  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:18] d2.utils.events INFO:  eta: 1:32:53  iter: 5919  total_loss: 5.329  loss_ce: 0.1282  loss_mask: 0.1938  loss_dice: 0.1456  loss_ce_0: 0.06657  loss_mask_0: 0.1871  loss_dice_0: 0.1419  loss_ce_1: 0.1281  loss_mask_1: 0.1794  loss_dice_1: 0.1534  loss_ce_2: 0.1281  loss_mask_2: 0.1874  loss_dice_2: 0.1473  loss_ce_3: 0.128  loss_mask_3: 0.1907  loss_dice_3: 0.1489  loss_ce_4: 0.1281  loss_mask_4: 0.1871  loss_dice_4: 0.1543  loss_ce_5: 0.128  loss_mask_5: 0.1984  loss_dice_5: 0.1468  loss_ce_6: 0.128  loss_mask_6: 0.1856  loss_dice_6: 0.1493  loss_ce_7: 0.1281  loss_mask_7: 0.1912  loss_dice_7: 0.1435  loss_ce_8: 0.128  loss_mask_8: 0.1854  loss_dice_8: 0.1514  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:20] d2.utils.events INFO:  eta: 1:32:49  iter: 5939  total_loss: 5.303  loss_ce: 0.1181  loss_mask: 0.2185  loss_dice: 0.1659  loss_ce_0: 0.06177  loss_mask_0: 0.2366  loss_dice_0: 0.1675  loss_ce_1: 0.1192  loss_mask_1: 0.232  loss_dice_1: 0.1622  loss_ce_2: 0.1187  loss_mask_2: 0.2376  loss_dice_2: 0.1661  loss_ce_3: 0.1186  loss_mask_3: 0.2334  loss_dice_3: 0.1644  loss_ce_4: 0.122  loss_mask_4: 0.2371  loss_dice_4: 0.1655  loss_ce_5: 0.1187  loss_mask_5: 0.226  loss_dice_5: 0.1688  loss_ce_6: 0.1184  loss_mask_6: 0.2265  loss_dice_6: 0.1701  loss_ce_7: 0.1208  loss_mask_7: 0.2298  loss_dice_7: 0.1677  loss_ce_8: 0.1186  loss_mask_8: 0.2241  loss_dice_8: 0.1676  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:23] d2.utils.events INFO:  eta: 1:32:46  iter: 5959  total_loss: 5.652  loss_ce: 0.1174  loss_mask: 0.2129  loss_dice: 0.1819  loss_ce_0: 0.06164  loss_mask_0: 0.2096  loss_dice_0: 0.1964  loss_ce_1: 0.1173  loss_mask_1: 0.1988  loss_dice_1: 0.1859  loss_ce_2: 0.1176  loss_mask_2: 0.1967  loss_dice_2: 0.1906  loss_ce_3: 0.1174  loss_mask_3: 0.2024  loss_dice_3: 0.1993  loss_ce_4: 0.1188  loss_mask_4: 0.209  loss_dice_4: 0.1946  loss_ce_5: 0.1174  loss_mask_5: 0.2078  loss_dice_5: 0.1862  loss_ce_6: 0.1173  loss_mask_6: 0.2182  loss_dice_6: 0.1898  loss_ce_7: 0.1181  loss_mask_7: 0.2022  loss_dice_7: 0.1914  loss_ce_8: 0.1177  loss_mask_8: 0.199  loss_dice_8: 0.1795  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:25] d2.utils.events INFO:  eta: 1:32:42  iter: 5979  total_loss: 4.746  loss_ce: 0.1011  loss_mask: 0.1809  loss_dice: 0.1605  loss_ce_0: 0.05932  loss_mask_0: 0.1957  loss_dice_0: 0.155  loss_ce_1: 0.09968  loss_mask_1: 0.1804  loss_dice_1: 0.1512  loss_ce_2: 0.09979  loss_mask_2: 0.183  loss_dice_2: 0.1583  loss_ce_3: 0.09904  loss_mask_3: 0.1803  loss_dice_3: 0.1611  loss_ce_4: 0.0981  loss_mask_4: 0.1886  loss_dice_4: 0.1558  loss_ce_5: 0.09922  loss_mask_5: 0.1745  loss_dice_5: 0.1596  loss_ce_6: 0.09927  loss_mask_6: 0.1882  loss_dice_6: 0.1613  loss_ce_7: 0.09923  loss_mask_7: 0.192  loss_dice_7: 0.1597  loss_ce_8: 0.1002  loss_mask_8: 0.1829  loss_dice_8: 0.1606  time: 0.1258  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:49:28] d2.utils.events INFO:  eta: 1:32:40  iter: 5999  total_loss: 4.897  loss_ce: 0.08906  loss_mask: 0.2008  loss_dice: 0.1345  loss_ce_0: 0.05657  loss_mask_0: 0.2256  loss_dice_0: 0.1392  loss_ce_1: 0.08743  loss_mask_1: 0.2116  loss_dice_1: 0.1426  loss_ce_2: 0.08817  loss_mask_2: 0.2057  loss_dice_2: 0.1387  loss_ce_3: 0.08759  loss_mask_3: 0.2029  loss_dice_3: 0.1369  loss_ce_4: 0.08533  loss_mask_4: 0.2048  loss_dice_4: 0.1376  loss_ce_5: 0.08742  loss_mask_5: 0.2142  loss_dice_5: 0.1403  loss_ce_6: 0.08742  loss_mask_6: 0.2138  loss_dice_6: 0.139  loss_ce_7: 0.08648  loss_mask_7: 0.204  loss_dice_7: 0.1421  loss_ce_8: 0.0881  loss_mask_8: 0.2139  loss_dice_8: 0.1407  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:31] d2.utils.events INFO:  eta: 1:32:41  iter: 6019  total_loss: 5.412  loss_ce: 0.1781  loss_mask: 0.2123  loss_dice: 0.1393  loss_ce_0: 0.07986  loss_mask_0: 0.2006  loss_dice_0: 0.1411  loss_ce_1: 0.1777  loss_mask_1: 0.2  loss_dice_1: 0.137  loss_ce_2: 0.1775  loss_mask_2: 0.2041  loss_dice_2: 0.1383  loss_ce_3: 0.1775  loss_mask_3: 0.2152  loss_dice_3: 0.1381  loss_ce_4: 0.1757  loss_mask_4: 0.2077  loss_dice_4: 0.1401  loss_ce_5: 0.1777  loss_mask_5: 0.2112  loss_dice_5: 0.1369  loss_ce_6: 0.178  loss_mask_6: 0.198  loss_dice_6: 0.139  loss_ce_7: 0.1773  loss_mask_7: 0.2083  loss_dice_7: 0.1368  loss_ce_8: 0.1781  loss_mask_8: 0.2099  loss_dice_8: 0.1406  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:33] d2.utils.events INFO:  eta: 1:32:42  iter: 6039  total_loss: 5.43  loss_ce: 0.09206  loss_mask: 0.1748  loss_dice: 0.1684  loss_ce_0: 0.05412  loss_mask_0: 0.1711  loss_dice_0: 0.1611  loss_ce_1: 0.09309  loss_mask_1: 0.1797  loss_dice_1: 0.1729  loss_ce_2: 0.09266  loss_mask_2: 0.1663  loss_dice_2: 0.1575  loss_ce_3: 0.0928  loss_mask_3: 0.1744  loss_dice_3: 0.1673  loss_ce_4: 0.09516  loss_mask_4: 0.1755  loss_dice_4: 0.1674  loss_ce_5: 0.09269  loss_mask_5: 0.1715  loss_dice_5: 0.1594  loss_ce_6: 0.09238  loss_mask_6: 0.1789  loss_dice_6: 0.1626  loss_ce_7: 0.09373  loss_mask_7: 0.1756  loss_dice_7: 0.1663  loss_ce_8: 0.09234  loss_mask_8: 0.1748  loss_dice_8: 0.1688  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:36] d2.utils.events INFO:  eta: 1:32:47  iter: 6059  total_loss: 5.026  loss_ce: 0.1308  loss_mask: 0.2038  loss_dice: 0.1391  loss_ce_0: 0.06723  loss_mask_0: 0.2086  loss_dice_0: 0.1394  loss_ce_1: 0.1307  loss_mask_1: 0.204  loss_dice_1: 0.1388  loss_ce_2: 0.1307  loss_mask_2: 0.2165  loss_dice_2: 0.1373  loss_ce_3: 0.1309  loss_mask_3: 0.2139  loss_dice_3: 0.1396  loss_ce_4: 0.1303  loss_mask_4: 0.2166  loss_dice_4: 0.1376  loss_ce_5: 0.1307  loss_mask_5: 0.218  loss_dice_5: 0.1395  loss_ce_6: 0.1308  loss_mask_6: 0.2055  loss_dice_6: 0.1404  loss_ce_7: 0.1306  loss_mask_7: 0.2139  loss_dice_7: 0.1378  loss_ce_8: 0.1308  loss_mask_8: 0.2127  loss_dice_8: 0.1394  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:38] d2.utils.events INFO:  eta: 1:32:47  iter: 6079  total_loss: 5.429  loss_ce: 0.1036  loss_mask: 0.2094  loss_dice: 0.1496  loss_ce_0: 0.05501  loss_mask_0: 0.224  loss_dice_0: 0.1459  loss_ce_1: 0.1045  loss_mask_1: 0.2162  loss_dice_1: 0.1476  loss_ce_2: 0.1041  loss_mask_2: 0.2208  loss_dice_2: 0.1463  loss_ce_3: 0.1046  loss_mask_3: 0.2105  loss_dice_3: 0.1465  loss_ce_4: 0.1067  loss_mask_4: 0.2157  loss_dice_4: 0.1484  loss_ce_5: 0.1044  loss_mask_5: 0.2117  loss_dice_5: 0.1379  loss_ce_6: 0.1043  loss_mask_6: 0.2103  loss_dice_6: 0.1393  loss_ce_7: 0.1053  loss_mask_7: 0.2136  loss_dice_7: 0.1446  loss_ce_8: 0.1044  loss_mask_8: 0.2154  loss_dice_8: 0.1423  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:41] d2.utils.events INFO:  eta: 1:32:46  iter: 6099  total_loss: 4.905  loss_ce: 0.1372  loss_mask: 0.2127  loss_dice: 0.1261  loss_ce_0: 0.07577  loss_mask_0: 0.2142  loss_dice_0: 0.1238  loss_ce_1: 0.1364  loss_mask_1: 0.2143  loss_dice_1: 0.1267  loss_ce_2: 0.1361  loss_mask_2: 0.2172  loss_dice_2: 0.1277  loss_ce_3: 0.1354  loss_mask_3: 0.2161  loss_dice_3: 0.1338  loss_ce_4: 0.1337  loss_mask_4: 0.1992  loss_dice_4: 0.1207  loss_ce_5: 0.1356  loss_mask_5: 0.2158  loss_dice_5: 0.1241  loss_ce_6: 0.1358  loss_mask_6: 0.2237  loss_dice_6: 0.1312  loss_ce_7: 0.1354  loss_mask_7: 0.207  loss_dice_7: 0.1242  loss_ce_8: 0.1358  loss_mask_8: 0.2082  loss_dice_8: 0.1238  time: 0.1258  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:43] d2.utils.events INFO:  eta: 1:32:44  iter: 6119  total_loss: 6.028  loss_ce: 0.1324  loss_mask: 0.1673  loss_dice: 0.1663  loss_ce_0: 0.06174  loss_mask_0: 0.1609  loss_dice_0: 0.1678  loss_ce_1: 0.1327  loss_mask_1: 0.1623  loss_dice_1: 0.1732  loss_ce_2: 0.1329  loss_mask_2: 0.1665  loss_dice_2: 0.1715  loss_ce_3: 0.1331  loss_mask_3: 0.1604  loss_dice_3: 0.165  loss_ce_4: 0.1332  loss_mask_4: 0.1673  loss_dice_4: 0.173  loss_ce_5: 0.133  loss_mask_5: 0.158  loss_dice_5: 0.1654  loss_ce_6: 0.133  loss_mask_6: 0.1671  loss_dice_6: 0.1624  loss_ce_7: 0.1327  loss_mask_7: 0.1579  loss_dice_7: 0.1668  loss_ce_8: 0.1329  loss_mask_8: 0.149  loss_dice_8: 0.1672  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:46] d2.utils.events INFO:  eta: 1:32:43  iter: 6139  total_loss: 6.142  loss_ce: 0.1259  loss_mask: 0.1943  loss_dice: 0.2072  loss_ce_0: 0.06132  loss_mask_0: 0.1954  loss_dice_0: 0.2138  loss_ce_1: 0.1258  loss_mask_1: 0.1921  loss_dice_1: 0.2202  loss_ce_2: 0.1257  loss_mask_2: 0.2015  loss_dice_2: 0.2187  loss_ce_3: 0.1256  loss_mask_3: 0.2081  loss_dice_3: 0.2202  loss_ce_4: 0.1255  loss_mask_4: 0.1918  loss_dice_4: 0.2183  loss_ce_5: 0.1257  loss_mask_5: 0.1883  loss_dice_5: 0.2228  loss_ce_6: 0.1257  loss_mask_6: 0.1847  loss_dice_6: 0.2114  loss_ce_7: 0.1257  loss_mask_7: 0.1965  loss_dice_7: 0.2064  loss_ce_8: 0.1257  loss_mask_8: 0.195  loss_dice_8: 0.2104  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:49] d2.utils.events INFO:  eta: 1:32:42  iter: 6159  total_loss: 5.556  loss_ce: 0.1023  loss_mask: 0.2198  loss_dice: 0.166  loss_ce_0: 0.05858  loss_mask_0: 0.2186  loss_dice_0: 0.1624  loss_ce_1: 0.1011  loss_mask_1: 0.2224  loss_dice_1: 0.1605  loss_ce_2: 0.1004  loss_mask_2: 0.2168  loss_dice_2: 0.1576  loss_ce_3: 0.09912  loss_mask_3: 0.2215  loss_dice_3: 0.1631  loss_ce_4: 0.09766  loss_mask_4: 0.2205  loss_dice_4: 0.1667  loss_ce_5: 0.09979  loss_mask_5: 0.2177  loss_dice_5: 0.1623  loss_ce_6: 0.1  loss_mask_6: 0.2235  loss_dice_6: 0.1647  loss_ce_7: 0.1011  loss_mask_7: 0.2216  loss_dice_7: 0.1651  loss_ce_8: 0.1006  loss_mask_8: 0.2295  loss_dice_8: 0.1623  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:51] d2.utils.events INFO:  eta: 1:32:42  iter: 6179  total_loss: 5.825  loss_ce: 0.08972  loss_mask: 0.2228  loss_dice: 0.156  loss_ce_0: 0.05606  loss_mask_0: 0.2161  loss_dice_0: 0.1551  loss_ce_1: 0.08923  loss_mask_1: 0.23  loss_dice_1: 0.1481  loss_ce_2: 0.08906  loss_mask_2: 0.2246  loss_dice_2: 0.1539  loss_ce_3: 0.08844  loss_mask_3: 0.2149  loss_dice_3: 0.155  loss_ce_4: 0.08822  loss_mask_4: 0.2169  loss_dice_4: 0.1516  loss_ce_5: 0.08871  loss_mask_5: 0.2194  loss_dice_5: 0.154  loss_ce_6: 0.08854  loss_mask_6: 0.2433  loss_dice_6: 0.1521  loss_ce_7: 0.08909  loss_mask_7: 0.2216  loss_dice_7: 0.1523  loss_ce_8: 0.08854  loss_mask_8: 0.2253  loss_dice_8: 0.1447  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:54] d2.utils.events INFO:  eta: 1:32:40  iter: 6199  total_loss: 5.606  loss_ce: 0.09983  loss_mask: 0.2412  loss_dice: 0.1672  loss_ce_0: 0.05649  loss_mask_0: 0.2394  loss_dice_0: 0.1693  loss_ce_1: 0.1001  loss_mask_1: 0.2353  loss_dice_1: 0.1706  loss_ce_2: 0.0999  loss_mask_2: 0.2409  loss_dice_2: 0.1661  loss_ce_3: 0.1  loss_mask_3: 0.2271  loss_dice_3: 0.1747  loss_ce_4: 0.1007  loss_mask_4: 0.2294  loss_dice_4: 0.1778  loss_ce_5: 0.09993  loss_mask_5: 0.2345  loss_dice_5: 0.1684  loss_ce_6: 0.09998  loss_mask_6: 0.2413  loss_dice_6: 0.1713  loss_ce_7: 0.1005  loss_mask_7: 0.2395  loss_dice_7: 0.1748  loss_ce_8: 0.09952  loss_mask_8: 0.2329  loss_dice_8: 0.1697  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:56] d2.utils.events INFO:  eta: 1:32:39  iter: 6219  total_loss: 5.232  loss_ce: 0.09381  loss_mask: 0.1904  loss_dice: 0.1527  loss_ce_0: 0.05528  loss_mask_0: 0.1843  loss_dice_0: 0.149  loss_ce_1: 0.09366  loss_mask_1: 0.1908  loss_dice_1: 0.1546  loss_ce_2: 0.0937  loss_mask_2: 0.1781  loss_dice_2: 0.1504  loss_ce_3: 0.09359  loss_mask_3: 0.1931  loss_dice_3: 0.1478  loss_ce_4: 0.09358  loss_mask_4: 0.188  loss_dice_4: 0.1524  loss_ce_5: 0.09344  loss_mask_5: 0.1883  loss_dice_5: 0.1471  loss_ce_6: 0.09331  loss_mask_6: 0.1863  loss_dice_6: 0.151  loss_ce_7: 0.09366  loss_mask_7: 0.1916  loss_dice_7: 0.1536  loss_ce_8: 0.09331  loss_mask_8: 0.1879  loss_dice_8: 0.1486  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:49:59] d2.utils.events INFO:  eta: 1:32:39  iter: 6239  total_loss: 4.989  loss_ce: 0.09234  loss_mask: 0.1954  loss_dice: 0.1303  loss_ce_0: 0.05409  loss_mask_0: 0.1831  loss_dice_0: 0.1175  loss_ce_1: 0.09249  loss_mask_1: 0.1921  loss_dice_1: 0.1156  loss_ce_2: 0.09245  loss_mask_2: 0.1853  loss_dice_2: 0.1269  loss_ce_3: 0.09241  loss_mask_3: 0.1815  loss_dice_3: 0.1253  loss_ce_4: 0.09261  loss_mask_4: 0.1924  loss_dice_4: 0.1289  loss_ce_5: 0.09223  loss_mask_5: 0.1865  loss_dice_5: 0.1229  loss_ce_6: 0.09213  loss_mask_6: 0.1831  loss_dice_6: 0.1254  loss_ce_7: 0.09238  loss_mask_7: 0.1857  loss_dice_7: 0.1208  loss_ce_8: 0.09227  loss_mask_8: 0.1857  loss_dice_8: 0.1253  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:01] d2.utils.events INFO:  eta: 1:32:33  iter: 6259  total_loss: 5.099  loss_ce: 0.09485  loss_mask: 0.2231  loss_dice: 0.1429  loss_ce_0: 0.05386  loss_mask_0: 0.223  loss_dice_0: 0.1415  loss_ce_1: 0.09492  loss_mask_1: 0.2257  loss_dice_1: 0.1439  loss_ce_2: 0.09492  loss_mask_2: 0.2118  loss_dice_2: 0.1403  loss_ce_3: 0.09499  loss_mask_3: 0.2225  loss_dice_3: 0.1427  loss_ce_4: 0.09542  loss_mask_4: 0.2247  loss_dice_4: 0.1428  loss_ce_5: 0.09499  loss_mask_5: 0.2269  loss_dice_5: 0.1429  loss_ce_6: 0.09478  loss_mask_6: 0.2172  loss_dice_6: 0.1457  loss_ce_7: 0.0951  loss_mask_7: 0.2219  loss_dice_7: 0.143  loss_ce_8: 0.09496  loss_mask_8: 0.2198  loss_dice_8: 0.1399  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:04] d2.utils.events INFO:  eta: 1:32:30  iter: 6279  total_loss: 5.394  loss_ce: 0.162  loss_mask: 0.2581  loss_dice: 0.1625  loss_ce_0: 0.0801  loss_mask_0: 0.2441  loss_dice_0: 0.1549  loss_ce_1: 0.1622  loss_mask_1: 0.2437  loss_dice_1: 0.1593  loss_ce_2: 0.1621  loss_mask_2: 0.2518  loss_dice_2: 0.154  loss_ce_3: 0.1616  loss_mask_3: 0.2509  loss_dice_3: 0.1577  loss_ce_4: 0.161  loss_mask_4: 0.2465  loss_dice_4: 0.154  loss_ce_5: 0.1619  loss_mask_5: 0.2465  loss_dice_5: 0.1584  loss_ce_6: 0.162  loss_mask_6: 0.2477  loss_dice_6: 0.1515  loss_ce_7: 0.1616  loss_mask_7: 0.2446  loss_dice_7: 0.1648  loss_ce_8: 0.1619  loss_mask_8: 0.2478  loss_dice_8: 0.1487  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:06] d2.utils.events INFO:  eta: 1:32:25  iter: 6299  total_loss: 5.014  loss_ce: 0.1419  loss_mask: 0.1865  loss_dice: 0.193  loss_ce_0: 0.07686  loss_mask_0: 0.1752  loss_dice_0: 0.1806  loss_ce_1: 0.1414  loss_mask_1: 0.1878  loss_dice_1: 0.1955  loss_ce_2: 0.1412  loss_mask_2: 0.1822  loss_dice_2: 0.1877  loss_ce_3: 0.1402  loss_mask_3: 0.1777  loss_dice_3: 0.1982  loss_ce_4: 0.1386  loss_mask_4: 0.1782  loss_dice_4: 0.196  loss_ce_5: 0.1407  loss_mask_5: 0.1649  loss_dice_5: 0.1861  loss_ce_6: 0.1403  loss_mask_6: 0.1729  loss_dice_6: 0.1968  loss_ce_7: 0.1406  loss_mask_7: 0.1824  loss_dice_7: 0.1928  loss_ce_8: 0.1409  loss_mask_8: 0.1908  loss_dice_8: 0.1898  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:50:09] d2.utils.events INFO:  eta: 1:32:23  iter: 6319  total_loss: 5.102  loss_ce: 0.1274  loss_mask: 0.2289  loss_dice: 0.1411  loss_ce_0: 0.07085  loss_mask_0: 0.2391  loss_dice_0: 0.1468  loss_ce_1: 0.1269  loss_mask_1: 0.2251  loss_dice_1: 0.1431  loss_ce_2: 0.1265  loss_mask_2: 0.2277  loss_dice_2: 0.148  loss_ce_3: 0.1265  loss_mask_3: 0.2312  loss_dice_3: 0.1492  loss_ce_4: 0.1267  loss_mask_4: 0.2289  loss_dice_4: 0.1396  loss_ce_5: 0.1265  loss_mask_5: 0.2232  loss_dice_5: 0.1412  loss_ce_6: 0.1265  loss_mask_6: 0.2175  loss_dice_6: 0.1417  loss_ce_7: 0.1264  loss_mask_7: 0.2354  loss_dice_7: 0.1461  loss_ce_8: 0.1266  loss_mask_8: 0.2304  loss_dice_8: 0.1422  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:50:11] d2.utils.events INFO:  eta: 1:32:18  iter: 6339  total_loss: 4.949  loss_ce: 0.1071  loss_mask: 0.2455  loss_dice: 0.1472  loss_ce_0: 0.06673  loss_mask_0: 0.2448  loss_dice_0: 0.1423  loss_ce_1: 0.1067  loss_mask_1: 0.2409  loss_dice_1: 0.1456  loss_ce_2: 0.1059  loss_mask_2: 0.2372  loss_dice_2: 0.1459  loss_ce_3: 0.1044  loss_mask_3: 0.2483  loss_dice_3: 0.1489  loss_ce_4: 0.1024  loss_mask_4: 0.2422  loss_dice_4: 0.1396  loss_ce_5: 0.1052  loss_mask_5: 0.2451  loss_dice_5: 0.1423  loss_ce_6: 0.1045  loss_mask_6: 0.2375  loss_dice_6: 0.1445  loss_ce_7: 0.1056  loss_mask_7: 0.2399  loss_dice_7: 0.143  loss_ce_8: 0.1054  loss_mask_8: 0.2357  loss_dice_8: 0.1457  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:50:14] d2.utils.events INFO:  eta: 1:32:13  iter: 6359  total_loss: 5.423  loss_ce: 0.1686  loss_mask: 0.2071  loss_dice: 0.1571  loss_ce_0: 0.06799  loss_mask_0: 0.2095  loss_dice_0: 0.1589  loss_ce_1: 0.1692  loss_mask_1: 0.2075  loss_dice_1: 0.1622  loss_ce_2: 0.1694  loss_mask_2: 0.2191  loss_dice_2: 0.1592  loss_ce_3: 0.1696  loss_mask_3: 0.2021  loss_dice_3: 0.155  loss_ce_4: 0.1701  loss_mask_4: 0.2062  loss_dice_4: 0.166  loss_ce_5: 0.1694  loss_mask_5: 0.2158  loss_dice_5: 0.1627  loss_ce_6: 0.1697  loss_mask_6: 0.2091  loss_dice_6: 0.1634  loss_ce_7: 0.1696  loss_mask_7: 0.2135  loss_dice_7: 0.1604  loss_ce_8: 0.1702  loss_mask_8: 0.2138  loss_dice_8: 0.1616  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:16] d2.utils.events INFO:  eta: 1:32:11  iter: 6379  total_loss: 5.749  loss_ce: 0.1291  loss_mask: 0.1682  loss_dice: 0.2456  loss_ce_0: 0.06607  loss_mask_0: 0.1629  loss_dice_0: 0.2512  loss_ce_1: 0.1299  loss_mask_1: 0.1686  loss_dice_1: 0.2567  loss_ce_2: 0.1292  loss_mask_2: 0.1749  loss_dice_2: 0.2444  loss_ce_3: 0.1291  loss_mask_3: 0.1712  loss_dice_3: 0.2509  loss_ce_4: 0.1299  loss_mask_4: 0.1684  loss_dice_4: 0.2347  loss_ce_5: 0.1291  loss_mask_5: 0.1727  loss_dice_5: 0.2481  loss_ce_6: 0.1291  loss_mask_6: 0.1655  loss_dice_6: 0.2509  loss_ce_7: 0.1295  loss_mask_7: 0.1682  loss_dice_7: 0.2456  loss_ce_8: 0.1292  loss_mask_8: 0.1645  loss_dice_8: 0.2373  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:19] d2.utils.events INFO:  eta: 1:32:08  iter: 6399  total_loss: 5.259  loss_ce: 0.1299  loss_mask: 0.1971  loss_dice: 0.1782  loss_ce_0: 0.06758  loss_mask_0: 0.213  loss_dice_0: 0.1752  loss_ce_1: 0.1304  loss_mask_1: 0.2028  loss_dice_1: 0.1787  loss_ce_2: 0.1313  loss_mask_2: 0.2007  loss_dice_2: 0.1795  loss_ce_3: 0.1313  loss_mask_3: 0.2011  loss_dice_3: 0.1792  loss_ce_4: 0.1319  loss_mask_4: 0.2149  loss_dice_4: 0.1775  loss_ce_5: 0.1314  loss_mask_5: 0.1958  loss_dice_5: 0.1768  loss_ce_6: 0.1317  loss_mask_6: 0.2132  loss_dice_6: 0.1826  loss_ce_7: 0.1303  loss_mask_7: 0.2046  loss_dice_7: 0.1746  loss_ce_8: 0.1309  loss_mask_8: 0.1997  loss_dice_8: 0.1809  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:50:22] d2.utils.events INFO:  eta: 1:32:06  iter: 6419  total_loss: 5.158  loss_ce: 0.1324  loss_mask: 0.2342  loss_dice: 0.1668  loss_ce_0: 0.06677  loss_mask_0: 0.2356  loss_dice_0: 0.1712  loss_ce_1: 0.1323  loss_mask_1: 0.232  loss_dice_1: 0.1698  loss_ce_2: 0.1321  loss_mask_2: 0.2384  loss_dice_2: 0.1641  loss_ce_3: 0.1319  loss_mask_3: 0.2381  loss_dice_3: 0.1664  loss_ce_4: 0.1315  loss_mask_4: 0.2377  loss_dice_4: 0.1695  loss_ce_5: 0.1319  loss_mask_5: 0.2349  loss_dice_5: 0.1668  loss_ce_6: 0.1319  loss_mask_6: 0.2334  loss_dice_6: 0.1713  loss_ce_7: 0.1324  loss_mask_7: 0.2298  loss_dice_7: 0.171  loss_ce_8: 0.1326  loss_mask_8: 0.2272  loss_dice_8: 0.1706  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:24] d2.utils.events INFO:  eta: 1:32:04  iter: 6439  total_loss: 5.324  loss_ce: 0.1312  loss_mask: 0.1893  loss_dice: 0.168  loss_ce_0: 0.06658  loss_mask_0: 0.1867  loss_dice_0: 0.1618  loss_ce_1: 0.1316  loss_mask_1: 0.1878  loss_dice_1: 0.1679  loss_ce_2: 0.1314  loss_mask_2: 0.19  loss_dice_2: 0.1714  loss_ce_3: 0.132  loss_mask_3: 0.1867  loss_dice_3: 0.1642  loss_ce_4: 0.1325  loss_mask_4: 0.1924  loss_dice_4: 0.1671  loss_ce_5: 0.1318  loss_mask_5: 0.1792  loss_dice_5: 0.1606  loss_ce_6: 0.1322  loss_mask_6: 0.1975  loss_dice_6: 0.1688  loss_ce_7: 0.1317  loss_mask_7: 0.194  loss_dice_7: 0.1695  loss_ce_8: 0.1314  loss_mask_8: 0.1913  loss_dice_8: 0.1656  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:50:27] d2.utils.events INFO:  eta: 1:32:01  iter: 6459  total_loss: 5.059  loss_ce: 0.1433  loss_mask: 0.2185  loss_dice: 0.1423  loss_ce_0: 0.06814  loss_mask_0: 0.2271  loss_dice_0: 0.1438  loss_ce_1: 0.1453  loss_mask_1: 0.2258  loss_dice_1: 0.1446  loss_ce_2: 0.1443  loss_mask_2: 0.2303  loss_dice_2: 0.1474  loss_ce_3: 0.1467  loss_mask_3: 0.2149  loss_dice_3: 0.1512  loss_ce_4: 0.1485  loss_mask_4: 0.2324  loss_dice_4: 0.1492  loss_ce_5: 0.1463  loss_mask_5: 0.2265  loss_dice_5: 0.1466  loss_ce_6: 0.148  loss_mask_6: 0.2239  loss_dice_6: 0.145  loss_ce_7: 0.1454  loss_mask_7: 0.2227  loss_dice_7: 0.1436  loss_ce_8: 0.1445  loss_mask_8: 0.2215  loss_dice_8: 0.1441  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:29] d2.utils.events INFO:  eta: 1:31:59  iter: 6479  total_loss: 5.216  loss_ce: 0.1284  loss_mask: 0.1958  loss_dice: 0.2041  loss_ce_0: 0.06622  loss_mask_0: 0.2076  loss_dice_0: 0.2043  loss_ce_1: 0.1292  loss_mask_1: 0.2029  loss_dice_1: 0.2057  loss_ce_2: 0.1296  loss_mask_2: 0.2056  loss_dice_2: 0.2037  loss_ce_3: 0.1304  loss_mask_3: 0.2113  loss_dice_3: 0.1989  loss_ce_4: 0.1319  loss_mask_4: 0.2013  loss_dice_4: 0.1911  loss_ce_5: 0.1311  loss_mask_5: 0.2107  loss_dice_5: 0.1948  loss_ce_6: 0.131  loss_mask_6: 0.2078  loss_dice_6: 0.1951  loss_ce_7: 0.1296  loss_mask_7: 0.21  loss_dice_7: 0.2002  loss_ce_8: 0.129  loss_mask_8: 0.2157  loss_dice_8: 0.1954  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:32] d2.utils.events INFO:  eta: 1:32:00  iter: 6499  total_loss: 4.735  loss_ce: 0.1305  loss_mask: 0.1827  loss_dice: 0.1516  loss_ce_0: 0.06734  loss_mask_0: 0.1857  loss_dice_0: 0.1536  loss_ce_1: 0.1301  loss_mask_1: 0.179  loss_dice_1: 0.1517  loss_ce_2: 0.1299  loss_mask_2: 0.1799  loss_dice_2: 0.1529  loss_ce_3: 0.1294  loss_mask_3: 0.1804  loss_dice_3: 0.1521  loss_ce_4: 0.1289  loss_mask_4: 0.1839  loss_dice_4: 0.15  loss_ce_5: 0.129  loss_mask_5: 0.1919  loss_dice_5: 0.1548  loss_ce_6: 0.1292  loss_mask_6: 0.1784  loss_dice_6: 0.1567  loss_ce_7: 0.1299  loss_mask_7: 0.1892  loss_dice_7: 0.1532  loss_ce_8: 0.1303  loss_mask_8: 0.1861  loss_dice_8: 0.1536  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:50:34] d2.utils.events INFO:  eta: 1:31:58  iter: 6519  total_loss: 5.028  loss_ce: 0.1283  loss_mask: 0.2213  loss_dice: 0.1372  loss_ce_0: 0.0665  loss_mask_0: 0.2255  loss_dice_0: 0.1433  loss_ce_1: 0.1282  loss_mask_1: 0.2269  loss_dice_1: 0.1432  loss_ce_2: 0.1283  loss_mask_2: 0.2175  loss_dice_2: 0.1352  loss_ce_3: 0.1281  loss_mask_3: 0.2151  loss_dice_3: 0.1395  loss_ce_4: 0.1278  loss_mask_4: 0.2262  loss_dice_4: 0.1354  loss_ce_5: 0.1279  loss_mask_5: 0.2176  loss_dice_5: 0.1379  loss_ce_6: 0.1278  loss_mask_6: 0.2233  loss_dice_6: 0.1399  loss_ce_7: 0.128  loss_mask_7: 0.2189  loss_dice_7: 0.1429  loss_ce_8: 0.1282  loss_mask_8: 0.212  loss_dice_8: 0.1378  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:50:37] d2.utils.events INFO:  eta: 1:31:53  iter: 6539  total_loss: 5.215  loss_ce: 0.1322  loss_mask: 0.1951  loss_dice: 0.1659  loss_ce_0: 0.06637  loss_mask_0: 0.1949  loss_dice_0: 0.1586  loss_ce_1: 0.1325  loss_mask_1: 0.2048  loss_dice_1: 0.1584  loss_ce_2: 0.1322  loss_mask_2: 0.1873  loss_dice_2: 0.1691  loss_ce_3: 0.1325  loss_mask_3: 0.2123  loss_dice_3: 0.1641  loss_ce_4: 0.1322  loss_mask_4: 0.1852  loss_dice_4: 0.1615  loss_ce_5: 0.1325  loss_mask_5: 0.1956  loss_dice_5: 0.1639  loss_ce_6: 0.1329  loss_mask_6: 0.193  loss_dice_6: 0.1543  loss_ce_7: 0.1325  loss_mask_7: 0.2063  loss_dice_7: 0.1658  loss_ce_8: 0.1324  loss_mask_8: 0.206  loss_dice_8: 0.1666  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:50:39] d2.utils.events INFO:  eta: 1:31:51  iter: 6559  total_loss: 5.487  loss_ce: 0.1264  loss_mask: 0.1827  loss_dice: 0.1772  loss_ce_0: 0.06587  loss_mask_0: 0.1914  loss_dice_0: 0.1869  loss_ce_1: 0.1263  loss_mask_1: 0.186  loss_dice_1: 0.1793  loss_ce_2: 0.1264  loss_mask_2: 0.1971  loss_dice_2: 0.1819  loss_ce_3: 0.1263  loss_mask_3: 0.1898  loss_dice_3: 0.1792  loss_ce_4: 0.1263  loss_mask_4: 0.1825  loss_dice_4: 0.1733  loss_ce_5: 0.1263  loss_mask_5: 0.191  loss_dice_5: 0.1735  loss_ce_6: 0.1263  loss_mask_6: 0.1819  loss_dice_6: 0.1747  loss_ce_7: 0.1263  loss_mask_7: 0.1877  loss_dice_7: 0.1798  loss_ce_8: 0.1264  loss_mask_8: 0.1827  loss_dice_8: 0.1743  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:42] d2.utils.events INFO:  eta: 1:31:48  iter: 6579  total_loss: 5.347  loss_ce: 0.1468  loss_mask: 0.2376  loss_dice: 0.1385  loss_ce_0: 0.06965  loss_mask_0: 0.2332  loss_dice_0: 0.1374  loss_ce_1: 0.1475  loss_mask_1: 0.2384  loss_dice_1: 0.1371  loss_ce_2: 0.1478  loss_mask_2: 0.2299  loss_dice_2: 0.1406  loss_ce_3: 0.1484  loss_mask_3: 0.2323  loss_dice_3: 0.1374  loss_ce_4: 0.1496  loss_mask_4: 0.2374  loss_dice_4: 0.1362  loss_ce_5: 0.1487  loss_mask_5: 0.2317  loss_dice_5: 0.1414  loss_ce_6: 0.1485  loss_mask_6: 0.2392  loss_dice_6: 0.142  loss_ce_7: 0.1477  loss_mask_7: 0.2343  loss_dice_7: 0.1411  loss_ce_8: 0.1481  loss_mask_8: 0.2384  loss_dice_8: 0.1421  time: 0.1259  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 14:50:44] d2.utils.events INFO:  eta: 1:31:48  iter: 6599  total_loss: 5.491  loss_ce: 0.1424  loss_mask: 0.2023  loss_dice: 0.1935  loss_ce_0: 0.07044  loss_mask_0: 0.2039  loss_dice_0: 0.1986  loss_ce_1: 0.1419  loss_mask_1: 0.2057  loss_dice_1: 0.1949  loss_ce_2: 0.1421  loss_mask_2: 0.2021  loss_dice_2: 0.1983  loss_ce_3: 0.1416  loss_mask_3: 0.1985  loss_dice_3: 0.1942  loss_ce_4: 0.141  loss_mask_4: 0.2005  loss_dice_4: 0.1859  loss_ce_5: 0.1415  loss_mask_5: 0.1992  loss_dice_5: 0.195  loss_ce_6: 0.1416  loss_mask_6: 0.2088  loss_dice_6: 0.1922  loss_ce_7: 0.142  loss_mask_7: 0.2066  loss_dice_7: 0.1986  loss_ce_8: 0.1424  loss_mask_8: 0.1962  loss_dice_8: 0.1945  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:47] d2.utils.events INFO:  eta: 1:31:45  iter: 6619  total_loss: 5.336  loss_ce: 0.1287  loss_mask: 0.2367  loss_dice: 0.1474  loss_ce_0: 0.06825  loss_mask_0: 0.2396  loss_dice_0: 0.1433  loss_ce_1: 0.1289  loss_mask_1: 0.2335  loss_dice_1: 0.1449  loss_ce_2: 0.129  loss_mask_2: 0.2396  loss_dice_2: 0.149  loss_ce_3: 0.1291  loss_mask_3: 0.2295  loss_dice_3: 0.1431  loss_ce_4: 0.1291  loss_mask_4: 0.2449  loss_dice_4: 0.1434  loss_ce_5: 0.1291  loss_mask_5: 0.2264  loss_dice_5: 0.1418  loss_ce_6: 0.1292  loss_mask_6: 0.2432  loss_dice_6: 0.147  loss_ce_7: 0.129  loss_mask_7: 0.2341  loss_dice_7: 0.1472  loss_ce_8: 0.1288  loss_mask_8: 0.2329  loss_dice_8: 0.1475  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:49] d2.utils.events INFO:  eta: 1:31:43  iter: 6639  total_loss: 5.221  loss_ce: 0.1281  loss_mask: 0.2238  loss_dice: 0.1342  loss_ce_0: 0.06598  loss_mask_0: 0.2212  loss_dice_0: 0.1351  loss_ce_1: 0.1284  loss_mask_1: 0.222  loss_dice_1: 0.1372  loss_ce_2: 0.1283  loss_mask_2: 0.2261  loss_dice_2: 0.1421  loss_ce_3: 0.1286  loss_mask_3: 0.2353  loss_dice_3: 0.1376  loss_ce_4: 0.1286  loss_mask_4: 0.2268  loss_dice_4: 0.1406  loss_ce_5: 0.1286  loss_mask_5: 0.2225  loss_dice_5: 0.14  loss_ce_6: 0.1286  loss_mask_6: 0.2265  loss_dice_6: 0.1382  loss_ce_7: 0.1284  loss_mask_7: 0.2287  loss_dice_7: 0.141  loss_ce_8: 0.1284  loss_mask_8: 0.2252  loss_dice_8: 0.1407  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:52] d2.utils.events INFO:  eta: 1:31:42  iter: 6659  total_loss: 4.831  loss_ce: 0.1263  loss_mask: 0.1888  loss_dice: 0.146  loss_ce_0: 0.06634  loss_mask_0: 0.1993  loss_dice_0: 0.1539  loss_ce_1: 0.1262  loss_mask_1: 0.1929  loss_dice_1: 0.1502  loss_ce_2: 0.1261  loss_mask_2: 0.1997  loss_dice_2: 0.1571  loss_ce_3: 0.1261  loss_mask_3: 0.1998  loss_dice_3: 0.1561  loss_ce_4: 0.126  loss_mask_4: 0.1954  loss_dice_4: 0.1536  loss_ce_5: 0.1261  loss_mask_5: 0.199  loss_dice_5: 0.1451  loss_ce_6: 0.1261  loss_mask_6: 0.1946  loss_dice_6: 0.1522  loss_ce_7: 0.1261  loss_mask_7: 0.2049  loss_dice_7: 0.156  loss_ce_8: 0.1261  loss_mask_8: 0.1995  loss_dice_8: 0.1528  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:50:54] d2.utils.events INFO:  eta: 1:31:40  iter: 6679  total_loss: 4.948  loss_ce: 0.1295  loss_mask: 0.1961  loss_dice: 0.1336  loss_ce_0: 0.06595  loss_mask_0: 0.2031  loss_dice_0: 0.1352  loss_ce_1: 0.1296  loss_mask_1: 0.2083  loss_dice_1: 0.144  loss_ce_2: 0.1297  loss_mask_2: 0.2002  loss_dice_2: 0.1364  loss_ce_3: 0.1298  loss_mask_3: 0.2039  loss_dice_3: 0.1414  loss_ce_4: 0.13  loss_mask_4: 0.1982  loss_dice_4: 0.1427  loss_ce_5: 0.1298  loss_mask_5: 0.2005  loss_dice_5: 0.1377  loss_ce_6: 0.1299  loss_mask_6: 0.2053  loss_dice_6: 0.1466  loss_ce_7: 0.1298  loss_mask_7: 0.1992  loss_dice_7: 0.1418  loss_ce_8: 0.1296  loss_mask_8: 0.2079  loss_dice_8: 0.1429  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:50:57] d2.utils.events INFO:  eta: 1:31:37  iter: 6699  total_loss: 5.187  loss_ce: 0.1196  loss_mask: 0.178  loss_dice: 0.1548  loss_ce_0: 0.06444  loss_mask_0: 0.1866  loss_dice_0: 0.1536  loss_ce_1: 0.1197  loss_mask_1: 0.1934  loss_dice_1: 0.1485  loss_ce_2: 0.1199  loss_mask_2: 0.191  loss_dice_2: 0.1486  loss_ce_3: 0.1199  loss_mask_3: 0.1928  loss_dice_3: 0.1501  loss_ce_4: 0.1201  loss_mask_4: 0.1894  loss_dice_4: 0.1566  loss_ce_5: 0.12  loss_mask_5: 0.1933  loss_dice_5: 0.1555  loss_ce_6: 0.1197  loss_mask_6: 0.1801  loss_dice_6: 0.1527  loss_ce_7: 0.12  loss_mask_7: 0.1916  loss_dice_7: 0.1555  loss_ce_8: 0.1196  loss_mask_8: 0.1859  loss_dice_8: 0.1523  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:00] d2.utils.events INFO:  eta: 1:31:33  iter: 6719  total_loss: 5.243  loss_ce: 0.1083  loss_mask: 0.2191  loss_dice: 0.1604  loss_ce_0: 0.0618  loss_mask_0: 0.2229  loss_dice_0: 0.1485  loss_ce_1: 0.1078  loss_mask_1: 0.23  loss_dice_1: 0.1542  loss_ce_2: 0.1079  loss_mask_2: 0.2254  loss_dice_2: 0.1543  loss_ce_3: 0.1073  loss_mask_3: 0.2251  loss_dice_3: 0.1515  loss_ce_4: 0.1072  loss_mask_4: 0.2255  loss_dice_4: 0.1519  loss_ce_5: 0.1075  loss_mask_5: 0.2175  loss_dice_5: 0.1487  loss_ce_6: 0.1071  loss_mask_6: 0.231  loss_dice_6: 0.1539  loss_ce_7: 0.1075  loss_mask_7: 0.2223  loss_dice_7: 0.1517  loss_ce_8: 0.1079  loss_mask_8: 0.2199  loss_dice_8: 0.1516  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:02] d2.utils.events INFO:  eta: 1:31:31  iter: 6739  total_loss: 4.675  loss_ce: 0.1589  loss_mask: 0.2123  loss_dice: 0.1412  loss_ce_0: 0.07172  loss_mask_0: 0.2108  loss_dice_0: 0.1397  loss_ce_1: 0.16  loss_mask_1: 0.2072  loss_dice_1: 0.1347  loss_ce_2: 0.1596  loss_mask_2: 0.2052  loss_dice_2: 0.139  loss_ce_3: 0.1608  loss_mask_3: 0.2115  loss_dice_3: 0.1432  loss_ce_4: 0.1611  loss_mask_4: 0.2141  loss_dice_4: 0.1393  loss_ce_5: 0.1603  loss_mask_5: 0.2106  loss_dice_5: 0.1386  loss_ce_6: 0.161  loss_mask_6: 0.2069  loss_dice_6: 0.1376  loss_ce_7: 0.1604  loss_mask_7: 0.2181  loss_dice_7: 0.1436  loss_ce_8: 0.1598  loss_mask_8: 0.2088  loss_dice_8: 0.1355  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:05] d2.utils.events INFO:  eta: 1:31:28  iter: 6759  total_loss: 5.284  loss_ce: 0.1526  loss_mask: 0.211  loss_dice: 0.1513  loss_ce_0: 0.0732  loss_mask_0: 0.2096  loss_dice_0: 0.1579  loss_ce_1: 0.1526  loss_mask_1: 0.2095  loss_dice_1: 0.1465  loss_ce_2: 0.1523  loss_mask_2: 0.2228  loss_dice_2: 0.1508  loss_ce_3: 0.1525  loss_mask_3: 0.2129  loss_dice_3: 0.1529  loss_ce_4: 0.1521  loss_mask_4: 0.2177  loss_dice_4: 0.1508  loss_ce_5: 0.1525  loss_mask_5: 0.2196  loss_dice_5: 0.1539  loss_ce_6: 0.1528  loss_mask_6: 0.2119  loss_dice_6: 0.1483  loss_ce_7: 0.1526  loss_mask_7: 0.2105  loss_dice_7: 0.1571  loss_ce_8: 0.1527  loss_mask_8: 0.211  loss_dice_8: 0.1476  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:07] d2.utils.events INFO:  eta: 1:31:21  iter: 6779  total_loss: 4.873  loss_ce: 0.124  loss_mask: 0.1809  loss_dice: 0.1645  loss_ce_0: 0.06127  loss_mask_0: 0.1737  loss_dice_0: 0.1602  loss_ce_1: 0.1246  loss_mask_1: 0.1699  loss_dice_1: 0.1598  loss_ce_2: 0.1251  loss_mask_2: 0.1828  loss_dice_2: 0.1583  loss_ce_3: 0.1256  loss_mask_3: 0.1798  loss_dice_3: 0.1579  loss_ce_4: 0.1269  loss_mask_4: 0.1852  loss_dice_4: 0.1703  loss_ce_5: 0.1259  loss_mask_5: 0.1724  loss_dice_5: 0.1579  loss_ce_6: 0.1256  loss_mask_6: 0.1792  loss_dice_6: 0.1598  loss_ce_7: 0.1253  loss_mask_7: 0.1738  loss_dice_7: 0.1655  loss_ce_8: 0.1251  loss_mask_8: 0.1785  loss_dice_8: 0.1579  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:10] d2.utils.events INFO:  eta: 1:31:19  iter: 6799  total_loss: 5.801  loss_ce: 0.1276  loss_mask: 0.1957  loss_dice: 0.2594  loss_ce_0: 0.06264  loss_mask_0: 0.1959  loss_dice_0: 0.2497  loss_ce_1: 0.1277  loss_mask_1: 0.1975  loss_dice_1: 0.2576  loss_ce_2: 0.1275  loss_mask_2: 0.1996  loss_dice_2: 0.2673  loss_ce_3: 0.1274  loss_mask_3: 0.1978  loss_dice_3: 0.2539  loss_ce_4: 0.1279  loss_mask_4: 0.19  loss_dice_4: 0.2454  loss_ce_5: 0.1275  loss_mask_5: 0.1982  loss_dice_5: 0.2599  loss_ce_6: 0.1274  loss_mask_6: 0.1934  loss_dice_6: 0.2623  loss_ce_7: 0.1274  loss_mask_7: 0.199  loss_dice_7: 0.2673  loss_ce_8: 0.1276  loss_mask_8: 0.199  loss_dice_8: 0.2655  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:12] d2.utils.events INFO:  eta: 1:31:15  iter: 6819  total_loss: 5.359  loss_ce: 0.125  loss_mask: 0.1602  loss_dice: 0.1944  loss_ce_0: 0.06224  loss_mask_0: 0.1665  loss_dice_0: 0.2008  loss_ce_1: 0.1252  loss_mask_1: 0.1552  loss_dice_1: 0.1976  loss_ce_2: 0.1254  loss_mask_2: 0.1576  loss_dice_2: 0.1925  loss_ce_3: 0.1256  loss_mask_3: 0.1531  loss_dice_3: 0.1987  loss_ce_4: 0.1259  loss_mask_4: 0.1607  loss_dice_4: 0.1984  loss_ce_5: 0.1252  loss_mask_5: 0.158  loss_dice_5: 0.209  loss_ce_6: 0.1255  loss_mask_6: 0.1674  loss_dice_6: 0.2017  loss_ce_7: 0.1253  loss_mask_7: 0.1614  loss_dice_7: 0.2012  loss_ce_8: 0.1258  loss_mask_8: 0.1489  loss_dice_8: 0.2004  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:15] d2.utils.events INFO:  eta: 1:31:12  iter: 6839  total_loss: 4.718  loss_ce: 0.1285  loss_mask: 0.1979  loss_dice: 0.1152  loss_ce_0: 0.06595  loss_mask_0: 0.1995  loss_dice_0: 0.1169  loss_ce_1: 0.1283  loss_mask_1: 0.1969  loss_dice_1: 0.1164  loss_ce_2: 0.128  loss_mask_2: 0.2026  loss_dice_2: 0.1134  loss_ce_3: 0.1279  loss_mask_3: 0.1902  loss_dice_3: 0.1186  loss_ce_4: 0.1277  loss_mask_4: 0.2015  loss_dice_4: 0.1138  loss_ce_5: 0.128  loss_mask_5: 0.2011  loss_dice_5: 0.1126  loss_ce_6: 0.1281  loss_mask_6: 0.1996  loss_dice_6: 0.1182  loss_ce_7: 0.1281  loss_mask_7: 0.1913  loss_dice_7: 0.1156  loss_ce_8: 0.1283  loss_mask_8: 0.1935  loss_dice_8: 0.1153  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:17] d2.utils.events INFO:  eta: 1:31:11  iter: 6859  total_loss: 5.952  loss_ce: 0.1162  loss_mask: 0.1661  loss_dice: 0.1744  loss_ce_0: 0.06081  loss_mask_0: 0.1663  loss_dice_0: 0.1774  loss_ce_1: 0.1162  loss_mask_1: 0.1656  loss_dice_1: 0.1795  loss_ce_2: 0.1167  loss_mask_2: 0.1725  loss_dice_2: 0.182  loss_ce_3: 0.1168  loss_mask_3: 0.1696  loss_dice_3: 0.1763  loss_ce_4: 0.1173  loss_mask_4: 0.1698  loss_dice_4: 0.1766  loss_ce_5: 0.1169  loss_mask_5: 0.1723  loss_dice_5: 0.1787  loss_ce_6: 0.1167  loss_mask_6: 0.1736  loss_dice_6: 0.1738  loss_ce_7: 0.1166  loss_mask_7: 0.1627  loss_dice_7: 0.1786  loss_ce_8: 0.1165  loss_mask_8: 0.1734  loss_dice_8: 0.1715  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:20] d2.utils.events INFO:  eta: 1:31:07  iter: 6879  total_loss: 5.174  loss_ce: 0.1283  loss_mask: 0.2442  loss_dice: 0.1523  loss_ce_0: 0.06578  loss_mask_0: 0.2338  loss_dice_0: 0.1478  loss_ce_1: 0.1283  loss_mask_1: 0.2411  loss_dice_1: 0.1566  loss_ce_2: 0.1283  loss_mask_2: 0.2337  loss_dice_2: 0.1573  loss_ce_3: 0.1283  loss_mask_3: 0.2398  loss_dice_3: 0.1469  loss_ce_4: 0.1285  loss_mask_4: 0.2424  loss_dice_4: 0.1591  loss_ce_5: 0.1283  loss_mask_5: 0.2436  loss_dice_5: 0.1524  loss_ce_6: 0.1283  loss_mask_6: 0.2392  loss_dice_6: 0.1561  loss_ce_7: 0.1283  loss_mask_7: 0.2409  loss_dice_7: 0.1469  loss_ce_8: 0.1283  loss_mask_8: 0.2373  loss_dice_8: 0.1543  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:22] d2.utils.events INFO:  eta: 1:31:04  iter: 6899  total_loss: 4.76  loss_ce: 0.1253  loss_mask: 0.2143  loss_dice: 0.1494  loss_ce_0: 0.06229  loss_mask_0: 0.2068  loss_dice_0: 0.1509  loss_ce_1: 0.1256  loss_mask_1: 0.2115  loss_dice_1: 0.1476  loss_ce_2: 0.126  loss_mask_2: 0.2144  loss_dice_2: 0.1487  loss_ce_3: 0.1264  loss_mask_3: 0.2059  loss_dice_3: 0.1509  loss_ce_4: 0.1271  loss_mask_4: 0.2077  loss_dice_4: 0.1443  loss_ce_5: 0.1263  loss_mask_5: 0.2088  loss_dice_5: 0.1504  loss_ce_6: 0.1263  loss_mask_6: 0.216  loss_dice_6: 0.1501  loss_ce_7: 0.1259  loss_mask_7: 0.2128  loss_dice_7: 0.1567  loss_ce_8: 0.126  loss_mask_8: 0.201  loss_dice_8: 0.1501  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:25] d2.utils.events INFO:  eta: 1:31:03  iter: 6919  total_loss: 4.858  loss_ce: 0.1143  loss_mask: 0.2207  loss_dice: 0.1303  loss_ce_0: 0.06092  loss_mask_0: 0.2342  loss_dice_0: 0.1303  loss_ce_1: 0.1138  loss_mask_1: 0.2406  loss_dice_1: 0.1379  loss_ce_2: 0.114  loss_mask_2: 0.2378  loss_dice_2: 0.1392  loss_ce_3: 0.1136  loss_mask_3: 0.2328  loss_dice_3: 0.1383  loss_ce_4: 0.113  loss_mask_4: 0.2234  loss_dice_4: 0.1321  loss_ce_5: 0.1133  loss_mask_5: 0.2235  loss_dice_5: 0.1342  loss_ce_6: 0.1135  loss_mask_6: 0.2177  loss_dice_6: 0.1373  loss_ce_7: 0.1135  loss_mask_7: 0.224  loss_dice_7: 0.133  loss_ce_8: 0.1139  loss_mask_8: 0.2385  loss_dice_8: 0.1336  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:28] d2.utils.events INFO:  eta: 1:31:04  iter: 6939  total_loss: 4.826  loss_ce: 0.1344  loss_mask: 0.2138  loss_dice: 0.1394  loss_ce_0: 0.06948  loss_mask_0: 0.2168  loss_dice_0: 0.1437  loss_ce_1: 0.1346  loss_mask_1: 0.2044  loss_dice_1: 0.1415  loss_ce_2: 0.1342  loss_mask_2: 0.2168  loss_dice_2: 0.1451  loss_ce_3: 0.134  loss_mask_3: 0.2103  loss_dice_3: 0.1394  loss_ce_4: 0.1334  loss_mask_4: 0.2102  loss_dice_4: 0.145  loss_ce_5: 0.1339  loss_mask_5: 0.2136  loss_dice_5: 0.1433  loss_ce_6: 0.134  loss_mask_6: 0.2069  loss_dice_6: 0.1463  loss_ce_7: 0.134  loss_mask_7: 0.2167  loss_dice_7: 0.1422  loss_ce_8: 0.1341  loss_mask_8: 0.2148  loss_dice_8: 0.147  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:30] d2.utils.events INFO:  eta: 1:31:04  iter: 6959  total_loss: 5.476  loss_ce: 0.1277  loss_mask: 0.2264  loss_dice: 0.1583  loss_ce_0: 0.06264  loss_mask_0: 0.2153  loss_dice_0: 0.1569  loss_ce_1: 0.1276  loss_mask_1: 0.2333  loss_dice_1: 0.1679  loss_ce_2: 0.1279  loss_mask_2: 0.2283  loss_dice_2: 0.1607  loss_ce_3: 0.1283  loss_mask_3: 0.2223  loss_dice_3: 0.1573  loss_ce_4: 0.1291  loss_mask_4: 0.2348  loss_dice_4: 0.1694  loss_ce_5: 0.1283  loss_mask_5: 0.2285  loss_dice_5: 0.1636  loss_ce_6: 0.1283  loss_mask_6: 0.2253  loss_dice_6: 0.1618  loss_ce_7: 0.1283  loss_mask_7: 0.2227  loss_dice_7: 0.1658  loss_ce_8: 0.1281  loss_mask_8: 0.2256  loss_dice_8: 0.164  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:33] d2.utils.events INFO:  eta: 1:31:01  iter: 6979  total_loss: 4.905  loss_ce: 0.1163  loss_mask: 0.2043  loss_dice: 0.145  loss_ce_0: 0.06109  loss_mask_0: 0.2013  loss_dice_0: 0.1416  loss_ce_1: 0.1163  loss_mask_1: 0.2034  loss_dice_1: 0.1402  loss_ce_2: 0.1165  loss_mask_2: 0.2039  loss_dice_2: 0.1397  loss_ce_3: 0.1164  loss_mask_3: 0.2083  loss_dice_3: 0.1432  loss_ce_4: 0.1161  loss_mask_4: 0.2028  loss_dice_4: 0.1424  loss_ce_5: 0.1163  loss_mask_5: 0.2175  loss_dice_5: 0.1401  loss_ce_6: 0.1161  loss_mask_6: 0.1972  loss_dice_6: 0.1407  loss_ce_7: 0.1161  loss_mask_7: 0.2051  loss_dice_7: 0.1414  loss_ce_8: 0.1163  loss_mask_8: 0.2011  loss_dice_8: 0.139  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:35] d2.utils.events INFO:  eta: 1:30:59  iter: 6999  total_loss: 5.095  loss_ce: 0.1171  loss_mask: 0.2056  loss_dice: 0.2071  loss_ce_0: 0.06122  loss_mask_0: 0.2035  loss_dice_0: 0.2091  loss_ce_1: 0.1171  loss_mask_1: 0.2041  loss_dice_1: 0.2055  loss_ce_2: 0.1173  loss_mask_2: 0.2028  loss_dice_2: 0.2178  loss_ce_3: 0.1173  loss_mask_3: 0.1973  loss_dice_3: 0.2146  loss_ce_4: 0.1171  loss_mask_4: 0.209  loss_dice_4: 0.2067  loss_ce_5: 0.1172  loss_mask_5: 0.199  loss_dice_5: 0.2159  loss_ce_6: 0.117  loss_mask_6: 0.2039  loss_dice_6: 0.2125  loss_ce_7: 0.117  loss_mask_7: 0.2036  loss_dice_7: 0.2067  loss_ce_8: 0.1171  loss_mask_8: 0.1992  loss_dice_8: 0.1893  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:38] d2.utils.events INFO:  eta: 1:30:52  iter: 7019  total_loss: 4.953  loss_ce: 0.1121  loss_mask: 0.224  loss_dice: 0.1378  loss_ce_0: 0.05992  loss_mask_0: 0.2254  loss_dice_0: 0.1339  loss_ce_1: 0.1119  loss_mask_1: 0.2278  loss_dice_1: 0.1345  loss_ce_2: 0.112  loss_mask_2: 0.2317  loss_dice_2: 0.1373  loss_ce_3: 0.112  loss_mask_3: 0.2302  loss_dice_3: 0.1336  loss_ce_4: 0.1122  loss_mask_4: 0.2254  loss_dice_4: 0.1381  loss_ce_5: 0.112  loss_mask_5: 0.217  loss_dice_5: 0.1377  loss_ce_6: 0.112  loss_mask_6: 0.2149  loss_dice_6: 0.131  loss_ce_7: 0.1121  loss_mask_7: 0.2254  loss_dice_7: 0.1391  loss_ce_8: 0.1121  loss_mask_8: 0.2223  loss_dice_8: 0.1338  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:40] d2.utils.events INFO:  eta: 1:30:50  iter: 7039  total_loss: 5.757  loss_ce: 0.1144  loss_mask: 0.1926  loss_dice: 0.1695  loss_ce_0: 0.06022  loss_mask_0: 0.1826  loss_dice_0: 0.17  loss_ce_1: 0.1143  loss_mask_1: 0.1907  loss_dice_1: 0.1761  loss_ce_2: 0.1144  loss_mask_2: 0.2042  loss_dice_2: 0.1731  loss_ce_3: 0.1146  loss_mask_3: 0.1911  loss_dice_3: 0.1672  loss_ce_4: 0.1148  loss_mask_4: 0.193  loss_dice_4: 0.1772  loss_ce_5: 0.1145  loss_mask_5: 0.2032  loss_dice_5: 0.1764  loss_ce_6: 0.1145  loss_mask_6: 0.2034  loss_dice_6: 0.1789  loss_ce_7: 0.1144  loss_mask_7: 0.2006  loss_dice_7: 0.177  loss_ce_8: 0.1145  loss_mask_8: 0.2008  loss_dice_8: 0.1771  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:43] d2.utils.events INFO:  eta: 1:30:45  iter: 7059  total_loss: 5.024  loss_ce: 0.1024  loss_mask: 0.231  loss_dice: 0.1801  loss_ce_0: 0.05798  loss_mask_0: 0.2264  loss_dice_0: 0.1752  loss_ce_1: 0.1021  loss_mask_1: 0.2309  loss_dice_1: 0.1754  loss_ce_2: 0.1023  loss_mask_2: 0.2248  loss_dice_2: 0.1776  loss_ce_3: 0.1018  loss_mask_3: 0.2415  loss_dice_3: 0.1783  loss_ce_4: 0.1016  loss_mask_4: 0.225  loss_dice_4: 0.1766  loss_ce_5: 0.1019  loss_mask_5: 0.2276  loss_dice_5: 0.1745  loss_ce_6: 0.1017  loss_mask_6: 0.2315  loss_dice_6: 0.1777  loss_ce_7: 0.1016  loss_mask_7: 0.2188  loss_dice_7: 0.1722  loss_ce_8: 0.102  loss_mask_8: 0.232  loss_dice_8: 0.1762  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:45] d2.utils.events INFO:  eta: 1:30:41  iter: 7079  total_loss: 5.228  loss_ce: 0.1844  loss_mask: 0.2102  loss_dice: 0.1401  loss_ce_0: 0.07846  loss_mask_0: 0.2109  loss_dice_0: 0.1352  loss_ce_1: 0.1848  loss_mask_1: 0.2012  loss_dice_1: 0.1429  loss_ce_2: 0.1846  loss_mask_2: 0.2108  loss_dice_2: 0.1403  loss_ce_3: 0.1858  loss_mask_3: 0.2045  loss_dice_3: 0.1412  loss_ce_4: 0.1858  loss_mask_4: 0.2065  loss_dice_4: 0.1406  loss_ce_5: 0.1854  loss_mask_5: 0.2011  loss_dice_5: 0.1313  loss_ce_6: 0.1861  loss_mask_6: 0.2195  loss_dice_6: 0.14  loss_ce_7: 0.1859  loss_mask_7: 0.2174  loss_dice_7: 0.1428  loss_ce_8: 0.1851  loss_mask_8: 0.209  loss_dice_8: 0.1405  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:48] d2.utils.events INFO:  eta: 1:30:36  iter: 7099  total_loss: 6.07  loss_ce: 0.09564  loss_mask: 0.2084  loss_dice: 0.1711  loss_ce_0: 0.0544  loss_mask_0: 0.2148  loss_dice_0: 0.1685  loss_ce_1: 0.09564  loss_mask_1: 0.206  loss_dice_1: 0.1751  loss_ce_2: 0.09532  loss_mask_2: 0.2231  loss_dice_2: 0.1736  loss_ce_3: 0.09528  loss_mask_3: 0.2162  loss_dice_3: 0.1783  loss_ce_4: 0.09593  loss_mask_4: 0.2163  loss_dice_4: 0.1776  loss_ce_5: 0.0956  loss_mask_5: 0.2217  loss_dice_5: 0.1812  loss_ce_6: 0.0955  loss_mask_6: 0.2256  loss_dice_6: 0.1659  loss_ce_7: 0.09612  loss_mask_7: 0.2062  loss_dice_7: 0.1773  loss_ce_8: 0.09583  loss_mask_8: 0.2116  loss_dice_8: 0.1775  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:50] d2.utils.events INFO:  eta: 1:30:32  iter: 7119  total_loss: 5.413  loss_ce: 0.1006  loss_mask: 0.1916  loss_dice: 0.1976  loss_ce_0: 0.05454  loss_mask_0: 0.1911  loss_dice_0: 0.2024  loss_ce_1: 0.1008  loss_mask_1: 0.1939  loss_dice_1: 0.2068  loss_ce_2: 0.1005  loss_mask_2: 0.1865  loss_dice_2: 0.2107  loss_ce_3: 0.1008  loss_mask_3: 0.1921  loss_dice_3: 0.2129  loss_ce_4: 0.1015  loss_mask_4: 0.1889  loss_dice_4: 0.2115  loss_ce_5: 0.1009  loss_mask_5: 0.1954  loss_dice_5: 0.2093  loss_ce_6: 0.1007  loss_mask_6: 0.186  loss_dice_6: 0.2129  loss_ce_7: 0.1013  loss_mask_7: 0.1914  loss_dice_7: 0.2153  loss_ce_8: 0.1009  loss_mask_8: 0.1882  loss_dice_8: 0.2107  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:51:53] d2.utils.events INFO:  eta: 1:30:27  iter: 7139  total_loss: 4.504  loss_ce: 0.1437  loss_mask: 0.1908  loss_dice: 0.1258  loss_ce_0: 0.07512  loss_mask_0: 0.1905  loss_dice_0: 0.1249  loss_ce_1: 0.143  loss_mask_1: 0.193  loss_dice_1: 0.1255  loss_ce_2: 0.1434  loss_mask_2: 0.1921  loss_dice_2: 0.1269  loss_ce_3: 0.1424  loss_mask_3: 0.1968  loss_dice_3: 0.1238  loss_ce_4: 0.1409  loss_mask_4: 0.2025  loss_dice_4: 0.1241  loss_ce_5: 0.1425  loss_mask_5: 0.1921  loss_dice_5: 0.1255  loss_ce_6: 0.1425  loss_mask_6: 0.187  loss_dice_6: 0.1256  loss_ce_7: 0.142  loss_mask_7: 0.191  loss_dice_7: 0.131  loss_ce_8: 0.1429  loss_mask_8: 0.1907  loss_dice_8: 0.1275  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:55] d2.utils.events INFO:  eta: 1:30:25  iter: 7159  total_loss: 4.928  loss_ce: 0.1313  loss_mask: 0.1477  loss_dice: 0.1884  loss_ce_0: 0.07113  loss_mask_0: 0.157  loss_dice_0: 0.1982  loss_ce_1: 0.1317  loss_mask_1: 0.1465  loss_dice_1: 0.1866  loss_ce_2: 0.1319  loss_mask_2: 0.1549  loss_dice_2: 0.1907  loss_ce_3: 0.131  loss_mask_3: 0.1487  loss_dice_3: 0.1929  loss_ce_4: 0.1301  loss_mask_4: 0.1478  loss_dice_4: 0.1903  loss_ce_5: 0.1312  loss_mask_5: 0.1483  loss_dice_5: 0.1925  loss_ce_6: 0.131  loss_mask_6: 0.1368  loss_dice_6: 0.1844  loss_ce_7: 0.1305  loss_mask_7: 0.1413  loss_dice_7: 0.2023  loss_ce_8: 0.1315  loss_mask_8: 0.1476  loss_dice_8: 0.1833  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:51:58] d2.utils.events INFO:  eta: 1:30:20  iter: 7179  total_loss: 5.053  loss_ce: 0.114  loss_mask: 0.2005  loss_dice: 0.1446  loss_ce_0: 0.06638  loss_mask_0: 0.1981  loss_dice_0: 0.1396  loss_ce_1: 0.1124  loss_mask_1: 0.1992  loss_dice_1: 0.1408  loss_ce_2: 0.1132  loss_mask_2: 0.1931  loss_dice_2: 0.1422  loss_ce_3: 0.1113  loss_mask_3: 0.1985  loss_dice_3: 0.1381  loss_ce_4: 0.1081  loss_mask_4: 0.2031  loss_dice_4: 0.1462  loss_ce_5: 0.1115  loss_mask_5: 0.1992  loss_dice_5: 0.1447  loss_ce_6: 0.1111  loss_mask_6: 0.1981  loss_dice_6: 0.1465  loss_ce_7: 0.1103  loss_mask_7: 0.197  loss_dice_7: 0.1383  loss_ce_8: 0.1118  loss_mask_8: 0.201  loss_dice_8: 0.1421  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:00] d2.utils.events INFO:  eta: 1:30:17  iter: 7199  total_loss: 4.633  loss_ce: 0.1354  loss_mask: 0.1979  loss_dice: 0.1434  loss_ce_0: 0.06449  loss_mask_0: 0.2091  loss_dice_0: 0.1452  loss_ce_1: 0.135  loss_mask_1: 0.2018  loss_dice_1: 0.1408  loss_ce_2: 0.1349  loss_mask_2: 0.2198  loss_dice_2: 0.1476  loss_ce_3: 0.1341  loss_mask_3: 0.2064  loss_dice_3: 0.1417  loss_ce_4: 0.1328  loss_mask_4: 0.201  loss_dice_4: 0.1418  loss_ce_5: 0.1341  loss_mask_5: 0.2003  loss_dice_5: 0.1405  loss_ce_6: 0.1342  loss_mask_6: 0.2116  loss_dice_6: 0.144  loss_ce_7: 0.1338  loss_mask_7: 0.1981  loss_dice_7: 0.1352  loss_ce_8: 0.1352  loss_mask_8: 0.2024  loss_dice_8: 0.1395  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:03] d2.utils.events INFO:  eta: 1:30:15  iter: 7219  total_loss: 5.147  loss_ce: 0.1262  loss_mask: 0.2161  loss_dice: 0.1327  loss_ce_0: 0.06401  loss_mask_0: 0.2198  loss_dice_0: 0.1299  loss_ce_1: 0.1262  loss_mask_1: 0.2035  loss_dice_1: 0.1353  loss_ce_2: 0.1261  loss_mask_2: 0.2116  loss_dice_2: 0.1379  loss_ce_3: 0.1265  loss_mask_3: 0.2061  loss_dice_3: 0.1326  loss_ce_4: 0.1248  loss_mask_4: 0.2165  loss_dice_4: 0.1345  loss_ce_5: 0.1267  loss_mask_5: 0.2097  loss_dice_5: 0.1299  loss_ce_6: 0.1268  loss_mask_6: 0.2109  loss_dice_6: 0.1324  loss_ce_7: 0.1266  loss_mask_7: 0.2112  loss_dice_7: 0.1346  loss_ce_8: 0.1261  loss_mask_8: 0.2123  loss_dice_8: 0.13  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:06] d2.utils.events INFO:  eta: 1:30:11  iter: 7239  total_loss: 5.054  loss_ce: 0.1364  loss_mask: 0.2162  loss_dice: 0.1487  loss_ce_0: 0.06745  loss_mask_0: 0.2198  loss_dice_0: 0.1463  loss_ce_1: 0.1374  loss_mask_1: 0.2174  loss_dice_1: 0.1464  loss_ce_2: 0.1371  loss_mask_2: 0.2044  loss_dice_2: 0.1499  loss_ce_3: 0.138  loss_mask_3: 0.217  loss_dice_3: 0.1471  loss_ce_4: 0.1392  loss_mask_4: 0.2227  loss_dice_4: 0.1457  loss_ce_5: 0.1377  loss_mask_5: 0.2165  loss_dice_5: 0.1475  loss_ce_6: 0.1379  loss_mask_6: 0.212  loss_dice_6: 0.1455  loss_ce_7: 0.1379  loss_mask_7: 0.2204  loss_dice_7: 0.1502  loss_ce_8: 0.1374  loss_mask_8: 0.212  loss_dice_8: 0.145  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:08] d2.utils.events INFO:  eta: 1:30:09  iter: 7259  total_loss: 4.863  loss_ce: 0.1357  loss_mask: 0.2091  loss_dice: 0.1436  loss_ce_0: 0.06788  loss_mask_0: 0.2144  loss_dice_0: 0.1385  loss_ce_1: 0.1358  loss_mask_1: 0.2181  loss_dice_1: 0.1386  loss_ce_2: 0.1354  loss_mask_2: 0.2219  loss_dice_2: 0.1401  loss_ce_3: 0.1351  loss_mask_3: 0.2224  loss_dice_3: 0.1408  loss_ce_4: 0.1349  loss_mask_4: 0.2127  loss_dice_4: 0.138  loss_ce_5: 0.1354  loss_mask_5: 0.2221  loss_dice_5: 0.1425  loss_ce_6: 0.1355  loss_mask_6: 0.2262  loss_dice_6: 0.1473  loss_ce_7: 0.1356  loss_mask_7: 0.2201  loss_dice_7: 0.1416  loss_ce_8: 0.1358  loss_mask_8: 0.216  loss_dice_8: 0.1377  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:11] d2.utils.events INFO:  eta: 1:30:07  iter: 7279  total_loss: 4.744  loss_ce: 0.129  loss_mask: 0.2092  loss_dice: 0.136  loss_ce_0: 0.06619  loss_mask_0: 0.2108  loss_dice_0: 0.1369  loss_ce_1: 0.129  loss_mask_1: 0.212  loss_dice_1: 0.1372  loss_ce_2: 0.1292  loss_mask_2: 0.215  loss_dice_2: 0.1401  loss_ce_3: 0.1293  loss_mask_3: 0.217  loss_dice_3: 0.1398  loss_ce_4: 0.1295  loss_mask_4: 0.2167  loss_dice_4: 0.1381  loss_ce_5: 0.1293  loss_mask_5: 0.2098  loss_dice_5: 0.141  loss_ce_6: 0.1293  loss_mask_6: 0.2151  loss_dice_6: 0.1342  loss_ce_7: 0.1293  loss_mask_7: 0.2166  loss_dice_7: 0.1347  loss_ce_8: 0.129  loss_mask_8: 0.2116  loss_dice_8: 0.1343  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:13] d2.utils.events INFO:  eta: 1:30:05  iter: 7299  total_loss: 5.116  loss_ce: 0.1366  loss_mask: 0.2211  loss_dice: 0.1298  loss_ce_0: 0.06533  loss_mask_0: 0.2237  loss_dice_0: 0.1299  loss_ce_1: 0.137  loss_mask_1: 0.2142  loss_dice_1: 0.1306  loss_ce_2: 0.1376  loss_mask_2: 0.218  loss_dice_2: 0.1286  loss_ce_3: 0.1388  loss_mask_3: 0.2196  loss_dice_3: 0.1318  loss_ce_4: 0.1398  loss_mask_4: 0.2199  loss_dice_4: 0.132  loss_ce_5: 0.138  loss_mask_5: 0.2242  loss_dice_5: 0.1299  loss_ce_6: 0.1384  loss_mask_6: 0.2101  loss_dice_6: 0.127  loss_ce_7: 0.1379  loss_mask_7: 0.2204  loss_dice_7: 0.1323  loss_ce_8: 0.1374  loss_mask_8: 0.2216  loss_dice_8: 0.128  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:16] d2.utils.events INFO:  eta: 1:30:01  iter: 7319  total_loss: 4.871  loss_ce: 0.1126  loss_mask: 0.1733  loss_dice: 0.171  loss_ce_0: 0.06342  loss_mask_0: 0.1838  loss_dice_0: 0.1694  loss_ce_1: 0.1126  loss_mask_1: 0.179  loss_dice_1: 0.1681  loss_ce_2: 0.112  loss_mask_2: 0.1704  loss_dice_2: 0.1743  loss_ce_3: 0.1115  loss_mask_3: 0.1799  loss_dice_3: 0.1715  loss_ce_4: 0.1108  loss_mask_4: 0.188  loss_dice_4: 0.161  loss_ce_5: 0.1119  loss_mask_5: 0.1822  loss_dice_5: 0.1749  loss_ce_6: 0.1117  loss_mask_6: 0.1776  loss_dice_6: 0.1755  loss_ce_7: 0.1119  loss_mask_7: 0.1802  loss_dice_7: 0.174  loss_ce_8: 0.1119  loss_mask_8: 0.1784  loss_dice_8: 0.1764  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:18] d2.utils.events INFO:  eta: 1:29:57  iter: 7339  total_loss: 5.453  loss_ce: 0.0959  loss_mask: 0.2134  loss_dice: 0.1827  loss_ce_0: 0.05934  loss_mask_0: 0.2213  loss_dice_0: 0.1794  loss_ce_1: 0.09583  loss_mask_1: 0.213  loss_dice_1: 0.1782  loss_ce_2: 0.09532  loss_mask_2: 0.2067  loss_dice_2: 0.184  loss_ce_3: 0.09437  loss_mask_3: 0.2111  loss_dice_3: 0.181  loss_ce_4: 0.09315  loss_mask_4: 0.2196  loss_dice_4: 0.1804  loss_ce_5: 0.09477  loss_mask_5: 0.2183  loss_dice_5: 0.1889  loss_ce_6: 0.09438  loss_mask_6: 0.2137  loss_dice_6: 0.1815  loss_ce_7: 0.09517  loss_mask_7: 0.2138  loss_dice_7: 0.1844  loss_ce_8: 0.09506  loss_mask_8: 0.2139  loss_dice_8: 0.1808  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:21] d2.utils.events INFO:  eta: 1:29:53  iter: 7359  total_loss: 4.699  loss_ce: 0.08988  loss_mask: 0.2253  loss_dice: 0.1161  loss_ce_0: 0.05671  loss_mask_0: 0.2309  loss_dice_0: 0.1219  loss_ce_1: 0.08968  loss_mask_1: 0.2333  loss_dice_1: 0.1136  loss_ce_2: 0.08954  loss_mask_2: 0.2312  loss_dice_2: 0.1184  loss_ce_3: 0.08895  loss_mask_3: 0.2351  loss_dice_3: 0.1185  loss_ce_4: 0.08816  loss_mask_4: 0.2353  loss_dice_4: 0.1193  loss_ce_5: 0.08919  loss_mask_5: 0.2337  loss_dice_5: 0.1165  loss_ce_6: 0.08892  loss_mask_6: 0.2405  loss_dice_6: 0.1166  loss_ce_7: 0.0895  loss_mask_7: 0.2359  loss_dice_7: 0.1169  loss_ce_8: 0.08919  loss_mask_8: 0.237  loss_dice_8: 0.1235  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:23] d2.utils.events INFO:  eta: 1:29:47  iter: 7379  total_loss: 5.42  loss_ce: 0.1691  loss_mask: 0.2044  loss_dice: 0.1587  loss_ce_0: 0.07757  loss_mask_0: 0.2066  loss_dice_0: 0.1591  loss_ce_1: 0.1686  loss_mask_1: 0.2021  loss_dice_1: 0.1635  loss_ce_2: 0.1683  loss_mask_2: 0.1982  loss_dice_2: 0.1638  loss_ce_3: 0.168  loss_mask_3: 0.1952  loss_dice_3: 0.1637  loss_ce_4: 0.1674  loss_mask_4: 0.2045  loss_dice_4: 0.1662  loss_ce_5: 0.1683  loss_mask_5: 0.196  loss_dice_5: 0.1606  loss_ce_6: 0.169  loss_mask_6: 0.204  loss_dice_6: 0.1634  loss_ce_7: 0.169  loss_mask_7: 0.1972  loss_dice_7: 0.1616  loss_ce_8: 0.1694  loss_mask_8: 0.204  loss_dice_8: 0.1621  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:26] d2.utils.events INFO:  eta: 1:29:45  iter: 7399  total_loss: 5.37  loss_ce: 0.1312  loss_mask: 0.1965  loss_dice: 0.156  loss_ce_0: 0.06604  loss_mask_0: 0.2049  loss_dice_0: 0.1557  loss_ce_1: 0.1313  loss_mask_1: 0.2112  loss_dice_1: 0.1615  loss_ce_2: 0.1312  loss_mask_2: 0.2018  loss_dice_2: 0.1653  loss_ce_3: 0.1312  loss_mask_3: 0.2088  loss_dice_3: 0.1573  loss_ce_4: 0.1313  loss_mask_4: 0.2113  loss_dice_4: 0.1606  loss_ce_5: 0.1313  loss_mask_5: 0.2129  loss_dice_5: 0.1623  loss_ce_6: 0.1313  loss_mask_6: 0.2084  loss_dice_6: 0.1592  loss_ce_7: 0.1313  loss_mask_7: 0.2055  loss_dice_7: 0.1621  loss_ce_8: 0.1313  loss_mask_8: 0.1893  loss_dice_8: 0.1615  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:28] d2.utils.events INFO:  eta: 1:29:41  iter: 7419  total_loss: 5.452  loss_ce: 0.149  loss_mask: 0.2056  loss_dice: 0.1447  loss_ce_0: 0.07599  loss_mask_0: 0.203  loss_dice_0: 0.1528  loss_ce_1: 0.1487  loss_mask_1: 0.2054  loss_dice_1: 0.1467  loss_ce_2: 0.1482  loss_mask_2: 0.2051  loss_dice_2: 0.1458  loss_ce_3: 0.1477  loss_mask_3: 0.2017  loss_dice_3: 0.1528  loss_ce_4: 0.1464  loss_mask_4: 0.2075  loss_dice_4: 0.1482  loss_ce_5: 0.148  loss_mask_5: 0.2031  loss_dice_5: 0.1503  loss_ce_6: 0.1479  loss_mask_6: 0.2038  loss_dice_6: 0.1474  loss_ce_7: 0.1487  loss_mask_7: 0.2068  loss_dice_7: 0.1469  loss_ce_8: 0.1485  loss_mask_8: 0.2037  loss_dice_8: 0.1515  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:31] d2.utils.events INFO:  eta: 1:29:35  iter: 7439  total_loss: 5.255  loss_ce: 0.1424  loss_mask: 0.2216  loss_dice: 0.1638  loss_ce_0: 0.07418  loss_mask_0: 0.2202  loss_dice_0: 0.1717  loss_ce_1: 0.1425  loss_mask_1: 0.2288  loss_dice_1: 0.1647  loss_ce_2: 0.142  loss_mask_2: 0.2294  loss_dice_2: 0.1684  loss_ce_3: 0.1415  loss_mask_3: 0.2212  loss_dice_3: 0.1707  loss_ce_4: 0.1407  loss_mask_4: 0.2286  loss_dice_4: 0.1701  loss_ce_5: 0.1418  loss_mask_5: 0.2267  loss_dice_5: 0.1675  loss_ce_6: 0.1416  loss_mask_6: 0.2311  loss_dice_6: 0.1762  loss_ce_7: 0.1421  loss_mask_7: 0.2308  loss_dice_7: 0.1724  loss_ce_8: 0.1417  loss_mask_8: 0.2193  loss_dice_8: 0.1699  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:33] d2.utils.events INFO:  eta: 1:29:31  iter: 7459  total_loss: 5.939  loss_ce: 0.1293  loss_mask: 0.2066  loss_dice: 0.1683  loss_ce_0: 0.07108  loss_mask_0: 0.2205  loss_dice_0: 0.1581  loss_ce_1: 0.1288  loss_mask_1: 0.2121  loss_dice_1: 0.169  loss_ce_2: 0.1287  loss_mask_2: 0.2135  loss_dice_2: 0.1681  loss_ce_3: 0.1282  loss_mask_3: 0.2209  loss_dice_3: 0.165  loss_ce_4: 0.1275  loss_mask_4: 0.2259  loss_dice_4: 0.17  loss_ce_5: 0.1286  loss_mask_5: 0.2159  loss_dice_5: 0.1676  loss_ce_6: 0.1281  loss_mask_6: 0.2211  loss_dice_6: 0.1665  loss_ce_7: 0.1282  loss_mask_7: 0.2146  loss_dice_7: 0.1629  loss_ce_8: 0.1282  loss_mask_8: 0.2117  loss_dice_8: 0.166  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:36] d2.utils.events INFO:  eta: 1:29:29  iter: 7479  total_loss: 5.313  loss_ce: 0.1261  loss_mask: 0.2401  loss_dice: 0.1376  loss_ce_0: 0.06841  loss_mask_0: 0.2278  loss_dice_0: 0.1357  loss_ce_1: 0.1258  loss_mask_1: 0.2339  loss_dice_1: 0.136  loss_ce_2: 0.1255  loss_mask_2: 0.2405  loss_dice_2: 0.1411  loss_ce_3: 0.1252  loss_mask_3: 0.2365  loss_dice_3: 0.1379  loss_ce_4: 0.1243  loss_mask_4: 0.2343  loss_dice_4: 0.1391  loss_ce_5: 0.1254  loss_mask_5: 0.2389  loss_dice_5: 0.1397  loss_ce_6: 0.1248  loss_mask_6: 0.2352  loss_dice_6: 0.1374  loss_ce_7: 0.1252  loss_mask_7: 0.2401  loss_dice_7: 0.1369  loss_ce_8: 0.1251  loss_mask_8: 0.2344  loss_dice_8: 0.1432  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:38] d2.utils.events INFO:  eta: 1:29:26  iter: 7499  total_loss: 5.354  loss_ce: 0.1091  loss_mask: 0.1906  loss_dice: 0.1469  loss_ce_0: 0.06499  loss_mask_0: 0.1877  loss_dice_0: 0.147  loss_ce_1: 0.1083  loss_mask_1: 0.1942  loss_dice_1: 0.1458  loss_ce_2: 0.1076  loss_mask_2: 0.1933  loss_dice_2: 0.1543  loss_ce_3: 0.1073  loss_mask_3: 0.1919  loss_dice_3: 0.1491  loss_ce_4: 0.1062  loss_mask_4: 0.2141  loss_dice_4: 0.1507  loss_ce_5: 0.1079  loss_mask_5: 0.1981  loss_dice_5: 0.15  loss_ce_6: 0.1072  loss_mask_6: 0.2114  loss_dice_6: 0.1644  loss_ce_7: 0.1072  loss_mask_7: 0.1944  loss_dice_7: 0.1535  loss_ce_8: 0.1074  loss_mask_8: 0.1904  loss_dice_8: 0.1564  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:41] d2.utils.events INFO:  eta: 1:29:25  iter: 7519  total_loss: 5.16  loss_ce: 0.1023  loss_mask: 0.2295  loss_dice: 0.1445  loss_ce_0: 0.06337  loss_mask_0: 0.2231  loss_dice_0: 0.1417  loss_ce_1: 0.1019  loss_mask_1: 0.232  loss_dice_1: 0.1407  loss_ce_2: 0.1015  loss_mask_2: 0.2291  loss_dice_2: 0.1461  loss_ce_3: 0.101  loss_mask_3: 0.2368  loss_dice_3: 0.1508  loss_ce_4: 0.1002  loss_mask_4: 0.2338  loss_dice_4: 0.1449  loss_ce_5: 0.1014  loss_mask_5: 0.2404  loss_dice_5: 0.1446  loss_ce_6: 0.1007  loss_mask_6: 0.2355  loss_dice_6: 0.1407  loss_ce_7: 0.1009  loss_mask_7: 0.2172  loss_dice_7: 0.1378  loss_ce_8: 0.101  loss_mask_8: 0.2272  loss_dice_8: 0.1413  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:43] d2.utils.events INFO:  eta: 1:29:25  iter: 7539  total_loss: 5.26  loss_ce: 0.1145  loss_mask: 0.1732  loss_dice: 0.1611  loss_ce_0: 0.06184  loss_mask_0: 0.1874  loss_dice_0: 0.1565  loss_ce_1: 0.1148  loss_mask_1: 0.1834  loss_dice_1: 0.1631  loss_ce_2: 0.1157  loss_mask_2: 0.1734  loss_dice_2: 0.1624  loss_ce_3: 0.1156  loss_mask_3: 0.1843  loss_dice_3: 0.1565  loss_ce_4: 0.116  loss_mask_4: 0.1847  loss_dice_4: 0.1678  loss_ce_5: 0.1149  loss_mask_5: 0.1779  loss_dice_5: 0.1584  loss_ce_6: 0.1149  loss_mask_6: 0.18  loss_dice_6: 0.1576  loss_ce_7: 0.1157  loss_mask_7: 0.1686  loss_dice_7: 0.1602  loss_ce_8: 0.1152  loss_mask_8: 0.1778  loss_dice_8: 0.1555  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:46] d2.utils.events INFO:  eta: 1:29:22  iter: 7559  total_loss: 5.03  loss_ce: 0.1122  loss_mask: 0.2425  loss_dice: 0.1238  loss_ce_0: 0.06071  loss_mask_0: 0.2463  loss_dice_0: 0.1243  loss_ce_1: 0.1123  loss_mask_1: 0.2432  loss_dice_1: 0.1215  loss_ce_2: 0.113  loss_mask_2: 0.2404  loss_dice_2: 0.1226  loss_ce_3: 0.1127  loss_mask_3: 0.2453  loss_dice_3: 0.1219  loss_ce_4: 0.1129  loss_mask_4: 0.2446  loss_dice_4: 0.1237  loss_ce_5: 0.1122  loss_mask_5: 0.252  loss_dice_5: 0.1248  loss_ce_6: 0.1123  loss_mask_6: 0.2369  loss_dice_6: 0.1224  loss_ce_7: 0.113  loss_mask_7: 0.2474  loss_dice_7: 0.1236  loss_ce_8: 0.1128  loss_mask_8: 0.2481  loss_dice_8: 0.1233  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:52:49] d2.utils.events INFO:  eta: 1:29:20  iter: 7579  total_loss: 5.249  loss_ce: 0.1286  loss_mask: 0.2151  loss_dice: 0.1876  loss_ce_0: 0.06498  loss_mask_0: 0.2117  loss_dice_0: 0.1986  loss_ce_1: 0.1288  loss_mask_1: 0.2033  loss_dice_1: 0.1998  loss_ce_2: 0.1292  loss_mask_2: 0.2084  loss_dice_2: 0.2  loss_ce_3: 0.1294  loss_mask_3: 0.2154  loss_dice_3: 0.1975  loss_ce_4: 0.1298  loss_mask_4: 0.2182  loss_dice_4: 0.1997  loss_ce_5: 0.1293  loss_mask_5: 0.2186  loss_dice_5: 0.202  loss_ce_6: 0.1293  loss_mask_6: 0.2165  loss_dice_6: 0.2121  loss_ce_7: 0.1292  loss_mask_7: 0.2103  loss_dice_7: 0.2034  loss_ce_8: 0.1292  loss_mask_8: 0.2138  loss_dice_8: 0.1926  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:51] d2.utils.events INFO:  eta: 1:29:19  iter: 7599  total_loss: 5.876  loss_ce: 0.1252  loss_mask: 0.1918  loss_dice: 0.2214  loss_ce_0: 0.06188  loss_mask_0: 0.1926  loss_dice_0: 0.2104  loss_ce_1: 0.1253  loss_mask_1: 0.1796  loss_dice_1: 0.2219  loss_ce_2: 0.1257  loss_mask_2: 0.2021  loss_dice_2: 0.2325  loss_ce_3: 0.126  loss_mask_3: 0.1934  loss_dice_3: 0.227  loss_ce_4: 0.1263  loss_mask_4: 0.1879  loss_dice_4: 0.2104  loss_ce_5: 0.1256  loss_mask_5: 0.1898  loss_dice_5: 0.2282  loss_ce_6: 0.1258  loss_mask_6: 0.1875  loss_dice_6: 0.235  loss_ce_7: 0.1259  loss_mask_7: 0.2015  loss_dice_7: 0.2407  loss_ce_8: 0.1259  loss_mask_8: 0.1891  loss_dice_8: 0.2207  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:54] d2.utils.events INFO:  eta: 1:29:17  iter: 7619  total_loss: 5.669  loss_ce: 0.1136  loss_mask: 0.232  loss_dice: 0.1452  loss_ce_0: 0.06035  loss_mask_0: 0.2325  loss_dice_0: 0.1445  loss_ce_1: 0.1133  loss_mask_1: 0.2371  loss_dice_1: 0.1457  loss_ce_2: 0.1134  loss_mask_2: 0.2206  loss_dice_2: 0.1427  loss_ce_3: 0.1129  loss_mask_3: 0.2212  loss_dice_3: 0.1422  loss_ce_4: 0.1128  loss_mask_4: 0.2352  loss_dice_4: 0.1528  loss_ce_5: 0.1128  loss_mask_5: 0.2255  loss_dice_5: 0.1528  loss_ce_6: 0.1128  loss_mask_6: 0.2263  loss_dice_6: 0.1488  loss_ce_7: 0.1133  loss_mask_7: 0.2311  loss_dice_7: 0.1468  loss_ce_8: 0.1134  loss_mask_8: 0.2333  loss_dice_8: 0.1441  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:56] d2.utils.events INFO:  eta: 1:29:18  iter: 7639  total_loss: 4.857  loss_ce: 0.1339  loss_mask: 0.2062  loss_dice: 0.1184  loss_ce_0: 0.06884  loss_mask_0: 0.1966  loss_dice_0: 0.111  loss_ce_1: 0.1338  loss_mask_1: 0.1958  loss_dice_1: 0.117  loss_ce_2: 0.134  loss_mask_2: 0.1943  loss_dice_2: 0.1204  loss_ce_3: 0.1343  loss_mask_3: 0.195  loss_dice_3: 0.1132  loss_ce_4: 0.1345  loss_mask_4: 0.2037  loss_dice_4: 0.1157  loss_ce_5: 0.1343  loss_mask_5: 0.1977  loss_dice_5: 0.114  loss_ce_6: 0.1344  loss_mask_6: 0.1968  loss_dice_6: 0.1134  loss_ce_7: 0.1342  loss_mask_7: 0.2041  loss_dice_7: 0.1167  loss_ce_8: 0.1341  loss_mask_8: 0.1968  loss_dice_8: 0.1152  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:52:59] d2.utils.events INFO:  eta: 1:29:17  iter: 7659  total_loss: 5.147  loss_ce: 0.133  loss_mask: 0.2306  loss_dice: 0.131  loss_ce_0: 0.06365  loss_mask_0: 0.229  loss_dice_0: 0.1328  loss_ce_1: 0.1331  loss_mask_1: 0.2348  loss_dice_1: 0.1353  loss_ce_2: 0.1334  loss_mask_2: 0.2328  loss_dice_2: 0.133  loss_ce_3: 0.1338  loss_mask_3: 0.2185  loss_dice_3: 0.1314  loss_ce_4: 0.134  loss_mask_4: 0.2345  loss_dice_4: 0.1379  loss_ce_5: 0.1336  loss_mask_5: 0.2317  loss_dice_5: 0.1359  loss_ce_6: 0.134  loss_mask_6: 0.2264  loss_dice_6: 0.1334  loss_ce_7: 0.1339  loss_mask_7: 0.2365  loss_dice_7: 0.1311  loss_ce_8: 0.1338  loss_mask_8: 0.2326  loss_dice_8: 0.135  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:01] d2.utils.events INFO:  eta: 1:29:15  iter: 7679  total_loss: 5.003  loss_ce: 0.1277  loss_mask: 0.1875  loss_dice: 0.1515  loss_ce_0: 0.06509  loss_mask_0: 0.1753  loss_dice_0: 0.1565  loss_ce_1: 0.1277  loss_mask_1: 0.1751  loss_dice_1: 0.1534  loss_ce_2: 0.1277  loss_mask_2: 0.1854  loss_dice_2: 0.1493  loss_ce_3: 0.1277  loss_mask_3: 0.1728  loss_dice_3: 0.1541  loss_ce_4: 0.1277  loss_mask_4: 0.1744  loss_dice_4: 0.1534  loss_ce_5: 0.1277  loss_mask_5: 0.1782  loss_dice_5: 0.1514  loss_ce_6: 0.1277  loss_mask_6: 0.174  loss_dice_6: 0.1542  loss_ce_7: 0.1277  loss_mask_7: 0.1708  loss_dice_7: 0.1532  loss_ce_8: 0.1277  loss_mask_8: 0.1765  loss_dice_8: 0.1499  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:04] d2.utils.events INFO:  eta: 1:29:12  iter: 7699  total_loss: 4.981  loss_ce: 0.1301  loss_mask: 0.2329  loss_dice: 0.13  loss_ce_0: 0.06408  loss_mask_0: 0.2384  loss_dice_0: 0.1274  loss_ce_1: 0.1304  loss_mask_1: 0.2258  loss_dice_1: 0.1345  loss_ce_2: 0.1304  loss_mask_2: 0.2385  loss_dice_2: 0.1319  loss_ce_3: 0.1307  loss_mask_3: 0.2299  loss_dice_3: 0.1305  loss_ce_4: 0.1309  loss_mask_4: 0.2339  loss_dice_4: 0.1329  loss_ce_5: 0.1306  loss_mask_5: 0.231  loss_dice_5: 0.1376  loss_ce_6: 0.1307  loss_mask_6: 0.2329  loss_dice_6: 0.1328  loss_ce_7: 0.1306  loss_mask_7: 0.2344  loss_dice_7: 0.1298  loss_ce_8: 0.1304  loss_mask_8: 0.2373  loss_dice_8: 0.1291  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:06] d2.utils.events INFO:  eta: 1:29:08  iter: 7719  total_loss: 4.972  loss_ce: 0.1273  loss_mask: 0.2012  loss_dice: 0.1758  loss_ce_0: 0.06502  loss_mask_0: 0.2053  loss_dice_0: 0.1847  loss_ce_1: 0.1273  loss_mask_1: 0.1894  loss_dice_1: 0.1823  loss_ce_2: 0.1273  loss_mask_2: 0.1894  loss_dice_2: 0.1814  loss_ce_3: 0.1273  loss_mask_3: 0.2049  loss_dice_3: 0.1963  loss_ce_4: 0.1273  loss_mask_4: 0.1906  loss_dice_4: 0.1868  loss_ce_5: 0.1272  loss_mask_5: 0.1952  loss_dice_5: 0.1834  loss_ce_6: 0.1273  loss_mask_6: 0.1923  loss_dice_6: 0.1808  loss_ce_7: 0.1273  loss_mask_7: 0.1962  loss_dice_7: 0.1857  loss_ce_8: 0.1273  loss_mask_8: 0.2047  loss_dice_8: 0.1813  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:09] d2.utils.events INFO:  eta: 1:29:04  iter: 7739  total_loss: 5.647  loss_ce: 0.1094  loss_mask: 0.2348  loss_dice: 0.1891  loss_ce_0: 0.0602  loss_mask_0: 0.2381  loss_dice_0: 0.1939  loss_ce_1: 0.1092  loss_mask_1: 0.2453  loss_dice_1: 0.2008  loss_ce_2: 0.109  loss_mask_2: 0.2504  loss_dice_2: 0.1855  loss_ce_3: 0.1089  loss_mask_3: 0.2388  loss_dice_3: 0.1961  loss_ce_4: 0.1087  loss_mask_4: 0.2362  loss_dice_4: 0.1885  loss_ce_5: 0.1088  loss_mask_5: 0.2381  loss_dice_5: 0.1951  loss_ce_6: 0.1088  loss_mask_6: 0.2384  loss_dice_6: 0.2003  loss_ce_7: 0.1091  loss_mask_7: 0.235  loss_dice_7: 0.1934  loss_ce_8: 0.1092  loss_mask_8: 0.2355  loss_dice_8: 0.1976  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:11] d2.utils.events INFO:  eta: 1:29:03  iter: 7759  total_loss: 5.425  loss_ce: 0.09663  loss_mask: 0.1921  loss_dice: 0.1959  loss_ce_0: 0.05729  loss_mask_0: 0.1988  loss_dice_0: 0.1942  loss_ce_1: 0.09656  loss_mask_1: 0.2067  loss_dice_1: 0.1953  loss_ce_2: 0.09645  loss_mask_2: 0.2009  loss_dice_2: 0.1926  loss_ce_3: 0.0959  loss_mask_3: 0.2013  loss_dice_3: 0.1955  loss_ce_4: 0.09557  loss_mask_4: 0.1989  loss_dice_4: 0.1833  loss_ce_5: 0.09593  loss_mask_5: 0.2024  loss_dice_5: 0.1907  loss_ce_6: 0.09547  loss_mask_6: 0.1942  loss_dice_6: 0.1891  loss_ce_7: 0.09572  loss_mask_7: 0.2098  loss_dice_7: 0.1938  loss_ce_8: 0.09609  loss_mask_8: 0.2007  loss_dice_8: 0.1898  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:14] d2.utils.events INFO:  eta: 1:29:01  iter: 7779  total_loss: 5.093  loss_ce: 0.09384  loss_mask: 0.241  loss_dice: 0.1473  loss_ce_0: 0.05526  loss_mask_0: 0.2432  loss_dice_0: 0.1509  loss_ce_1: 0.09355  loss_mask_1: 0.2392  loss_dice_1: 0.1583  loss_ce_2: 0.0938  loss_mask_2: 0.2425  loss_dice_2: 0.152  loss_ce_3: 0.09338  loss_mask_3: 0.2425  loss_dice_3: 0.1597  loss_ce_4: 0.09358  loss_mask_4: 0.2395  loss_dice_4: 0.158  loss_ce_5: 0.09362  loss_mask_5: 0.2419  loss_dice_5: 0.1627  loss_ce_6: 0.09341  loss_mask_6: 0.2379  loss_dice_6: 0.1545  loss_ce_7: 0.09362  loss_mask_7: 0.2489  loss_dice_7: 0.1564  loss_ce_8: 0.09363  loss_mask_8: 0.25  loss_dice_8: 0.1565  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:16] d2.utils.events INFO:  eta: 1:28:58  iter: 7799  total_loss: 5.488  loss_ce: 0.09405  loss_mask: 0.2181  loss_dice: 0.1769  loss_ce_0: 0.05475  loss_mask_0: 0.2267  loss_dice_0: 0.1716  loss_ce_1: 0.09373  loss_mask_1: 0.2297  loss_dice_1: 0.1809  loss_ce_2: 0.09391  loss_mask_2: 0.2274  loss_dice_2: 0.1737  loss_ce_3: 0.09363  loss_mask_3: 0.224  loss_dice_3: 0.1666  loss_ce_4: 0.0938  loss_mask_4: 0.2282  loss_dice_4: 0.1767  loss_ce_5: 0.0938  loss_mask_5: 0.2272  loss_dice_5: 0.1837  loss_ce_6: 0.09359  loss_mask_6: 0.2192  loss_dice_6: 0.1817  loss_ce_7: 0.0938  loss_mask_7: 0.2248  loss_dice_7: 0.183  loss_ce_8: 0.09384  loss_mask_8: 0.2239  loss_dice_8: 0.1727  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:19] d2.utils.events INFO:  eta: 1:28:56  iter: 7819  total_loss: 5.195  loss_ce: 0.09641  loss_mask: 0.2192  loss_dice: 0.1425  loss_ce_0: 0.05358  loss_mask_0: 0.2087  loss_dice_0: 0.1401  loss_ce_1: 0.0959  loss_mask_1: 0.2118  loss_dice_1: 0.1471  loss_ce_2: 0.09616  loss_mask_2: 0.2193  loss_dice_2: 0.14  loss_ce_3: 0.09616  loss_mask_3: 0.2071  loss_dice_3: 0.139  loss_ce_4: 0.0963  loss_mask_4: 0.2107  loss_dice_4: 0.1428  loss_ce_5: 0.09608  loss_mask_5: 0.204  loss_dice_5: 0.1428  loss_ce_6: 0.09612  loss_mask_6: 0.2139  loss_dice_6: 0.1424  loss_ce_7: 0.09652  loss_mask_7: 0.2209  loss_dice_7: 0.144  loss_ce_8: 0.09634  loss_mask_8: 0.2118  loss_dice_8: 0.1428  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:21] d2.utils.events INFO:  eta: 1:28:55  iter: 7839  total_loss: 5.339  loss_ce: 0.1377  loss_mask: 0.2015  loss_dice: 0.1667  loss_ce_0: 0.07383  loss_mask_0: 0.1939  loss_dice_0: 0.1722  loss_ce_1: 0.1384  loss_mask_1: 0.1867  loss_dice_1: 0.1633  loss_ce_2: 0.1377  loss_mask_2: 0.1882  loss_dice_2: 0.1619  loss_ce_3: 0.1367  loss_mask_3: 0.1989  loss_dice_3: 0.163  loss_ce_4: 0.1361  loss_mask_4: 0.2026  loss_dice_4: 0.1622  loss_ce_5: 0.1371  loss_mask_5: 0.1832  loss_dice_5: 0.1666  loss_ce_6: 0.1365  loss_mask_6: 0.1912  loss_dice_6: 0.1663  loss_ce_7: 0.1365  loss_mask_7: 0.1956  loss_dice_7: 0.1598  loss_ce_8: 0.1369  loss_mask_8: 0.2033  loss_dice_8: 0.1591  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:24] d2.utils.events INFO:  eta: 1:28:51  iter: 7859  total_loss: 4.966  loss_ce: 0.1188  loss_mask: 0.1917  loss_dice: 0.1695  loss_ce_0: 0.05751  loss_mask_0: 0.1911  loss_dice_0: 0.1654  loss_ce_1: 0.1183  loss_mask_1: 0.1985  loss_dice_1: 0.1764  loss_ce_2: 0.1188  loss_mask_2: 0.19  loss_dice_2: 0.1704  loss_ce_3: 0.1197  loss_mask_3: 0.1898  loss_dice_3: 0.1625  loss_ce_4: 0.1202  loss_mask_4: 0.1825  loss_dice_4: 0.1692  loss_ce_5: 0.1192  loss_mask_5: 0.1862  loss_dice_5: 0.1609  loss_ce_6: 0.1198  loss_mask_6: 0.1872  loss_dice_6: 0.1705  loss_ce_7: 0.1198  loss_mask_7: 0.1896  loss_dice_7: 0.1677  loss_ce_8: 0.1195  loss_mask_8: 0.1819  loss_dice_8: 0.1648  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:27] d2.utils.events INFO:  eta: 1:28:49  iter: 7879  total_loss: 4.976  loss_ce: 0.1493  loss_mask: 0.1678  loss_dice: 0.1668  loss_ce_0: 0.0744  loss_mask_0: 0.1657  loss_dice_0: 0.1643  loss_ce_1: 0.1491  loss_mask_1: 0.1613  loss_dice_1: 0.1637  loss_ce_2: 0.1493  loss_mask_2: 0.166  loss_dice_2: 0.169  loss_ce_3: 0.1494  loss_mask_3: 0.1648  loss_dice_3: 0.1715  loss_ce_4: 0.1494  loss_mask_4: 0.1674  loss_dice_4: 0.1618  loss_ce_5: 0.1495  loss_mask_5: 0.1583  loss_dice_5: 0.1621  loss_ce_6: 0.1494  loss_mask_6: 0.1676  loss_dice_6: 0.1725  loss_ce_7: 0.1491  loss_mask_7: 0.1641  loss_dice_7: 0.1628  loss_ce_8: 0.1493  loss_mask_8: 0.1546  loss_dice_8: 0.1568  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:29] d2.utils.events INFO:  eta: 1:28:43  iter: 7899  total_loss: 5.141  loss_ce: 0.1271  loss_mask: 0.2179  loss_dice: 0.1495  loss_ce_0: 0.0651  loss_mask_0: 0.2206  loss_dice_0: 0.1492  loss_ce_1: 0.127  loss_mask_1: 0.2215  loss_dice_1: 0.1561  loss_ce_2: 0.1271  loss_mask_2: 0.2195  loss_dice_2: 0.1524  loss_ce_3: 0.127  loss_mask_3: 0.2218  loss_dice_3: 0.148  loss_ce_4: 0.127  loss_mask_4: 0.2249  loss_dice_4: 0.1494  loss_ce_5: 0.127  loss_mask_5: 0.2199  loss_dice_5: 0.1494  loss_ce_6: 0.127  loss_mask_6: 0.221  loss_dice_6: 0.1486  loss_ce_7: 0.127  loss_mask_7: 0.2199  loss_dice_7: 0.1522  loss_ce_8: 0.1271  loss_mask_8: 0.2197  loss_dice_8: 0.15  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:32] d2.utils.events INFO:  eta: 1:28:38  iter: 7919  total_loss: 4.519  loss_ce: 0.1286  loss_mask: 0.2036  loss_dice: 0.125  loss_ce_0: 0.06185  loss_mask_0: 0.1903  loss_dice_0: 0.1216  loss_ce_1: 0.1287  loss_mask_1: 0.1961  loss_dice_1: 0.1232  loss_ce_2: 0.129  loss_mask_2: 0.1912  loss_dice_2: 0.117  loss_ce_3: 0.1292  loss_mask_3: 0.1944  loss_dice_3: 0.1174  loss_ce_4: 0.1295  loss_mask_4: 0.1915  loss_dice_4: 0.1268  loss_ce_5: 0.1289  loss_mask_5: 0.191  loss_dice_5: 0.1211  loss_ce_6: 0.1292  loss_mask_6: 0.1992  loss_dice_6: 0.1208  loss_ce_7: 0.1288  loss_mask_7: 0.1988  loss_dice_7: 0.1208  loss_ce_8: 0.129  loss_mask_8: 0.1897  loss_dice_8: 0.1228  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:34] d2.utils.events INFO:  eta: 1:28:33  iter: 7939  total_loss: 4.945  loss_ce: 0.1283  loss_mask: 0.2158  loss_dice: 0.1642  loss_ce_0: 0.0678  loss_mask_0: 0.2116  loss_dice_0: 0.1571  loss_ce_1: 0.1281  loss_mask_1: 0.2165  loss_dice_1: 0.1586  loss_ce_2: 0.1284  loss_mask_2: 0.2147  loss_dice_2: 0.1593  loss_ce_3: 0.1285  loss_mask_3: 0.2139  loss_dice_3: 0.1601  loss_ce_4: 0.1285  loss_mask_4: 0.2139  loss_dice_4: 0.1656  loss_ce_5: 0.1285  loss_mask_5: 0.2158  loss_dice_5: 0.16  loss_ce_6: 0.1284  loss_mask_6: 0.2104  loss_dice_6: 0.1605  loss_ce_7: 0.1284  loss_mask_7: 0.2175  loss_dice_7: 0.1617  loss_ce_8: 0.1283  loss_mask_8: 0.2197  loss_dice_8: 0.1601  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:37] d2.utils.events INFO:  eta: 1:28:29  iter: 7959  total_loss: 4.975  loss_ce: 0.1342  loss_mask: 0.2322  loss_dice: 0.1384  loss_ce_0: 0.06392  loss_mask_0: 0.2343  loss_dice_0: 0.1376  loss_ce_1: 0.1344  loss_mask_1: 0.2274  loss_dice_1: 0.1378  loss_ce_2: 0.1347  loss_mask_2: 0.2394  loss_dice_2: 0.1397  loss_ce_3: 0.135  loss_mask_3: 0.2277  loss_dice_3: 0.1407  loss_ce_4: 0.1353  loss_mask_4: 0.2315  loss_dice_4: 0.1401  loss_ce_5: 0.1347  loss_mask_5: 0.2356  loss_dice_5: 0.1399  loss_ce_6: 0.135  loss_mask_6: 0.2331  loss_dice_6: 0.1391  loss_ce_7: 0.1345  loss_mask_7: 0.233  loss_dice_7: 0.1343  loss_ce_8: 0.1348  loss_mask_8: 0.2371  loss_dice_8: 0.1357  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:39] d2.utils.events INFO:  eta: 1:28:28  iter: 7979  total_loss: 4.78  loss_ce: 0.128  loss_mask: 0.1872  loss_dice: 0.138  loss_ce_0: 0.06514  loss_mask_0: 0.1859  loss_dice_0: 0.1412  loss_ce_1: 0.128  loss_mask_1: 0.1953  loss_dice_1: 0.1406  loss_ce_2: 0.1281  loss_mask_2: 0.186  loss_dice_2: 0.1411  loss_ce_3: 0.1282  loss_mask_3: 0.1912  loss_dice_3: 0.1387  loss_ce_4: 0.1282  loss_mask_4: 0.1902  loss_dice_4: 0.1445  loss_ce_5: 0.1281  loss_mask_5: 0.1961  loss_dice_5: 0.1408  loss_ce_6: 0.1281  loss_mask_6: 0.1976  loss_dice_6: 0.1427  loss_ce_7: 0.1281  loss_mask_7: 0.1886  loss_dice_7: 0.1439  loss_ce_8: 0.128  loss_mask_8: 0.1955  loss_dice_8: 0.1435  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:42] d2.utils.events INFO:  eta: 1:28:26  iter: 7999  total_loss: 4.569  loss_ce: 0.1312  loss_mask: 0.1909  loss_dice: 0.1295  loss_ce_0: 0.06542  loss_mask_0: 0.1858  loss_dice_0: 0.1314  loss_ce_1: 0.1313  loss_mask_1: 0.1982  loss_dice_1: 0.1212  loss_ce_2: 0.1316  loss_mask_2: 0.1941  loss_dice_2: 0.1275  loss_ce_3: 0.1322  loss_mask_3: 0.2009  loss_dice_3: 0.1284  loss_ce_4: 0.1327  loss_mask_4: 0.1889  loss_dice_4: 0.1317  loss_ce_5: 0.1321  loss_mask_5: 0.1963  loss_dice_5: 0.1312  loss_ce_6: 0.1321  loss_mask_6: 0.1992  loss_dice_6: 0.136  loss_ce_7: 0.1319  loss_mask_7: 0.1857  loss_dice_7: 0.1291  loss_ce_8: 0.1314  loss_mask_8: 0.1839  loss_dice_8: 0.1294  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:44] d2.utils.events INFO:  eta: 1:28:25  iter: 8019  total_loss: 4.966  loss_ce: 0.1301  loss_mask: 0.2107  loss_dice: 0.1537  loss_ce_0: 0.06515  loss_mask_0: 0.2075  loss_dice_0: 0.1558  loss_ce_1: 0.1301  loss_mask_1: 0.2039  loss_dice_1: 0.1503  loss_ce_2: 0.1303  loss_mask_2: 0.2041  loss_dice_2: 0.1506  loss_ce_3: 0.1305  loss_mask_3: 0.2177  loss_dice_3: 0.1564  loss_ce_4: 0.1308  loss_mask_4: 0.2165  loss_dice_4: 0.1539  loss_ce_5: 0.1305  loss_mask_5: 0.2085  loss_dice_5: 0.149  loss_ce_6: 0.1304  loss_mask_6: 0.2045  loss_dice_6: 0.1537  loss_ce_7: 0.13  loss_mask_7: 0.2148  loss_dice_7: 0.1551  loss_ce_8: 0.1301  loss_mask_8: 0.2127  loss_dice_8: 0.1536  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:53:47] d2.utils.events INFO:  eta: 1:28:20  iter: 8039  total_loss: 4.968  loss_ce: 0.1264  loss_mask: 0.1841  loss_dice: 0.1655  loss_ce_0: 0.06496  loss_mask_0: 0.184  loss_dice_0: 0.1695  loss_ce_1: 0.1264  loss_mask_1: 0.1786  loss_dice_1: 0.1706  loss_ce_2: 0.1266  loss_mask_2: 0.1834  loss_dice_2: 0.1687  loss_ce_3: 0.1266  loss_mask_3: 0.1916  loss_dice_3: 0.1702  loss_ce_4: 0.1268  loss_mask_4: 0.1924  loss_dice_4: 0.1735  loss_ce_5: 0.1267  loss_mask_5: 0.1908  loss_dice_5: 0.162  loss_ce_6: 0.1266  loss_mask_6: 0.191  loss_dice_6: 0.1604  loss_ce_7: 0.1264  loss_mask_7: 0.1863  loss_dice_7: 0.1696  loss_ce_8: 0.1262  loss_mask_8: 0.1856  loss_dice_8: 0.1631  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:49] d2.utils.events INFO:  eta: 1:28:19  iter: 8059  total_loss: 5.283  loss_ce: 0.1281  loss_mask: 0.1729  loss_dice: 0.1533  loss_ce_0: 0.06489  loss_mask_0: 0.1769  loss_dice_0: 0.1569  loss_ce_1: 0.1281  loss_mask_1: 0.175  loss_dice_1: 0.1536  loss_ce_2: 0.1281  loss_mask_2: 0.1812  loss_dice_2: 0.161  loss_ce_3: 0.1282  loss_mask_3: 0.1743  loss_dice_3: 0.1562  loss_ce_4: 0.1282  loss_mask_4: 0.1777  loss_dice_4: 0.1594  loss_ce_5: 0.1282  loss_mask_5: 0.1845  loss_dice_5: 0.1575  loss_ce_6: 0.1282  loss_mask_6: 0.1769  loss_dice_6: 0.1576  loss_ce_7: 0.1281  loss_mask_7: 0.1826  loss_dice_7: 0.1604  loss_ce_8: 0.1282  loss_mask_8: 0.1726  loss_dice_8: 0.1517  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:52] d2.utils.events INFO:  eta: 1:28:13  iter: 8079  total_loss: 5.333  loss_ce: 0.1532  loss_mask: 0.2202  loss_dice: 0.1703  loss_ce_0: 0.07005  loss_mask_0: 0.2193  loss_dice_0: 0.1663  loss_ce_1: 0.1531  loss_mask_1: 0.2127  loss_dice_1: 0.1667  loss_ce_2: 0.1536  loss_mask_2: 0.2082  loss_dice_2: 0.1669  loss_ce_3: 0.1539  loss_mask_3: 0.2169  loss_dice_3: 0.1632  loss_ce_4: 0.1542  loss_mask_4: 0.218  loss_dice_4: 0.1603  loss_ce_5: 0.1538  loss_mask_5: 0.2124  loss_dice_5: 0.168  loss_ce_6: 0.1542  loss_mask_6: 0.2209  loss_dice_6: 0.1713  loss_ce_7: 0.1535  loss_mask_7: 0.2173  loss_dice_7: 0.1739  loss_ce_8: 0.154  loss_mask_8: 0.2182  loss_dice_8: 0.1682  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:54] d2.utils.events INFO:  eta: 1:28:10  iter: 8099  total_loss: 4.868  loss_ce: 0.1395  loss_mask: 0.22  loss_dice: 0.1368  loss_ce_0: 0.06907  loss_mask_0: 0.212  loss_dice_0: 0.1359  loss_ce_1: 0.1392  loss_mask_1: 0.2129  loss_dice_1: 0.1352  loss_ce_2: 0.1391  loss_mask_2: 0.2112  loss_dice_2: 0.1389  loss_ce_3: 0.139  loss_mask_3: 0.214  loss_dice_3: 0.14  loss_ce_4: 0.1383  loss_mask_4: 0.2209  loss_dice_4: 0.1369  loss_ce_5: 0.139  loss_mask_5: 0.2182  loss_dice_5: 0.1393  loss_ce_6: 0.1392  loss_mask_6: 0.2118  loss_dice_6: 0.1387  loss_ce_7: 0.1392  loss_mask_7: 0.2136  loss_dice_7: 0.1392  loss_ce_8: 0.1397  loss_mask_8: 0.2156  loss_dice_8: 0.1383  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:53:57] d2.utils.events INFO:  eta: 1:28:08  iter: 8119  total_loss: 4.951  loss_ce: 0.1302  loss_mask: 0.1693  loss_dice: 0.1602  loss_ce_0: 0.06499  loss_mask_0: 0.1772  loss_dice_0: 0.1558  loss_ce_1: 0.1304  loss_mask_1: 0.1783  loss_dice_1: 0.1702  loss_ce_2: 0.1302  loss_mask_2: 0.1848  loss_dice_2: 0.1612  loss_ce_3: 0.13  loss_mask_3: 0.1755  loss_dice_3: 0.1622  loss_ce_4: 0.1291  loss_mask_4: 0.1747  loss_dice_4: 0.1629  loss_ce_5: 0.1301  loss_mask_5: 0.1745  loss_dice_5: 0.1609  loss_ce_6: 0.1301  loss_mask_6: 0.1761  loss_dice_6: 0.1617  loss_ce_7: 0.1302  loss_mask_7: 0.168  loss_dice_7: 0.1675  loss_ce_8: 0.1303  loss_mask_8: 0.1762  loss_dice_8: 0.1603  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:54:00] d2.utils.events INFO:  eta: 1:28:05  iter: 8139  total_loss: 4.797  loss_ce: 0.1326  loss_mask: 0.2121  loss_dice: 0.1633  loss_ce_0: 0.06525  loss_mask_0: 0.1954  loss_dice_0: 0.1647  loss_ce_1: 0.1324  loss_mask_1: 0.2009  loss_dice_1: 0.1598  loss_ce_2: 0.1327  loss_mask_2: 0.2072  loss_dice_2: 0.1577  loss_ce_3: 0.1324  loss_mask_3: 0.203  loss_dice_3: 0.1555  loss_ce_4: 0.1323  loss_mask_4: 0.2158  loss_dice_4: 0.1568  loss_ce_5: 0.1326  loss_mask_5: 0.2018  loss_dice_5: 0.1614  loss_ce_6: 0.1329  loss_mask_6: 0.2092  loss_dice_6: 0.1552  loss_ce_7: 0.1326  loss_mask_7: 0.1957  loss_dice_7: 0.1588  loss_ce_8: 0.1329  loss_mask_8: 0.1987  loss_dice_8: 0.1551  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:02] d2.utils.events INFO:  eta: 1:28:01  iter: 8159  total_loss: 4.582  loss_ce: 0.13  loss_mask: 0.1656  loss_dice: 0.1293  loss_ce_0: 0.06541  loss_mask_0: 0.1596  loss_dice_0: 0.1232  loss_ce_1: 0.1301  loss_mask_1: 0.1627  loss_dice_1: 0.1249  loss_ce_2: 0.1302  loss_mask_2: 0.1626  loss_dice_2: 0.1287  loss_ce_3: 0.1304  loss_mask_3: 0.1617  loss_dice_3: 0.1263  loss_ce_4: 0.1306  loss_mask_4: 0.1613  loss_dice_4: 0.1244  loss_ce_5: 0.1303  loss_mask_5: 0.1659  loss_dice_5: 0.128  loss_ce_6: 0.1304  loss_mask_6: 0.166  loss_dice_6: 0.1272  loss_ce_7: 0.1304  loss_mask_7: 0.1566  loss_dice_7: 0.1253  loss_ce_8: 0.1302  loss_mask_8: 0.1614  loss_dice_8: 0.127  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:05] d2.utils.events INFO:  eta: 1:27:58  iter: 8179  total_loss: 5.676  loss_ce: 0.128  loss_mask: 0.2009  loss_dice: 0.1868  loss_ce_0: 0.06505  loss_mask_0: 0.1957  loss_dice_0: 0.1823  loss_ce_1: 0.1281  loss_mask_1: 0.1774  loss_dice_1: 0.1876  loss_ce_2: 0.128  loss_mask_2: 0.1903  loss_dice_2: 0.1827  loss_ce_3: 0.128  loss_mask_3: 0.1892  loss_dice_3: 0.1879  loss_ce_4: 0.1287  loss_mask_4: 0.1819  loss_dice_4: 0.1755  loss_ce_5: 0.128  loss_mask_5: 0.2024  loss_dice_5: 0.1879  loss_ce_6: 0.128  loss_mask_6: 0.1812  loss_dice_6: 0.1866  loss_ce_7: 0.1283  loss_mask_7: 0.1919  loss_dice_7: 0.1801  loss_ce_8: 0.128  loss_mask_8: 0.1983  loss_dice_8: 0.1768  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:07] d2.utils.events INFO:  eta: 1:27:55  iter: 8199  total_loss: 5.316  loss_ce: 0.1319  loss_mask: 0.2215  loss_dice: 0.1625  loss_ce_0: 0.06622  loss_mask_0: 0.2205  loss_dice_0: 0.1619  loss_ce_1: 0.1328  loss_mask_1: 0.2195  loss_dice_1: 0.1598  loss_ce_2: 0.1321  loss_mask_2: 0.2185  loss_dice_2: 0.1585  loss_ce_3: 0.1323  loss_mask_3: 0.2239  loss_dice_3: 0.1639  loss_ce_4: 0.1326  loss_mask_4: 0.2137  loss_dice_4: 0.1599  loss_ce_5: 0.1322  loss_mask_5: 0.2281  loss_dice_5: 0.1612  loss_ce_6: 0.1323  loss_mask_6: 0.2117  loss_dice_6: 0.1614  loss_ce_7: 0.1329  loss_mask_7: 0.212  loss_dice_7: 0.1624  loss_ce_8: 0.1321  loss_mask_8: 0.2173  loss_dice_8: 0.1581  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:10] d2.utils.events INFO:  eta: 1:27:52  iter: 8219  total_loss: 4.826  loss_ce: 0.1372  loss_mask: 0.1429  loss_dice: 0.2104  loss_ce_0: 0.06607  loss_mask_0: 0.1498  loss_dice_0: 0.2117  loss_ce_1: 0.1377  loss_mask_1: 0.152  loss_dice_1: 0.1928  loss_ce_2: 0.1377  loss_mask_2: 0.1355  loss_dice_2: 0.2169  loss_ce_3: 0.1391  loss_mask_3: 0.1499  loss_dice_3: 0.2019  loss_ce_4: 0.1403  loss_mask_4: 0.1484  loss_dice_4: 0.2052  loss_ce_5: 0.1389  loss_mask_5: 0.1417  loss_dice_5: 0.2053  loss_ce_6: 0.1391  loss_mask_6: 0.1431  loss_dice_6: 0.2044  loss_ce_7: 0.1385  loss_mask_7: 0.1423  loss_dice_7: 0.2083  loss_ce_8: 0.1382  loss_mask_8: 0.1419  loss_dice_8: 0.2015  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:12] d2.utils.events INFO:  eta: 1:27:50  iter: 8239  total_loss: 4.697  loss_ce: 0.1154  loss_mask: 0.1721  loss_dice: 0.1444  loss_ce_0: 0.06255  loss_mask_0: 0.1683  loss_dice_0: 0.1427  loss_ce_1: 0.1155  loss_mask_1: 0.1799  loss_dice_1: 0.1413  loss_ce_2: 0.1155  loss_mask_2: 0.1823  loss_dice_2: 0.152  loss_ce_3: 0.1153  loss_mask_3: 0.1806  loss_dice_3: 0.1468  loss_ce_4: 0.1153  loss_mask_4: 0.1767  loss_dice_4: 0.145  loss_ce_5: 0.1153  loss_mask_5: 0.1805  loss_dice_5: 0.147  loss_ce_6: 0.115  loss_mask_6: 0.1804  loss_dice_6: 0.1483  loss_ce_7: 0.1155  loss_mask_7: 0.1738  loss_dice_7: 0.1472  loss_ce_8: 0.1151  loss_mask_8: 0.1781  loss_dice_8: 0.1413  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:15] d2.utils.events INFO:  eta: 1:27:47  iter: 8259  total_loss: 5.894  loss_ce: 0.1583  loss_mask: 0.19  loss_dice: 0.201  loss_ce_0: 0.07209  loss_mask_0: 0.1848  loss_dice_0: 0.2091  loss_ce_1: 0.1589  loss_mask_1: 0.1846  loss_dice_1: 0.2084  loss_ce_2: 0.159  loss_mask_2: 0.1829  loss_dice_2: 0.2076  loss_ce_3: 0.1597  loss_mask_3: 0.1952  loss_dice_3: 0.2169  loss_ce_4: 0.1599  loss_mask_4: 0.1915  loss_dice_4: 0.2099  loss_ce_5: 0.1594  loss_mask_5: 0.1835  loss_dice_5: 0.2038  loss_ce_6: 0.16  loss_mask_6: 0.1957  loss_dice_6: 0.2103  loss_ce_7: 0.1596  loss_mask_7: 0.1829  loss_dice_7: 0.1979  loss_ce_8: 0.1591  loss_mask_8: 0.1846  loss_dice_8: 0.2009  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:17] d2.utils.events INFO:  eta: 1:27:45  iter: 8279  total_loss: 4.785  loss_ce: 0.1074  loss_mask: 0.2211  loss_dice: 0.14  loss_ce_0: 0.05876  loss_mask_0: 0.2198  loss_dice_0: 0.1387  loss_ce_1: 0.1075  loss_mask_1: 0.2215  loss_dice_1: 0.1386  loss_ce_2: 0.1072  loss_mask_2: 0.2175  loss_dice_2: 0.1373  loss_ce_3: 0.1073  loss_mask_3: 0.2243  loss_dice_3: 0.1403  loss_ce_4: 0.1077  loss_mask_4: 0.2163  loss_dice_4: 0.1391  loss_ce_5: 0.1073  loss_mask_5: 0.2174  loss_dice_5: 0.1357  loss_ce_6: 0.1071  loss_mask_6: 0.2166  loss_dice_6: 0.1361  loss_ce_7: 0.1074  loss_mask_7: 0.224  loss_dice_7: 0.1344  loss_ce_8: 0.1073  loss_mask_8: 0.2184  loss_dice_8: 0.1413  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:54:20] d2.utils.events INFO:  eta: 1:27:42  iter: 8299  total_loss: 5.343  loss_ce: 0.1041  loss_mask: 0.2056  loss_dice: 0.1649  loss_ce_0: 0.05772  loss_mask_0: 0.2114  loss_dice_0: 0.1647  loss_ce_1: 0.1041  loss_mask_1: 0.2095  loss_dice_1: 0.1697  loss_ce_2: 0.1038  loss_mask_2: 0.2054  loss_dice_2: 0.1536  loss_ce_3: 0.1038  loss_mask_3: 0.2068  loss_dice_3: 0.1536  loss_ce_4: 0.1039  loss_mask_4: 0.2094  loss_dice_4: 0.1628  loss_ce_5: 0.1038  loss_mask_5: 0.2126  loss_dice_5: 0.1584  loss_ce_6: 0.1034  loss_mask_6: 0.2021  loss_dice_6: 0.1531  loss_ce_7: 0.1038  loss_mask_7: 0.1996  loss_dice_7: 0.152  loss_ce_8: 0.1039  loss_mask_8: 0.1979  loss_dice_8: 0.1584  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:22] d2.utils.events INFO:  eta: 1:27:42  iter: 8319  total_loss: 4.606  loss_ce: 0.1574  loss_mask: 0.1623  loss_dice: 0.1631  loss_ce_0: 0.07451  loss_mask_0: 0.164  loss_dice_0: 0.1583  loss_ce_1: 0.1578  loss_mask_1: 0.164  loss_dice_1: 0.1604  loss_ce_2: 0.1576  loss_mask_2: 0.1697  loss_dice_2: 0.1578  loss_ce_3: 0.1577  loss_mask_3: 0.1673  loss_dice_3: 0.1562  loss_ce_4: 0.1573  loss_mask_4: 0.164  loss_dice_4: 0.1597  loss_ce_5: 0.1577  loss_mask_5: 0.1734  loss_dice_5: 0.1591  loss_ce_6: 0.158  loss_mask_6: 0.1679  loss_dice_6: 0.1518  loss_ce_7: 0.1577  loss_mask_7: 0.1652  loss_dice_7: 0.161  loss_ce_8: 0.1576  loss_mask_8: 0.1639  loss_dice_8: 0.1556  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:54:25] d2.utils.events INFO:  eta: 1:27:43  iter: 8339  total_loss: 5.075  loss_ce: 0.1108  loss_mask: 0.2115  loss_dice: 0.1395  loss_ce_0: 0.0577  loss_mask_0: 0.2002  loss_dice_0: 0.1364  loss_ce_1: 0.1107  loss_mask_1: 0.2182  loss_dice_1: 0.1346  loss_ce_2: 0.1111  loss_mask_2: 0.2139  loss_dice_2: 0.1361  loss_ce_3: 0.1113  loss_mask_3: 0.2202  loss_dice_3: 0.1345  loss_ce_4: 0.1117  loss_mask_4: 0.2113  loss_dice_4: 0.1365  loss_ce_5: 0.1113  loss_mask_5: 0.2143  loss_dice_5: 0.1354  loss_ce_6: 0.1113  loss_mask_6: 0.2068  loss_dice_6: 0.135  loss_ce_7: 0.1113  loss_mask_7: 0.2137  loss_dice_7: 0.1361  loss_ce_8: 0.1112  loss_mask_8: 0.2138  loss_dice_8: 0.1342  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:27] d2.utils.events INFO:  eta: 1:27:41  iter: 8359  total_loss: 5.269  loss_ce: 0.1055  loss_mask: 0.1787  loss_dice: 0.1749  loss_ce_0: 0.05664  loss_mask_0: 0.1828  loss_dice_0: 0.1692  loss_ce_1: 0.1054  loss_mask_1: 0.1698  loss_dice_1: 0.1642  loss_ce_2: 0.1056  loss_mask_2: 0.1776  loss_dice_2: 0.1707  loss_ce_3: 0.1053  loss_mask_3: 0.1779  loss_dice_3: 0.1734  loss_ce_4: 0.1055  loss_mask_4: 0.1863  loss_dice_4: 0.1735  loss_ce_5: 0.1054  loss_mask_5: 0.1745  loss_dice_5: 0.1719  loss_ce_6: 0.1053  loss_mask_6: 0.1847  loss_dice_6: 0.1729  loss_ce_7: 0.1055  loss_mask_7: 0.1945  loss_dice_7: 0.1727  loss_ce_8: 0.1056  loss_mask_8: 0.178  loss_dice_8: 0.1684  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:54:30] d2.utils.events INFO:  eta: 1:27:41  iter: 8379  total_loss: 5.049  loss_ce: 0.1299  loss_mask: 0.1749  loss_dice: 0.1553  loss_ce_0: 0.0657  loss_mask_0: 0.1771  loss_dice_0: 0.148  loss_ce_1: 0.1302  loss_mask_1: 0.1801  loss_dice_1: 0.1468  loss_ce_2: 0.1301  loss_mask_2: 0.1858  loss_dice_2: 0.1437  loss_ce_3: 0.1301  loss_mask_3: 0.1747  loss_dice_3: 0.1556  loss_ce_4: 0.1301  loss_mask_4: 0.1768  loss_dice_4: 0.153  loss_ce_5: 0.1301  loss_mask_5: 0.1786  loss_dice_5: 0.1488  loss_ce_6: 0.13  loss_mask_6: 0.1803  loss_dice_6: 0.1385  loss_ce_7: 0.1299  loss_mask_7: 0.1853  loss_dice_7: 0.1479  loss_ce_8: 0.13  loss_mask_8: 0.1759  loss_dice_8: 0.1475  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:33] d2.utils.events INFO:  eta: 1:27:42  iter: 8399  total_loss: 5.255  loss_ce: 0.1372  loss_mask: 0.1948  loss_dice: 0.1493  loss_ce_0: 0.07227  loss_mask_0: 0.2037  loss_dice_0: 0.1451  loss_ce_1: 0.1375  loss_mask_1: 0.2023  loss_dice_1: 0.148  loss_ce_2: 0.1371  loss_mask_2: 0.2142  loss_dice_2: 0.1514  loss_ce_3: 0.136  loss_mask_3: 0.2063  loss_dice_3: 0.1476  loss_ce_4: 0.1351  loss_mask_4: 0.2017  loss_dice_4: 0.1485  loss_ce_5: 0.1369  loss_mask_5: 0.1972  loss_dice_5: 0.1416  loss_ce_6: 0.1358  loss_mask_6: 0.2009  loss_dice_6: 0.1426  loss_ce_7: 0.136  loss_mask_7: 0.2074  loss_dice_7: 0.1506  loss_ce_8: 0.1371  loss_mask_8: 0.2037  loss_dice_8: 0.1472  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:35] d2.utils.events INFO:  eta: 1:27:40  iter: 8419  total_loss: 4.949  loss_ce: 0.1282  loss_mask: 0.2056  loss_dice: 0.1468  loss_ce_0: 0.0607  loss_mask_0: 0.1963  loss_dice_0: 0.148  loss_ce_1: 0.1278  loss_mask_1: 0.2033  loss_dice_1: 0.1465  loss_ce_2: 0.1283  loss_mask_2: 0.1916  loss_dice_2: 0.1411  loss_ce_3: 0.1288  loss_mask_3: 0.2081  loss_dice_3: 0.1476  loss_ce_4: 0.1289  loss_mask_4: 0.2153  loss_dice_4: 0.1565  loss_ce_5: 0.1284  loss_mask_5: 0.2028  loss_dice_5: 0.1459  loss_ce_6: 0.1288  loss_mask_6: 0.1974  loss_dice_6: 0.1464  loss_ce_7: 0.1281  loss_mask_7: 0.2069  loss_dice_7: 0.1506  loss_ce_8: 0.1283  loss_mask_8: 0.1979  loss_dice_8: 0.1509  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:38] d2.utils.events INFO:  eta: 1:27:39  iter: 8439  total_loss: 5.495  loss_ce: 0.1273  loss_mask: 0.1941  loss_dice: 0.2364  loss_ce_0: 0.067  loss_mask_0: 0.1984  loss_dice_0: 0.2472  loss_ce_1: 0.1272  loss_mask_1: 0.1905  loss_dice_1: 0.2256  loss_ce_2: 0.1275  loss_mask_2: 0.1891  loss_dice_2: 0.2385  loss_ce_3: 0.1277  loss_mask_3: 0.1968  loss_dice_3: 0.2286  loss_ce_4: 0.1275  loss_mask_4: 0.1933  loss_dice_4: 0.2531  loss_ce_5: 0.1274  loss_mask_5: 0.1952  loss_dice_5: 0.2451  loss_ce_6: 0.1277  loss_mask_6: 0.1966  loss_dice_6: 0.2328  loss_ce_7: 0.1273  loss_mask_7: 0.1962  loss_dice_7: 0.2363  loss_ce_8: 0.1274  loss_mask_8: 0.1863  loss_dice_8: 0.2456  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:40] d2.utils.events INFO:  eta: 1:27:39  iter: 8459  total_loss: 5.435  loss_ce: 0.1336  loss_mask: 0.2069  loss_dice: 0.167  loss_ce_0: 0.0647  loss_mask_0: 0.2047  loss_dice_0: 0.1752  loss_ce_1: 0.1335  loss_mask_1: 0.2086  loss_dice_1: 0.1746  loss_ce_2: 0.1339  loss_mask_2: 0.2041  loss_dice_2: 0.178  loss_ce_3: 0.1344  loss_mask_3: 0.2165  loss_dice_3: 0.1853  loss_ce_4: 0.1343  loss_mask_4: 0.2012  loss_dice_4: 0.1831  loss_ce_5: 0.1341  loss_mask_5: 0.2146  loss_dice_5: 0.1782  loss_ce_6: 0.1347  loss_mask_6: 0.2078  loss_dice_6: 0.1781  loss_ce_7: 0.1339  loss_mask_7: 0.2048  loss_dice_7: 0.1805  loss_ce_8: 0.1342  loss_mask_8: 0.2101  loss_dice_8: 0.1781  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:43] d2.utils.events INFO:  eta: 1:27:36  iter: 8479  total_loss: 5.258  loss_ce: 0.1246  loss_mask: 0.1824  loss_dice: 0.1809  loss_ce_0: 0.06298  loss_mask_0: 0.1799  loss_dice_0: 0.1814  loss_ce_1: 0.1252  loss_mask_1: 0.1835  loss_dice_1: 0.1894  loss_ce_2: 0.1247  loss_mask_2: 0.1866  loss_dice_2: 0.1897  loss_ce_3: 0.1244  loss_mask_3: 0.1821  loss_dice_3: 0.1878  loss_ce_4: 0.1243  loss_mask_4: 0.1853  loss_dice_4: 0.186  loss_ce_5: 0.1247  loss_mask_5: 0.1873  loss_dice_5: 0.1813  loss_ce_6: 0.1246  loss_mask_6: 0.1832  loss_dice_6: 0.1864  loss_ce_7: 0.1248  loss_mask_7: 0.1871  loss_dice_7: 0.189  loss_ce_8: 0.1247  loss_mask_8: 0.1867  loss_dice_8: 0.1867  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:45] d2.utils.events INFO:  eta: 1:27:32  iter: 8499  total_loss: 4.801  loss_ce: 0.1494  loss_mask: 0.2027  loss_dice: 0.1446  loss_ce_0: 0.07063  loss_mask_0: 0.19  loss_dice_0: 0.1493  loss_ce_1: 0.1499  loss_mask_1: 0.1903  loss_dice_1: 0.1532  loss_ce_2: 0.1492  loss_mask_2: 0.1889  loss_dice_2: 0.1498  loss_ce_3: 0.1504  loss_mask_3: 0.1943  loss_dice_3: 0.1454  loss_ce_4: 0.1525  loss_mask_4: 0.1991  loss_dice_4: 0.1481  loss_ce_5: 0.15  loss_mask_5: 0.1931  loss_dice_5: 0.1431  loss_ce_6: 0.1506  loss_mask_6: 0.1908  loss_dice_6: 0.1437  loss_ce_7: 0.1506  loss_mask_7: 0.1808  loss_dice_7: 0.1428  loss_ce_8: 0.1498  loss_mask_8: 0.1845  loss_dice_8: 0.147  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:48] d2.utils.events INFO:  eta: 1:27:25  iter: 8519  total_loss: 5.124  loss_ce: 0.1267  loss_mask: 0.1921  loss_dice: 0.163  loss_ce_0: 0.0649  loss_mask_0: 0.189  loss_dice_0: 0.1659  loss_ce_1: 0.1267  loss_mask_1: 0.1899  loss_dice_1: 0.1683  loss_ce_2: 0.1267  loss_mask_2: 0.1932  loss_dice_2: 0.1682  loss_ce_3: 0.1266  loss_mask_3: 0.1953  loss_dice_3: 0.1682  loss_ce_4: 0.1266  loss_mask_4: 0.1813  loss_dice_4: 0.1634  loss_ce_5: 0.1266  loss_mask_5: 0.1919  loss_dice_5: 0.163  loss_ce_6: 0.1266  loss_mask_6: 0.189  loss_dice_6: 0.1701  loss_ce_7: 0.1267  loss_mask_7: 0.1881  loss_dice_7: 0.1638  loss_ce_8: 0.1267  loss_mask_8: 0.189  loss_dice_8: 0.1618  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:54:50] d2.utils.events INFO:  eta: 1:27:23  iter: 8539  total_loss: 5.776  loss_ce: 0.1315  loss_mask: 0.2019  loss_dice: 0.2318  loss_ce_0: 0.06498  loss_mask_0: 0.2001  loss_dice_0: 0.2162  loss_ce_1: 0.1318  loss_mask_1: 0.1917  loss_dice_1: 0.2226  loss_ce_2: 0.1319  loss_mask_2: 0.195  loss_dice_2: 0.2148  loss_ce_3: 0.1329  loss_mask_3: 0.1953  loss_dice_3: 0.2226  loss_ce_4: 0.1333  loss_mask_4: 0.2022  loss_dice_4: 0.2184  loss_ce_5: 0.1324  loss_mask_5: 0.1999  loss_dice_5: 0.222  loss_ce_6: 0.1328  loss_mask_6: 0.1992  loss_dice_6: 0.2186  loss_ce_7: 0.1326  loss_mask_7: 0.2028  loss_dice_7: 0.2215  loss_ce_8: 0.1327  loss_mask_8: 0.1973  loss_dice_8: 0.2175  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:53] d2.utils.events INFO:  eta: 1:27:19  iter: 8559  total_loss: 5.02  loss_ce: 0.1278  loss_mask: 0.1647  loss_dice: 0.148  loss_ce_0: 0.06495  loss_mask_0: 0.1662  loss_dice_0: 0.1585  loss_ce_1: 0.1278  loss_mask_1: 0.1671  loss_dice_1: 0.149  loss_ce_2: 0.1278  loss_mask_2: 0.1638  loss_dice_2: 0.1448  loss_ce_3: 0.1279  loss_mask_3: 0.1621  loss_dice_3: 0.1489  loss_ce_4: 0.128  loss_mask_4: 0.1675  loss_dice_4: 0.1489  loss_ce_5: 0.1279  loss_mask_5: 0.1598  loss_dice_5: 0.1483  loss_ce_6: 0.1279  loss_mask_6: 0.1715  loss_dice_6: 0.155  loss_ce_7: 0.1278  loss_mask_7: 0.1633  loss_dice_7: 0.1426  loss_ce_8: 0.1278  loss_mask_8: 0.1713  loss_dice_8: 0.1426  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:54:55] d2.utils.events INFO:  eta: 1:27:18  iter: 8579  total_loss: 5.52  loss_ce: 0.1267  loss_mask: 0.2031  loss_dice: 0.1782  loss_ce_0: 0.06327  loss_mask_0: 0.1976  loss_dice_0: 0.172  loss_ce_1: 0.1268  loss_mask_1: 0.1981  loss_dice_1: 0.1682  loss_ce_2: 0.1265  loss_mask_2: 0.2041  loss_dice_2: 0.1722  loss_ce_3: 0.1263  loss_mask_3: 0.2062  loss_dice_3: 0.1804  loss_ce_4: 0.1261  loss_mask_4: 0.2043  loss_dice_4: 0.1768  loss_ce_5: 0.1261  loss_mask_5: 0.2039  loss_dice_5: 0.1719  loss_ce_6: 0.1263  loss_mask_6: 0.2041  loss_dice_6: 0.1701  loss_ce_7: 0.1265  loss_mask_7: 0.2031  loss_dice_7: 0.1767  loss_ce_8: 0.1271  loss_mask_8: 0.1983  loss_dice_8: 0.1712  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:54:58] d2.utils.events INFO:  eta: 1:27:13  iter: 8599  total_loss: 5.004  loss_ce: 0.12  loss_mask: 0.2094  loss_dice: 0.1358  loss_ce_0: 0.0622  loss_mask_0: 0.2099  loss_dice_0: 0.1396  loss_ce_1: 0.1198  loss_mask_1: 0.2077  loss_dice_1: 0.1434  loss_ce_2: 0.1199  loss_mask_2: 0.2124  loss_dice_2: 0.1363  loss_ce_3: 0.1198  loss_mask_3: 0.2078  loss_dice_3: 0.142  loss_ce_4: 0.1195  loss_mask_4: 0.2111  loss_dice_4: 0.1337  loss_ce_5: 0.1198  loss_mask_5: 0.2017  loss_dice_5: 0.1352  loss_ce_6: 0.1196  loss_mask_6: 0.1981  loss_dice_6: 0.1367  loss_ce_7: 0.1196  loss_mask_7: 0.1977  loss_dice_7: 0.141  loss_ce_8: 0.1198  loss_mask_8: 0.2115  loss_dice_8: 0.146  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:00] d2.utils.events INFO:  eta: 1:27:12  iter: 8619  total_loss: 5.112  loss_ce: 0.1282  loss_mask: 0.2147  loss_dice: 0.1606  loss_ce_0: 0.06492  loss_mask_0: 0.1978  loss_dice_0: 0.1628  loss_ce_1: 0.1281  loss_mask_1: 0.2057  loss_dice_1: 0.1582  loss_ce_2: 0.1283  loss_mask_2: 0.2169  loss_dice_2: 0.1595  loss_ce_3: 0.1283  loss_mask_3: 0.2026  loss_dice_3: 0.1613  loss_ce_4: 0.1284  loss_mask_4: 0.2142  loss_dice_4: 0.1612  loss_ce_5: 0.1283  loss_mask_5: 0.2057  loss_dice_5: 0.1568  loss_ce_6: 0.1284  loss_mask_6: 0.1988  loss_dice_6: 0.1567  loss_ce_7: 0.1283  loss_mask_7: 0.2055  loss_dice_7: 0.1623  loss_ce_8: 0.1283  loss_mask_8: 0.204  loss_dice_8: 0.1614  time: 0.1259  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:03] d2.utils.events INFO:  eta: 1:27:09  iter: 8639  total_loss: 4.604  loss_ce: 0.1166  loss_mask: 0.2024  loss_dice: 0.1324  loss_ce_0: 0.06117  loss_mask_0: 0.1988  loss_dice_0: 0.1281  loss_ce_1: 0.1165  loss_mask_1: 0.2095  loss_dice_1: 0.1316  loss_ce_2: 0.1168  loss_mask_2: 0.2076  loss_dice_2: 0.1321  loss_ce_3: 0.1168  loss_mask_3: 0.2107  loss_dice_3: 0.1372  loss_ce_4: 0.1169  loss_mask_4: 0.2036  loss_dice_4: 0.1376  loss_ce_5: 0.1168  loss_mask_5: 0.2081  loss_dice_5: 0.1391  loss_ce_6: 0.1167  loss_mask_6: 0.2  loss_dice_6: 0.1314  loss_ce_7: 0.1167  loss_mask_7: 0.2028  loss_dice_7: 0.1342  loss_ce_8: 0.1166  loss_mask_8: 0.2024  loss_dice_8: 0.1296  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:06] d2.utils.events INFO:  eta: 1:27:04  iter: 8659  total_loss: 5.028  loss_ce: 0.1245  loss_mask: 0.2446  loss_dice: 0.1413  loss_ce_0: 0.06235  loss_mask_0: 0.2396  loss_dice_0: 0.1315  loss_ce_1: 0.1248  loss_mask_1: 0.2394  loss_dice_1: 0.142  loss_ce_2: 0.1249  loss_mask_2: 0.2444  loss_dice_2: 0.1363  loss_ce_3: 0.1253  loss_mask_3: 0.2383  loss_dice_3: 0.1419  loss_ce_4: 0.1261  loss_mask_4: 0.2467  loss_dice_4: 0.1398  loss_ce_5: 0.1256  loss_mask_5: 0.2495  loss_dice_5: 0.1384  loss_ce_6: 0.1256  loss_mask_6: 0.2386  loss_dice_6: 0.1408  loss_ce_7: 0.1255  loss_mask_7: 0.2413  loss_dice_7: 0.1379  loss_ce_8: 0.125  loss_mask_8: 0.2333  loss_dice_8: 0.1365  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:08] d2.utils.events INFO:  eta: 1:27:02  iter: 8679  total_loss: 5.204  loss_ce: 0.1238  loss_mask: 0.2725  loss_dice: 0.1528  loss_ce_0: 0.06226  loss_mask_0: 0.2683  loss_dice_0: 0.152  loss_ce_1: 0.1241  loss_mask_1: 0.2721  loss_dice_1: 0.1513  loss_ce_2: 0.1242  loss_mask_2: 0.2732  loss_dice_2: 0.151  loss_ce_3: 0.1245  loss_mask_3: 0.2667  loss_dice_3: 0.1484  loss_ce_4: 0.125  loss_mask_4: 0.2746  loss_dice_4: 0.1525  loss_ce_5: 0.1246  loss_mask_5: 0.2588  loss_dice_5: 0.1478  loss_ce_6: 0.1245  loss_mask_6: 0.2718  loss_dice_6: 0.1505  loss_ce_7: 0.1247  loss_mask_7: 0.2655  loss_dice_7: 0.1547  loss_ce_8: 0.1243  loss_mask_8: 0.2829  loss_dice_8: 0.1506  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:11] d2.utils.events INFO:  eta: 1:27:01  iter: 8699  total_loss: 4.585  loss_ce: 0.1192  loss_mask: 0.2013  loss_dice: 0.121  loss_ce_0: 0.06139  loss_mask_0: 0.1995  loss_dice_0: 0.121  loss_ce_1: 0.1192  loss_mask_1: 0.2047  loss_dice_1: 0.1161  loss_ce_2: 0.1192  loss_mask_2: 0.1972  loss_dice_2: 0.1153  loss_ce_3: 0.119  loss_mask_3: 0.2066  loss_dice_3: 0.1158  loss_ce_4: 0.1189  loss_mask_4: 0.1964  loss_dice_4: 0.1197  loss_ce_5: 0.1188  loss_mask_5: 0.1994  loss_dice_5: 0.1141  loss_ce_6: 0.1187  loss_mask_6: 0.1988  loss_dice_6: 0.12  loss_ce_7: 0.1189  loss_mask_7: 0.1947  loss_dice_7: 0.1142  loss_ce_8: 0.1194  loss_mask_8: 0.2038  loss_dice_8: 0.1172  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:13] d2.utils.events INFO:  eta: 1:27:00  iter: 8719  total_loss: 5.308  loss_ce: 0.1428  loss_mask: 0.2023  loss_dice: 0.1424  loss_ce_0: 0.06991  loss_mask_0: 0.1966  loss_dice_0: 0.145  loss_ce_1: 0.1431  loss_mask_1: 0.2092  loss_dice_1: 0.1494  loss_ce_2: 0.1427  loss_mask_2: 0.2002  loss_dice_2: 0.149  loss_ce_3: 0.1425  loss_mask_3: 0.2025  loss_dice_3: 0.1447  loss_ce_4: 0.1422  loss_mask_4: 0.2062  loss_dice_4: 0.149  loss_ce_5: 0.1425  loss_mask_5: 0.2107  loss_dice_5: 0.1484  loss_ce_6: 0.1426  loss_mask_6: 0.2083  loss_dice_6: 0.1462  loss_ce_7: 0.1425  loss_mask_7: 0.2032  loss_dice_7: 0.145  loss_ce_8: 0.1427  loss_mask_8: 0.206  loss_dice_8: 0.1515  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:16] d2.utils.events INFO:  eta: 1:26:56  iter: 8739  total_loss: 4.575  loss_ce: 0.1273  loss_mask: 0.1595  loss_dice: 0.1571  loss_ce_0: 0.06267  loss_mask_0: 0.1527  loss_dice_0: 0.1651  loss_ce_1: 0.1271  loss_mask_1: 0.1574  loss_dice_1: 0.1636  loss_ce_2: 0.1274  loss_mask_2: 0.1564  loss_dice_2: 0.1608  loss_ce_3: 0.1274  loss_mask_3: 0.1579  loss_dice_3: 0.162  loss_ce_4: 0.1272  loss_mask_4: 0.1551  loss_dice_4: 0.1613  loss_ce_5: 0.1273  loss_mask_5: 0.1505  loss_dice_5: 0.1657  loss_ce_6: 0.1275  loss_mask_6: 0.1616  loss_dice_6: 0.1654  loss_ce_7: 0.1274  loss_mask_7: 0.1608  loss_dice_7: 0.1587  loss_ce_8: 0.1274  loss_mask_8: 0.1524  loss_dice_8: 0.161  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:18] d2.utils.events INFO:  eta: 1:26:53  iter: 8759  total_loss: 4.777  loss_ce: 0.1274  loss_mask: 0.2207  loss_dice: 0.1295  loss_ce_0: 0.06328  loss_mask_0: 0.2291  loss_dice_0: 0.1257  loss_ce_1: 0.1274  loss_mask_1: 0.2181  loss_dice_1: 0.1249  loss_ce_2: 0.1273  loss_mask_2: 0.2199  loss_dice_2: 0.129  loss_ce_3: 0.1273  loss_mask_3: 0.2189  loss_dice_3: 0.1279  loss_ce_4: 0.1272  loss_mask_4: 0.2307  loss_dice_4: 0.133  loss_ce_5: 0.1274  loss_mask_5: 0.2313  loss_dice_5: 0.1311  loss_ce_6: 0.1273  loss_mask_6: 0.2185  loss_dice_6: 0.1274  loss_ce_7: 0.1274  loss_mask_7: 0.2204  loss_dice_7: 0.1317  loss_ce_8: 0.1275  loss_mask_8: 0.2269  loss_dice_8: 0.1307  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:21] d2.utils.events INFO:  eta: 1:26:52  iter: 8779  total_loss: 4.538  loss_ce: 0.1225  loss_mask: 0.1689  loss_dice: 0.1387  loss_ce_0: 0.06219  loss_mask_0: 0.1678  loss_dice_0: 0.1366  loss_ce_1: 0.1227  loss_mask_1: 0.1809  loss_dice_1: 0.1368  loss_ce_2: 0.1227  loss_mask_2: 0.1697  loss_dice_2: 0.1322  loss_ce_3: 0.1222  loss_mask_3: 0.1775  loss_dice_3: 0.1353  loss_ce_4: 0.122  loss_mask_4: 0.1724  loss_dice_4: 0.1374  loss_ce_5: 0.122  loss_mask_5: 0.1781  loss_dice_5: 0.1373  loss_ce_6: 0.1221  loss_mask_6: 0.1726  loss_dice_6: 0.1372  loss_ce_7: 0.1223  loss_mask_7: 0.1817  loss_dice_7: 0.1362  loss_ce_8: 0.1227  loss_mask_8: 0.1706  loss_dice_8: 0.1323  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:23] d2.utils.events INFO:  eta: 1:26:51  iter: 8799  total_loss: 5.445  loss_ce: 0.1277  loss_mask: 0.1347  loss_dice: 0.2046  loss_ce_0: 0.06473  loss_mask_0: 0.1321  loss_dice_0: 0.1959  loss_ce_1: 0.1276  loss_mask_1: 0.139  loss_dice_1: 0.1905  loss_ce_2: 0.1277  loss_mask_2: 0.1257  loss_dice_2: 0.2033  loss_ce_3: 0.1278  loss_mask_3: 0.1235  loss_dice_3: 0.1994  loss_ce_4: 0.1276  loss_mask_4: 0.1346  loss_dice_4: 0.204  loss_ce_5: 0.1278  loss_mask_5: 0.1309  loss_dice_5: 0.1937  loss_ce_6: 0.1278  loss_mask_6: 0.1386  loss_dice_6: 0.2084  loss_ce_7: 0.1277  loss_mask_7: 0.1362  loss_dice_7: 0.2012  loss_ce_8: 0.1277  loss_mask_8: 0.1298  loss_dice_8: 0.2004  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:26] d2.utils.events INFO:  eta: 1:26:48  iter: 8819  total_loss: 5.205  loss_ce: 0.1163  loss_mask: 0.1909  loss_dice: 0.1727  loss_ce_0: 0.06104  loss_mask_0: 0.1828  loss_dice_0: 0.1707  loss_ce_1: 0.1154  loss_mask_1: 0.1913  loss_dice_1: 0.1738  loss_ce_2: 0.1165  loss_mask_2: 0.1848  loss_dice_2: 0.1701  loss_ce_3: 0.1161  loss_mask_3: 0.1902  loss_dice_3: 0.1693  loss_ce_4: 0.1148  loss_mask_4: 0.1811  loss_dice_4: 0.1661  loss_ce_5: 0.1162  loss_mask_5: 0.1851  loss_dice_5: 0.1677  loss_ce_6: 0.1161  loss_mask_6: 0.1855  loss_dice_6: 0.1692  loss_ce_7: 0.1153  loss_mask_7: 0.1793  loss_dice_7: 0.1753  loss_ce_8: 0.1165  loss_mask_8: 0.1966  loss_dice_8: 0.1724  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:28] d2.utils.events INFO:  eta: 1:26:44  iter: 8839  total_loss: 4.899  loss_ce: 0.1438  loss_mask: 0.1782  loss_dice: 0.1562  loss_ce_0: 0.07011  loss_mask_0: 0.1765  loss_dice_0: 0.1578  loss_ce_1: 0.144  loss_mask_1: 0.1708  loss_dice_1: 0.1492  loss_ce_2: 0.1437  loss_mask_2: 0.1845  loss_dice_2: 0.1531  loss_ce_3: 0.1434  loss_mask_3: 0.1761  loss_dice_3: 0.1541  loss_ce_4: 0.1431  loss_mask_4: 0.1791  loss_dice_4: 0.1549  loss_ce_5: 0.1435  loss_mask_5: 0.1733  loss_dice_5: 0.1592  loss_ce_6: 0.1436  loss_mask_6: 0.1791  loss_dice_6: 0.1612  loss_ce_7: 0.1435  loss_mask_7: 0.1699  loss_dice_7: 0.1608  loss_ce_8: 0.1439  loss_mask_8: 0.1695  loss_dice_8: 0.1608  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:31] d2.utils.events INFO:  eta: 1:26:40  iter: 8859  total_loss: 4.915  loss_ce: 0.127  loss_mask: 0.2114  loss_dice: 0.1428  loss_ce_0: 0.06398  loss_mask_0: 0.2161  loss_dice_0: 0.1472  loss_ce_1: 0.1271  loss_mask_1: 0.2118  loss_dice_1: 0.1496  loss_ce_2: 0.1275  loss_mask_2: 0.2169  loss_dice_2: 0.1473  loss_ce_3: 0.1279  loss_mask_3: 0.2129  loss_dice_3: 0.1441  loss_ce_4: 0.1278  loss_mask_4: 0.2226  loss_dice_4: 0.1514  loss_ce_5: 0.1278  loss_mask_5: 0.2237  loss_dice_5: 0.1502  loss_ce_6: 0.1278  loss_mask_6: 0.2129  loss_dice_6: 0.146  loss_ce_7: 0.1278  loss_mask_7: 0.2178  loss_dice_7: 0.1482  loss_ce_8: 0.1269  loss_mask_8: 0.2173  loss_dice_8: 0.1445  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:33] d2.utils.events INFO:  eta: 1:26:38  iter: 8879  total_loss: 5.479  loss_ce: 0.1278  loss_mask: 0.2323  loss_dice: 0.1627  loss_ce_0: 0.06354  loss_mask_0: 0.2448  loss_dice_0: 0.1645  loss_ce_1: 0.1278  loss_mask_1: 0.234  loss_dice_1: 0.1587  loss_ce_2: 0.1279  loss_mask_2: 0.2334  loss_dice_2: 0.1659  loss_ce_3: 0.128  loss_mask_3: 0.2298  loss_dice_3: 0.1605  loss_ce_4: 0.128  loss_mask_4: 0.2338  loss_dice_4: 0.1611  loss_ce_5: 0.128  loss_mask_5: 0.2379  loss_dice_5: 0.1547  loss_ce_6: 0.128  loss_mask_6: 0.2543  loss_dice_6: 0.1606  loss_ce_7: 0.1279  loss_mask_7: 0.2398  loss_dice_7: 0.1605  loss_ce_8: 0.1279  loss_mask_8: 0.2319  loss_dice_8: 0.1609  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:36] d2.utils.events INFO:  eta: 1:26:38  iter: 8899  total_loss: 4.931  loss_ce: 0.1338  loss_mask: 0.1655  loss_dice: 0.144  loss_ce_0: 0.06651  loss_mask_0: 0.1667  loss_dice_0: 0.1472  loss_ce_1: 0.1344  loss_mask_1: 0.1743  loss_dice_1: 0.141  loss_ce_2: 0.1341  loss_mask_2: 0.1621  loss_dice_2: 0.1434  loss_ce_3: 0.1348  loss_mask_3: 0.1765  loss_dice_3: 0.1401  loss_ce_4: 0.1362  loss_mask_4: 0.1659  loss_dice_4: 0.1379  loss_ce_5: 0.1347  loss_mask_5: 0.1641  loss_dice_5: 0.1437  loss_ce_6: 0.1347  loss_mask_6: 0.1649  loss_dice_6: 0.1403  loss_ce_7: 0.1349  loss_mask_7: 0.1645  loss_dice_7: 0.1417  loss_ce_8: 0.1338  loss_mask_8: 0.1742  loss_dice_8: 0.143  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:38] d2.utils.events INFO:  eta: 1:26:35  iter: 8919  total_loss: 4.736  loss_ce: 0.1113  loss_mask: 0.188  loss_dice: 0.1604  loss_ce_0: 0.06177  loss_mask_0: 0.1856  loss_dice_0: 0.1645  loss_ce_1: 0.1106  loss_mask_1: 0.1911  loss_dice_1: 0.1677  loss_ce_2: 0.1103  loss_mask_2: 0.191  loss_dice_2: 0.1603  loss_ce_3: 0.1092  loss_mask_3: 0.1891  loss_dice_3: 0.1651  loss_ce_4: 0.1078  loss_mask_4: 0.188  loss_dice_4: 0.1747  loss_ce_5: 0.109  loss_mask_5: 0.1858  loss_dice_5: 0.1636  loss_ce_6: 0.1091  loss_mask_6: 0.1953  loss_dice_6: 0.1624  loss_ce_7: 0.1097  loss_mask_7: 0.1963  loss_dice_7: 0.1681  loss_ce_8: 0.1104  loss_mask_8: 0.1843  loss_dice_8: 0.1615  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:41] d2.utils.events INFO:  eta: 1:26:30  iter: 8939  total_loss: 5.154  loss_ce: 0.1506  loss_mask: 0.2076  loss_dice: 0.1365  loss_ce_0: 0.07061  loss_mask_0: 0.2129  loss_dice_0: 0.1501  loss_ce_1: 0.1503  loss_mask_1: 0.2061  loss_dice_1: 0.1418  loss_ce_2: 0.1505  loss_mask_2: 0.206  loss_dice_2: 0.1484  loss_ce_3: 0.1502  loss_mask_3: 0.2165  loss_dice_3: 0.1447  loss_ce_4: 0.1496  loss_mask_4: 0.2064  loss_dice_4: 0.1381  loss_ce_5: 0.15  loss_mask_5: 0.2116  loss_dice_5: 0.1389  loss_ce_6: 0.1506  loss_mask_6: 0.211  loss_dice_6: 0.1344  loss_ce_7: 0.1507  loss_mask_7: 0.2159  loss_dice_7: 0.152  loss_ce_8: 0.1511  loss_mask_8: 0.2146  loss_dice_8: 0.1491  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:44] d2.utils.events INFO:  eta: 1:26:27  iter: 8959  total_loss: 4.767  loss_ce: 0.1205  loss_mask: 0.2171  loss_dice: 0.143  loss_ce_0: 0.06157  loss_mask_0: 0.2179  loss_dice_0: 0.1416  loss_ce_1: 0.1203  loss_mask_1: 0.2194  loss_dice_1: 0.1421  loss_ce_2: 0.1205  loss_mask_2: 0.2185  loss_dice_2: 0.145  loss_ce_3: 0.1214  loss_mask_3: 0.2205  loss_dice_3: 0.143  loss_ce_4: 0.1222  loss_mask_4: 0.219  loss_dice_4: 0.1465  loss_ce_5: 0.1214  loss_mask_5: 0.2161  loss_dice_5: 0.1414  loss_ce_6: 0.1216  loss_mask_6: 0.2218  loss_dice_6: 0.1419  loss_ce_7: 0.1205  loss_mask_7: 0.2204  loss_dice_7: 0.1445  loss_ce_8: 0.1205  loss_mask_8: 0.2234  loss_dice_8: 0.1427  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:46] d2.utils.events INFO:  eta: 1:26:25  iter: 8979  total_loss: 5.407  loss_ce: 0.1344  loss_mask: 0.2015  loss_dice: 0.1737  loss_ce_0: 0.06799  loss_mask_0: 0.2133  loss_dice_0: 0.1779  loss_ce_1: 0.1339  loss_mask_1: 0.2095  loss_dice_1: 0.1751  loss_ce_2: 0.1346  loss_mask_2: 0.2164  loss_dice_2: 0.1723  loss_ce_3: 0.1345  loss_mask_3: 0.2105  loss_dice_3: 0.1748  loss_ce_4: 0.1339  loss_mask_4: 0.2023  loss_dice_4: 0.1733  loss_ce_5: 0.1346  loss_mask_5: 0.2118  loss_dice_5: 0.1674  loss_ce_6: 0.1344  loss_mask_6: 0.2149  loss_dice_6: 0.1733  loss_ce_7: 0.1342  loss_mask_7: 0.2179  loss_dice_7: 0.1728  loss_ce_8: 0.1342  loss_mask_8: 0.2154  loss_dice_8: 0.1713  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:49] d2.utils.events INFO:  eta: 1:26:19  iter: 8999  total_loss: 4.779  loss_ce: 0.1269  loss_mask: 0.1477  loss_dice: 0.1346  loss_ce_0: 0.06486  loss_mask_0: 0.1374  loss_dice_0: 0.1332  loss_ce_1: 0.1268  loss_mask_1: 0.1463  loss_dice_1: 0.1393  loss_ce_2: 0.1268  loss_mask_2: 0.1526  loss_dice_2: 0.1366  loss_ce_3: 0.1265  loss_mask_3: 0.1427  loss_dice_3: 0.1391  loss_ce_4: 0.126  loss_mask_4: 0.1411  loss_dice_4: 0.1404  loss_ce_5: 0.1265  loss_mask_5: 0.1506  loss_dice_5: 0.1426  loss_ce_6: 0.1261  loss_mask_6: 0.1539  loss_dice_6: 0.1381  loss_ce_7: 0.1268  loss_mask_7: 0.1384  loss_dice_7: 0.1386  loss_ce_8: 0.1267  loss_mask_8: 0.1499  loss_dice_8: 0.1317  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:51] d2.utils.events INFO:  eta: 1:26:18  iter: 9019  total_loss: 4.754  loss_ce: 0.1238  loss_mask: 0.1751  loss_dice: 0.1601  loss_ce_0: 0.06472  loss_mask_0: 0.1843  loss_dice_0: 0.165  loss_ce_1: 0.1234  loss_mask_1: 0.1771  loss_dice_1: 0.1578  loss_ce_2: 0.1234  loss_mask_2: 0.1697  loss_dice_2: 0.1612  loss_ce_3: 0.1235  loss_mask_3: 0.1743  loss_dice_3: 0.1598  loss_ce_4: 0.1235  loss_mask_4: 0.1704  loss_dice_4: 0.1604  loss_ce_5: 0.1235  loss_mask_5: 0.1738  loss_dice_5: 0.1578  loss_ce_6: 0.1234  loss_mask_6: 0.1743  loss_dice_6: 0.1611  loss_ce_7: 0.1237  loss_mask_7: 0.1712  loss_dice_7: 0.1594  loss_ce_8: 0.1235  loss_mask_8: 0.1713  loss_dice_8: 0.1571  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:54] d2.utils.events INFO:  eta: 1:26:12  iter: 9039  total_loss: 4.418  loss_ce: 0.1395  loss_mask: 0.187  loss_dice: 0.1373  loss_ce_0: 0.06716  loss_mask_0: 0.1924  loss_dice_0: 0.1408  loss_ce_1: 0.1401  loss_mask_1: 0.1897  loss_dice_1: 0.1332  loss_ce_2: 0.1399  loss_mask_2: 0.1979  loss_dice_2: 0.1265  loss_ce_3: 0.1401  loss_mask_3: 0.1876  loss_dice_3: 0.1319  loss_ce_4: 0.1401  loss_mask_4: 0.1928  loss_dice_4: 0.139  loss_ce_5: 0.1401  loss_mask_5: 0.1899  loss_dice_5: 0.1331  loss_ce_6: 0.1404  loss_mask_6: 0.1848  loss_dice_6: 0.128  loss_ce_7: 0.14  loss_mask_7: 0.1888  loss_dice_7: 0.1317  loss_ce_8: 0.1399  loss_mask_8: 0.1904  loss_dice_8: 0.1332  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:55:56] d2.utils.events INFO:  eta: 1:26:08  iter: 9059  total_loss: 5.116  loss_ce: 0.127  loss_mask: 0.1897  loss_dice: 0.1448  loss_ce_0: 0.06465  loss_mask_0: 0.1874  loss_dice_0: 0.1454  loss_ce_1: 0.127  loss_mask_1: 0.1861  loss_dice_1: 0.1436  loss_ce_2: 0.1269  loss_mask_2: 0.1825  loss_dice_2: 0.1464  loss_ce_3: 0.127  loss_mask_3: 0.1894  loss_dice_3: 0.1487  loss_ce_4: 0.1269  loss_mask_4: 0.1914  loss_dice_4: 0.1502  loss_ce_5: 0.1269  loss_mask_5: 0.1824  loss_dice_5: 0.1501  loss_ce_6: 0.127  loss_mask_6: 0.1814  loss_dice_6: 0.1452  loss_ce_7: 0.1269  loss_mask_7: 0.1849  loss_dice_7: 0.1494  loss_ce_8: 0.1269  loss_mask_8: 0.1875  loss_dice_8: 0.1555  time: 0.1259  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:55:59] d2.utils.events INFO:  eta: 1:26:10  iter: 9079  total_loss: 5.383  loss_ce: 0.1283  loss_mask: 0.1973  loss_dice: 0.1702  loss_ce_0: 0.06473  loss_mask_0: 0.1975  loss_dice_0: 0.1755  loss_ce_1: 0.128  loss_mask_1: 0.2061  loss_dice_1: 0.172  loss_ce_2: 0.1287  loss_mask_2: 0.1977  loss_dice_2: 0.1717  loss_ce_3: 0.1285  loss_mask_3: 0.2002  loss_dice_3: 0.1687  loss_ce_4: 0.128  loss_mask_4: 0.1996  loss_dice_4: 0.1716  loss_ce_5: 0.1283  loss_mask_5: 0.201  loss_dice_5: 0.1746  loss_ce_6: 0.1283  loss_mask_6: 0.1938  loss_dice_6: 0.1692  loss_ce_7: 0.1278  loss_mask_7: 0.204  loss_dice_7: 0.1744  loss_ce_8: 0.1288  loss_mask_8: 0.2002  loss_dice_8: 0.1776  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:01] d2.utils.events INFO:  eta: 1:26:10  iter: 9099  total_loss: 5.178  loss_ce: 0.1276  loss_mask: 0.1703  loss_dice: 0.1547  loss_ce_0: 0.06427  loss_mask_0: 0.1695  loss_dice_0: 0.174  loss_ce_1: 0.1278  loss_mask_1: 0.175  loss_dice_1: 0.1695  loss_ce_2: 0.1276  loss_mask_2: 0.1732  loss_dice_2: 0.1675  loss_ce_3: 0.1276  loss_mask_3: 0.1757  loss_dice_3: 0.1632  loss_ce_4: 0.1277  loss_mask_4: 0.1786  loss_dice_4: 0.168  loss_ce_5: 0.1277  loss_mask_5: 0.1663  loss_dice_5: 0.1591  loss_ce_6: 0.1277  loss_mask_6: 0.173  loss_dice_6: 0.1633  loss_ce_7: 0.1277  loss_mask_7: 0.1721  loss_dice_7: 0.1672  loss_ce_8: 0.1277  loss_mask_8: 0.172  loss_dice_8: 0.1689  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:56:04] d2.utils.events INFO:  eta: 1:26:10  iter: 9119  total_loss: 4.961  loss_ce: 0.1294  loss_mask: 0.1858  loss_dice: 0.1307  loss_ce_0: 0.0653  loss_mask_0: 0.1872  loss_dice_0: 0.1325  loss_ce_1: 0.1295  loss_mask_1: 0.1969  loss_dice_1: 0.1324  loss_ce_2: 0.1295  loss_mask_2: 0.1814  loss_dice_2: 0.1336  loss_ce_3: 0.1297  loss_mask_3: 0.182  loss_dice_3: 0.1334  loss_ce_4: 0.1303  loss_mask_4: 0.1747  loss_dice_4: 0.1313  loss_ce_5: 0.1297  loss_mask_5: 0.1887  loss_dice_5: 0.1338  loss_ce_6: 0.1301  loss_mask_6: 0.1846  loss_dice_6: 0.1325  loss_ce_7: 0.1298  loss_mask_7: 0.1794  loss_dice_7: 0.1349  loss_ce_8: 0.1296  loss_mask_8: 0.1814  loss_dice_8: 0.1373  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:06] d2.utils.events INFO:  eta: 1:26:08  iter: 9139  total_loss: 4.738  loss_ce: 0.127  loss_mask: 0.1856  loss_dice: 0.1402  loss_ce_0: 0.06469  loss_mask_0: 0.1751  loss_dice_0: 0.1404  loss_ce_1: 0.1268  loss_mask_1: 0.1809  loss_dice_1: 0.135  loss_ce_2: 0.1268  loss_mask_2: 0.1711  loss_dice_2: 0.1479  loss_ce_3: 0.1268  loss_mask_3: 0.1887  loss_dice_3: 0.1413  loss_ce_4: 0.1264  loss_mask_4: 0.1762  loss_dice_4: 0.1386  loss_ce_5: 0.1269  loss_mask_5: 0.1779  loss_dice_5: 0.1343  loss_ce_6: 0.1269  loss_mask_6: 0.1891  loss_dice_6: 0.1362  loss_ce_7: 0.1269  loss_mask_7: 0.1826  loss_dice_7: 0.1371  loss_ce_8: 0.1269  loss_mask_8: 0.1788  loss_dice_8: 0.1365  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:09] d2.utils.events INFO:  eta: 1:26:08  iter: 9159  total_loss: 5.586  loss_ce: 0.1287  loss_mask: 0.2444  loss_dice: 0.158  loss_ce_0: 0.06448  loss_mask_0: 0.2584  loss_dice_0: 0.1606  loss_ce_1: 0.1277  loss_mask_1: 0.2455  loss_dice_1: 0.1616  loss_ce_2: 0.1288  loss_mask_2: 0.2397  loss_dice_2: 0.1545  loss_ce_3: 0.1287  loss_mask_3: 0.2348  loss_dice_3: 0.15  loss_ce_4: 0.1277  loss_mask_4: 0.2462  loss_dice_4: 0.1562  loss_ce_5: 0.1288  loss_mask_5: 0.2471  loss_dice_5: 0.1537  loss_ce_6: 0.1289  loss_mask_6: 0.2412  loss_dice_6: 0.1543  loss_ce_7: 0.1277  loss_mask_7: 0.2383  loss_dice_7: 0.1547  loss_ce_8: 0.1287  loss_mask_8: 0.2376  loss_dice_8: 0.1557  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:11] d2.utils.events INFO:  eta: 1:26:07  iter: 9179  total_loss: 4.893  loss_ce: 0.116  loss_mask: 0.2083  loss_dice: 0.1525  loss_ce_0: 0.06176  loss_mask_0: 0.2184  loss_dice_0: 0.1521  loss_ce_1: 0.1147  loss_mask_1: 0.2125  loss_dice_1: 0.1516  loss_ce_2: 0.1153  loss_mask_2: 0.2083  loss_dice_2: 0.1529  loss_ce_3: 0.1147  loss_mask_3: 0.213  loss_dice_3: 0.1435  loss_ce_4: 0.1146  loss_mask_4: 0.2044  loss_dice_4: 0.1475  loss_ce_5: 0.1146  loss_mask_5: 0.2191  loss_dice_5: 0.1461  loss_ce_6: 0.1154  loss_mask_6: 0.2172  loss_dice_6: 0.1527  loss_ce_7: 0.1158  loss_mask_7: 0.2104  loss_dice_7: 0.1447  loss_ce_8: 0.1149  loss_mask_8: 0.2121  loss_dice_8: 0.1532  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:14] d2.utils.events INFO:  eta: 1:26:05  iter: 9199  total_loss: 5.043  loss_ce: 0.1039  loss_mask: 0.1969  loss_dice: 0.1405  loss_ce_0: 0.0583  loss_mask_0: 0.2058  loss_dice_0: 0.1298  loss_ce_1: 0.1051  loss_mask_1: 0.2116  loss_dice_1: 0.1328  loss_ce_2: 0.1049  loss_mask_2: 0.209  loss_dice_2: 0.1302  loss_ce_3: 0.1044  loss_mask_3: 0.2091  loss_dice_3: 0.1332  loss_ce_4: 0.1048  loss_mask_4: 0.2104  loss_dice_4: 0.1363  loss_ce_5: 0.1042  loss_mask_5: 0.2102  loss_dice_5: 0.1327  loss_ce_6: 0.1047  loss_mask_6: 0.214  loss_dice_6: 0.1416  loss_ce_7: 0.105  loss_mask_7: 0.2129  loss_dice_7: 0.1394  loss_ce_8: 0.1038  loss_mask_8: 0.2048  loss_dice_8: 0.134  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:17] d2.utils.events INFO:  eta: 1:26:02  iter: 9219  total_loss: 5.206  loss_ce: 0.1061  loss_mask: 0.1869  loss_dice: 0.1565  loss_ce_0: 0.0574  loss_mask_0: 0.1749  loss_dice_0: 0.1666  loss_ce_1: 0.1076  loss_mask_1: 0.1796  loss_dice_1: 0.1585  loss_ce_2: 0.1077  loss_mask_2: 0.1794  loss_dice_2: 0.1541  loss_ce_3: 0.1081  loss_mask_3: 0.1821  loss_dice_3: 0.1609  loss_ce_4: 0.1092  loss_mask_4: 0.1758  loss_dice_4: 0.1632  loss_ce_5: 0.1085  loss_mask_5: 0.1821  loss_dice_5: 0.1582  loss_ce_6: 0.1086  loss_mask_6: 0.1782  loss_dice_6: 0.166  loss_ce_7: 0.1087  loss_mask_7: 0.1722  loss_dice_7: 0.1584  loss_ce_8: 0.1071  loss_mask_8: 0.1826  loss_dice_8: 0.1586  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:56:19] d2.utils.events INFO:  eta: 1:25:57  iter: 9239  total_loss: 5.295  loss_ce: 0.1056  loss_mask: 0.2173  loss_dice: 0.1531  loss_ce_0: 0.05701  loss_mask_0: 0.2273  loss_dice_0: 0.1544  loss_ce_1: 0.1059  loss_mask_1: 0.2179  loss_dice_1: 0.1508  loss_ce_2: 0.1065  loss_mask_2: 0.2266  loss_dice_2: 0.1538  loss_ce_3: 0.1068  loss_mask_3: 0.2154  loss_dice_3: 0.1536  loss_ce_4: 0.1074  loss_mask_4: 0.2267  loss_dice_4: 0.1574  loss_ce_5: 0.1071  loss_mask_5: 0.2291  loss_dice_5: 0.1572  loss_ce_6: 0.1072  loss_mask_6: 0.2267  loss_dice_6: 0.1556  loss_ce_7: 0.1072  loss_mask_7: 0.2188  loss_dice_7: 0.1553  loss_ce_8: 0.1062  loss_mask_8: 0.2163  loss_dice_8: 0.1473  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:56:22] d2.utils.events INFO:  eta: 1:25:56  iter: 9259  total_loss: 4.981  loss_ce: 0.08511  loss_mask: 0.2491  loss_dice: 0.1645  loss_ce_0: 0.05201  loss_mask_0: 0.2515  loss_dice_0: 0.1559  loss_ce_1: 0.08271  loss_mask_1: 0.2484  loss_dice_1: 0.1627  loss_ce_2: 0.08188  loss_mask_2: 0.2382  loss_dice_2: 0.1615  loss_ce_3: 0.0805  loss_mask_3: 0.2438  loss_dice_3: 0.165  loss_ce_4: 0.07819  loss_mask_4: 0.24  loss_dice_4: 0.1577  loss_ce_5: 0.07928  loss_mask_5: 0.2395  loss_dice_5: 0.1588  loss_ce_6: 0.07869  loss_mask_6: 0.2405  loss_dice_6: 0.1594  loss_ce_7: 0.07991  loss_mask_7: 0.2448  loss_dice_7: 0.1661  loss_ce_8: 0.08136  loss_mask_8: 0.2397  loss_dice_8: 0.153  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:56:24] d2.utils.events INFO:  eta: 1:25:52  iter: 9279  total_loss: 5.373  loss_ce: 0.06816  loss_mask: 0.1956  loss_dice: 0.1836  loss_ce_0: 0.04799  loss_mask_0: 0.1994  loss_dice_0: 0.1667  loss_ce_1: 0.06587  loss_mask_1: 0.2099  loss_dice_1: 0.1787  loss_ce_2: 0.06584  loss_mask_2: 0.1992  loss_dice_2: 0.1714  loss_ce_3: 0.06509  loss_mask_3: 0.2002  loss_dice_3: 0.1796  loss_ce_4: 0.06447  loss_mask_4: 0.1955  loss_dice_4: 0.1789  loss_ce_5: 0.06501  loss_mask_5: 0.2067  loss_dice_5: 0.1781  loss_ce_6: 0.06416  loss_mask_6: 0.2012  loss_dice_6: 0.1759  loss_ce_7: 0.06471  loss_mask_7: 0.2031  loss_dice_7: 0.1753  loss_ce_8: 0.06508  loss_mask_8: 0.1973  loss_dice_8: 0.1696  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:27] d2.utils.events INFO:  eta: 1:25:54  iter: 9299  total_loss: 5.393  loss_ce: 0.07044  loss_mask: 0.2187  loss_dice: 0.1784  loss_ce_0: 0.04646  loss_mask_0: 0.2135  loss_dice_0: 0.1729  loss_ce_1: 0.0699  loss_mask_1: 0.2242  loss_dice_1: 0.1709  loss_ce_2: 0.06903  loss_mask_2: 0.2195  loss_dice_2: 0.169  loss_ce_3: 0.06875  loss_mask_3: 0.2227  loss_dice_3: 0.1649  loss_ce_4: 0.06932  loss_mask_4: 0.2222  loss_dice_4: 0.1738  loss_ce_5: 0.06907  loss_mask_5: 0.2212  loss_dice_5: 0.1673  loss_ce_6: 0.06833  loss_mask_6: 0.2143  loss_dice_6: 0.1658  loss_ce_7: 0.06936  loss_mask_7: 0.2079  loss_dice_7: 0.1691  loss_ce_8: 0.06847  loss_mask_8: 0.2112  loss_dice_8: 0.172  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:29] d2.utils.events INFO:  eta: 1:25:50  iter: 9319  total_loss: 5.865  loss_ce: 0.1957  loss_mask: 0.1972  loss_dice: 0.1672  loss_ce_0: 0.08928  loss_mask_0: 0.2069  loss_dice_0: 0.1673  loss_ce_1: 0.1948  loss_mask_1: 0.2062  loss_dice_1: 0.1614  loss_ce_2: 0.1968  loss_mask_2: 0.2054  loss_dice_2: 0.1749  loss_ce_3: 0.1956  loss_mask_3: 0.2129  loss_dice_3: 0.1713  loss_ce_4: 0.1927  loss_mask_4: 0.218  loss_dice_4: 0.1718  loss_ce_5: 0.1948  loss_mask_5: 0.1987  loss_dice_5: 0.1717  loss_ce_6: 0.1947  loss_mask_6: 0.2059  loss_dice_6: 0.1702  loss_ce_7: 0.1929  loss_mask_7: 0.2097  loss_dice_7: 0.1744  loss_ce_8: 0.1966  loss_mask_8: 0.2204  loss_dice_8: 0.1723  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:32] d2.utils.events INFO:  eta: 1:25:48  iter: 9339  total_loss: 5.44  loss_ce: 0.08782  loss_mask: 0.1896  loss_dice: 0.2079  loss_ce_0: 0.04743  loss_mask_0: 0.191  loss_dice_0: 0.2025  loss_ce_1: 0.08874  loss_mask_1: 0.1881  loss_dice_1: 0.211  loss_ce_2: 0.08772  loss_mask_2: 0.184  loss_dice_2: 0.2014  loss_ce_3: 0.08892  loss_mask_3: 0.192  loss_dice_3: 0.2114  loss_ce_4: 0.09106  loss_mask_4: 0.1876  loss_dice_4: 0.208  loss_ce_5: 0.08959  loss_mask_5: 0.1866  loss_dice_5: 0.2095  loss_ce_6: 0.09011  loss_mask_6: 0.1863  loss_dice_6: 0.2028  loss_ce_7: 0.09071  loss_mask_7: 0.192  loss_dice_7: 0.2027  loss_ce_8: 0.08814  loss_mask_8: 0.1895  loss_dice_8: 0.1953  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:34] d2.utils.events INFO:  eta: 1:25:48  iter: 9359  total_loss: 6.517  loss_ce: 0.09733  loss_mask: 0.2085  loss_dice: 0.2377  loss_ce_0: 0.04977  loss_mask_0: 0.209  loss_dice_0: 0.2366  loss_ce_1: 0.09851  loss_mask_1: 0.2096  loss_dice_1: 0.234  loss_ce_2: 0.09785  loss_mask_2: 0.222  loss_dice_2: 0.2458  loss_ce_3: 0.09889  loss_mask_3: 0.2109  loss_dice_3: 0.2356  loss_ce_4: 0.1005  loss_mask_4: 0.2084  loss_dice_4: 0.2315  loss_ce_5: 0.09914  loss_mask_5: 0.2072  loss_dice_5: 0.2173  loss_ce_6: 0.09997  loss_mask_6: 0.2136  loss_dice_6: 0.2428  loss_ce_7: 0.1  loss_mask_7: 0.2059  loss_dice_7: 0.2313  loss_ce_8: 0.09837  loss_mask_8: 0.2025  loss_dice_8: 0.2349  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:56:37] d2.utils.events INFO:  eta: 1:25:48  iter: 9379  total_loss: 5.363  loss_ce: 0.1464  loss_mask: 0.1988  loss_dice: 0.163  loss_ce_0: 0.07865  loss_mask_0: 0.1856  loss_dice_0: 0.1619  loss_ce_1: 0.1449  loss_mask_1: 0.1966  loss_dice_1: 0.1559  loss_ce_2: 0.1449  loss_mask_2: 0.2017  loss_dice_2: 0.1579  loss_ce_3: 0.1428  loss_mask_3: 0.1931  loss_dice_3: 0.1563  loss_ce_4: 0.1404  loss_mask_4: 0.1982  loss_dice_4: 0.1597  loss_ce_5: 0.142  loss_mask_5: 0.1898  loss_dice_5: 0.1599  loss_ce_6: 0.1409  loss_mask_6: 0.2027  loss_dice_6: 0.1632  loss_ce_7: 0.1421  loss_mask_7: 0.1982  loss_dice_7: 0.1562  loss_ce_8: 0.1443  loss_mask_8: 0.1958  loss_dice_8: 0.163  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 14:56:39] d2.utils.events INFO:  eta: 1:25:42  iter: 9399  total_loss: 5.226  loss_ce: 0.1268  loss_mask: 0.2236  loss_dice: 0.1397  loss_ce_0: 0.06461  loss_mask_0: 0.2228  loss_dice_0: 0.1421  loss_ce_1: 0.1263  loss_mask_1: 0.2283  loss_dice_1: 0.1356  loss_ce_2: 0.1263  loss_mask_2: 0.2277  loss_dice_2: 0.1378  loss_ce_3: 0.1276  loss_mask_3: 0.2247  loss_dice_3: 0.137  loss_ce_4: 0.1282  loss_mask_4: 0.2278  loss_dice_4: 0.1366  loss_ce_5: 0.1282  loss_mask_5: 0.2333  loss_dice_5: 0.139  loss_ce_6: 0.1282  loss_mask_6: 0.2193  loss_dice_6: 0.1342  loss_ce_7: 0.1283  loss_mask_7: 0.2213  loss_dice_7: 0.1391  loss_ce_8: 0.1265  loss_mask_8: 0.2263  loss_dice_8: 0.1455  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:42] d2.utils.events INFO:  eta: 1:25:40  iter: 9419  total_loss: 5.259  loss_ce: 0.1424  loss_mask: 0.2157  loss_dice: 0.1295  loss_ce_0: 0.0624  loss_mask_0: 0.2099  loss_dice_0: 0.142  loss_ce_1: 0.1434  loss_mask_1: 0.2221  loss_dice_1: 0.135  loss_ce_2: 0.1438  loss_mask_2: 0.2106  loss_dice_2: 0.1382  loss_ce_3: 0.1469  loss_mask_3: 0.2153  loss_dice_3: 0.1387  loss_ce_4: 0.1478  loss_mask_4: 0.2185  loss_dice_4: 0.1402  loss_ce_5: 0.147  loss_mask_5: 0.213  loss_dice_5: 0.1368  loss_ce_6: 0.1482  loss_mask_6: 0.207  loss_dice_6: 0.1409  loss_ce_7: 0.1447  loss_mask_7: 0.208  loss_dice_7: 0.1385  loss_ce_8: 0.1443  loss_mask_8: 0.2102  loss_dice_8: 0.1385  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:56:45] d2.utils.events INFO:  eta: 1:25:37  iter: 9439  total_loss: 4.846  loss_ce: 0.1187  loss_mask: 0.184  loss_dice: 0.1573  loss_ce_0: 0.06536  loss_mask_0: 0.1825  loss_dice_0: 0.1538  loss_ce_1: 0.1182  loss_mask_1: 0.1773  loss_dice_1: 0.1506  loss_ce_2: 0.1184  loss_mask_2: 0.1718  loss_dice_2: 0.1553  loss_ce_3: 0.1185  loss_mask_3: 0.1738  loss_dice_3: 0.148  loss_ce_4: 0.119  loss_mask_4: 0.1838  loss_dice_4: 0.1497  loss_ce_5: 0.1195  loss_mask_5: 0.1899  loss_dice_5: 0.1616  loss_ce_6: 0.1184  loss_mask_6: 0.1782  loss_dice_6: 0.1551  loss_ce_7: 0.1181  loss_mask_7: 0.1736  loss_dice_7: 0.1567  loss_ce_8: 0.1176  loss_mask_8: 0.1841  loss_dice_8: 0.1546  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:56:47] d2.utils.events INFO:  eta: 1:25:37  iter: 9459  total_loss: 4.901  loss_ce: 0.1389  loss_mask: 0.1838  loss_dice: 0.1582  loss_ce_0: 0.0646  loss_mask_0: 0.1906  loss_dice_0: 0.1642  loss_ce_1: 0.1392  loss_mask_1: 0.1822  loss_dice_1: 0.1506  loss_ce_2: 0.1393  loss_mask_2: 0.1923  loss_dice_2: 0.1578  loss_ce_3: 0.1394  loss_mask_3: 0.1971  loss_dice_3: 0.1607  loss_ce_4: 0.1391  loss_mask_4: 0.1973  loss_dice_4: 0.1615  loss_ce_5: 0.1388  loss_mask_5: 0.1836  loss_dice_5: 0.1606  loss_ce_6: 0.1393  loss_mask_6: 0.1765  loss_dice_6: 0.1511  loss_ce_7: 0.1395  loss_mask_7: 0.1993  loss_dice_7: 0.1604  loss_ce_8: 0.1398  loss_mask_8: 0.1875  loss_dice_8: 0.1578  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:56:50] d2.utils.events INFO:  eta: 1:25:35  iter: 9479  total_loss: 4.857  loss_ce: 0.1323  loss_mask: 0.1875  loss_dice: 0.146  loss_ce_0: 0.06474  loss_mask_0: 0.1811  loss_dice_0: 0.1483  loss_ce_1: 0.1295  loss_mask_1: 0.1835  loss_dice_1: 0.151  loss_ce_2: 0.1322  loss_mask_2: 0.1912  loss_dice_2: 0.1524  loss_ce_3: 0.1319  loss_mask_3: 0.1906  loss_dice_3: 0.1479  loss_ce_4: 0.1295  loss_mask_4: 0.175  loss_dice_4: 0.1466  loss_ce_5: 0.1311  loss_mask_5: 0.1886  loss_dice_5: 0.1429  loss_ce_6: 0.1314  loss_mask_6: 0.185  loss_dice_6: 0.1439  loss_ce_7: 0.1306  loss_mask_7: 0.1838  loss_dice_7: 0.147  loss_ce_8: 0.1326  loss_mask_8: 0.1885  loss_dice_8: 0.1447  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:56:52] d2.utils.events INFO:  eta: 1:25:33  iter: 9499  total_loss: 5.493  loss_ce: 0.1305  loss_mask: 0.2037  loss_dice: 0.1729  loss_ce_0: 0.06509  loss_mask_0: 0.2139  loss_dice_0: 0.171  loss_ce_1: 0.1293  loss_mask_1: 0.2025  loss_dice_1: 0.1825  loss_ce_2: 0.1306  loss_mask_2: 0.2081  loss_dice_2: 0.1701  loss_ce_3: 0.1305  loss_mask_3: 0.2056  loss_dice_3: 0.1746  loss_ce_4: 0.1301  loss_mask_4: 0.2061  loss_dice_4: 0.1661  loss_ce_5: 0.1302  loss_mask_5: 0.216  loss_dice_5: 0.175  loss_ce_6: 0.1301  loss_mask_6: 0.2094  loss_dice_6: 0.1722  loss_ce_7: 0.1305  loss_mask_7: 0.2062  loss_dice_7: 0.1726  loss_ce_8: 0.1305  loss_mask_8: 0.2086  loss_dice_8: 0.1735  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:55] d2.utils.events INFO:  eta: 1:25:31  iter: 9519  total_loss: 4.63  loss_ce: 0.1252  loss_mask: 0.2209  loss_dice: 0.1488  loss_ce_0: 0.06414  loss_mask_0: 0.2211  loss_dice_0: 0.1535  loss_ce_1: 0.1256  loss_mask_1: 0.2253  loss_dice_1: 0.1577  loss_ce_2: 0.125  loss_mask_2: 0.2096  loss_dice_2: 0.1507  loss_ce_3: 0.1243  loss_mask_3: 0.23  loss_dice_3: 0.1575  loss_ce_4: 0.1237  loss_mask_4: 0.2313  loss_dice_4: 0.1577  loss_ce_5: 0.1239  loss_mask_5: 0.2203  loss_dice_5: 0.1546  loss_ce_6: 0.1237  loss_mask_6: 0.2162  loss_dice_6: 0.1491  loss_ce_7: 0.1247  loss_mask_7: 0.2167  loss_dice_7: 0.1538  loss_ce_8: 0.125  loss_mask_8: 0.2157  loss_dice_8: 0.1517  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:56:57] d2.utils.events INFO:  eta: 1:25:28  iter: 9539  total_loss: 4.959  loss_ce: 0.1157  loss_mask: 0.17  loss_dice: 0.1576  loss_ce_0: 0.06225  loss_mask_0: 0.1731  loss_dice_0: 0.1612  loss_ce_1: 0.1158  loss_mask_1: 0.1784  loss_dice_1: 0.1639  loss_ce_2: 0.1151  loss_mask_2: 0.1759  loss_dice_2: 0.1648  loss_ce_3: 0.1141  loss_mask_3: 0.1693  loss_dice_3: 0.1721  loss_ce_4: 0.1132  loss_mask_4: 0.1765  loss_dice_4: 0.1565  loss_ce_5: 0.1136  loss_mask_5: 0.1832  loss_dice_5: 0.165  loss_ce_6: 0.1133  loss_mask_6: 0.1736  loss_dice_6: 0.1633  loss_ce_7: 0.1146  loss_mask_7: 0.1762  loss_dice_7: 0.1653  loss_ce_8: 0.1151  loss_mask_8: 0.1833  loss_dice_8: 0.1615  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:00] d2.utils.events INFO:  eta: 1:25:25  iter: 9559  total_loss: 4.995  loss_ce: 0.1287  loss_mask: 0.2159  loss_dice: 0.1299  loss_ce_0: 0.06457  loss_mask_0: 0.2232  loss_dice_0: 0.1299  loss_ce_1: 0.1283  loss_mask_1: 0.2278  loss_dice_1: 0.1295  loss_ce_2: 0.1288  loss_mask_2: 0.2219  loss_dice_2: 0.1251  loss_ce_3: 0.1287  loss_mask_3: 0.2269  loss_dice_3: 0.1302  loss_ce_4: 0.1284  loss_mask_4: 0.2231  loss_dice_4: 0.1266  loss_ce_5: 0.1287  loss_mask_5: 0.2212  loss_dice_5: 0.1285  loss_ce_6: 0.1288  loss_mask_6: 0.2296  loss_dice_6: 0.1275  loss_ce_7: 0.1285  loss_mask_7: 0.2174  loss_dice_7: 0.1311  loss_ce_8: 0.1288  loss_mask_8: 0.226  loss_dice_8: 0.1287  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:57:02] d2.utils.events INFO:  eta: 1:25:20  iter: 9579  total_loss: 4.796  loss_ce: 0.1055  loss_mask: 0.2126  loss_dice: 0.1486  loss_ce_0: 0.05884  loss_mask_0: 0.2234  loss_dice_0: 0.1507  loss_ce_1: 0.1057  loss_mask_1: 0.2299  loss_dice_1: 0.155  loss_ce_2: 0.1053  loss_mask_2: 0.2258  loss_dice_2: 0.1516  loss_ce_3: 0.105  loss_mask_3: 0.2282  loss_dice_3: 0.149  loss_ce_4: 0.1053  loss_mask_4: 0.2285  loss_dice_4: 0.1523  loss_ce_5: 0.1052  loss_mask_5: 0.2165  loss_dice_5: 0.1534  loss_ce_6: 0.1047  loss_mask_6: 0.2205  loss_dice_6: 0.1533  loss_ce_7: 0.1054  loss_mask_7: 0.2164  loss_dice_7: 0.15  loss_ce_8: 0.105  loss_mask_8: 0.2244  loss_dice_8: 0.1545  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:05] d2.utils.events INFO:  eta: 1:25:20  iter: 9599  total_loss: 4.892  loss_ce: 0.129  loss_mask: 0.1442  loss_dice: 0.1401  loss_ce_0: 0.06475  loss_mask_0: 0.1519  loss_dice_0: 0.1506  loss_ce_1: 0.1286  loss_mask_1: 0.1544  loss_dice_1: 0.1499  loss_ce_2: 0.129  loss_mask_2: 0.143  loss_dice_2: 0.1431  loss_ce_3: 0.129  loss_mask_3: 0.1576  loss_dice_3: 0.1441  loss_ce_4: 0.1287  loss_mask_4: 0.1589  loss_dice_4: 0.1419  loss_ce_5: 0.1289  loss_mask_5: 0.1418  loss_dice_5: 0.1458  loss_ce_6: 0.129  loss_mask_6: 0.1444  loss_dice_6: 0.1504  loss_ce_7: 0.1287  loss_mask_7: 0.1497  loss_dice_7: 0.1429  loss_ce_8: 0.129  loss_mask_8: 0.1547  loss_dice_8: 0.1407  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:07] d2.utils.events INFO:  eta: 1:25:18  iter: 9619  total_loss: 5.062  loss_ce: 0.1126  loss_mask: 0.2204  loss_dice: 0.1372  loss_ce_0: 0.05859  loss_mask_0: 0.2215  loss_dice_0: 0.137  loss_ce_1: 0.1124  loss_mask_1: 0.2224  loss_dice_1: 0.1387  loss_ce_2: 0.1129  loss_mask_2: 0.2138  loss_dice_2: 0.138  loss_ce_3: 0.1136  loss_mask_3: 0.2166  loss_dice_3: 0.1371  loss_ce_4: 0.1145  loss_mask_4: 0.2298  loss_dice_4: 0.1384  loss_ce_5: 0.114  loss_mask_5: 0.2237  loss_dice_5: 0.1434  loss_ce_6: 0.1139  loss_mask_6: 0.2327  loss_dice_6: 0.1373  loss_ce_7: 0.1136  loss_mask_7: 0.2363  loss_dice_7: 0.1402  loss_ce_8: 0.1132  loss_mask_8: 0.2266  loss_dice_8: 0.1375  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:10] d2.utils.events INFO:  eta: 1:25:15  iter: 9639  total_loss: 4.964  loss_ce: 0.1091  loss_mask: 0.2225  loss_dice: 0.1354  loss_ce_0: 0.05782  loss_mask_0: 0.2225  loss_dice_0: 0.1405  loss_ce_1: 0.1092  loss_mask_1: 0.2195  loss_dice_1: 0.1361  loss_ce_2: 0.1093  loss_mask_2: 0.2148  loss_dice_2: 0.139  loss_ce_3: 0.1093  loss_mask_3: 0.2166  loss_dice_3: 0.1367  loss_ce_4: 0.1095  loss_mask_4: 0.2168  loss_dice_4: 0.14  loss_ce_5: 0.1093  loss_mask_5: 0.2199  loss_dice_5: 0.1355  loss_ce_6: 0.1092  loss_mask_6: 0.2195  loss_dice_6: 0.1325  loss_ce_7: 0.1093  loss_mask_7: 0.2238  loss_dice_7: 0.1433  loss_ce_8: 0.1095  loss_mask_8: 0.2178  loss_dice_8: 0.1361  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:13] d2.utils.events INFO:  eta: 1:25:14  iter: 9659  total_loss: 4.767  loss_ce: 0.1042  loss_mask: 0.2027  loss_dice: 0.1561  loss_ce_0: 0.05632  loss_mask_0: 0.2022  loss_dice_0: 0.1642  loss_ce_1: 0.1034  loss_mask_1: 0.2002  loss_dice_1: 0.1636  loss_ce_2: 0.1041  loss_mask_2: 0.1977  loss_dice_2: 0.1542  loss_ce_3: 0.1041  loss_mask_3: 0.2055  loss_dice_3: 0.1582  loss_ce_4: 0.1034  loss_mask_4: 0.2001  loss_dice_4: 0.1584  loss_ce_5: 0.104  loss_mask_5: 0.2056  loss_dice_5: 0.1574  loss_ce_6: 0.1039  loss_mask_6: 0.1898  loss_dice_6: 0.1619  loss_ce_7: 0.1033  loss_mask_7: 0.2049  loss_dice_7: 0.1571  loss_ce_8: 0.1042  loss_mask_8: 0.192  loss_dice_8: 0.153  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:15] d2.utils.events INFO:  eta: 1:25:11  iter: 9679  total_loss: 4.799  loss_ce: 0.1571  loss_mask: 0.2151  loss_dice: 0.1274  loss_ce_0: 0.07423  loss_mask_0: 0.2202  loss_dice_0: 0.1239  loss_ce_1: 0.1566  loss_mask_1: 0.222  loss_dice_1: 0.132  loss_ce_2: 0.157  loss_mask_2: 0.212  loss_dice_2: 0.1335  loss_ce_3: 0.1571  loss_mask_3: 0.2126  loss_dice_3: 0.1271  loss_ce_4: 0.1567  loss_mask_4: 0.2164  loss_dice_4: 0.125  loss_ce_5: 0.1571  loss_mask_5: 0.2101  loss_dice_5: 0.1315  loss_ce_6: 0.1575  loss_mask_6: 0.2114  loss_dice_6: 0.1257  loss_ce_7: 0.1571  loss_mask_7: 0.2202  loss_dice_7: 0.1308  loss_ce_8: 0.1571  loss_mask_8: 0.2205  loss_dice_8: 0.1341  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:18] d2.utils.events INFO:  eta: 1:25:09  iter: 9699  total_loss: 5.046  loss_ce: 0.1072  loss_mask: 0.2058  loss_dice: 0.134  loss_ce_0: 0.05647  loss_mask_0: 0.2102  loss_dice_0: 0.1383  loss_ce_1: 0.1072  loss_mask_1: 0.2146  loss_dice_1: 0.137  loss_ce_2: 0.1074  loss_mask_2: 0.2183  loss_dice_2: 0.1419  loss_ce_3: 0.108  loss_mask_3: 0.2172  loss_dice_3: 0.1345  loss_ce_4: 0.1086  loss_mask_4: 0.2153  loss_dice_4: 0.1378  loss_ce_5: 0.108  loss_mask_5: 0.219  loss_dice_5: 0.136  loss_ce_6: 0.1079  loss_mask_6: 0.2052  loss_dice_6: 0.1361  loss_ce_7: 0.1079  loss_mask_7: 0.2194  loss_dice_7: 0.1341  loss_ce_8: 0.1076  loss_mask_8: 0.2033  loss_dice_8: 0.138  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:20] d2.utils.events INFO:  eta: 1:25:05  iter: 9719  total_loss: 5.342  loss_ce: 0.09628  loss_mask: 0.212  loss_dice: 0.1429  loss_ce_0: 0.05416  loss_mask_0: 0.2109  loss_dice_0: 0.1504  loss_ce_1: 0.09666  loss_mask_1: 0.2167  loss_dice_1: 0.1444  loss_ce_2: 0.09615  loss_mask_2: 0.2111  loss_dice_2: 0.1453  loss_ce_3: 0.09546  loss_mask_3: 0.2123  loss_dice_3: 0.1566  loss_ce_4: 0.09513  loss_mask_4: 0.2054  loss_dice_4: 0.1474  loss_ce_5: 0.09511  loss_mask_5: 0.212  loss_dice_5: 0.1452  loss_ce_6: 0.09492  loss_mask_6: 0.2114  loss_dice_6: 0.1452  loss_ce_7: 0.09557  loss_mask_7: 0.2134  loss_dice_7: 0.1545  loss_ce_8: 0.09593  loss_mask_8: 0.2049  loss_dice_8: 0.1443  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:57:23] d2.utils.events INFO:  eta: 1:25:03  iter: 9739  total_loss: 4.706  loss_ce: 0.1739  loss_mask: 0.1926  loss_dice: 0.1179  loss_ce_0: 0.07906  loss_mask_0: 0.1968  loss_dice_0: 0.1225  loss_ce_1: 0.1745  loss_mask_1: 0.1967  loss_dice_1: 0.1241  loss_ce_2: 0.1744  loss_mask_2: 0.2013  loss_dice_2: 0.1268  loss_ce_3: 0.1748  loss_mask_3: 0.191  loss_dice_3: 0.1216  loss_ce_4: 0.1744  loss_mask_4: 0.1968  loss_dice_4: 0.1283  loss_ce_5: 0.1745  loss_mask_5: 0.1892  loss_dice_5: 0.1242  loss_ce_6: 0.175  loss_mask_6: 0.1847  loss_dice_6: 0.1183  loss_ce_7: 0.1744  loss_mask_7: 0.1999  loss_dice_7: 0.1285  loss_ce_8: 0.1746  loss_mask_8: 0.1962  loss_dice_8: 0.1205  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:57:25] d2.utils.events INFO:  eta: 1:25:01  iter: 9759  total_loss: 5.376  loss_ce: 0.09821  loss_mask: 0.1881  loss_dice: 0.1619  loss_ce_0: 0.05301  loss_mask_0: 0.1823  loss_dice_0: 0.1644  loss_ce_1: 0.09791  loss_mask_1: 0.1834  loss_dice_1: 0.1655  loss_ce_2: 0.09833  loss_mask_2: 0.1954  loss_dice_2: 0.163  loss_ce_3: 0.09888  loss_mask_3: 0.1853  loss_dice_3: 0.1678  loss_ce_4: 0.09959  loss_mask_4: 0.1837  loss_dice_4: 0.1678  loss_ce_5: 0.09931  loss_mask_5: 0.1934  loss_dice_5: 0.1613  loss_ce_6: 0.09914  loss_mask_6: 0.1961  loss_dice_6: 0.1679  loss_ce_7: 0.09903  loss_mask_7: 0.1839  loss_dice_7: 0.1678  loss_ce_8: 0.09843  loss_mask_8: 0.1861  loss_dice_8: 0.1623  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:57:28] d2.utils.events INFO:  eta: 1:24:58  iter: 9779  total_loss: 4.846  loss_ce: 0.1543  loss_mask: 0.1759  loss_dice: 0.1112  loss_ce_0: 0.0763  loss_mask_0: 0.1675  loss_dice_0: 0.1104  loss_ce_1: 0.1537  loss_mask_1: 0.1712  loss_dice_1: 0.1176  loss_ce_2: 0.154  loss_mask_2: 0.1681  loss_dice_2: 0.1147  loss_ce_3: 0.1532  loss_mask_3: 0.1689  loss_dice_3: 0.1123  loss_ce_4: 0.152  loss_mask_4: 0.1668  loss_dice_4: 0.1143  loss_ce_5: 0.1529  loss_mask_5: 0.1773  loss_dice_5: 0.112  loss_ce_6: 0.153  loss_mask_6: 0.169  loss_dice_6: 0.1133  loss_ce_7: 0.153  loss_mask_7: 0.1747  loss_dice_7: 0.1162  loss_ce_8: 0.1539  loss_mask_8: 0.1754  loss_dice_8: 0.1139  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:30] d2.utils.events INFO:  eta: 1:24:56  iter: 9799  total_loss: 4.943  loss_ce: 0.1054  loss_mask: 0.2054  loss_dice: 0.1672  loss_ce_0: 0.05435  loss_mask_0: 0.2035  loss_dice_0: 0.1672  loss_ce_1: 0.1055  loss_mask_1: 0.2124  loss_dice_1: 0.1736  loss_ce_2: 0.1057  loss_mask_2: 0.2172  loss_dice_2: 0.1743  loss_ce_3: 0.1064  loss_mask_3: 0.2042  loss_dice_3: 0.1716  loss_ce_4: 0.1071  loss_mask_4: 0.2172  loss_dice_4: 0.175  loss_ce_5: 0.1066  loss_mask_5: 0.2051  loss_dice_5: 0.1713  loss_ce_6: 0.1066  loss_mask_6: 0.2119  loss_dice_6: 0.173  loss_ce_7: 0.1062  loss_mask_7: 0.21  loss_dice_7: 0.1733  loss_ce_8: 0.1058  loss_mask_8: 0.2062  loss_dice_8: 0.176  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:33] d2.utils.events INFO:  eta: 1:24:55  iter: 9819  total_loss: 4.96  loss_ce: 0.1004  loss_mask: 0.1829  loss_dice: 0.1567  loss_ce_0: 0.05363  loss_mask_0: 0.1869  loss_dice_0: 0.1458  loss_ce_1: 0.1003  loss_mask_1: 0.1921  loss_dice_1: 0.1505  loss_ce_2: 0.1004  loss_mask_2: 0.1807  loss_dice_2: 0.1547  loss_ce_3: 0.1003  loss_mask_3: 0.1854  loss_dice_3: 0.1542  loss_ce_4: 0.1002  loss_mask_4: 0.1764  loss_dice_4: 0.1552  loss_ce_5: 0.1002  loss_mask_5: 0.1881  loss_dice_5: 0.1505  loss_ce_6: 0.1002  loss_mask_6: 0.1871  loss_dice_6: 0.1498  loss_ce_7: 0.1002  loss_mask_7: 0.1863  loss_dice_7: 0.1469  loss_ce_8: 0.1006  loss_mask_8: 0.1762  loss_dice_8: 0.152  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:57:35] d2.utils.events INFO:  eta: 1:24:53  iter: 9839  total_loss: 5.369  loss_ce: 0.1499  loss_mask: 0.2184  loss_dice: 0.1534  loss_ce_0: 0.07543  loss_mask_0: 0.2196  loss_dice_0: 0.1478  loss_ce_1: 0.1501  loss_mask_1: 0.2149  loss_dice_1: 0.1576  loss_ce_2: 0.1495  loss_mask_2: 0.2149  loss_dice_2: 0.1531  loss_ce_3: 0.1484  loss_mask_3: 0.2222  loss_dice_3: 0.1553  loss_ce_4: 0.1474  loss_mask_4: 0.2131  loss_dice_4: 0.1523  loss_ce_5: 0.1476  loss_mask_5: 0.2206  loss_dice_5: 0.1564  loss_ce_6: 0.1478  loss_mask_6: 0.2222  loss_dice_6: 0.1556  loss_ce_7: 0.1482  loss_mask_7: 0.2216  loss_dice_7: 0.1555  loss_ce_8: 0.1492  loss_mask_8: 0.219  loss_dice_8: 0.1545  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:57:38] d2.utils.events INFO:  eta: 1:24:50  iter: 9859  total_loss: 5.683  loss_ce: 0.1315  loss_mask: 0.2161  loss_dice: 0.1613  loss_ce_0: 0.07083  loss_mask_0: 0.2157  loss_dice_0: 0.1566  loss_ce_1: 0.1324  loss_mask_1: 0.2191  loss_dice_1: 0.1586  loss_ce_2: 0.1303  loss_mask_2: 0.2177  loss_dice_2: 0.1658  loss_ce_3: 0.1289  loss_mask_3: 0.2175  loss_dice_3: 0.156  loss_ce_4: 0.1303  loss_mask_4: 0.2205  loss_dice_4: 0.1568  loss_ce_5: 0.1288  loss_mask_5: 0.2309  loss_dice_5: 0.1646  loss_ce_6: 0.1288  loss_mask_6: 0.2132  loss_dice_6: 0.1561  loss_ce_7: 0.1306  loss_mask_7: 0.2163  loss_dice_7: 0.1619  loss_ce_8: 0.1303  loss_mask_8: 0.227  loss_dice_8: 0.1597  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:57:40] d2.utils.events INFO:  eta: 1:24:46  iter: 9879  total_loss: 5.644  loss_ce: 0.1273  loss_mask: 0.2195  loss_dice: 0.1703  loss_ce_0: 0.05976  loss_mask_0: 0.2187  loss_dice_0: 0.1699  loss_ce_1: 0.1277  loss_mask_1: 0.2211  loss_dice_1: 0.159  loss_ce_2: 0.1284  loss_mask_2: 0.2283  loss_dice_2: 0.1608  loss_ce_3: 0.1291  loss_mask_3: 0.2267  loss_dice_3: 0.1721  loss_ce_4: 0.1293  loss_mask_4: 0.2153  loss_dice_4: 0.1661  loss_ce_5: 0.1296  loss_mask_5: 0.2187  loss_dice_5: 0.1617  loss_ce_6: 0.1294  loss_mask_6: 0.2148  loss_dice_6: 0.1715  loss_ce_7: 0.1284  loss_mask_7: 0.2184  loss_dice_7: 0.1704  loss_ce_8: 0.1282  loss_mask_8: 0.2208  loss_dice_8: 0.1683  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:57:43] d2.utils.events INFO:  eta: 1:24:43  iter: 9899  total_loss: 6.752  loss_ce: 0.1224  loss_mask: 0.2125  loss_dice: 0.2144  loss_ce_0: 0.05953  loss_mask_0: 0.21  loss_dice_0: 0.2101  loss_ce_1: 0.1224  loss_mask_1: 0.2245  loss_dice_1: 0.2213  loss_ce_2: 0.1229  loss_mask_2: 0.2128  loss_dice_2: 0.2149  loss_ce_3: 0.1232  loss_mask_3: 0.2252  loss_dice_3: 0.2243  loss_ce_4: 0.1223  loss_mask_4: 0.2151  loss_dice_4: 0.2154  loss_ce_5: 0.1227  loss_mask_5: 0.2085  loss_dice_5: 0.211  loss_ce_6: 0.1231  loss_mask_6: 0.2059  loss_dice_6: 0.216  loss_ce_7: 0.1223  loss_mask_7: 0.2042  loss_dice_7: 0.2218  loss_ce_8: 0.123  loss_mask_8: 0.2212  loss_dice_8: 0.2178  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:46] d2.utils.events INFO:  eta: 1:24:43  iter: 9919  total_loss: 5.587  loss_ce: 0.1181  loss_mask: 0.2088  loss_dice: 0.1974  loss_ce_0: 0.05929  loss_mask_0: 0.2131  loss_dice_0: 0.1962  loss_ce_1: 0.1181  loss_mask_1: 0.2106  loss_dice_1: 0.1942  loss_ce_2: 0.118  loss_mask_2: 0.2145  loss_dice_2: 0.1928  loss_ce_3: 0.118  loss_mask_3: 0.2051  loss_dice_3: 0.2012  loss_ce_4: 0.1178  loss_mask_4: 0.2238  loss_dice_4: 0.1997  loss_ce_5: 0.1178  loss_mask_5: 0.2084  loss_dice_5: 0.1981  loss_ce_6: 0.1181  loss_mask_6: 0.2047  loss_dice_6: 0.1978  loss_ce_7: 0.1182  loss_mask_7: 0.2097  loss_dice_7: 0.1855  loss_ce_8: 0.1182  loss_mask_8: 0.2002  loss_dice_8: 0.1943  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:48] d2.utils.events INFO:  eta: 1:24:46  iter: 9939  total_loss: 5.09  loss_ce: 0.1442  loss_mask: 0.2562  loss_dice: 0.1321  loss_ce_0: 0.07117  loss_mask_0: 0.258  loss_dice_0: 0.1294  loss_ce_1: 0.144  loss_mask_1: 0.2661  loss_dice_1: 0.132  loss_ce_2: 0.1443  loss_mask_2: 0.2587  loss_dice_2: 0.1298  loss_ce_3: 0.1448  loss_mask_3: 0.2492  loss_dice_3: 0.1289  loss_ce_4: 0.1454  loss_mask_4: 0.2531  loss_dice_4: 0.1339  loss_ce_5: 0.1451  loss_mask_5: 0.248  loss_dice_5: 0.125  loss_ce_6: 0.145  loss_mask_6: 0.2471  loss_dice_6: 0.1293  loss_ce_7: 0.1445  loss_mask_7: 0.2517  loss_dice_7: 0.1275  loss_ce_8: 0.1442  loss_mask_8: 0.259  loss_dice_8: 0.1299  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:51] d2.utils.events INFO:  eta: 1:24:46  iter: 9959  total_loss: 4.84  loss_ce: 0.1062  loss_mask: 0.2187  loss_dice: 0.1315  loss_ce_0: 0.05705  loss_mask_0: 0.226  loss_dice_0: 0.132  loss_ce_1: 0.1062  loss_mask_1: 0.2291  loss_dice_1: 0.134  loss_ce_2: 0.106  loss_mask_2: 0.2198  loss_dice_2: 0.1326  loss_ce_3: 0.1057  loss_mask_3: 0.225  loss_dice_3: 0.1287  loss_ce_4: 0.1053  loss_mask_4: 0.2147  loss_dice_4: 0.131  loss_ce_5: 0.1055  loss_mask_5: 0.2121  loss_dice_5: 0.1322  loss_ce_6: 0.1054  loss_mask_6: 0.2249  loss_dice_6: 0.1337  loss_ce_7: 0.1057  loss_mask_7: 0.2166  loss_dice_7: 0.1285  loss_ce_8: 0.106  loss_mask_8: 0.2168  loss_dice_8: 0.1332  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 14:57:53] d2.utils.events INFO:  eta: 1:24:45  iter: 9979  total_loss: 5.037  loss_ce: 0.1031  loss_mask: 0.2053  loss_dice: 0.1472  loss_ce_0: 0.05614  loss_mask_0: 0.2022  loss_dice_0: 0.1515  loss_ce_1: 0.103  loss_mask_1: 0.2088  loss_dice_1: 0.1514  loss_ce_2: 0.1029  loss_mask_2: 0.2044  loss_dice_2: 0.1527  loss_ce_3: 0.1028  loss_mask_3: 0.2011  loss_dice_3: 0.1552  loss_ce_4: 0.1028  loss_mask_4: 0.198  loss_dice_4: 0.1566  loss_ce_5: 0.1029  loss_mask_5: 0.2033  loss_dice_5: 0.15  loss_ce_6: 0.1027  loss_mask_6: 0.1977  loss_dice_6: 0.1572  loss_ce_7: 0.103  loss_mask_7: 0.2006  loss_dice_7: 0.1593  loss_ce_8: 0.1029  loss_mask_8: 0.2001  loss_dice_8: 0.1532  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 14:57:56] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0009999.pth
[04/13 14:57:56] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 14:57:56] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 14:57:56] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 14:57:56] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 14:57:56] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 14:57:59] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0005 s/iter. Inference: 0.0526 s/iter. Eval: 0.2303 s/iter. Total: 0.2835 s/iter. ETA=0:03:59
[04/13 14:58:04] d2.evaluation.evaluator INFO: Inference done 29/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2307 s/iter. Total: 0.2842 s/iter. ETA=0:03:55
[04/13 14:58:10] d2.evaluation.evaluator INFO: Inference done 47/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2321 s/iter. Total: 0.2856 s/iter. ETA=0:03:51
[04/13 14:58:15] d2.evaluation.evaluator INFO: Inference done 65/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2314 s/iter. Total: 0.2849 s/iter. ETA=0:03:45
[04/13 14:58:20] d2.evaluation.evaluator INFO: Inference done 83/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2315 s/iter. Total: 0.2850 s/iter. ETA=0:03:40
[04/13 14:58:25] d2.evaluation.evaluator INFO: Inference done 101/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2313 s/iter. Total: 0.2850 s/iter. ETA=0:03:35
[04/13 14:58:30] d2.evaluation.evaluator INFO: Inference done 119/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2312 s/iter. Total: 0.2849 s/iter. ETA=0:03:29
[04/13 14:58:35] d2.evaluation.evaluator INFO: Inference done 137/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2307 s/iter. Total: 0.2844 s/iter. ETA=0:03:24
[04/13 14:58:40] d2.evaluation.evaluator INFO: Inference done 155/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2313 s/iter. Total: 0.2850 s/iter. ETA=0:03:19
[04/13 14:58:46] d2.evaluation.evaluator INFO: Inference done 173/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2316 s/iter. Total: 0.2852 s/iter. ETA=0:03:14
[04/13 14:58:51] d2.evaluation.evaluator INFO: Inference done 191/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2319 s/iter. Total: 0.2855 s/iter. ETA=0:03:09
[04/13 14:58:56] d2.evaluation.evaluator INFO: Inference done 209/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2321 s/iter. Total: 0.2858 s/iter. ETA=0:03:04
[04/13 14:59:01] d2.evaluation.evaluator INFO: Inference done 227/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2323 s/iter. Total: 0.2860 s/iter. ETA=0:02:59
[04/13 14:59:06] d2.evaluation.evaluator INFO: Inference done 245/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2324 s/iter. Total: 0.2861 s/iter. ETA=0:02:54
[04/13 14:59:12] d2.evaluation.evaluator INFO: Inference done 263/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2328 s/iter. Total: 0.2865 s/iter. ETA=0:02:49
[04/13 14:59:17] d2.evaluation.evaluator INFO: Inference done 281/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2329 s/iter. Total: 0.2866 s/iter. ETA=0:02:44
[04/13 14:59:22] d2.evaluation.evaluator INFO: Inference done 299/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2332 s/iter. Total: 0.2869 s/iter. ETA=0:02:39
[04/13 14:59:27] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2333 s/iter. Total: 0.2870 s/iter. ETA=0:02:34
[04/13 14:59:32] d2.evaluation.evaluator INFO: Inference done 335/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:02:29
[04/13 14:59:38] d2.evaluation.evaluator INFO: Inference done 353/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2339 s/iter. Total: 0.2876 s/iter. ETA=0:02:24
[04/13 14:59:43] d2.evaluation.evaluator INFO: Inference done 371/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2341 s/iter. Total: 0.2879 s/iter. ETA=0:02:19
[04/13 14:59:48] d2.evaluation.evaluator INFO: Inference done 388/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2344 s/iter. Total: 0.2882 s/iter. ETA=0:02:14
[04/13 14:59:53] d2.evaluation.evaluator INFO: Inference done 406/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2346 s/iter. Total: 0.2884 s/iter. ETA=0:02:09
[04/13 14:59:59] d2.evaluation.evaluator INFO: Inference done 424/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2347 s/iter. Total: 0.2885 s/iter. ETA=0:02:04
[04/13 15:00:04] d2.evaluation.evaluator INFO: Inference done 442/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2349 s/iter. Total: 0.2886 s/iter. ETA=0:01:59
[04/13 15:00:09] d2.evaluation.evaluator INFO: Inference done 460/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2350 s/iter. Total: 0.2888 s/iter. ETA=0:01:54
[04/13 15:00:14] d2.evaluation.evaluator INFO: Inference done 478/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2352 s/iter. Total: 0.2890 s/iter. ETA=0:01:49
[04/13 15:00:20] d2.evaluation.evaluator INFO: Inference done 496/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2353 s/iter. Total: 0.2890 s/iter. ETA=0:01:44
[04/13 15:00:25] d2.evaluation.evaluator INFO: Inference done 514/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2354 s/iter. Total: 0.2891 s/iter. ETA=0:01:38
[04/13 15:00:30] d2.evaluation.evaluator INFO: Inference done 532/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2355 s/iter. Total: 0.2893 s/iter. ETA=0:01:33
[04/13 15:00:35] d2.evaluation.evaluator INFO: Inference done 550/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2356 s/iter. Total: 0.2893 s/iter. ETA=0:01:28
[04/13 15:00:40] d2.evaluation.evaluator INFO: Inference done 567/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2358 s/iter. Total: 0.2896 s/iter. ETA=0:01:23
[04/13 15:00:46] d2.evaluation.evaluator INFO: Inference done 584/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2361 s/iter. Total: 0.2899 s/iter. ETA=0:01:18
[04/13 15:00:51] d2.evaluation.evaluator INFO: Inference done 601/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2365 s/iter. Total: 0.2902 s/iter. ETA=0:01:14
[04/13 15:00:56] d2.evaluation.evaluator INFO: Inference done 618/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2368 s/iter. Total: 0.2905 s/iter. ETA=0:01:09
[04/13 15:01:01] d2.evaluation.evaluator INFO: Inference done 635/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2371 s/iter. Total: 0.2908 s/iter. ETA=0:01:04
[04/13 15:01:06] d2.evaluation.evaluator INFO: Inference done 652/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2374 s/iter. Total: 0.2912 s/iter. ETA=0:00:59
[04/13 15:01:11] d2.evaluation.evaluator INFO: Inference done 669/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2377 s/iter. Total: 0.2915 s/iter. ETA=0:00:54
[04/13 15:01:16] d2.evaluation.evaluator INFO: Inference done 686/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2379 s/iter. Total: 0.2918 s/iter. ETA=0:00:49
[04/13 15:01:21] d2.evaluation.evaluator INFO: Inference done 703/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2382 s/iter. Total: 0.2920 s/iter. ETA=0:00:44
[04/13 15:01:27] d2.evaluation.evaluator INFO: Inference done 720/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2386 s/iter. Total: 0.2924 s/iter. ETA=0:00:39
[04/13 15:01:32] d2.evaluation.evaluator INFO: Inference done 737/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2388 s/iter. Total: 0.2926 s/iter. ETA=0:00:34
[04/13 15:01:37] d2.evaluation.evaluator INFO: Inference done 754/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2391 s/iter. Total: 0.2929 s/iter. ETA=0:00:29
[04/13 15:01:42] d2.evaluation.evaluator INFO: Inference done 771/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2393 s/iter. Total: 0.2931 s/iter. ETA=0:00:24
[04/13 15:01:47] d2.evaluation.evaluator INFO: Inference done 788/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2395 s/iter. Total: 0.2934 s/iter. ETA=0:00:19
[04/13 15:01:53] d2.evaluation.evaluator INFO: Inference done 805/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2397 s/iter. Total: 0.2936 s/iter. ETA=0:00:14
[04/13 15:01:58] d2.evaluation.evaluator INFO: Inference done 822/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2399 s/iter. Total: 0.2937 s/iter. ETA=0:00:09
[04/13 15:02:03] d2.evaluation.evaluator INFO: Inference done 839/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2400 s/iter. Total: 0.2938 s/iter. ETA=0:00:04
[04/13 15:02:08] d2.evaluation.evaluator INFO: Inference done 856/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2402 s/iter. Total: 0.2940 s/iter. ETA=0:00:00
[04/13 15:02:08] d2.evaluation.evaluator INFO: Total inference time: 0:04:10.303946 (0.294129 s / iter per device, on 1 devices)
[04/13 15:02:08] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:45 (0.052913 s / iter per device, on 1 devices)
[04/13 15:02:09] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 15:02:09] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 15:02:10] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 15:02:11] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 15:02:11] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.28 seconds.
[04/13 15:02:11] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 15:02:11] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 15:02:11] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 15:02:11] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 15:02:11] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 15:02:15] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 15:02:17] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 2.04 seconds.
[04/13 15:02:17] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 15:02:17] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 15:02:17] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 61.523 | 61.523 | 61.523 |  nan  |  nan  | 61.523 |
[04/13 15:02:17] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 15:02:17] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 70.592 | defect     | 52.453 |
[04/13 15:02:17] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 15:02:17] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 15:02:17] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 15:02:17] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 15:02:17] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 15:02:17] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 15:02:17] d2.evaluation.testing INFO: copypaste: 61.5229,61.5229,61.5229,nan,nan,61.5229
[04/13 15:02:17] d2.utils.events INFO:  eta: 1:24:45  iter: 9999  total_loss: 4.952  loss_ce: 0.1297  loss_mask: 0.2053  loss_dice: 0.1372  loss_ce_0: 0.06507  loss_mask_0: 0.2158  loss_dice_0: 0.1352  loss_ce_1: 0.1298  loss_mask_1: 0.2089  loss_dice_1: 0.1413  loss_ce_2: 0.1298  loss_mask_2: 0.2084  loss_dice_2: 0.1363  loss_ce_3: 0.1297  loss_mask_3: 0.2104  loss_dice_3: 0.1398  loss_ce_4: 0.1296  loss_mask_4: 0.2123  loss_dice_4: 0.1403  loss_ce_5: 0.1296  loss_mask_5: 0.2115  loss_dice_5: 0.1415  loss_ce_6: 0.1296  loss_mask_6: 0.2094  loss_dice_6: 0.1399  loss_ce_7: 0.1296  loss_mask_7: 0.2166  loss_dice_7: 0.1319  loss_ce_8: 0.1296  loss_mask_8: 0.2132  loss_dice_8: 0.146  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:02:19] d2.utils.events INFO:  eta: 1:24:42  iter: 10019  total_loss: 4.816  loss_ce: 0.09887  loss_mask: 0.206  loss_dice: 0.1452  loss_ce_0: 0.05409  loss_mask_0: 0.2095  loss_dice_0: 0.1488  loss_ce_1: 0.09843  loss_mask_1: 0.2033  loss_dice_1: 0.1444  loss_ce_2: 0.09869  loss_mask_2: 0.2089  loss_dice_2: 0.1474  loss_ce_3: 0.09891  loss_mask_3: 0.2007  loss_dice_3: 0.1466  loss_ce_4: 0.09951  loss_mask_4: 0.2077  loss_dice_4: 0.1491  loss_ce_5: 0.09936  loss_mask_5: 0.2032  loss_dice_5: 0.1487  loss_ce_6: 0.09932  loss_mask_6: 0.1985  loss_dice_6: 0.1423  loss_ce_7: 0.09936  loss_mask_7: 0.2019  loss_dice_7: 0.1473  loss_ce_8: 0.09917  loss_mask_8: 0.2119  loss_dice_8: 0.1523  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:02:22] d2.utils.events INFO:  eta: 1:24:41  iter: 10039  total_loss: 5.296  loss_ce: 0.132  loss_mask: 0.1883  loss_dice: 0.1579  loss_ce_0: 0.06535  loss_mask_0: 0.2039  loss_dice_0: 0.1478  loss_ce_1: 0.1318  loss_mask_1: 0.1868  loss_dice_1: 0.1465  loss_ce_2: 0.1319  loss_mask_2: 0.1809  loss_dice_2: 0.147  loss_ce_3: 0.1323  loss_mask_3: 0.2004  loss_dice_3: 0.1503  loss_ce_4: 0.1323  loss_mask_4: 0.186  loss_dice_4: 0.1531  loss_ce_5: 0.1322  loss_mask_5: 0.1876  loss_dice_5: 0.1534  loss_ce_6: 0.1324  loss_mask_6: 0.1907  loss_dice_6: 0.1483  loss_ce_7: 0.1323  loss_mask_7: 0.1954  loss_dice_7: 0.1482  loss_ce_8: 0.132  loss_mask_8: 0.2016  loss_dice_8: 0.1479  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:02:25] d2.utils.events INFO:  eta: 1:24:39  iter: 10059  total_loss: 5.308  loss_ce: 0.1302  loss_mask: 0.215  loss_dice: 0.1487  loss_ce_0: 0.06533  loss_mask_0: 0.2184  loss_dice_0: 0.1489  loss_ce_1: 0.1303  loss_mask_1: 0.2339  loss_dice_1: 0.1426  loss_ce_2: 0.1302  loss_mask_2: 0.2126  loss_dice_2: 0.1497  loss_ce_3: 0.1301  loss_mask_3: 0.2149  loss_dice_3: 0.1545  loss_ce_4: 0.1301  loss_mask_4: 0.2149  loss_dice_4: 0.1489  loss_ce_5: 0.1301  loss_mask_5: 0.2141  loss_dice_5: 0.1414  loss_ce_6: 0.1301  loss_mask_6: 0.2207  loss_dice_6: 0.1524  loss_ce_7: 0.1301  loss_mask_7: 0.2142  loss_dice_7: 0.1522  loss_ce_8: 0.1301  loss_mask_8: 0.2148  loss_dice_8: 0.1435  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:02:27] d2.utils.events INFO:  eta: 1:24:37  iter: 10079  total_loss: 4.802  loss_ce: 0.1477  loss_mask: 0.1837  loss_dice: 0.1327  loss_ce_0: 0.07457  loss_mask_0: 0.1776  loss_dice_0: 0.1287  loss_ce_1: 0.1477  loss_mask_1: 0.1825  loss_dice_1: 0.1308  loss_ce_2: 0.1475  loss_mask_2: 0.1797  loss_dice_2: 0.1333  loss_ce_3: 0.1464  loss_mask_3: 0.1824  loss_dice_3: 0.1309  loss_ce_4: 0.1454  loss_mask_4: 0.1795  loss_dice_4: 0.1286  loss_ce_5: 0.146  loss_mask_5: 0.1911  loss_dice_5: 0.1355  loss_ce_6: 0.146  loss_mask_6: 0.1893  loss_dice_6: 0.1356  loss_ce_7: 0.1466  loss_mask_7: 0.1794  loss_dice_7: 0.1357  loss_ce_8: 0.147  loss_mask_8: 0.1796  loss_dice_8: 0.1339  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:02:30] d2.utils.events INFO:  eta: 1:24:37  iter: 10099  total_loss: 5  loss_ce: 0.1302  loss_mask: 0.1847  loss_dice: 0.1464  loss_ce_0: 0.06082  loss_mask_0: 0.1778  loss_dice_0: 0.1504  loss_ce_1: 0.1303  loss_mask_1: 0.1774  loss_dice_1: 0.138  loss_ce_2: 0.1308  loss_mask_2: 0.1703  loss_dice_2: 0.143  loss_ce_3: 0.1303  loss_mask_3: 0.1722  loss_dice_3: 0.1482  loss_ce_4: 0.1296  loss_mask_4: 0.181  loss_dice_4: 0.1472  loss_ce_5: 0.13  loss_mask_5: 0.1823  loss_dice_5: 0.1531  loss_ce_6: 0.1299  loss_mask_6: 0.174  loss_dice_6: 0.1449  loss_ce_7: 0.1308  loss_mask_7: 0.1813  loss_dice_7: 0.1467  loss_ce_8: 0.1309  loss_mask_8: 0.1823  loss_dice_8: 0.1468  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:02:32] d2.utils.events INFO:  eta: 1:24:33  iter: 10119  total_loss: 5.336  loss_ce: 0.1229  loss_mask: 0.2128  loss_dice: 0.1509  loss_ce_0: 0.0653  loss_mask_0: 0.2143  loss_dice_0: 0.1502  loss_ce_1: 0.1236  loss_mask_1: 0.2105  loss_dice_1: 0.1531  loss_ce_2: 0.1225  loss_mask_2: 0.2176  loss_dice_2: 0.1542  loss_ce_3: 0.121  loss_mask_3: 0.208  loss_dice_3: 0.1514  loss_ce_4: 0.1205  loss_mask_4: 0.2184  loss_dice_4: 0.1481  loss_ce_5: 0.1213  loss_mask_5: 0.2089  loss_dice_5: 0.1472  loss_ce_6: 0.1209  loss_mask_6: 0.2078  loss_dice_6: 0.1463  loss_ce_7: 0.1222  loss_mask_7: 0.2172  loss_dice_7: 0.1491  loss_ce_8: 0.122  loss_mask_8: 0.2091  loss_dice_8: 0.1569  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:02:35] d2.utils.events INFO:  eta: 1:24:29  iter: 10139  total_loss: 4.858  loss_ce: 0.1141  loss_mask: 0.1992  loss_dice: 0.1195  loss_ce_0: 0.06435  loss_mask_0: 0.1979  loss_dice_0: 0.122  loss_ce_1: 0.1139  loss_mask_1: 0.1983  loss_dice_1: 0.1325  loss_ce_2: 0.1136  loss_mask_2: 0.1995  loss_dice_2: 0.1289  loss_ce_3: 0.1125  loss_mask_3: 0.1956  loss_dice_3: 0.1276  loss_ce_4: 0.112  loss_mask_4: 0.1964  loss_dice_4: 0.1266  loss_ce_5: 0.1132  loss_mask_5: 0.1988  loss_dice_5: 0.1254  loss_ce_6: 0.1125  loss_mask_6: 0.1958  loss_dice_6: 0.126  loss_ce_7: 0.1137  loss_mask_7: 0.1905  loss_dice_7: 0.1234  loss_ce_8: 0.1132  loss_mask_8: 0.1972  loss_dice_8: 0.1232  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:02:37] d2.utils.events INFO:  eta: 1:24:26  iter: 10159  total_loss: 4.82  loss_ce: 0.1474  loss_mask: 0.2234  loss_dice: 0.1456  loss_ce_0: 0.06776  loss_mask_0: 0.2209  loss_dice_0: 0.1392  loss_ce_1: 0.1472  loss_mask_1: 0.2314  loss_dice_1: 0.1462  loss_ce_2: 0.1471  loss_mask_2: 0.2313  loss_dice_2: 0.1385  loss_ce_3: 0.1473  loss_mask_3: 0.226  loss_dice_3: 0.1372  loss_ce_4: 0.1473  loss_mask_4: 0.226  loss_dice_4: 0.141  loss_ce_5: 0.1471  loss_mask_5: 0.2323  loss_dice_5: 0.1386  loss_ce_6: 0.1478  loss_mask_6: 0.2355  loss_dice_6: 0.1392  loss_ce_7: 0.1476  loss_mask_7: 0.2285  loss_dice_7: 0.1431  loss_ce_8: 0.1482  loss_mask_8: 0.2305  loss_dice_8: 0.1421  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:02:40] d2.utils.events INFO:  eta: 1:24:24  iter: 10179  total_loss: 4.56  loss_ce: 0.1111  loss_mask: 0.1763  loss_dice: 0.1226  loss_ce_0: 0.06064  loss_mask_0: 0.1792  loss_dice_0: 0.1259  loss_ce_1: 0.1111  loss_mask_1: 0.1621  loss_dice_1: 0.1261  loss_ce_2: 0.1112  loss_mask_2: 0.1731  loss_dice_2: 0.1334  loss_ce_3: 0.1113  loss_mask_3: 0.1679  loss_dice_3: 0.1266  loss_ce_4: 0.1114  loss_mask_4: 0.1669  loss_dice_4: 0.1292  loss_ce_5: 0.1113  loss_mask_5: 0.1723  loss_dice_5: 0.1331  loss_ce_6: 0.1109  loss_mask_6: 0.1752  loss_dice_6: 0.1322  loss_ce_7: 0.1107  loss_mask_7: 0.171  loss_dice_7: 0.1289  loss_ce_8: 0.1103  loss_mask_8: 0.1788  loss_dice_8: 0.1273  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:02:42] d2.utils.events INFO:  eta: 1:24:20  iter: 10199  total_loss: 5.342  loss_ce: 0.1498  loss_mask: 0.1822  loss_dice: 0.1601  loss_ce_0: 0.07073  loss_mask_0: 0.1823  loss_dice_0: 0.1583  loss_ce_1: 0.1494  loss_mask_1: 0.1901  loss_dice_1: 0.1569  loss_ce_2: 0.1498  loss_mask_2: 0.183  loss_dice_2: 0.1611  loss_ce_3: 0.1496  loss_mask_3: 0.1848  loss_dice_3: 0.1526  loss_ce_4: 0.1488  loss_mask_4: 0.1807  loss_dice_4: 0.1526  loss_ce_5: 0.1496  loss_mask_5: 0.1787  loss_dice_5: 0.1573  loss_ce_6: 0.1499  loss_mask_6: 0.1795  loss_dice_6: 0.1594  loss_ce_7: 0.1501  loss_mask_7: 0.1832  loss_dice_7: 0.1594  loss_ce_8: 0.1505  loss_mask_8: 0.1869  loss_dice_8: 0.1573  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:02:45] d2.utils.events INFO:  eta: 1:24:16  iter: 10219  total_loss: 4.672  loss_ce: 0.137  loss_mask: 0.199  loss_dice: 0.1374  loss_ce_0: 0.06562  loss_mask_0: 0.1889  loss_dice_0: 0.1368  loss_ce_1: 0.1372  loss_mask_1: 0.1886  loss_dice_1: 0.1352  loss_ce_2: 0.1376  loss_mask_2: 0.1879  loss_dice_2: 0.1378  loss_ce_3: 0.1382  loss_mask_3: 0.194  loss_dice_3: 0.1303  loss_ce_4: 0.1388  loss_mask_4: 0.1775  loss_dice_4: 0.1345  loss_ce_5: 0.1382  loss_mask_5: 0.1782  loss_dice_5: 0.1312  loss_ce_6: 0.1381  loss_mask_6: 0.1923  loss_dice_6: 0.1346  loss_ce_7: 0.1375  loss_mask_7: 0.1972  loss_dice_7: 0.1382  loss_ce_8: 0.1374  loss_mask_8: 0.1878  loss_dice_8: 0.1336  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:02:47] d2.utils.events INFO:  eta: 1:24:17  iter: 10239  total_loss: 4.985  loss_ce: 0.1278  loss_mask: 0.2261  loss_dice: 0.1393  loss_ce_0: 0.06478  loss_mask_0: 0.2147  loss_dice_0: 0.1367  loss_ce_1: 0.1277  loss_mask_1: 0.2185  loss_dice_1: 0.1398  loss_ce_2: 0.1278  loss_mask_2: 0.2193  loss_dice_2: 0.1391  loss_ce_3: 0.1278  loss_mask_3: 0.2269  loss_dice_3: 0.1393  loss_ce_4: 0.1277  loss_mask_4: 0.2242  loss_dice_4: 0.1378  loss_ce_5: 0.1278  loss_mask_5: 0.2182  loss_dice_5: 0.1324  loss_ce_6: 0.1279  loss_mask_6: 0.217  loss_dice_6: 0.1334  loss_ce_7: 0.1279  loss_mask_7: 0.2156  loss_dice_7: 0.132  loss_ce_8: 0.1279  loss_mask_8: 0.2146  loss_dice_8: 0.1425  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:02:50] d2.utils.events INFO:  eta: 1:24:15  iter: 10259  total_loss: 5.295  loss_ce: 0.1191  loss_mask: 0.1483  loss_dice: 0.2252  loss_ce_0: 0.06388  loss_mask_0: 0.1505  loss_dice_0: 0.2265  loss_ce_1: 0.1201  loss_mask_1: 0.1517  loss_dice_1: 0.2226  loss_ce_2: 0.1202  loss_mask_2: 0.1411  loss_dice_2: 0.2235  loss_ce_3: 0.1199  loss_mask_3: 0.1521  loss_dice_3: 0.2213  loss_ce_4: 0.1202  loss_mask_4: 0.14  loss_dice_4: 0.2231  loss_ce_5: 0.1204  loss_mask_5: 0.1397  loss_dice_5: 0.2324  loss_ce_6: 0.1194  loss_mask_6: 0.1487  loss_dice_6: 0.2179  loss_ce_7: 0.1204  loss_mask_7: 0.153  loss_dice_7: 0.2327  loss_ce_8: 0.1192  loss_mask_8: 0.1484  loss_dice_8: 0.2192  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:02:53] d2.utils.events INFO:  eta: 1:24:12  iter: 10279  total_loss: 4.954  loss_ce: 0.1099  loss_mask: 0.1786  loss_dice: 0.154  loss_ce_0: 0.06094  loss_mask_0: 0.1783  loss_dice_0: 0.1572  loss_ce_1: 0.1102  loss_mask_1: 0.1744  loss_dice_1: 0.1553  loss_ce_2: 0.11  loss_mask_2: 0.1802  loss_dice_2: 0.1566  loss_ce_3: 0.1091  loss_mask_3: 0.1831  loss_dice_3: 0.1603  loss_ce_4: 0.1086  loss_mask_4: 0.1866  loss_dice_4: 0.1535  loss_ce_5: 0.1091  loss_mask_5: 0.1837  loss_dice_5: 0.1573  loss_ce_6: 0.1085  loss_mask_6: 0.1773  loss_dice_6: 0.1582  loss_ce_7: 0.1095  loss_mask_7: 0.1834  loss_dice_7: 0.1519  loss_ce_8: 0.1095  loss_mask_8: 0.1879  loss_dice_8: 0.1522  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:02:55] d2.utils.events INFO:  eta: 1:24:08  iter: 10299  total_loss: 5.176  loss_ce: 0.1027  loss_mask: 0.2205  loss_dice: 0.1388  loss_ce_0: 0.05759  loss_mask_0: 0.2186  loss_dice_0: 0.1332  loss_ce_1: 0.1025  loss_mask_1: 0.2224  loss_dice_1: 0.138  loss_ce_2: 0.1023  loss_mask_2: 0.2164  loss_dice_2: 0.1396  loss_ce_3: 0.1022  loss_mask_3: 0.2222  loss_dice_3: 0.1343  loss_ce_4: 0.1023  loss_mask_4: 0.222  loss_dice_4: 0.1367  loss_ce_5: 0.1025  loss_mask_5: 0.2109  loss_dice_5: 0.1358  loss_ce_6: 0.1022  loss_mask_6: 0.2002  loss_dice_6: 0.1435  loss_ce_7: 0.1026  loss_mask_7: 0.2168  loss_dice_7: 0.141  loss_ce_8: 0.1025  loss_mask_8: 0.2075  loss_dice_8: 0.1327  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:02:58] d2.utils.events INFO:  eta: 1:24:04  iter: 10319  total_loss: 5.385  loss_ce: 0.1299  loss_mask: 0.2291  loss_dice: 0.1345  loss_ce_0: 0.06472  loss_mask_0: 0.2413  loss_dice_0: 0.1329  loss_ce_1: 0.13  loss_mask_1: 0.2395  loss_dice_1: 0.1385  loss_ce_2: 0.1299  loss_mask_2: 0.2295  loss_dice_2: 0.1432  loss_ce_3: 0.13  loss_mask_3: 0.2301  loss_dice_3: 0.1358  loss_ce_4: 0.13  loss_mask_4: 0.229  loss_dice_4: 0.1383  loss_ce_5: 0.13  loss_mask_5: 0.2356  loss_dice_5: 0.1398  loss_ce_6: 0.1301  loss_mask_6: 0.229  loss_dice_6: 0.1449  loss_ce_7: 0.13  loss_mask_7: 0.237  loss_dice_7: 0.1345  loss_ce_8: 0.1299  loss_mask_8: 0.2345  loss_dice_8: 0.1341  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:00] d2.utils.events INFO:  eta: 1:24:03  iter: 10339  total_loss: 4.928  loss_ce: 0.1041  loss_mask: 0.2003  loss_dice: 0.1358  loss_ce_0: 0.05697  loss_mask_0: 0.1977  loss_dice_0: 0.1296  loss_ce_1: 0.104  loss_mask_1: 0.2006  loss_dice_1: 0.1387  loss_ce_2: 0.1042  loss_mask_2: 0.1976  loss_dice_2: 0.1357  loss_ce_3: 0.1043  loss_mask_3: 0.2031  loss_dice_3: 0.1364  loss_ce_4: 0.1047  loss_mask_4: 0.1926  loss_dice_4: 0.1414  loss_ce_5: 0.1045  loss_mask_5: 0.1893  loss_dice_5: 0.1399  loss_ce_6: 0.1042  loss_mask_6: 0.2018  loss_dice_6: 0.1445  loss_ce_7: 0.1043  loss_mask_7: 0.1882  loss_dice_7: 0.1369  loss_ce_8: 0.1042  loss_mask_8: 0.2026  loss_dice_8: 0.1322  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:03] d2.utils.events INFO:  eta: 1:24:00  iter: 10359  total_loss: 5.188  loss_ce: 0.1082  loss_mask: 0.2251  loss_dice: 0.1563  loss_ce_0: 0.05716  loss_mask_0: 0.2218  loss_dice_0: 0.1521  loss_ce_1: 0.1082  loss_mask_1: 0.2173  loss_dice_1: 0.1582  loss_ce_2: 0.1085  loss_mask_2: 0.2106  loss_dice_2: 0.1629  loss_ce_3: 0.1087  loss_mask_3: 0.2191  loss_dice_3: 0.1503  loss_ce_4: 0.1094  loss_mask_4: 0.219  loss_dice_4: 0.1588  loss_ce_5: 0.109  loss_mask_5: 0.2154  loss_dice_5: 0.1552  loss_ce_6: 0.1089  loss_mask_6: 0.2178  loss_dice_6: 0.1597  loss_ce_7: 0.1089  loss_mask_7: 0.2257  loss_dice_7: 0.1493  loss_ce_8: 0.1087  loss_mask_8: 0.2064  loss_dice_8: 0.1572  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:03:05] d2.utils.events INFO:  eta: 1:23:57  iter: 10379  total_loss: 4.731  loss_ce: 0.1121  loss_mask: 0.1872  loss_dice: 0.1309  loss_ce_0: 0.05802  loss_mask_0: 0.1895  loss_dice_0: 0.1262  loss_ce_1: 0.1121  loss_mask_1: 0.1907  loss_dice_1: 0.1248  loss_ce_2: 0.1124  loss_mask_2: 0.192  loss_dice_2: 0.1348  loss_ce_3: 0.1127  loss_mask_3: 0.1831  loss_dice_3: 0.1243  loss_ce_4: 0.1134  loss_mask_4: 0.1984  loss_dice_4: 0.1292  loss_ce_5: 0.1129  loss_mask_5: 0.1924  loss_dice_5: 0.126  loss_ce_6: 0.1129  loss_mask_6: 0.1837  loss_dice_6: 0.1302  loss_ce_7: 0.1127  loss_mask_7: 0.1877  loss_dice_7: 0.1224  loss_ce_8: 0.1125  loss_mask_8: 0.1921  loss_dice_8: 0.1298  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:03:08] d2.utils.events INFO:  eta: 1:23:54  iter: 10399  total_loss: 5.353  loss_ce: 0.1446  loss_mask: 0.2177  loss_dice: 0.1426  loss_ce_0: 0.07138  loss_mask_0: 0.2218  loss_dice_0: 0.1395  loss_ce_1: 0.1443  loss_mask_1: 0.22  loss_dice_1: 0.1539  loss_ce_2: 0.1442  loss_mask_2: 0.2237  loss_dice_2: 0.1489  loss_ce_3: 0.1442  loss_mask_3: 0.2152  loss_dice_3: 0.1463  loss_ce_4: 0.1439  loss_mask_4: 0.2158  loss_dice_4: 0.1455  loss_ce_5: 0.1442  loss_mask_5: 0.2121  loss_dice_5: 0.149  loss_ce_6: 0.1443  loss_mask_6: 0.2112  loss_dice_6: 0.1477  loss_ce_7: 0.1443  loss_mask_7: 0.2199  loss_dice_7: 0.1493  loss_ce_8: 0.1443  loss_mask_8: 0.2013  loss_dice_8: 0.155  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:10] d2.utils.events INFO:  eta: 1:23:51  iter: 10419  total_loss: 4.981  loss_ce: 0.1149  loss_mask: 0.2235  loss_dice: 0.1669  loss_ce_0: 0.05871  loss_mask_0: 0.2364  loss_dice_0: 0.1619  loss_ce_1: 0.1149  loss_mask_1: 0.235  loss_dice_1: 0.1666  loss_ce_2: 0.1152  loss_mask_2: 0.2273  loss_dice_2: 0.1655  loss_ce_3: 0.1155  loss_mask_3: 0.223  loss_dice_3: 0.1733  loss_ce_4: 0.1157  loss_mask_4: 0.2143  loss_dice_4: 0.1499  loss_ce_5: 0.1154  loss_mask_5: 0.2256  loss_dice_5: 0.1681  loss_ce_6: 0.1154  loss_mask_6: 0.2308  loss_dice_6: 0.1601  loss_ce_7: 0.1152  loss_mask_7: 0.2176  loss_dice_7: 0.1668  loss_ce_8: 0.1152  loss_mask_8: 0.2312  loss_dice_8: 0.171  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:03:13] d2.utils.events INFO:  eta: 1:23:48  iter: 10439  total_loss: 5.132  loss_ce: 0.1041  loss_mask: 0.2334  loss_dice: 0.1273  loss_ce_0: 0.05632  loss_mask_0: 0.215  loss_dice_0: 0.1264  loss_ce_1: 0.1039  loss_mask_1: 0.2267  loss_dice_1: 0.1294  loss_ce_2: 0.1039  loss_mask_2: 0.2263  loss_dice_2: 0.1256  loss_ce_3: 0.1033  loss_mask_3: 0.2091  loss_dice_3: 0.125  loss_ce_4: 0.1022  loss_mask_4: 0.2271  loss_dice_4: 0.1264  loss_ce_5: 0.103  loss_mask_5: 0.2145  loss_dice_5: 0.1264  loss_ce_6: 0.1029  loss_mask_6: 0.2196  loss_dice_6: 0.1253  loss_ce_7: 0.1032  loss_mask_7: 0.2221  loss_dice_7: 0.126  loss_ce_8: 0.1038  loss_mask_8: 0.2162  loss_dice_8: 0.1243  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:15] d2.utils.events INFO:  eta: 1:23:43  iter: 10459  total_loss: 4.804  loss_ce: 0.09053  loss_mask: 0.2121  loss_dice: 0.1482  loss_ce_0: 0.05301  loss_mask_0: 0.2322  loss_dice_0: 0.1526  loss_ce_1: 0.09022  loss_mask_1: 0.2119  loss_dice_1: 0.1524  loss_ce_2: 0.09008  loss_mask_2: 0.2139  loss_dice_2: 0.1512  loss_ce_3: 0.08935  loss_mask_3: 0.2049  loss_dice_3: 0.1431  loss_ce_4: 0.08766  loss_mask_4: 0.2127  loss_dice_4: 0.1507  loss_ce_5: 0.08923  loss_mask_5: 0.206  loss_dice_5: 0.1524  loss_ce_6: 0.08873  loss_mask_6: 0.2048  loss_dice_6: 0.1536  loss_ce_7: 0.08915  loss_mask_7: 0.2116  loss_dice_7: 0.1517  loss_ce_8: 0.08973  loss_mask_8: 0.2187  loss_dice_8: 0.1502  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:03:18] d2.utils.events INFO:  eta: 1:23:39  iter: 10479  total_loss: 4.936  loss_ce: 0.08044  loss_mask: 0.2266  loss_dice: 0.1339  loss_ce_0: 0.05004  loss_mask_0: 0.2198  loss_dice_0: 0.1265  loss_ce_1: 0.08024  loss_mask_1: 0.2248  loss_dice_1: 0.1264  loss_ce_2: 0.0805  loss_mask_2: 0.2294  loss_dice_2: 0.1326  loss_ce_3: 0.07999  loss_mask_3: 0.221  loss_dice_3: 0.13  loss_ce_4: 0.07812  loss_mask_4: 0.2305  loss_dice_4: 0.1315  loss_ce_5: 0.07992  loss_mask_5: 0.2335  loss_dice_5: 0.1309  loss_ce_6: 0.07904  loss_mask_6: 0.2355  loss_dice_6: 0.1297  loss_ce_7: 0.07914  loss_mask_7: 0.2255  loss_dice_7: 0.1259  loss_ce_8: 0.07977  loss_mask_8: 0.2211  loss_dice_8: 0.1297  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:03:20] d2.utils.events INFO:  eta: 1:23:37  iter: 10499  total_loss: 4.953  loss_ce: 0.2062  loss_mask: 0.207  loss_dice: 0.1345  loss_ce_0: 0.08547  loss_mask_0: 0.2189  loss_dice_0: 0.1347  loss_ce_1: 0.2068  loss_mask_1: 0.2063  loss_dice_1: 0.1323  loss_ce_2: 0.2066  loss_mask_2: 0.1989  loss_dice_2: 0.1323  loss_ce_3: 0.2066  loss_mask_3: 0.2071  loss_dice_3: 0.1382  loss_ce_4: 0.2069  loss_mask_4: 0.2118  loss_dice_4: 0.1332  loss_ce_5: 0.2062  loss_mask_5: 0.2045  loss_dice_5: 0.135  loss_ce_6: 0.2074  loss_mask_6: 0.2036  loss_dice_6: 0.1377  loss_ce_7: 0.2068  loss_mask_7: 0.1917  loss_dice_7: 0.1347  loss_ce_8: 0.2068  loss_mask_8: 0.2039  loss_dice_8: 0.1286  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:23] d2.utils.events INFO:  eta: 1:23:35  iter: 10519  total_loss: 4.919  loss_ce: 0.1335  loss_mask: 0.197  loss_dice: 0.1171  loss_ce_0: 0.06653  loss_mask_0: 0.2041  loss_dice_0: 0.1177  loss_ce_1: 0.1334  loss_mask_1: 0.205  loss_dice_1: 0.1149  loss_ce_2: 0.1336  loss_mask_2: 0.2038  loss_dice_2: 0.1135  loss_ce_3: 0.1335  loss_mask_3: 0.1939  loss_dice_3: 0.1176  loss_ce_4: 0.1325  loss_mask_4: 0.2011  loss_dice_4: 0.1129  loss_ce_5: 0.1333  loss_mask_5: 0.1914  loss_dice_5: 0.1112  loss_ce_6: 0.1332  loss_mask_6: 0.2036  loss_dice_6: 0.118  loss_ce_7: 0.1329  loss_mask_7: 0.1967  loss_dice_7: 0.1185  loss_ce_8: 0.1333  loss_mask_8: 0.2082  loss_dice_8: 0.1146  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:03:26] d2.utils.events INFO:  eta: 1:23:35  iter: 10539  total_loss: 4.827  loss_ce: 0.1657  loss_mask: 0.2125  loss_dice: 0.1384  loss_ce_0: 0.08199  loss_mask_0: 0.2105  loss_dice_0: 0.135  loss_ce_1: 0.1655  loss_mask_1: 0.2151  loss_dice_1: 0.1371  loss_ce_2: 0.1655  loss_mask_2: 0.2022  loss_dice_2: 0.1383  loss_ce_3: 0.1649  loss_mask_3: 0.2007  loss_dice_3: 0.1359  loss_ce_4: 0.1606  loss_mask_4: 0.217  loss_dice_4: 0.1408  loss_ce_5: 0.1648  loss_mask_5: 0.2063  loss_dice_5: 0.1368  loss_ce_6: 0.1642  loss_mask_6: 0.2154  loss_dice_6: 0.1336  loss_ce_7: 0.1631  loss_mask_7: 0.2092  loss_dice_7: 0.1353  loss_ce_8: 0.1646  loss_mask_8: 0.2038  loss_dice_8: 0.1349  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:28] d2.utils.events INFO:  eta: 1:23:36  iter: 10559  total_loss: 4.714  loss_ce: 0.09822  loss_mask: 0.1869  loss_dice: 0.1368  loss_ce_0: 0.05083  loss_mask_0: 0.1932  loss_dice_0: 0.1424  loss_ce_1: 0.09853  loss_mask_1: 0.1949  loss_dice_1: 0.1491  loss_ce_2: 0.09854  loss_mask_2: 0.1869  loss_dice_2: 0.1463  loss_ce_3: 0.09883  loss_mask_3: 0.1869  loss_dice_3: 0.1392  loss_ce_4: 0.102  loss_mask_4: 0.1907  loss_dice_4: 0.1416  loss_ce_5: 0.09883  loss_mask_5: 0.1888  loss_dice_5: 0.1333  loss_ce_6: 0.09939  loss_mask_6: 0.1917  loss_dice_6: 0.1427  loss_ce_7: 0.09999  loss_mask_7: 0.1905  loss_dice_7: 0.1452  loss_ce_8: 0.09902  loss_mask_8: 0.1925  loss_dice_8: 0.1508  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:31] d2.utils.events INFO:  eta: 1:23:33  iter: 10579  total_loss: 5.174  loss_ce: 0.1457  loss_mask: 0.2095  loss_dice: 0.1522  loss_ce_0: 0.07577  loss_mask_0: 0.2015  loss_dice_0: 0.1421  loss_ce_1: 0.1456  loss_mask_1: 0.2103  loss_dice_1: 0.1446  loss_ce_2: 0.145  loss_mask_2: 0.2039  loss_dice_2: 0.1482  loss_ce_3: 0.1443  loss_mask_3: 0.2114  loss_dice_3: 0.1486  loss_ce_4: 0.1409  loss_mask_4: 0.2048  loss_dice_4: 0.1522  loss_ce_5: 0.1445  loss_mask_5: 0.2115  loss_dice_5: 0.1489  loss_ce_6: 0.1438  loss_mask_6: 0.2097  loss_dice_6: 0.1454  loss_ce_7: 0.1438  loss_mask_7: 0.216  loss_dice_7: 0.1474  loss_ce_8: 0.1445  loss_mask_8: 0.2147  loss_dice_8: 0.1469  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:33] d2.utils.events INFO:  eta: 1:23:27  iter: 10599  total_loss: 5.149  loss_ce: 0.1193  loss_mask: 0.2374  loss_dice: 0.1462  loss_ce_0: 0.05684  loss_mask_0: 0.236  loss_dice_0: 0.147  loss_ce_1: 0.1193  loss_mask_1: 0.2284  loss_dice_1: 0.1493  loss_ce_2: 0.1203  loss_mask_2: 0.2321  loss_dice_2: 0.1457  loss_ce_3: 0.1209  loss_mask_3: 0.2322  loss_dice_3: 0.1461  loss_ce_4: 0.1244  loss_mask_4: 0.2449  loss_dice_4: 0.1528  loss_ce_5: 0.1203  loss_mask_5: 0.24  loss_dice_5: 0.1502  loss_ce_6: 0.1213  loss_mask_6: 0.2407  loss_dice_6: 0.1478  loss_ce_7: 0.1211  loss_mask_7: 0.2384  loss_dice_7: 0.1496  loss_ce_8: 0.1207  loss_mask_8: 0.2293  loss_dice_8: 0.1455  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:36] d2.utils.events INFO:  eta: 1:23:22  iter: 10619  total_loss: 5.283  loss_ce: 0.1393  loss_mask: 0.2283  loss_dice: 0.1474  loss_ce_0: 0.07205  loss_mask_0: 0.2267  loss_dice_0: 0.1473  loss_ce_1: 0.1396  loss_mask_1: 0.2299  loss_dice_1: 0.1464  loss_ce_2: 0.1392  loss_mask_2: 0.2278  loss_dice_2: 0.1469  loss_ce_3: 0.1395  loss_mask_3: 0.223  loss_dice_3: 0.1439  loss_ce_4: 0.1384  loss_mask_4: 0.2251  loss_dice_4: 0.1473  loss_ce_5: 0.1398  loss_mask_5: 0.2226  loss_dice_5: 0.1445  loss_ce_6: 0.139  loss_mask_6: 0.2287  loss_dice_6: 0.149  loss_ce_7: 0.1391  loss_mask_7: 0.2303  loss_dice_7: 0.1491  loss_ce_8: 0.1385  loss_mask_8: 0.2315  loss_dice_8: 0.1479  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:03:38] d2.utils.events INFO:  eta: 1:23:18  iter: 10639  total_loss: 5.034  loss_ce: 0.1284  loss_mask: 0.2046  loss_dice: 0.1311  loss_ce_0: 0.06472  loss_mask_0: 0.2106  loss_dice_0: 0.1316  loss_ce_1: 0.1285  loss_mask_1: 0.1937  loss_dice_1: 0.1302  loss_ce_2: 0.1286  loss_mask_2: 0.2026  loss_dice_2: 0.1261  loss_ce_3: 0.1286  loss_mask_3: 0.1978  loss_dice_3: 0.1312  loss_ce_4: 0.1288  loss_mask_4: 0.2002  loss_dice_4: 0.128  loss_ce_5: 0.1286  loss_mask_5: 0.1949  loss_dice_5: 0.133  loss_ce_6: 0.1286  loss_mask_6: 0.2024  loss_dice_6: 0.1327  loss_ce_7: 0.1286  loss_mask_7: 0.1916  loss_dice_7: 0.1297  loss_ce_8: 0.1284  loss_mask_8: 0.1972  loss_dice_8: 0.1278  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:41] d2.utils.events INFO:  eta: 1:23:11  iter: 10659  total_loss: 5.716  loss_ce: 0.1278  loss_mask: 0.2118  loss_dice: 0.1855  loss_ce_0: 0.06143  loss_mask_0: 0.2242  loss_dice_0: 0.1825  loss_ce_1: 0.1279  loss_mask_1: 0.2146  loss_dice_1: 0.1912  loss_ce_2: 0.1279  loss_mask_2: 0.221  loss_dice_2: 0.1852  loss_ce_3: 0.1282  loss_mask_3: 0.2053  loss_dice_3: 0.1824  loss_ce_4: 0.1292  loss_mask_4: 0.2084  loss_dice_4: 0.1767  loss_ce_5: 0.1281  loss_mask_5: 0.218  loss_dice_5: 0.182  loss_ce_6: 0.1283  loss_mask_6: 0.2076  loss_dice_6: 0.1851  loss_ce_7: 0.1283  loss_mask_7: 0.2072  loss_dice_7: 0.1791  loss_ce_8: 0.1281  loss_mask_8: 0.2078  loss_dice_8: 0.1907  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:03:43] d2.utils.events INFO:  eta: 1:23:08  iter: 10679  total_loss: 5.107  loss_ce: 0.1325  loss_mask: 0.1895  loss_dice: 0.1713  loss_ce_0: 0.06843  loss_mask_0: 0.1978  loss_dice_0: 0.1778  loss_ce_1: 0.134  loss_mask_1: 0.1858  loss_dice_1: 0.1775  loss_ce_2: 0.1329  loss_mask_2: 0.1875  loss_dice_2: 0.1769  loss_ce_3: 0.1328  loss_mask_3: 0.1877  loss_dice_3: 0.1705  loss_ce_4: 0.1333  loss_mask_4: 0.1897  loss_dice_4: 0.1792  loss_ce_5: 0.1333  loss_mask_5: 0.1817  loss_dice_5: 0.1772  loss_ce_6: 0.1324  loss_mask_6: 0.194  loss_dice_6: 0.1727  loss_ce_7: 0.1331  loss_mask_7: 0.1947  loss_dice_7: 0.1733  loss_ce_8: 0.1321  loss_mask_8: 0.1881  loss_dice_8: 0.1707  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:03:46] d2.utils.events INFO:  eta: 1:23:06  iter: 10699  total_loss: 4.724  loss_ce: 0.1134  loss_mask: 0.2086  loss_dice: 0.132  loss_ce_0: 0.05912  loss_mask_0: 0.2009  loss_dice_0: 0.1295  loss_ce_1: 0.1136  loss_mask_1: 0.2055  loss_dice_1: 0.1353  loss_ce_2: 0.1133  loss_mask_2: 0.208  loss_dice_2: 0.133  loss_ce_3: 0.1129  loss_mask_3: 0.197  loss_dice_3: 0.1299  loss_ce_4: 0.1118  loss_mask_4: 0.2005  loss_dice_4: 0.1322  loss_ce_5: 0.1131  loss_mask_5: 0.2009  loss_dice_5: 0.1284  loss_ce_6: 0.113  loss_mask_6: 0.2029  loss_dice_6: 0.136  loss_ce_7: 0.1131  loss_mask_7: 0.199  loss_dice_7: 0.1321  loss_ce_8: 0.1132  loss_mask_8: 0.2012  loss_dice_8: 0.1319  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:48] d2.utils.events INFO:  eta: 1:23:08  iter: 10719  total_loss: 4.895  loss_ce: 0.1416  loss_mask: 0.1826  loss_dice: 0.1561  loss_ce_0: 0.06968  loss_mask_0: 0.1944  loss_dice_0: 0.153  loss_ce_1: 0.1414  loss_mask_1: 0.1892  loss_dice_1: 0.1443  loss_ce_2: 0.1413  loss_mask_2: 0.1918  loss_dice_2: 0.1492  loss_ce_3: 0.1409  loss_mask_3: 0.1925  loss_dice_3: 0.1499  loss_ce_4: 0.1409  loss_mask_4: 0.1852  loss_dice_4: 0.1549  loss_ce_5: 0.1409  loss_mask_5: 0.1883  loss_dice_5: 0.1478  loss_ce_6: 0.1412  loss_mask_6: 0.1928  loss_dice_6: 0.1504  loss_ce_7: 0.1412  loss_mask_7: 0.2029  loss_dice_7: 0.1496  loss_ce_8: 0.1417  loss_mask_8: 0.1881  loss_dice_8: 0.1482  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:51] d2.utils.events INFO:  eta: 1:23:07  iter: 10739  total_loss: 5.404  loss_ce: 0.1241  loss_mask: 0.2049  loss_dice: 0.1755  loss_ce_0: 0.06146  loss_mask_0: 0.1924  loss_dice_0: 0.1736  loss_ce_1: 0.124  loss_mask_1: 0.1954  loss_dice_1: 0.1717  loss_ce_2: 0.1246  loss_mask_2: 0.199  loss_dice_2: 0.1788  loss_ce_3: 0.1256  loss_mask_3: 0.1937  loss_dice_3: 0.1773  loss_ce_4: 0.1268  loss_mask_4: 0.1962  loss_dice_4: 0.1748  loss_ce_5: 0.1255  loss_mask_5: 0.1993  loss_dice_5: 0.1732  loss_ce_6: 0.1254  loss_mask_6: 0.1853  loss_dice_6: 0.1655  loss_ce_7: 0.1253  loss_mask_7: 0.1924  loss_dice_7: 0.1745  loss_ce_8: 0.1246  loss_mask_8: 0.1955  loss_dice_8: 0.1711  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:53] d2.utils.events INFO:  eta: 1:23:06  iter: 10759  total_loss: 4.877  loss_ce: 0.1277  loss_mask: 0.1485  loss_dice: 0.1701  loss_ce_0: 0.06439  loss_mask_0: 0.1599  loss_dice_0: 0.1668  loss_ce_1: 0.1277  loss_mask_1: 0.1576  loss_dice_1: 0.1669  loss_ce_2: 0.1278  loss_mask_2: 0.1626  loss_dice_2: 0.1733  loss_ce_3: 0.1279  loss_mask_3: 0.1653  loss_dice_3: 0.1649  loss_ce_4: 0.1279  loss_mask_4: 0.1642  loss_dice_4: 0.1711  loss_ce_5: 0.1279  loss_mask_5: 0.1599  loss_dice_5: 0.1669  loss_ce_6: 0.1278  loss_mask_6: 0.1623  loss_dice_6: 0.1629  loss_ce_7: 0.1278  loss_mask_7: 0.1572  loss_dice_7: 0.1678  loss_ce_8: 0.1278  loss_mask_8: 0.1595  loss_dice_8: 0.1668  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:56] d2.utils.events INFO:  eta: 1:23:05  iter: 10779  total_loss: 5.116  loss_ce: 0.1318  loss_mask: 0.1912  loss_dice: 0.155  loss_ce_0: 0.06629  loss_mask_0: 0.1795  loss_dice_0: 0.163  loss_ce_1: 0.1318  loss_mask_1: 0.1918  loss_dice_1: 0.1533  loss_ce_2: 0.1315  loss_mask_2: 0.1921  loss_dice_2: 0.1528  loss_ce_3: 0.131  loss_mask_3: 0.1922  loss_dice_3: 0.1584  loss_ce_4: 0.1307  loss_mask_4: 0.1957  loss_dice_4: 0.158  loss_ce_5: 0.131  loss_mask_5: 0.1909  loss_dice_5: 0.1631  loss_ce_6: 0.1311  loss_mask_6: 0.1797  loss_dice_6: 0.1604  loss_ce_7: 0.1311  loss_mask_7: 0.184  loss_dice_7: 0.1623  loss_ce_8: 0.1315  loss_mask_8: 0.1969  loss_dice_8: 0.1556  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:03:59] d2.utils.events INFO:  eta: 1:23:04  iter: 10799  total_loss: 5.12  loss_ce: 0.1273  loss_mask: 0.1786  loss_dice: 0.1777  loss_ce_0: 0.06426  loss_mask_0: 0.1713  loss_dice_0: 0.1647  loss_ce_1: 0.1287  loss_mask_1: 0.1703  loss_dice_1: 0.1682  loss_ce_2: 0.1274  loss_mask_2: 0.1693  loss_dice_2: 0.1647  loss_ce_3: 0.1275  loss_mask_3: 0.1728  loss_dice_3: 0.1656  loss_ce_4: 0.1282  loss_mask_4: 0.171  loss_dice_4: 0.1632  loss_ce_5: 0.1275  loss_mask_5: 0.1818  loss_dice_5: 0.1781  loss_ce_6: 0.1275  loss_mask_6: 0.1729  loss_dice_6: 0.1726  loss_ce_7: 0.1282  loss_mask_7: 0.1726  loss_dice_7: 0.174  loss_ce_8: 0.1273  loss_mask_8: 0.1812  loss_dice_8: 0.1702  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:01] d2.utils.events INFO:  eta: 1:23:01  iter: 10819  total_loss: 4.656  loss_ce: 0.1164  loss_mask: 0.1759  loss_dice: 0.1496  loss_ce_0: 0.06271  loss_mask_0: 0.1791  loss_dice_0: 0.1479  loss_ce_1: 0.1164  loss_mask_1: 0.1843  loss_dice_1: 0.1484  loss_ce_2: 0.1162  loss_mask_2: 0.1757  loss_dice_2: 0.158  loss_ce_3: 0.1158  loss_mask_3: 0.1867  loss_dice_3: 0.1572  loss_ce_4: 0.1154  loss_mask_4: 0.1801  loss_dice_4: 0.1562  loss_ce_5: 0.1162  loss_mask_5: 0.1837  loss_dice_5: 0.1516  loss_ce_6: 0.1158  loss_mask_6: 0.179  loss_dice_6: 0.1558  loss_ce_7: 0.116  loss_mask_7: 0.1884  loss_dice_7: 0.1476  loss_ce_8: 0.1158  loss_mask_8: 0.1744  loss_dice_8: 0.1498  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:04] d2.utils.events INFO:  eta: 1:22:59  iter: 10839  total_loss: 4.876  loss_ce: 0.1132  loss_mask: 0.2268  loss_dice: 0.1379  loss_ce_0: 0.06128  loss_mask_0: 0.215  loss_dice_0: 0.134  loss_ce_1: 0.1134  loss_mask_1: 0.2228  loss_dice_1: 0.1438  loss_ce_2: 0.1129  loss_mask_2: 0.2196  loss_dice_2: 0.1421  loss_ce_3: 0.1127  loss_mask_3: 0.2296  loss_dice_3: 0.1448  loss_ce_4: 0.1122  loss_mask_4: 0.2222  loss_dice_4: 0.1337  loss_ce_5: 0.1131  loss_mask_5: 0.2274  loss_dice_5: 0.1396  loss_ce_6: 0.1127  loss_mask_6: 0.2219  loss_dice_6: 0.1373  loss_ce_7: 0.1131  loss_mask_7: 0.2155  loss_dice_7: 0.141  loss_ce_8: 0.1127  loss_mask_8: 0.2175  loss_dice_8: 0.1355  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:06] d2.utils.events INFO:  eta: 1:22:54  iter: 10859  total_loss: 4.365  loss_ce: 0.1033  loss_mask: 0.1833  loss_dice: 0.1195  loss_ce_0: 0.05806  loss_mask_0: 0.1721  loss_dice_0: 0.1189  loss_ce_1: 0.1035  loss_mask_1: 0.1825  loss_dice_1: 0.1124  loss_ce_2: 0.1028  loss_mask_2: 0.1809  loss_dice_2: 0.1185  loss_ce_3: 0.1023  loss_mask_3: 0.1827  loss_dice_3: 0.1193  loss_ce_4: 0.1016  loss_mask_4: 0.1818  loss_dice_4: 0.1203  loss_ce_5: 0.1029  loss_mask_5: 0.1841  loss_dice_5: 0.1212  loss_ce_6: 0.1027  loss_mask_6: 0.1777  loss_dice_6: 0.1191  loss_ce_7: 0.1033  loss_mask_7: 0.1887  loss_dice_7: 0.1269  loss_ce_8: 0.1028  loss_mask_8: 0.1793  loss_dice_8: 0.1222  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:09] d2.utils.events INFO:  eta: 1:22:51  iter: 10879  total_loss: 4.726  loss_ce: 0.09697  loss_mask: 0.2139  loss_dice: 0.1244  loss_ce_0: 0.05502  loss_mask_0: 0.2043  loss_dice_0: 0.1252  loss_ce_1: 0.09835  loss_mask_1: 0.2011  loss_dice_1: 0.1215  loss_ce_2: 0.09708  loss_mask_2: 0.2087  loss_dice_2: 0.1242  loss_ce_3: 0.09708  loss_mask_3: 0.2033  loss_dice_3: 0.1181  loss_ce_4: 0.09723  loss_mask_4: 0.1974  loss_dice_4: 0.1176  loss_ce_5: 0.09704  loss_mask_5: 0.2078  loss_dice_5: 0.1246  loss_ce_6: 0.09656  loss_mask_6: 0.2029  loss_dice_6: 0.1217  loss_ce_7: 0.09738  loss_mask_7: 0.2081  loss_dice_7: 0.1269  loss_ce_8: 0.09645  loss_mask_8: 0.1966  loss_dice_8: 0.1179  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:11] d2.utils.events INFO:  eta: 1:22:49  iter: 10899  total_loss: 5.183  loss_ce: 0.1527  loss_mask: 0.1719  loss_dice: 0.1778  loss_ce_0: 0.07306  loss_mask_0: 0.1689  loss_dice_0: 0.1773  loss_ce_1: 0.1524  loss_mask_1: 0.1629  loss_dice_1: 0.1921  loss_ce_2: 0.1522  loss_mask_2: 0.1645  loss_dice_2: 0.1835  loss_ce_3: 0.1513  loss_mask_3: 0.1664  loss_dice_3: 0.1804  loss_ce_4: 0.1505  loss_mask_4: 0.1613  loss_dice_4: 0.1797  loss_ce_5: 0.1522  loss_mask_5: 0.1623  loss_dice_5: 0.1803  loss_ce_6: 0.1528  loss_mask_6: 0.1743  loss_dice_6: 0.184  loss_ce_7: 0.1528  loss_mask_7: 0.1682  loss_dice_7: 0.1904  loss_ce_8: 0.1531  loss_mask_8: 0.1686  loss_dice_8: 0.1797  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:14] d2.utils.events INFO:  eta: 1:22:47  iter: 10919  total_loss: 4.89  loss_ce: 0.1353  loss_mask: 0.2  loss_dice: 0.1341  loss_ce_0: 0.06949  loss_mask_0: 0.2022  loss_dice_0: 0.1313  loss_ce_1: 0.1357  loss_mask_1: 0.196  loss_dice_1: 0.1316  loss_ce_2: 0.1354  loss_mask_2: 0.2053  loss_dice_2: 0.1331  loss_ce_3: 0.1345  loss_mask_3: 0.2058  loss_dice_3: 0.1323  loss_ce_4: 0.1334  loss_mask_4: 0.2018  loss_dice_4: 0.1338  loss_ce_5: 0.1353  loss_mask_5: 0.1929  loss_dice_5: 0.1308  loss_ce_6: 0.135  loss_mask_6: 0.197  loss_dice_6: 0.1397  loss_ce_7: 0.1353  loss_mask_7: 0.2099  loss_dice_7: 0.1263  loss_ce_8: 0.1352  loss_mask_8: 0.197  loss_dice_8: 0.1359  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:16] d2.utils.events INFO:  eta: 1:22:44  iter: 10939  total_loss: 4.883  loss_ce: 0.122  loss_mask: 0.1788  loss_dice: 0.1765  loss_ce_0: 0.05987  loss_mask_0: 0.1651  loss_dice_0: 0.1712  loss_ce_1: 0.1214  loss_mask_1: 0.1753  loss_dice_1: 0.1837  loss_ce_2: 0.1218  loss_mask_2: 0.181  loss_dice_2: 0.1694  loss_ce_3: 0.1225  loss_mask_3: 0.1823  loss_dice_3: 0.185  loss_ce_4: 0.1236  loss_mask_4: 0.1777  loss_dice_4: 0.18  loss_ce_5: 0.1219  loss_mask_5: 0.1826  loss_dice_5: 0.1773  loss_ce_6: 0.1223  loss_mask_6: 0.1746  loss_dice_6: 0.1775  loss_ce_7: 0.122  loss_mask_7: 0.1731  loss_dice_7: 0.171  loss_ce_8: 0.1221  loss_mask_8: 0.1678  loss_dice_8: 0.1758  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:19] d2.utils.events INFO:  eta: 1:22:41  iter: 10959  total_loss: 4.894  loss_ce: 0.1195  loss_mask: 0.2152  loss_dice: 0.146  loss_ce_0: 0.0597  loss_mask_0: 0.2046  loss_dice_0: 0.1405  loss_ce_1: 0.119  loss_mask_1: 0.2068  loss_dice_1: 0.1475  loss_ce_2: 0.119  loss_mask_2: 0.198  loss_dice_2: 0.1383  loss_ce_3: 0.119  loss_mask_3: 0.2098  loss_dice_3: 0.1435  loss_ce_4: 0.1192  loss_mask_4: 0.2036  loss_dice_4: 0.1462  loss_ce_5: 0.1187  loss_mask_5: 0.2068  loss_dice_5: 0.1397  loss_ce_6: 0.1194  loss_mask_6: 0.1994  loss_dice_6: 0.1442  loss_ce_7: 0.1192  loss_mask_7: 0.2086  loss_dice_7: 0.1442  loss_ce_8: 0.1196  loss_mask_8: 0.2096  loss_dice_8: 0.1458  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:21] d2.utils.events INFO:  eta: 1:22:36  iter: 10979  total_loss: 5.266  loss_ce: 0.1431  loss_mask: 0.2216  loss_dice: 0.1367  loss_ce_0: 0.06993  loss_mask_0: 0.2181  loss_dice_0: 0.1376  loss_ce_1: 0.143  loss_mask_1: 0.2215  loss_dice_1: 0.1316  loss_ce_2: 0.1431  loss_mask_2: 0.2231  loss_dice_2: 0.1335  loss_ce_3: 0.1433  loss_mask_3: 0.2243  loss_dice_3: 0.1322  loss_ce_4: 0.1434  loss_mask_4: 0.2188  loss_dice_4: 0.1335  loss_ce_5: 0.1432  loss_mask_5: 0.2204  loss_dice_5: 0.1351  loss_ce_6: 0.1433  loss_mask_6: 0.2196  loss_dice_6: 0.1345  loss_ce_7: 0.1432  loss_mask_7: 0.2049  loss_dice_7: 0.1327  loss_ce_8: 0.1435  loss_mask_8: 0.2139  loss_dice_8: 0.1325  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:24] d2.utils.events INFO:  eta: 1:22:30  iter: 10999  total_loss: 4.801  loss_ce: 0.1161  loss_mask: 0.2028  loss_dice: 0.1603  loss_ce_0: 0.0594  loss_mask_0: 0.2133  loss_dice_0: 0.1652  loss_ce_1: 0.1162  loss_mask_1: 0.2191  loss_dice_1: 0.165  loss_ce_2: 0.1162  loss_mask_2: 0.2189  loss_dice_2: 0.1625  loss_ce_3: 0.1164  loss_mask_3: 0.2069  loss_dice_3: 0.1603  loss_ce_4: 0.1167  loss_mask_4: 0.2188  loss_dice_4: 0.1582  loss_ce_5: 0.1163  loss_mask_5: 0.2054  loss_dice_5: 0.1554  loss_ce_6: 0.1162  loss_mask_6: 0.2194  loss_dice_6: 0.1613  loss_ce_7: 0.1162  loss_mask_7: 0.2065  loss_dice_7: 0.1599  loss_ce_8: 0.1158  loss_mask_8: 0.2169  loss_dice_8: 0.1609  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:04:27] d2.utils.events INFO:  eta: 1:22:30  iter: 11019  total_loss: 4.961  loss_ce: 0.1449  loss_mask: 0.2132  loss_dice: 0.1385  loss_ce_0: 0.07025  loss_mask_0: 0.2069  loss_dice_0: 0.1383  loss_ce_1: 0.145  loss_mask_1: 0.2072  loss_dice_1: 0.139  loss_ce_2: 0.1453  loss_mask_2: 0.212  loss_dice_2: 0.1418  loss_ce_3: 0.1455  loss_mask_3: 0.2074  loss_dice_3: 0.1429  loss_ce_4: 0.1457  loss_mask_4: 0.2134  loss_dice_4: 0.1398  loss_ce_5: 0.1453  loss_mask_5: 0.2092  loss_dice_5: 0.1405  loss_ce_6: 0.1452  loss_mask_6: 0.2046  loss_dice_6: 0.1435  loss_ce_7: 0.1451  loss_mask_7: 0.2239  loss_dice_7: 0.1392  loss_ce_8: 0.1452  loss_mask_8: 0.2181  loss_dice_8: 0.1407  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:29] d2.utils.events INFO:  eta: 1:22:26  iter: 11039  total_loss: 4.866  loss_ce: 0.1283  loss_mask: 0.2254  loss_dice: 0.1446  loss_ce_0: 0.06431  loss_mask_0: 0.2313  loss_dice_0: 0.1437  loss_ce_1: 0.1284  loss_mask_1: 0.22  loss_dice_1: 0.1392  loss_ce_2: 0.1284  loss_mask_2: 0.2287  loss_dice_2: 0.1403  loss_ce_3: 0.1284  loss_mask_3: 0.229  loss_dice_3: 0.143  loss_ce_4: 0.1284  loss_mask_4: 0.2257  loss_dice_4: 0.1437  loss_ce_5: 0.1284  loss_mask_5: 0.2242  loss_dice_5: 0.1455  loss_ce_6: 0.1284  loss_mask_6: 0.2234  loss_dice_6: 0.1407  loss_ce_7: 0.1283  loss_mask_7: 0.2251  loss_dice_7: 0.1415  loss_ce_8: 0.1285  loss_mask_8: 0.2312  loss_dice_8: 0.1458  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:32] d2.utils.events INFO:  eta: 1:22:24  iter: 11059  total_loss: 4.971  loss_ce: 0.1363  loss_mask: 0.2244  loss_dice: 0.1629  loss_ce_0: 0.06844  loss_mask_0: 0.216  loss_dice_0: 0.1605  loss_ce_1: 0.1364  loss_mask_1: 0.2315  loss_dice_1: 0.1663  loss_ce_2: 0.1364  loss_mask_2: 0.2214  loss_dice_2: 0.1605  loss_ce_3: 0.1358  loss_mask_3: 0.219  loss_dice_3: 0.1584  loss_ce_4: 0.1352  loss_mask_4: 0.2167  loss_dice_4: 0.1606  loss_ce_5: 0.136  loss_mask_5: 0.2277  loss_dice_5: 0.1558  loss_ce_6: 0.136  loss_mask_6: 0.2226  loss_dice_6: 0.1684  loss_ce_7: 0.1363  loss_mask_7: 0.2235  loss_dice_7: 0.1638  loss_ce_8: 0.1364  loss_mask_8: 0.2232  loss_dice_8: 0.1621  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:34] d2.utils.events INFO:  eta: 1:22:17  iter: 11079  total_loss: 4.792  loss_ce: 0.1277  loss_mask: 0.1736  loss_dice: 0.1665  loss_ce_0: 0.06415  loss_mask_0: 0.1709  loss_dice_0: 0.166  loss_ce_1: 0.1272  loss_mask_1: 0.1642  loss_dice_1: 0.1628  loss_ce_2: 0.1277  loss_mask_2: 0.1591  loss_dice_2: 0.1643  loss_ce_3: 0.1279  loss_mask_3: 0.167  loss_dice_3: 0.1677  loss_ce_4: 0.1283  loss_mask_4: 0.1545  loss_dice_4: 0.1749  loss_ce_5: 0.1277  loss_mask_5: 0.1706  loss_dice_5: 0.1689  loss_ce_6: 0.1278  loss_mask_6: 0.1562  loss_dice_6: 0.1675  loss_ce_7: 0.1277  loss_mask_7: 0.166  loss_dice_7: 0.1627  loss_ce_8: 0.1277  loss_mask_8: 0.1684  loss_dice_8: 0.163  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:37] d2.utils.events INFO:  eta: 1:22:11  iter: 11099  total_loss: 4.89  loss_ce: 0.1307  loss_mask: 0.2019  loss_dice: 0.1503  loss_ce_0: 0.06651  loss_mask_0: 0.2105  loss_dice_0: 0.148  loss_ce_1: 0.1309  loss_mask_1: 0.2055  loss_dice_1: 0.1503  loss_ce_2: 0.1309  loss_mask_2: 0.208  loss_dice_2: 0.1497  loss_ce_3: 0.1308  loss_mask_3: 0.2037  loss_dice_3: 0.156  loss_ce_4: 0.1309  loss_mask_4: 0.2044  loss_dice_4: 0.145  loss_ce_5: 0.131  loss_mask_5: 0.2083  loss_dice_5: 0.1458  loss_ce_6: 0.1308  loss_mask_6: 0.2008  loss_dice_6: 0.1439  loss_ce_7: 0.1309  loss_mask_7: 0.1982  loss_dice_7: 0.1487  loss_ce_8: 0.1307  loss_mask_8: 0.2097  loss_dice_8: 0.1497  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:39] d2.utils.events INFO:  eta: 1:22:10  iter: 11119  total_loss: 4.607  loss_ce: 0.1286  loss_mask: 0.2026  loss_dice: 0.1193  loss_ce_0: 0.06372  loss_mask_0: 0.2048  loss_dice_0: 0.123  loss_ce_1: 0.1285  loss_mask_1: 0.2008  loss_dice_1: 0.1237  loss_ce_2: 0.1287  loss_mask_2: 0.2011  loss_dice_2: 0.1182  loss_ce_3: 0.1288  loss_mask_3: 0.2083  loss_dice_3: 0.1264  loss_ce_4: 0.1288  loss_mask_4: 0.2091  loss_dice_4: 0.1204  loss_ce_5: 0.1287  loss_mask_5: 0.2028  loss_dice_5: 0.1203  loss_ce_6: 0.1288  loss_mask_6: 0.2069  loss_dice_6: 0.1189  loss_ce_7: 0.1287  loss_mask_7: 0.2037  loss_dice_7: 0.1195  loss_ce_8: 0.1286  loss_mask_8: 0.1988  loss_dice_8: 0.116  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:42] d2.utils.events INFO:  eta: 1:22:05  iter: 11139  total_loss: 5.12  loss_ce: 0.1298  loss_mask: 0.2318  loss_dice: 0.1545  loss_ce_0: 0.06496  loss_mask_0: 0.2334  loss_dice_0: 0.1566  loss_ce_1: 0.1295  loss_mask_1: 0.2277  loss_dice_1: 0.1616  loss_ce_2: 0.1299  loss_mask_2: 0.2353  loss_dice_2: 0.1623  loss_ce_3: 0.13  loss_mask_3: 0.2341  loss_dice_3: 0.156  loss_ce_4: 0.1302  loss_mask_4: 0.2321  loss_dice_4: 0.1581  loss_ce_5: 0.13  loss_mask_5: 0.2384  loss_dice_5: 0.1564  loss_ce_6: 0.13  loss_mask_6: 0.2323  loss_dice_6: 0.1489  loss_ce_7: 0.1299  loss_mask_7: 0.224  loss_dice_7: 0.1575  loss_ce_8: 0.13  loss_mask_8: 0.2382  loss_dice_8: 0.161  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:44] d2.utils.events INFO:  eta: 1:22:02  iter: 11159  total_loss: 4.777  loss_ce: 0.1301  loss_mask: 0.1767  loss_dice: 0.1799  loss_ce_0: 0.06422  loss_mask_0: 0.1671  loss_dice_0: 0.1658  loss_ce_1: 0.13  loss_mask_1: 0.1756  loss_dice_1: 0.1821  loss_ce_2: 0.1299  loss_mask_2: 0.1793  loss_dice_2: 0.1878  loss_ce_3: 0.1301  loss_mask_3: 0.1829  loss_dice_3: 0.1781  loss_ce_4: 0.1301  loss_mask_4: 0.1873  loss_dice_4: 0.1714  loss_ce_5: 0.1299  loss_mask_5: 0.1864  loss_dice_5: 0.1818  loss_ce_6: 0.1301  loss_mask_6: 0.1751  loss_dice_6: 0.1777  loss_ce_7: 0.13  loss_mask_7: 0.1661  loss_dice_7: 0.1831  loss_ce_8: 0.13  loss_mask_8: 0.1828  loss_dice_8: 0.1819  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:47] d2.utils.events INFO:  eta: 1:21:59  iter: 11179  total_loss: 4.764  loss_ce: 0.1273  loss_mask: 0.1407  loss_dice: 0.1742  loss_ce_0: 0.06397  loss_mask_0: 0.139  loss_dice_0: 0.1771  loss_ce_1: 0.127  loss_mask_1: 0.1535  loss_dice_1: 0.1768  loss_ce_2: 0.1273  loss_mask_2: 0.1461  loss_dice_2: 0.1731  loss_ce_3: 0.1272  loss_mask_3: 0.1526  loss_dice_3: 0.1766  loss_ce_4: 0.1273  loss_mask_4: 0.1472  loss_dice_4: 0.1874  loss_ce_5: 0.1272  loss_mask_5: 0.1491  loss_dice_5: 0.1822  loss_ce_6: 0.1272  loss_mask_6: 0.148  loss_dice_6: 0.179  loss_ce_7: 0.1271  loss_mask_7: 0.1488  loss_dice_7: 0.179  loss_ce_8: 0.1272  loss_mask_8: 0.1546  loss_dice_8: 0.1917  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:04:49] d2.utils.events INFO:  eta: 1:21:58  iter: 11199  total_loss: 5.137  loss_ce: 0.1269  loss_mask: 0.2267  loss_dice: 0.1476  loss_ce_0: 0.06364  loss_mask_0: 0.2177  loss_dice_0: 0.142  loss_ce_1: 0.1269  loss_mask_1: 0.2218  loss_dice_1: 0.146  loss_ce_2: 0.1271  loss_mask_2: 0.2116  loss_dice_2: 0.1462  loss_ce_3: 0.1272  loss_mask_3: 0.2226  loss_dice_3: 0.1417  loss_ce_4: 0.1275  loss_mask_4: 0.2106  loss_dice_4: 0.1426  loss_ce_5: 0.1272  loss_mask_5: 0.22  loss_dice_5: 0.142  loss_ce_6: 0.1272  loss_mask_6: 0.2184  loss_dice_6: 0.1483  loss_ce_7: 0.1273  loss_mask_7: 0.2238  loss_dice_7: 0.143  loss_ce_8: 0.1269  loss_mask_8: 0.2198  loss_dice_8: 0.1433  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:04:52] d2.utils.events INFO:  eta: 1:21:56  iter: 11219  total_loss: 4.951  loss_ce: 0.1226  loss_mask: 0.2496  loss_dice: 0.1348  loss_ce_0: 0.06255  loss_mask_0: 0.2346  loss_dice_0: 0.1267  loss_ce_1: 0.1224  loss_mask_1: 0.2436  loss_dice_1: 0.1313  loss_ce_2: 0.1225  loss_mask_2: 0.2471  loss_dice_2: 0.1305  loss_ce_3: 0.1224  loss_mask_3: 0.2407  loss_dice_3: 0.1337  loss_ce_4: 0.1223  loss_mask_4: 0.2518  loss_dice_4: 0.1342  loss_ce_5: 0.1224  loss_mask_5: 0.2472  loss_dice_5: 0.1362  loss_ce_6: 0.1222  loss_mask_6: 0.2396  loss_dice_6: 0.1314  loss_ce_7: 0.1226  loss_mask_7: 0.244  loss_dice_7: 0.1317  loss_ce_8: 0.1224  loss_mask_8: 0.245  loss_dice_8: 0.1285  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:04:54] d2.utils.events INFO:  eta: 1:21:53  iter: 11239  total_loss: 4.713  loss_ce: 0.1171  loss_mask: 0.188  loss_dice: 0.1654  loss_ce_0: 0.06113  loss_mask_0: 0.1877  loss_dice_0: 0.1632  loss_ce_1: 0.1171  loss_mask_1: 0.1921  loss_dice_1: 0.1577  loss_ce_2: 0.1171  loss_mask_2: 0.1809  loss_dice_2: 0.1665  loss_ce_3: 0.1168  loss_mask_3: 0.1924  loss_dice_3: 0.1668  loss_ce_4: 0.1165  loss_mask_4: 0.1861  loss_dice_4: 0.1648  loss_ce_5: 0.1168  loss_mask_5: 0.1858  loss_dice_5: 0.1627  loss_ce_6: 0.1164  loss_mask_6: 0.1832  loss_dice_6: 0.1656  loss_ce_7: 0.1167  loss_mask_7: 0.1882  loss_dice_7: 0.1588  loss_ce_8: 0.1168  loss_mask_8: 0.1855  loss_dice_8: 0.1605  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:04:57] d2.utils.events INFO:  eta: 1:21:49  iter: 11259  total_loss: 4.729  loss_ce: 0.1488  loss_mask: 0.1779  loss_dice: 0.1443  loss_ce_0: 0.0702  loss_mask_0: 0.1763  loss_dice_0: 0.1358  loss_ce_1: 0.1491  loss_mask_1: 0.1757  loss_dice_1: 0.1433  loss_ce_2: 0.1491  loss_mask_2: 0.1773  loss_dice_2: 0.142  loss_ce_3: 0.1492  loss_mask_3: 0.1775  loss_dice_3: 0.1457  loss_ce_4: 0.1493  loss_mask_4: 0.1774  loss_dice_4: 0.1412  loss_ce_5: 0.149  loss_mask_5: 0.1783  loss_dice_5: 0.1411  loss_ce_6: 0.1492  loss_mask_6: 0.1855  loss_dice_6: 0.1433  loss_ce_7: 0.1489  loss_mask_7: 0.1729  loss_dice_7: 0.1422  loss_ce_8: 0.1489  loss_mask_8: 0.1791  loss_dice_8: 0.1437  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:00] d2.utils.events INFO:  eta: 1:21:47  iter: 11279  total_loss: 5.001  loss_ce: 0.1189  loss_mask: 0.1672  loss_dice: 0.132  loss_ce_0: 0.06065  loss_mask_0: 0.1745  loss_dice_0: 0.1351  loss_ce_1: 0.1188  loss_mask_1: 0.1711  loss_dice_1: 0.139  loss_ce_2: 0.1193  loss_mask_2: 0.1675  loss_dice_2: 0.1422  loss_ce_3: 0.1192  loss_mask_3: 0.1595  loss_dice_3: 0.144  loss_ce_4: 0.1197  loss_mask_4: 0.164  loss_dice_4: 0.1351  loss_ce_5: 0.12  loss_mask_5: 0.1765  loss_dice_5: 0.1397  loss_ce_6: 0.1198  loss_mask_6: 0.1687  loss_dice_6: 0.1402  loss_ce_7: 0.1199  loss_mask_7: 0.1611  loss_dice_7: 0.1367  loss_ce_8: 0.1197  loss_mask_8: 0.1637  loss_dice_8: 0.1374  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:05:02] d2.utils.events INFO:  eta: 1:21:45  iter: 11299  total_loss: 4.888  loss_ce: 0.1189  loss_mask: 0.226  loss_dice: 0.1394  loss_ce_0: 0.06076  loss_mask_0: 0.2223  loss_dice_0: 0.1434  loss_ce_1: 0.1192  loss_mask_1: 0.2264  loss_dice_1: 0.1384  loss_ce_2: 0.1198  loss_mask_2: 0.2249  loss_dice_2: 0.1384  loss_ce_3: 0.1198  loss_mask_3: 0.2235  loss_dice_3: 0.1441  loss_ce_4: 0.1196  loss_mask_4: 0.2152  loss_dice_4: 0.1414  loss_ce_5: 0.1205  loss_mask_5: 0.2166  loss_dice_5: 0.1297  loss_ce_6: 0.1199  loss_mask_6: 0.2253  loss_dice_6: 0.1355  loss_ce_7: 0.1197  loss_mask_7: 0.2233  loss_dice_7: 0.1356  loss_ce_8: 0.1198  loss_mask_8: 0.2237  loss_dice_8: 0.1432  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:05:05] d2.utils.events INFO:  eta: 1:21:44  iter: 11319  total_loss: 4.939  loss_ce: 0.1113  loss_mask: 0.1991  loss_dice: 0.148  loss_ce_0: 0.05849  loss_mask_0: 0.1945  loss_dice_0: 0.1473  loss_ce_1: 0.1111  loss_mask_1: 0.1941  loss_dice_1: 0.1456  loss_ce_2: 0.1093  loss_mask_2: 0.2033  loss_dice_2: 0.1513  loss_ce_3: 0.1082  loss_mask_3: 0.1895  loss_dice_3: 0.1419  loss_ce_4: 0.1063  loss_mask_4: 0.2007  loss_dice_4: 0.1495  loss_ce_5: 0.1071  loss_mask_5: 0.1962  loss_dice_5: 0.1478  loss_ce_6: 0.1065  loss_mask_6: 0.1959  loss_dice_6: 0.1437  loss_ce_7: 0.1083  loss_mask_7: 0.1922  loss_dice_7: 0.1513  loss_ce_8: 0.1092  loss_mask_8: 0.1857  loss_dice_8: 0.1447  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:07] d2.utils.events INFO:  eta: 1:21:37  iter: 11339  total_loss: 5.004  loss_ce: 0.1067  loss_mask: 0.1687  loss_dice: 0.175  loss_ce_0: 0.05715  loss_mask_0: 0.176  loss_dice_0: 0.1714  loss_ce_1: 0.1066  loss_mask_1: 0.1626  loss_dice_1: 0.1765  loss_ce_2: 0.1053  loss_mask_2: 0.1704  loss_dice_2: 0.1796  loss_ce_3: 0.1051  loss_mask_3: 0.1732  loss_dice_3: 0.1747  loss_ce_4: 0.1048  loss_mask_4: 0.1689  loss_dice_4: 0.1682  loss_ce_5: 0.1049  loss_mask_5: 0.1689  loss_dice_5: 0.1692  loss_ce_6: 0.1045  loss_mask_6: 0.1754  loss_dice_6: 0.1742  loss_ce_7: 0.1055  loss_mask_7: 0.1698  loss_dice_7: 0.164  loss_ce_8: 0.105  loss_mask_8: 0.1683  loss_dice_8: 0.1738  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:10] d2.utils.events INFO:  eta: 1:21:32  iter: 11359  total_loss: 5.068  loss_ce: 0.131  loss_mask: 0.1986  loss_dice: 0.1494  loss_ce_0: 0.06437  loss_mask_0: 0.1969  loss_dice_0: 0.1451  loss_ce_1: 0.131  loss_mask_1: 0.1961  loss_dice_1: 0.1444  loss_ce_2: 0.1313  loss_mask_2: 0.193  loss_dice_2: 0.1427  loss_ce_3: 0.1313  loss_mask_3: 0.1941  loss_dice_3: 0.1451  loss_ce_4: 0.1312  loss_mask_4: 0.183  loss_dice_4: 0.1412  loss_ce_5: 0.1312  loss_mask_5: 0.1888  loss_dice_5: 0.1405  loss_ce_6: 0.1312  loss_mask_6: 0.1954  loss_dice_6: 0.1494  loss_ce_7: 0.1312  loss_mask_7: 0.1962  loss_dice_7: 0.1482  loss_ce_8: 0.1313  loss_mask_8: 0.1893  loss_dice_8: 0.1399  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:12] d2.utils.events INFO:  eta: 1:21:30  iter: 11379  total_loss: 5.442  loss_ce: 0.1301  loss_mask: 0.2176  loss_dice: 0.1624  loss_ce_0: 0.06467  loss_mask_0: 0.2244  loss_dice_0: 0.1683  loss_ce_1: 0.1303  loss_mask_1: 0.2188  loss_dice_1: 0.1627  loss_ce_2: 0.1301  loss_mask_2: 0.2232  loss_dice_2: 0.1682  loss_ce_3: 0.1301  loss_mask_3: 0.2159  loss_dice_3: 0.1644  loss_ce_4: 0.1301  loss_mask_4: 0.22  loss_dice_4: 0.1752  loss_ce_5: 0.1299  loss_mask_5: 0.2132  loss_dice_5: 0.165  loss_ce_6: 0.1299  loss_mask_6: 0.2145  loss_dice_6: 0.1638  loss_ce_7: 0.1301  loss_mask_7: 0.2284  loss_dice_7: 0.1691  loss_ce_8: 0.1302  loss_mask_8: 0.2147  loss_dice_8: 0.1655  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:15] d2.utils.events INFO:  eta: 1:21:29  iter: 11399  total_loss: 5.033  loss_ce: 0.1516  loss_mask: 0.2279  loss_dice: 0.1331  loss_ce_0: 0.07378  loss_mask_0: 0.222  loss_dice_0: 0.1359  loss_ce_1: 0.1518  loss_mask_1: 0.2243  loss_dice_1: 0.1362  loss_ce_2: 0.1509  loss_mask_2: 0.23  loss_dice_2: 0.1355  loss_ce_3: 0.1495  loss_mask_3: 0.2234  loss_dice_3: 0.1333  loss_ce_4: 0.148  loss_mask_4: 0.2266  loss_dice_4: 0.1343  loss_ce_5: 0.1492  loss_mask_5: 0.2246  loss_dice_5: 0.1356  loss_ce_6: 0.1485  loss_mask_6: 0.2279  loss_dice_6: 0.1344  loss_ce_7: 0.1497  loss_mask_7: 0.2245  loss_dice_7: 0.1377  loss_ce_8: 0.1508  loss_mask_8: 0.2177  loss_dice_8: 0.1359  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:17] d2.utils.events INFO:  eta: 1:21:27  iter: 11419  total_loss: 4.744  loss_ce: 0.1102  loss_mask: 0.1563  loss_dice: 0.1729  loss_ce_0: 0.05617  loss_mask_0: 0.144  loss_dice_0: 0.1697  loss_ce_1: 0.1098  loss_mask_1: 0.146  loss_dice_1: 0.1629  loss_ce_2: 0.1111  loss_mask_2: 0.1569  loss_dice_2: 0.1649  loss_ce_3: 0.1125  loss_mask_3: 0.1357  loss_dice_3: 0.1707  loss_ce_4: 0.1136  loss_mask_4: 0.1455  loss_dice_4: 0.1604  loss_ce_5: 0.1126  loss_mask_5: 0.1441  loss_dice_5: 0.1618  loss_ce_6: 0.1135  loss_mask_6: 0.1482  loss_dice_6: 0.1688  loss_ce_7: 0.1118  loss_mask_7: 0.1524  loss_dice_7: 0.1724  loss_ce_8: 0.1113  loss_mask_8: 0.1514  loss_dice_8: 0.1722  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:20] d2.utils.events INFO:  eta: 1:21:25  iter: 11439  total_loss: 4.857  loss_ce: 0.09846  loss_mask: 0.1814  loss_dice: 0.1283  loss_ce_0: 0.05345  loss_mask_0: 0.1962  loss_dice_0: 0.1422  loss_ce_1: 0.09885  loss_mask_1: 0.1854  loss_dice_1: 0.1341  loss_ce_2: 0.09792  loss_mask_2: 0.1837  loss_dice_2: 0.129  loss_ce_3: 0.09703  loss_mask_3: 0.1907  loss_dice_3: 0.1343  loss_ce_4: 0.09614  loss_mask_4: 0.1857  loss_dice_4: 0.1371  loss_ce_5: 0.09614  loss_mask_5: 0.1833  loss_dice_5: 0.1291  loss_ce_6: 0.09643  loss_mask_6: 0.1924  loss_dice_6: 0.1353  loss_ce_7: 0.09692  loss_mask_7: 0.1878  loss_dice_7: 0.1319  loss_ce_8: 0.09825  loss_mask_8: 0.1897  loss_dice_8: 0.1356  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:05:22] d2.utils.events INFO:  eta: 1:21:25  iter: 11459  total_loss: 4.969  loss_ce: 0.08916  loss_mask: 0.1991  loss_dice: 0.1437  loss_ce_0: 0.0513  loss_mask_0: 0.1905  loss_dice_0: 0.1557  loss_ce_1: 0.08979  loss_mask_1: 0.1956  loss_dice_1: 0.1487  loss_ce_2: 0.08831  loss_mask_2: 0.1951  loss_dice_2: 0.1576  loss_ce_3: 0.08721  loss_mask_3: 0.2038  loss_dice_3: 0.1497  loss_ce_4: 0.08643  loss_mask_4: 0.1899  loss_dice_4: 0.1507  loss_ce_5: 0.08667  loss_mask_5: 0.1909  loss_dice_5: 0.1515  loss_ce_6: 0.08626  loss_mask_6: 0.2012  loss_dice_6: 0.1547  loss_ce_7: 0.08735  loss_mask_7: 0.1948  loss_dice_7: 0.1527  loss_ce_8: 0.08831  loss_mask_8: 0.2028  loss_dice_8: 0.1471  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:25] d2.utils.events INFO:  eta: 1:21:24  iter: 11479  total_loss: 4.64  loss_ce: 0.08416  loss_mask: 0.2045  loss_dice: 0.1246  loss_ce_0: 0.04934  loss_mask_0: 0.2226  loss_dice_0: 0.1212  loss_ce_1: 0.08404  loss_mask_1: 0.214  loss_dice_1: 0.1206  loss_ce_2: 0.08349  loss_mask_2: 0.2159  loss_dice_2: 0.1263  loss_ce_3: 0.08337  loss_mask_3: 0.2173  loss_dice_3: 0.1265  loss_ce_4: 0.08358  loss_mask_4: 0.224  loss_dice_4: 0.126  loss_ce_5: 0.08366  loss_mask_5: 0.2066  loss_dice_5: 0.1271  loss_ce_6: 0.08323  loss_mask_6: 0.2246  loss_dice_6: 0.1222  loss_ce_7: 0.08364  loss_mask_7: 0.2173  loss_dice_7: 0.1328  loss_ce_8: 0.08353  loss_mask_8: 0.215  loss_dice_8: 0.1222  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:05:27] d2.utils.events INFO:  eta: 1:21:20  iter: 11499  total_loss: 5.121  loss_ce: 0.07914  loss_mask: 0.2093  loss_dice: 0.1396  loss_ce_0: 0.04776  loss_mask_0: 0.2113  loss_dice_0: 0.1388  loss_ce_1: 0.07924  loss_mask_1: 0.2148  loss_dice_1: 0.141  loss_ce_2: 0.07868  loss_mask_2: 0.2162  loss_dice_2: 0.1407  loss_ce_3: 0.07841  loss_mask_3: 0.2058  loss_dice_3: 0.1411  loss_ce_4: 0.07858  loss_mask_4: 0.2062  loss_dice_4: 0.1446  loss_ce_5: 0.07866  loss_mask_5: 0.2143  loss_dice_5: 0.1401  loss_ce_6: 0.07817  loss_mask_6: 0.208  loss_dice_6: 0.1401  loss_ce_7: 0.07864  loss_mask_7: 0.2089  loss_dice_7: 0.1405  loss_ce_8: 0.07866  loss_mask_8: 0.2198  loss_dice_8: 0.1384  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:05:30] d2.utils.events INFO:  eta: 1:21:15  iter: 11519  total_loss: 4.747  loss_ce: 0.168  loss_mask: 0.2026  loss_dice: 0.1393  loss_ce_0: 0.08134  loss_mask_0: 0.2116  loss_dice_0: 0.1338  loss_ce_1: 0.1691  loss_mask_1: 0.1988  loss_dice_1: 0.1342  loss_ce_2: 0.1679  loss_mask_2: 0.1976  loss_dice_2: 0.1306  loss_ce_3: 0.1658  loss_mask_3: 0.2008  loss_dice_3: 0.1312  loss_ce_4: 0.1642  loss_mask_4: 0.2022  loss_dice_4: 0.1306  loss_ce_5: 0.1651  loss_mask_5: 0.2033  loss_dice_5: 0.1321  loss_ce_6: 0.1641  loss_mask_6: 0.2052  loss_dice_6: 0.1332  loss_ce_7: 0.1658  loss_mask_7: 0.1991  loss_dice_7: 0.1343  loss_ce_8: 0.1672  loss_mask_8: 0.1998  loss_dice_8: 0.135  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:05:32] d2.utils.events INFO:  eta: 1:21:11  iter: 11539  total_loss: 4.693  loss_ce: 0.1073  loss_mask: 0.2008  loss_dice: 0.1416  loss_ce_0: 0.053  loss_mask_0: 0.2002  loss_dice_0: 0.146  loss_ce_1: 0.1058  loss_mask_1: 0.2008  loss_dice_1: 0.1443  loss_ce_2: 0.1082  loss_mask_2: 0.2045  loss_dice_2: 0.1446  loss_ce_3: 0.1104  loss_mask_3: 0.2019  loss_dice_3: 0.1427  loss_ce_4: 0.1122  loss_mask_4: 0.2053  loss_dice_4: 0.1393  loss_ce_5: 0.1111  loss_mask_5: 0.2075  loss_dice_5: 0.1464  loss_ce_6: 0.1124  loss_mask_6: 0.2134  loss_dice_6: 0.1452  loss_ce_7: 0.1104  loss_mask_7: 0.1978  loss_dice_7: 0.1427  loss_ce_8: 0.1088  loss_mask_8: 0.2071  loss_dice_8: 0.1375  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:05:35] d2.utils.events INFO:  eta: 1:21:05  iter: 11559  total_loss: 4.959  loss_ce: 0.1285  loss_mask: 0.1917  loss_dice: 0.1785  loss_ce_0: 0.0648  loss_mask_0: 0.1843  loss_dice_0: 0.1805  loss_ce_1: 0.1288  loss_mask_1: 0.1841  loss_dice_1: 0.1859  loss_ce_2: 0.1285  loss_mask_2: 0.191  loss_dice_2: 0.1892  loss_ce_3: 0.1285  loss_mask_3: 0.1853  loss_dice_3: 0.1855  loss_ce_4: 0.1285  loss_mask_4: 0.1921  loss_dice_4: 0.181  loss_ce_5: 0.1285  loss_mask_5: 0.1976  loss_dice_5: 0.1843  loss_ce_6: 0.1285  loss_mask_6: 0.1895  loss_dice_6: 0.1874  loss_ce_7: 0.1286  loss_mask_7: 0.1976  loss_dice_7: 0.1826  loss_ce_8: 0.1285  loss_mask_8: 0.1972  loss_dice_8: 0.1876  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:37] d2.utils.events INFO:  eta: 1:21:02  iter: 11579  total_loss: 4.552  loss_ce: 0.1088  loss_mask: 0.1774  loss_dice: 0.1517  loss_ce_0: 0.05428  loss_mask_0: 0.1852  loss_dice_0: 0.153  loss_ce_1: 0.1107  loss_mask_1: 0.1818  loss_dice_1: 0.1396  loss_ce_2: 0.11  loss_mask_2: 0.1846  loss_dice_2: 0.1442  loss_ce_3: 0.1084  loss_mask_3: 0.1752  loss_dice_3: 0.1525  loss_ce_4: 0.1086  loss_mask_4: 0.1913  loss_dice_4: 0.1423  loss_ce_5: 0.1089  loss_mask_5: 0.1806  loss_dice_5: 0.1542  loss_ce_6: 0.1093  loss_mask_6: 0.1776  loss_dice_6: 0.1488  loss_ce_7: 0.1086  loss_mask_7: 0.1802  loss_dice_7: 0.1481  loss_ce_8: 0.1091  loss_mask_8: 0.1867  loss_dice_8: 0.1534  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:40] d2.utils.events INFO:  eta: 1:21:00  iter: 11599  total_loss: 5.411  loss_ce: 0.1073  loss_mask: 0.2001  loss_dice: 0.1772  loss_ce_0: 0.05493  loss_mask_0: 0.1834  loss_dice_0: 0.1648  loss_ce_1: 0.1077  loss_mask_1: 0.1882  loss_dice_1: 0.1788  loss_ce_2: 0.1077  loss_mask_2: 0.1782  loss_dice_2: 0.1668  loss_ce_3: 0.1075  loss_mask_3: 0.1815  loss_dice_3: 0.1685  loss_ce_4: 0.108  loss_mask_4: 0.1878  loss_dice_4: 0.1734  loss_ce_5: 0.1077  loss_mask_5: 0.1959  loss_dice_5: 0.1727  loss_ce_6: 0.1077  loss_mask_6: 0.1898  loss_dice_6: 0.1689  loss_ce_7: 0.1072  loss_mask_7: 0.1929  loss_dice_7: 0.1746  loss_ce_8: 0.1075  loss_mask_8: 0.1841  loss_dice_8: 0.1651  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:43] d2.utils.events INFO:  eta: 1:20:59  iter: 11619  total_loss: 4.831  loss_ce: 0.1007  loss_mask: 0.2231  loss_dice: 0.1325  loss_ce_0: 0.05357  loss_mask_0: 0.2312  loss_dice_0: 0.1337  loss_ce_1: 0.1003  loss_mask_1: 0.2277  loss_dice_1: 0.1354  loss_ce_2: 0.09932  loss_mask_2: 0.2275  loss_dice_2: 0.1278  loss_ce_3: 0.09941  loss_mask_3: 0.228  loss_dice_3: 0.1369  loss_ce_4: 0.09958  loss_mask_4: 0.2238  loss_dice_4: 0.1322  loss_ce_5: 0.09987  loss_mask_5: 0.2261  loss_dice_5: 0.1284  loss_ce_6: 0.09963  loss_mask_6: 0.2229  loss_dice_6: 0.1325  loss_ce_7: 0.09979  loss_mask_7: 0.2364  loss_dice_7: 0.1303  loss_ce_8: 0.1004  loss_mask_8: 0.2213  loss_dice_8: 0.136  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:05:45] d2.utils.events INFO:  eta: 1:20:57  iter: 11639  total_loss: 5.624  loss_ce: 0.1319  loss_mask: 0.2239  loss_dice: 0.1507  loss_ce_0: 0.06522  loss_mask_0: 0.2282  loss_dice_0: 0.1484  loss_ce_1: 0.1319  loss_mask_1: 0.2222  loss_dice_1: 0.1514  loss_ce_2: 0.1319  loss_mask_2: 0.2221  loss_dice_2: 0.1539  loss_ce_3: 0.1318  loss_mask_3: 0.217  loss_dice_3: 0.1567  loss_ce_4: 0.1319  loss_mask_4: 0.2216  loss_dice_4: 0.1558  loss_ce_5: 0.1319  loss_mask_5: 0.2257  loss_dice_5: 0.1509  loss_ce_6: 0.1321  loss_mask_6: 0.2214  loss_dice_6: 0.1491  loss_ce_7: 0.1321  loss_mask_7: 0.2232  loss_dice_7: 0.1515  loss_ce_8: 0.1319  loss_mask_8: 0.234  loss_dice_8: 0.1516  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:48] d2.utils.events INFO:  eta: 1:20:58  iter: 11659  total_loss: 5.648  loss_ce: 0.1606  loss_mask: 0.2289  loss_dice: 0.1734  loss_ce_0: 0.07685  loss_mask_0: 0.2261  loss_dice_0: 0.177  loss_ce_1: 0.1593  loss_mask_1: 0.2334  loss_dice_1: 0.182  loss_ce_2: 0.1586  loss_mask_2: 0.2249  loss_dice_2: 0.1765  loss_ce_3: 0.1575  loss_mask_3: 0.2294  loss_dice_3: 0.1724  loss_ce_4: 0.1577  loss_mask_4: 0.2354  loss_dice_4: 0.1732  loss_ce_5: 0.1589  loss_mask_5: 0.2239  loss_dice_5: 0.1743  loss_ce_6: 0.1586  loss_mask_6: 0.2125  loss_dice_6: 0.1716  loss_ce_7: 0.1592  loss_mask_7: 0.2258  loss_dice_7: 0.1761  loss_ce_8: 0.1598  loss_mask_8: 0.2283  loss_dice_8: 0.1732  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:05:50] d2.utils.events INFO:  eta: 1:20:57  iter: 11679  total_loss: 5.312  loss_ce: 0.1121  loss_mask: 0.2183  loss_dice: 0.1649  loss_ce_0: 0.05612  loss_mask_0: 0.2273  loss_dice_0: 0.1677  loss_ce_1: 0.1134  loss_mask_1: 0.2138  loss_dice_1: 0.1663  loss_ce_2: 0.1151  loss_mask_2: 0.2342  loss_dice_2: 0.1692  loss_ce_3: 0.1165  loss_mask_3: 0.233  loss_dice_3: 0.1695  loss_ce_4: 0.1166  loss_mask_4: 0.2269  loss_dice_4: 0.1662  loss_ce_5: 0.115  loss_mask_5: 0.2296  loss_dice_5: 0.1727  loss_ce_6: 0.1159  loss_mask_6: 0.2274  loss_dice_6: 0.1642  loss_ce_7: 0.1146  loss_mask_7: 0.229  loss_dice_7: 0.1662  loss_ce_8: 0.1135  loss_mask_8: 0.2232  loss_dice_8: 0.1668  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:53] d2.utils.events INFO:  eta: 1:20:53  iter: 11699  total_loss: 4.785  loss_ce: 0.1092  loss_mask: 0.226  loss_dice: 0.1308  loss_ce_0: 0.05537  loss_mask_0: 0.2247  loss_dice_0: 0.1325  loss_ce_1: 0.1087  loss_mask_1: 0.2217  loss_dice_1: 0.1316  loss_ce_2: 0.1097  loss_mask_2: 0.2267  loss_dice_2: 0.1352  loss_ce_3: 0.1102  loss_mask_3: 0.2269  loss_dice_3: 0.1363  loss_ce_4: 0.1099  loss_mask_4: 0.227  loss_dice_4: 0.1372  loss_ce_5: 0.1096  loss_mask_5: 0.233  loss_dice_5: 0.141  loss_ce_6: 0.1107  loss_mask_6: 0.2315  loss_dice_6: 0.134  loss_ce_7: 0.1101  loss_mask_7: 0.2245  loss_dice_7: 0.1366  loss_ce_8: 0.1101  loss_mask_8: 0.2319  loss_dice_8: 0.136  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:55] d2.utils.events INFO:  eta: 1:20:50  iter: 11719  total_loss: 4.92  loss_ce: 0.1403  loss_mask: 0.1954  loss_dice: 0.1306  loss_ce_0: 0.07096  loss_mask_0: 0.1812  loss_dice_0: 0.1325  loss_ce_1: 0.1391  loss_mask_1: 0.1887  loss_dice_1: 0.1358  loss_ce_2: 0.1386  loss_mask_2: 0.1885  loss_dice_2: 0.1306  loss_ce_3: 0.1384  loss_mask_3: 0.1959  loss_dice_3: 0.1336  loss_ce_4: 0.1381  loss_mask_4: 0.1807  loss_dice_4: 0.1326  loss_ce_5: 0.1389  loss_mask_5: 0.1881  loss_dice_5: 0.1312  loss_ce_6: 0.1387  loss_mask_6: 0.1918  loss_dice_6: 0.1283  loss_ce_7: 0.1395  loss_mask_7: 0.1829  loss_dice_7: 0.1344  loss_ce_8: 0.1395  loss_mask_8: 0.1843  loss_dice_8: 0.1369  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:05:58] d2.utils.events INFO:  eta: 1:20:47  iter: 11739  total_loss: 5.022  loss_ce: 0.1161  loss_mask: 0.2433  loss_dice: 0.1568  loss_ce_0: 0.05806  loss_mask_0: 0.2431  loss_dice_0: 0.1532  loss_ce_1: 0.1168  loss_mask_1: 0.2518  loss_dice_1: 0.1563  loss_ce_2: 0.1174  loss_mask_2: 0.2539  loss_dice_2: 0.1497  loss_ce_3: 0.1176  loss_mask_3: 0.2411  loss_dice_3: 0.1491  loss_ce_4: 0.1178  loss_mask_4: 0.2586  loss_dice_4: 0.1566  loss_ce_5: 0.1173  loss_mask_5: 0.2413  loss_dice_5: 0.1607  loss_ce_6: 0.1174  loss_mask_6: 0.2432  loss_dice_6: 0.1556  loss_ce_7: 0.1167  loss_mask_7: 0.2465  loss_dice_7: 0.1565  loss_ce_8: 0.1168  loss_mask_8: 0.248  loss_dice_8: 0.1553  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:00] d2.utils.events INFO:  eta: 1:20:44  iter: 11759  total_loss: 5.002  loss_ce: 0.1097  loss_mask: 0.2076  loss_dice: 0.1651  loss_ce_0: 0.05669  loss_mask_0: 0.216  loss_dice_0: 0.1678  loss_ce_1: 0.1093  loss_mask_1: 0.2172  loss_dice_1: 0.1609  loss_ce_2: 0.1094  loss_mask_2: 0.2161  loss_dice_2: 0.1693  loss_ce_3: 0.1093  loss_mask_3: 0.2146  loss_dice_3: 0.1618  loss_ce_4: 0.1092  loss_mask_4: 0.2092  loss_dice_4: 0.1641  loss_ce_5: 0.1095  loss_mask_5: 0.2137  loss_dice_5: 0.1617  loss_ce_6: 0.1095  loss_mask_6: 0.2166  loss_dice_6: 0.1571  loss_ce_7: 0.1095  loss_mask_7: 0.2149  loss_dice_7: 0.1601  loss_ce_8: 0.11  loss_mask_8: 0.2233  loss_dice_8: 0.163  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:03] d2.utils.events INFO:  eta: 1:20:38  iter: 11779  total_loss: 4.841  loss_ce: 0.1067  loss_mask: 0.1864  loss_dice: 0.1522  loss_ce_0: 0.05617  loss_mask_0: 0.1872  loss_dice_0: 0.1518  loss_ce_1: 0.1069  loss_mask_1: 0.1922  loss_dice_1: 0.1576  loss_ce_2: 0.1071  loss_mask_2: 0.1881  loss_dice_2: 0.1545  loss_ce_3: 0.1071  loss_mask_3: 0.1854  loss_dice_3: 0.1527  loss_ce_4: 0.1072  loss_mask_4: 0.1902  loss_dice_4: 0.1533  loss_ce_5: 0.107  loss_mask_5: 0.1918  loss_dice_5: 0.1554  loss_ce_6: 0.1068  loss_mask_6: 0.1793  loss_dice_6: 0.1536  loss_ce_7: 0.1067  loss_mask_7: 0.193  loss_dice_7: 0.152  loss_ce_8: 0.1069  loss_mask_8: 0.1857  loss_dice_8: 0.1565  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:05] d2.utils.events INFO:  eta: 1:20:34  iter: 11799  total_loss: 4.937  loss_ce: 0.1069  loss_mask: 0.2067  loss_dice: 0.1496  loss_ce_0: 0.05619  loss_mask_0: 0.2022  loss_dice_0: 0.148  loss_ce_1: 0.1069  loss_mask_1: 0.2024  loss_dice_1: 0.1492  loss_ce_2: 0.1072  loss_mask_2: 0.2055  loss_dice_2: 0.1471  loss_ce_3: 0.1072  loss_mask_3: 0.202  loss_dice_3: 0.1509  loss_ce_4: 0.1073  loss_mask_4: 0.2012  loss_dice_4: 0.1494  loss_ce_5: 0.1072  loss_mask_5: 0.203  loss_dice_5: 0.1474  loss_ce_6: 0.107  loss_mask_6: 0.2088  loss_dice_6: 0.1553  loss_ce_7: 0.1069  loss_mask_7: 0.2024  loss_dice_7: 0.1506  loss_ce_8: 0.107  loss_mask_8: 0.2034  loss_dice_8: 0.1492  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:08] d2.utils.events INFO:  eta: 1:20:32  iter: 11819  total_loss: 4.991  loss_ce: 0.14  loss_mask: 0.2311  loss_dice: 0.1303  loss_ce_0: 0.07001  loss_mask_0: 0.2212  loss_dice_0: 0.1272  loss_ce_1: 0.1393  loss_mask_1: 0.23  loss_dice_1: 0.1259  loss_ce_2: 0.1389  loss_mask_2: 0.2253  loss_dice_2: 0.124  loss_ce_3: 0.1385  loss_mask_3: 0.2303  loss_dice_3: 0.1233  loss_ce_4: 0.138  loss_mask_4: 0.2255  loss_dice_4: 0.126  loss_ce_5: 0.1387  loss_mask_5: 0.2336  loss_dice_5: 0.1288  loss_ce_6: 0.1386  loss_mask_6: 0.223  loss_dice_6: 0.1251  loss_ce_7: 0.1393  loss_mask_7: 0.2311  loss_dice_7: 0.1307  loss_ce_8: 0.1396  loss_mask_8: 0.2239  loss_dice_8: 0.1271  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:11] d2.utils.events INFO:  eta: 1:20:29  iter: 11839  total_loss: 5.405  loss_ce: 0.131  loss_mask: 0.1667  loss_dice: 0.1786  loss_ce_0: 0.06698  loss_mask_0: 0.1652  loss_dice_0: 0.1762  loss_ce_1: 0.1314  loss_mask_1: 0.1652  loss_dice_1: 0.1804  loss_ce_2: 0.132  loss_mask_2: 0.1644  loss_dice_2: 0.1784  loss_ce_3: 0.132  loss_mask_3: 0.1695  loss_dice_3: 0.1782  loss_ce_4: 0.132  loss_mask_4: 0.1737  loss_dice_4: 0.1817  loss_ce_5: 0.1318  loss_mask_5: 0.1745  loss_dice_5: 0.1772  loss_ce_6: 0.1319  loss_mask_6: 0.1667  loss_dice_6: 0.1763  loss_ce_7: 0.1312  loss_mask_7: 0.1624  loss_dice_7: 0.175  loss_ce_8: 0.1314  loss_mask_8: 0.1568  loss_dice_8: 0.1783  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:13] d2.utils.events INFO:  eta: 1:20:27  iter: 11859  total_loss: 5.318  loss_ce: 0.1289  loss_mask: 0.1199  loss_dice: 0.2348  loss_ce_0: 0.06233  loss_mask_0: 0.1257  loss_dice_0: 0.2508  loss_ce_1: 0.129  loss_mask_1: 0.1189  loss_dice_1: 0.2472  loss_ce_2: 0.1295  loss_mask_2: 0.1179  loss_dice_2: 0.25  loss_ce_3: 0.1296  loss_mask_3: 0.1229  loss_dice_3: 0.2593  loss_ce_4: 0.1298  loss_mask_4: 0.1159  loss_dice_4: 0.2307  loss_ce_5: 0.1295  loss_mask_5: 0.1251  loss_dice_5: 0.2473  loss_ce_6: 0.1295  loss_mask_6: 0.1269  loss_dice_6: 0.2451  loss_ce_7: 0.1293  loss_mask_7: 0.1363  loss_dice_7: 0.2543  loss_ce_8: 0.1292  loss_mask_8: 0.1289  loss_dice_8: 0.246  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:16] d2.utils.events INFO:  eta: 1:20:26  iter: 11879  total_loss: 4.901  loss_ce: 0.1223  loss_mask: 0.2229  loss_dice: 0.1377  loss_ce_0: 0.06086  loss_mask_0: 0.2338  loss_dice_0: 0.1387  loss_ce_1: 0.1217  loss_mask_1: 0.2313  loss_dice_1: 0.1402  loss_ce_2: 0.1222  loss_mask_2: 0.2323  loss_dice_2: 0.1408  loss_ce_3: 0.122  loss_mask_3: 0.2234  loss_dice_3: 0.1412  loss_ce_4: 0.1217  loss_mask_4: 0.2303  loss_dice_4: 0.1382  loss_ce_5: 0.122  loss_mask_5: 0.2255  loss_dice_5: 0.1344  loss_ce_6: 0.1225  loss_mask_6: 0.2296  loss_dice_6: 0.141  loss_ce_7: 0.1225  loss_mask_7: 0.2276  loss_dice_7: 0.1414  loss_ce_8: 0.1226  loss_mask_8: 0.2289  loss_dice_8: 0.14  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:18] d2.utils.events INFO:  eta: 1:20:23  iter: 11899  total_loss: 4.796  loss_ce: 0.1434  loss_mask: 0.202  loss_dice: 0.1328  loss_ce_0: 0.06973  loss_mask_0: 0.2029  loss_dice_0: 0.1264  loss_ce_1: 0.1438  loss_mask_1: 0.204  loss_dice_1: 0.1338  loss_ce_2: 0.1436  loss_mask_2: 0.1992  loss_dice_2: 0.1315  loss_ce_3: 0.1439  loss_mask_3: 0.2017  loss_dice_3: 0.1375  loss_ce_4: 0.1443  loss_mask_4: 0.2024  loss_dice_4: 0.1317  loss_ce_5: 0.144  loss_mask_5: 0.2045  loss_dice_5: 0.1343  loss_ce_6: 0.1441  loss_mask_6: 0.2018  loss_dice_6: 0.1319  loss_ce_7: 0.1438  loss_mask_7: 0.1946  loss_dice_7: 0.1317  loss_ce_8: 0.1434  loss_mask_8: 0.2062  loss_dice_8: 0.1328  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:21] d2.utils.events INFO:  eta: 1:20:19  iter: 11919  total_loss: 4.686  loss_ce: 0.1279  loss_mask: 0.161  loss_dice: 0.1369  loss_ce_0: 0.06553  loss_mask_0: 0.1586  loss_dice_0: 0.1357  loss_ce_1: 0.1279  loss_mask_1: 0.1679  loss_dice_1: 0.1382  loss_ce_2: 0.1277  loss_mask_2: 0.161  loss_dice_2: 0.1335  loss_ce_3: 0.1273  loss_mask_3: 0.155  loss_dice_3: 0.1383  loss_ce_4: 0.1267  loss_mask_4: 0.1563  loss_dice_4: 0.1358  loss_ce_5: 0.1275  loss_mask_5: 0.1618  loss_dice_5: 0.1387  loss_ce_6: 0.1276  loss_mask_6: 0.1552  loss_dice_6: 0.1355  loss_ce_7: 0.128  loss_mask_7: 0.1611  loss_dice_7: 0.1328  loss_ce_8: 0.1279  loss_mask_8: 0.1649  loss_dice_8: 0.135  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:23] d2.utils.events INFO:  eta: 1:20:16  iter: 11939  total_loss: 4.628  loss_ce: 0.1184  loss_mask: 0.182  loss_dice: 0.1386  loss_ce_0: 0.06292  loss_mask_0: 0.1792  loss_dice_0: 0.1387  loss_ce_1: 0.1175  loss_mask_1: 0.1874  loss_dice_1: 0.1454  loss_ce_2: 0.117  loss_mask_2: 0.1781  loss_dice_2: 0.1395  loss_ce_3: 0.1165  loss_mask_3: 0.1866  loss_dice_3: 0.1429  loss_ce_4: 0.1157  loss_mask_4: 0.1864  loss_dice_4: 0.1384  loss_ce_5: 0.1167  loss_mask_5: 0.1885  loss_dice_5: 0.139  loss_ce_6: 0.1164  loss_mask_6: 0.1825  loss_dice_6: 0.1451  loss_ce_7: 0.1173  loss_mask_7: 0.1904  loss_dice_7: 0.1418  loss_ce_8: 0.1178  loss_mask_8: 0.1838  loss_dice_8: 0.1435  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:26] d2.utils.events INFO:  eta: 1:20:11  iter: 11959  total_loss: 4.997  loss_ce: 0.1536  loss_mask: 0.1904  loss_dice: 0.1357  loss_ce_0: 0.06907  loss_mask_0: 0.1938  loss_dice_0: 0.1356  loss_ce_1: 0.1554  loss_mask_1: 0.1931  loss_dice_1: 0.1295  loss_ce_2: 0.1562  loss_mask_2: 0.193  loss_dice_2: 0.1239  loss_ce_3: 0.1569  loss_mask_3: 0.1879  loss_dice_3: 0.1223  loss_ce_4: 0.1582  loss_mask_4: 0.1779  loss_dice_4: 0.1307  loss_ce_5: 0.1558  loss_mask_5: 0.1882  loss_dice_5: 0.1281  loss_ce_6: 0.1561  loss_mask_6: 0.1897  loss_dice_6: 0.1319  loss_ce_7: 0.1544  loss_mask_7: 0.1941  loss_dice_7: 0.1334  loss_ce_8: 0.1541  loss_mask_8: 0.1964  loss_dice_8: 0.1276  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:28] d2.utils.events INFO:  eta: 1:20:10  iter: 11979  total_loss: 5.206  loss_ce: 0.09999  loss_mask: 0.2154  loss_dice: 0.17  loss_ce_0: 0.05691  loss_mask_0: 0.2246  loss_dice_0: 0.1691  loss_ce_1: 0.1001  loss_mask_1: 0.2139  loss_dice_1: 0.1732  loss_ce_2: 0.09991  loss_mask_2: 0.2201  loss_dice_2: 0.1689  loss_ce_3: 0.09972  loss_mask_3: 0.2199  loss_dice_3: 0.1661  loss_ce_4: 0.09946  loss_mask_4: 0.2247  loss_dice_4: 0.1717  loss_ce_5: 0.09988  loss_mask_5: 0.2169  loss_dice_5: 0.1648  loss_ce_6: 0.09917  loss_mask_6: 0.2168  loss_dice_6: 0.1682  loss_ce_7: 0.09962  loss_mask_7: 0.2226  loss_dice_7: 0.1737  loss_ce_8: 0.09939  loss_mask_8: 0.2245  loss_dice_8: 0.1689  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:31] d2.utils.events INFO:  eta: 1:20:08  iter: 11999  total_loss: 5.314  loss_ce: 0.1577  loss_mask: 0.2079  loss_dice: 0.166  loss_ce_0: 0.07279  loss_mask_0: 0.222  loss_dice_0: 0.167  loss_ce_1: 0.1569  loss_mask_1: 0.2025  loss_dice_1: 0.171  loss_ce_2: 0.1571  loss_mask_2: 0.2071  loss_dice_2: 0.1709  loss_ce_3: 0.1572  loss_mask_3: 0.2108  loss_dice_3: 0.1619  loss_ce_4: 0.1569  loss_mask_4: 0.2158  loss_dice_4: 0.1664  loss_ce_5: 0.1572  loss_mask_5: 0.2136  loss_dice_5: 0.1657  loss_ce_6: 0.158  loss_mask_6: 0.2086  loss_dice_6: 0.1693  loss_ce_7: 0.158  loss_mask_7: 0.2096  loss_dice_7: 0.1665  loss_ce_8: 0.1584  loss_mask_8: 0.2114  loss_dice_8: 0.1689  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:33] d2.utils.events INFO:  eta: 1:20:04  iter: 12019  total_loss: 4.771  loss_ce: 0.1157  loss_mask: 0.211  loss_dice: 0.1387  loss_ce_0: 0.05915  loss_mask_0: 0.2161  loss_dice_0: 0.1402  loss_ce_1: 0.1176  loss_mask_1: 0.2024  loss_dice_1: 0.1432  loss_ce_2: 0.1174  loss_mask_2: 0.2075  loss_dice_2: 0.1374  loss_ce_3: 0.1177  loss_mask_3: 0.2175  loss_dice_3: 0.1432  loss_ce_4: 0.1185  loss_mask_4: 0.2166  loss_dice_4: 0.1403  loss_ce_5: 0.1168  loss_mask_5: 0.2086  loss_dice_5: 0.1395  loss_ce_6: 0.116  loss_mask_6: 0.2129  loss_dice_6: 0.1393  loss_ce_7: 0.1153  loss_mask_7: 0.2084  loss_dice_7: 0.1348  loss_ce_8: 0.1149  loss_mask_8: 0.2076  loss_dice_8: 0.1381  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:36] d2.utils.events INFO:  eta: 1:20:00  iter: 12039  total_loss: 4.656  loss_ce: 0.1129  loss_mask: 0.2149  loss_dice: 0.1286  loss_ce_0: 0.05819  loss_mask_0: 0.2114  loss_dice_0: 0.132  loss_ce_1: 0.1127  loss_mask_1: 0.2064  loss_dice_1: 0.1308  loss_ce_2: 0.1123  loss_mask_2: 0.2102  loss_dice_2: 0.1255  loss_ce_3: 0.1124  loss_mask_3: 0.2091  loss_dice_3: 0.1342  loss_ce_4: 0.1128  loss_mask_4: 0.2146  loss_dice_4: 0.1365  loss_ce_5: 0.1123  loss_mask_5: 0.2116  loss_dice_5: 0.1325  loss_ce_6: 0.1123  loss_mask_6: 0.2158  loss_dice_6: 0.1312  loss_ce_7: 0.1123  loss_mask_7: 0.2098  loss_dice_7: 0.1342  loss_ce_8: 0.1124  loss_mask_8: 0.2084  loss_dice_8: 0.1314  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:38] d2.utils.events INFO:  eta: 1:19:56  iter: 12059  total_loss: 5.283  loss_ce: 0.128  loss_mask: 0.2349  loss_dice: 0.1427  loss_ce_0: 0.06417  loss_mask_0: 0.2396  loss_dice_0: 0.1395  loss_ce_1: 0.128  loss_mask_1: 0.2389  loss_dice_1: 0.1444  loss_ce_2: 0.1279  loss_mask_2: 0.2433  loss_dice_2: 0.1429  loss_ce_3: 0.1279  loss_mask_3: 0.2335  loss_dice_3: 0.144  loss_ce_4: 0.1279  loss_mask_4: 0.2333  loss_dice_4: 0.1417  loss_ce_5: 0.1279  loss_mask_5: 0.2417  loss_dice_5: 0.1406  loss_ce_6: 0.1279  loss_mask_6: 0.2402  loss_dice_6: 0.1419  loss_ce_7: 0.128  loss_mask_7: 0.2399  loss_dice_7: 0.1406  loss_ce_8: 0.1279  loss_mask_8: 0.2357  loss_dice_8: 0.1399  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:41] d2.utils.events INFO:  eta: 1:19:56  iter: 12079  total_loss: 5.458  loss_ce: 0.1298  loss_mask: 0.2222  loss_dice: 0.1593  loss_ce_0: 0.06612  loss_mask_0: 0.2289  loss_dice_0: 0.1526  loss_ce_1: 0.1296  loss_mask_1: 0.2291  loss_dice_1: 0.1581  loss_ce_2: 0.1296  loss_mask_2: 0.2294  loss_dice_2: 0.154  loss_ce_3: 0.1298  loss_mask_3: 0.2265  loss_dice_3: 0.1604  loss_ce_4: 0.1301  loss_mask_4: 0.2274  loss_dice_4: 0.1597  loss_ce_5: 0.1298  loss_mask_5: 0.2242  loss_dice_5: 0.1584  loss_ce_6: 0.1298  loss_mask_6: 0.2373  loss_dice_6: 0.1578  loss_ce_7: 0.1299  loss_mask_7: 0.2319  loss_dice_7: 0.1557  loss_ce_8: 0.1299  loss_mask_8: 0.2216  loss_dice_8: 0.1564  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:06:43] d2.utils.events INFO:  eta: 1:19:53  iter: 12099  total_loss: 5.022  loss_ce: 0.1229  loss_mask: 0.1895  loss_dice: 0.1458  loss_ce_0: 0.06383  loss_mask_0: 0.1788  loss_dice_0: 0.144  loss_ce_1: 0.1215  loss_mask_1: 0.1872  loss_dice_1: 0.1467  loss_ce_2: 0.1215  loss_mask_2: 0.1812  loss_dice_2: 0.1413  loss_ce_3: 0.1212  loss_mask_3: 0.1844  loss_dice_3: 0.1514  loss_ce_4: 0.1202  loss_mask_4: 0.1866  loss_dice_4: 0.1442  loss_ce_5: 0.1213  loss_mask_5: 0.1797  loss_dice_5: 0.144  loss_ce_6: 0.1212  loss_mask_6: 0.1919  loss_dice_6: 0.1481  loss_ce_7: 0.122  loss_mask_7: 0.1899  loss_dice_7: 0.1441  loss_ce_8: 0.1225  loss_mask_8: 0.1818  loss_dice_8: 0.1432  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:46] d2.utils.events INFO:  eta: 1:19:50  iter: 12119  total_loss: 4.734  loss_ce: 0.1154  loss_mask: 0.1432  loss_dice: 0.175  loss_ce_0: 0.06149  loss_mask_0: 0.1502  loss_dice_0: 0.1712  loss_ce_1: 0.1159  loss_mask_1: 0.1422  loss_dice_1: 0.1705  loss_ce_2: 0.116  loss_mask_2: 0.1396  loss_dice_2: 0.1657  loss_ce_3: 0.1159  loss_mask_3: 0.1456  loss_dice_3: 0.1677  loss_ce_4: 0.1159  loss_mask_4: 0.1523  loss_dice_4: 0.1733  loss_ce_5: 0.1159  loss_mask_5: 0.1423  loss_dice_5: 0.1667  loss_ce_6: 0.1152  loss_mask_6: 0.1411  loss_dice_6: 0.1655  loss_ce_7: 0.1154  loss_mask_7: 0.1454  loss_dice_7: 0.1811  loss_ce_8: 0.1151  loss_mask_8: 0.1452  loss_dice_8: 0.1723  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:48] d2.utils.events INFO:  eta: 1:19:44  iter: 12139  total_loss: 4.624  loss_ce: 0.1281  loss_mask: 0.2059  loss_dice: 0.117  loss_ce_0: 0.0643  loss_mask_0: 0.2047  loss_dice_0: 0.1135  loss_ce_1: 0.1282  loss_mask_1: 0.1985  loss_dice_1: 0.1191  loss_ce_2: 0.1282  loss_mask_2: 0.2054  loss_dice_2: 0.1161  loss_ce_3: 0.1281  loss_mask_3: 0.2133  loss_dice_3: 0.1168  loss_ce_4: 0.1282  loss_mask_4: 0.2062  loss_dice_4: 0.1178  loss_ce_5: 0.1281  loss_mask_5: 0.2054  loss_dice_5: 0.1141  loss_ce_6: 0.1281  loss_mask_6: 0.2001  loss_dice_6: 0.1193  loss_ce_7: 0.1281  loss_mask_7: 0.2086  loss_dice_7: 0.1199  loss_ce_8: 0.1281  loss_mask_8: 0.2141  loss_dice_8: 0.1161  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:51] d2.utils.events INFO:  eta: 1:19:40  iter: 12159  total_loss: 4.819  loss_ce: 0.1422  loss_mask: 0.1966  loss_dice: 0.159  loss_ce_0: 0.06793  loss_mask_0: 0.203  loss_dice_0: 0.1532  loss_ce_1: 0.1423  loss_mask_1: 0.2029  loss_dice_1: 0.1595  loss_ce_2: 0.142  loss_mask_2: 0.1998  loss_dice_2: 0.1616  loss_ce_3: 0.1423  loss_mask_3: 0.1893  loss_dice_3: 0.1544  loss_ce_4: 0.1422  loss_mask_4: 0.1933  loss_dice_4: 0.1588  loss_ce_5: 0.1421  loss_mask_5: 0.2045  loss_dice_5: 0.1585  loss_ce_6: 0.1425  loss_mask_6: 0.2061  loss_dice_6: 0.1537  loss_ce_7: 0.1422  loss_mask_7: 0.1988  loss_dice_7: 0.1565  loss_ce_8: 0.1423  loss_mask_8: 0.1928  loss_dice_8: 0.1611  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:53] d2.utils.events INFO:  eta: 1:19:35  iter: 12179  total_loss: 5.356  loss_ce: 0.1137  loss_mask: 0.2527  loss_dice: 0.1648  loss_ce_0: 0.05972  loss_mask_0: 0.2443  loss_dice_0: 0.1703  loss_ce_1: 0.1136  loss_mask_1: 0.2414  loss_dice_1: 0.1711  loss_ce_2: 0.114  loss_mask_2: 0.2585  loss_dice_2: 0.1728  loss_ce_3: 0.1137  loss_mask_3: 0.2606  loss_dice_3: 0.1609  loss_ce_4: 0.1139  loss_mask_4: 0.2411  loss_dice_4: 0.1699  loss_ce_5: 0.1139  loss_mask_5: 0.2489  loss_dice_5: 0.1724  loss_ce_6: 0.1137  loss_mask_6: 0.2507  loss_dice_6: 0.167  loss_ce_7: 0.1137  loss_mask_7: 0.2526  loss_dice_7: 0.1623  loss_ce_8: 0.1137  loss_mask_8: 0.2458  loss_dice_8: 0.1648  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:56] d2.utils.events INFO:  eta: 1:19:35  iter: 12199  total_loss: 4.782  loss_ce: 0.1076  loss_mask: 0.2301  loss_dice: 0.1433  loss_ce_0: 0.05768  loss_mask_0: 0.2294  loss_dice_0: 0.1383  loss_ce_1: 0.1073  loss_mask_1: 0.2172  loss_dice_1: 0.1451  loss_ce_2: 0.1075  loss_mask_2: 0.2207  loss_dice_2: 0.1395  loss_ce_3: 0.1073  loss_mask_3: 0.2222  loss_dice_3: 0.1411  loss_ce_4: 0.1072  loss_mask_4: 0.2322  loss_dice_4: 0.1412  loss_ce_5: 0.1073  loss_mask_5: 0.2293  loss_dice_5: 0.138  loss_ce_6: 0.107  loss_mask_6: 0.2226  loss_dice_6: 0.1385  loss_ce_7: 0.1072  loss_mask_7: 0.2259  loss_dice_7: 0.1358  loss_ce_8: 0.1074  loss_mask_8: 0.2267  loss_dice_8: 0.1483  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:06:59] d2.utils.events INFO:  eta: 1:19:34  iter: 12219  total_loss: 5.013  loss_ce: 0.1288  loss_mask: 0.2313  loss_dice: 0.1253  loss_ce_0: 0.06408  loss_mask_0: 0.2301  loss_dice_0: 0.1251  loss_ce_1: 0.1288  loss_mask_1: 0.2311  loss_dice_1: 0.1234  loss_ce_2: 0.1289  loss_mask_2: 0.2332  loss_dice_2: 0.1254  loss_ce_3: 0.1289  loss_mask_3: 0.2262  loss_dice_3: 0.1237  loss_ce_4: 0.1291  loss_mask_4: 0.2394  loss_dice_4: 0.1283  loss_ce_5: 0.1289  loss_mask_5: 0.2379  loss_dice_5: 0.1238  loss_ce_6: 0.129  loss_mask_6: 0.2331  loss_dice_6: 0.1256  loss_ce_7: 0.129  loss_mask_7: 0.2337  loss_dice_7: 0.1232  loss_ce_8: 0.1288  loss_mask_8: 0.2378  loss_dice_8: 0.1248  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:01] d2.utils.events INFO:  eta: 1:19:32  iter: 12239  total_loss: 5.116  loss_ce: 0.0994  loss_mask: 0.2416  loss_dice: 0.1546  loss_ce_0: 0.05501  loss_mask_0: 0.2352  loss_dice_0: 0.1539  loss_ce_1: 0.09894  loss_mask_1: 0.2411  loss_dice_1: 0.1577  loss_ce_2: 0.09895  loss_mask_2: 0.2377  loss_dice_2: 0.1593  loss_ce_3: 0.09891  loss_mask_3: 0.2379  loss_dice_3: 0.1583  loss_ce_4: 0.0988  loss_mask_4: 0.2342  loss_dice_4: 0.1591  loss_ce_5: 0.09895  loss_mask_5: 0.2317  loss_dice_5: 0.1566  loss_ce_6: 0.09843  loss_mask_6: 0.2376  loss_dice_6: 0.158  loss_ce_7: 0.09872  loss_mask_7: 0.2397  loss_dice_7: 0.1572  loss_ce_8: 0.09906  loss_mask_8: 0.2487  loss_dice_8: 0.1542  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:04] d2.utils.events INFO:  eta: 1:19:28  iter: 12259  total_loss: 4.949  loss_ce: 0.1604  loss_mask: 0.2142  loss_dice: 0.1356  loss_ce_0: 0.07484  loss_mask_0: 0.2214  loss_dice_0: 0.1353  loss_ce_1: 0.1604  loss_mask_1: 0.2276  loss_dice_1: 0.1402  loss_ce_2: 0.16  loss_mask_2: 0.214  loss_dice_2: 0.1396  loss_ce_3: 0.1601  loss_mask_3: 0.2244  loss_dice_3: 0.1436  loss_ce_4: 0.1598  loss_mask_4: 0.2224  loss_dice_4: 0.1363  loss_ce_5: 0.1598  loss_mask_5: 0.2247  loss_dice_5: 0.1436  loss_ce_6: 0.1603  loss_mask_6: 0.2272  loss_dice_6: 0.1399  loss_ce_7: 0.1602  loss_mask_7: 0.2261  loss_dice_7: 0.1391  loss_ce_8: 0.1603  loss_mask_8: 0.2236  loss_dice_8: 0.1356  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:07:06] d2.utils.events INFO:  eta: 1:19:29  iter: 12279  total_loss: 4.989  loss_ce: 0.111  loss_mask: 0.1937  loss_dice: 0.1339  loss_ce_0: 0.05658  loss_mask_0: 0.1984  loss_dice_0: 0.1397  loss_ce_1: 0.1117  loss_mask_1: 0.1931  loss_dice_1: 0.141  loss_ce_2: 0.1121  loss_mask_2: 0.2057  loss_dice_2: 0.1397  loss_ce_3: 0.1121  loss_mask_3: 0.195  loss_dice_3: 0.1274  loss_ce_4: 0.1127  loss_mask_4: 0.1951  loss_dice_4: 0.1409  loss_ce_5: 0.1122  loss_mask_5: 0.2209  loss_dice_5: 0.142  loss_ce_6: 0.1123  loss_mask_6: 0.2014  loss_dice_6: 0.1365  loss_ce_7: 0.112  loss_mask_7: 0.2073  loss_dice_7: 0.1376  loss_ce_8: 0.1116  loss_mask_8: 0.1934  loss_dice_8: 0.1349  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:09] d2.utils.events INFO:  eta: 1:19:27  iter: 12299  total_loss: 5.186  loss_ce: 0.1278  loss_mask: 0.198  loss_dice: 0.144  loss_ce_0: 0.06406  loss_mask_0: 0.2033  loss_dice_0: 0.1395  loss_ce_1: 0.1278  loss_mask_1: 0.2064  loss_dice_1: 0.1414  loss_ce_2: 0.1279  loss_mask_2: 0.1968  loss_dice_2: 0.1444  loss_ce_3: 0.1279  loss_mask_3: 0.2058  loss_dice_3: 0.1416  loss_ce_4: 0.1279  loss_mask_4: 0.2077  loss_dice_4: 0.1409  loss_ce_5: 0.1278  loss_mask_5: 0.215  loss_dice_5: 0.1423  loss_ce_6: 0.1278  loss_mask_6: 0.2116  loss_dice_6: 0.1442  loss_ce_7: 0.1278  loss_mask_7: 0.1992  loss_dice_7: 0.1435  loss_ce_8: 0.1278  loss_mask_8: 0.1981  loss_dice_8: 0.1424  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:11] d2.utils.events INFO:  eta: 1:19:24  iter: 12319  total_loss: 5.226  loss_ce: 0.1162  loss_mask: 0.1855  loss_dice: 0.184  loss_ce_0: 0.05815  loss_mask_0: 0.1866  loss_dice_0: 0.1743  loss_ce_1: 0.1163  loss_mask_1: 0.1887  loss_dice_1: 0.184  loss_ce_2: 0.1167  loss_mask_2: 0.1869  loss_dice_2: 0.1885  loss_ce_3: 0.1165  loss_mask_3: 0.1829  loss_dice_3: 0.1799  loss_ce_4: 0.1168  loss_mask_4: 0.1833  loss_dice_4: 0.1756  loss_ce_5: 0.1165  loss_mask_5: 0.1923  loss_dice_5: 0.1808  loss_ce_6: 0.1168  loss_mask_6: 0.1795  loss_dice_6: 0.1744  loss_ce_7: 0.1165  loss_mask_7: 0.1875  loss_dice_7: 0.1771  loss_ce_8: 0.1166  loss_mask_8: 0.1909  loss_dice_8: 0.1764  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:14] d2.utils.events INFO:  eta: 1:19:25  iter: 12339  total_loss: 5.235  loss_ce: 0.113  loss_mask: 0.2065  loss_dice: 0.1859  loss_ce_0: 0.05768  loss_mask_0: 0.1897  loss_dice_0: 0.1842  loss_ce_1: 0.1127  loss_mask_1: 0.1963  loss_dice_1: 0.1946  loss_ce_2: 0.113  loss_mask_2: 0.195  loss_dice_2: 0.1839  loss_ce_3: 0.1129  loss_mask_3: 0.1937  loss_dice_3: 0.1867  loss_ce_4: 0.1128  loss_mask_4: 0.1876  loss_dice_4: 0.185  loss_ce_5: 0.1127  loss_mask_5: 0.2046  loss_dice_5: 0.1846  loss_ce_6: 0.1128  loss_mask_6: 0.1999  loss_dice_6: 0.1793  loss_ce_7: 0.1128  loss_mask_7: 0.1951  loss_dice_7: 0.1877  loss_ce_8: 0.1131  loss_mask_8: 0.1976  loss_dice_8: 0.1808  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:16] d2.utils.events INFO:  eta: 1:19:25  iter: 12359  total_loss: 4.78  loss_ce: 0.1033  loss_mask: 0.2154  loss_dice: 0.1366  loss_ce_0: 0.05543  loss_mask_0: 0.222  loss_dice_0: 0.136  loss_ce_1: 0.1028  loss_mask_1: 0.2253  loss_dice_1: 0.1362  loss_ce_2: 0.1027  loss_mask_2: 0.2293  loss_dice_2: 0.1351  loss_ce_3: 0.1026  loss_mask_3: 0.2129  loss_dice_3: 0.1342  loss_ce_4: 0.1019  loss_mask_4: 0.2144  loss_dice_4: 0.1316  loss_ce_5: 0.1024  loss_mask_5: 0.2235  loss_dice_5: 0.1347  loss_ce_6: 0.1022  loss_mask_6: 0.2193  loss_dice_6: 0.1331  loss_ce_7: 0.1025  loss_mask_7: 0.2113  loss_dice_7: 0.1298  loss_ce_8: 0.1029  loss_mask_8: 0.2208  loss_dice_8: 0.1336  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:19] d2.utils.events INFO:  eta: 1:19:25  iter: 12379  total_loss: 4.705  loss_ce: 0.1527  loss_mask: 0.1936  loss_dice: 0.1249  loss_ce_0: 0.07297  loss_mask_0: 0.2047  loss_dice_0: 0.1331  loss_ce_1: 0.1522  loss_mask_1: 0.2011  loss_dice_1: 0.1343  loss_ce_2: 0.1522  loss_mask_2: 0.1997  loss_dice_2: 0.1322  loss_ce_3: 0.1521  loss_mask_3: 0.2049  loss_dice_3: 0.1309  loss_ce_4: 0.1517  loss_mask_4: 0.2026  loss_dice_4: 0.1298  loss_ce_5: 0.1519  loss_mask_5: 0.1957  loss_dice_5: 0.1283  loss_ce_6: 0.1521  loss_mask_6: 0.2023  loss_dice_6: 0.1319  loss_ce_7: 0.1521  loss_mask_7: 0.2037  loss_dice_7: 0.1297  loss_ce_8: 0.1527  loss_mask_8: 0.2061  loss_dice_8: 0.1281  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:22] d2.utils.events INFO:  eta: 1:19:23  iter: 12399  total_loss: 4.552  loss_ce: 0.1053  loss_mask: 0.1961  loss_dice: 0.1305  loss_ce_0: 0.0554  loss_mask_0: 0.194  loss_dice_0: 0.1262  loss_ce_1: 0.1056  loss_mask_1: 0.199  loss_dice_1: 0.1344  loss_ce_2: 0.1056  loss_mask_2: 0.196  loss_dice_2: 0.1397  loss_ce_3: 0.1056  loss_mask_3: 0.1951  loss_dice_3: 0.1374  loss_ce_4: 0.1058  loss_mask_4: 0.1972  loss_dice_4: 0.132  loss_ce_5: 0.1057  loss_mask_5: 0.2006  loss_dice_5: 0.1298  loss_ce_6: 0.1057  loss_mask_6: 0.1986  loss_dice_6: 0.1361  loss_ce_7: 0.1056  loss_mask_7: 0.1914  loss_dice_7: 0.1278  loss_ce_8: 0.1055  loss_mask_8: 0.1997  loss_dice_8: 0.1366  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:24] d2.utils.events INFO:  eta: 1:19:23  iter: 12419  total_loss: 4.919  loss_ce: 0.1287  loss_mask: 0.1635  loss_dice: 0.1875  loss_ce_0: 0.06423  loss_mask_0: 0.1533  loss_dice_0: 0.1795  loss_ce_1: 0.1288  loss_mask_1: 0.1617  loss_dice_1: 0.185  loss_ce_2: 0.1287  loss_mask_2: 0.1596  loss_dice_2: 0.1918  loss_ce_3: 0.1287  loss_mask_3: 0.1602  loss_dice_3: 0.1881  loss_ce_4: 0.1287  loss_mask_4: 0.156  loss_dice_4: 0.1885  loss_ce_5: 0.1288  loss_mask_5: 0.1725  loss_dice_5: 0.1837  loss_ce_6: 0.1287  loss_mask_6: 0.1537  loss_dice_6: 0.1809  loss_ce_7: 0.1287  loss_mask_7: 0.1454  loss_dice_7: 0.1878  loss_ce_8: 0.1288  loss_mask_8: 0.1537  loss_dice_8: 0.1798  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:27] d2.utils.events INFO:  eta: 1:19:21  iter: 12439  total_loss: 5.607  loss_ce: 0.1049  loss_mask: 0.2481  loss_dice: 0.1482  loss_ce_0: 0.05509  loss_mask_0: 0.2525  loss_dice_0: 0.1515  loss_ce_1: 0.105  loss_mask_1: 0.2513  loss_dice_1: 0.1568  loss_ce_2: 0.1051  loss_mask_2: 0.2486  loss_dice_2: 0.1476  loss_ce_3: 0.1053  loss_mask_3: 0.2525  loss_dice_3: 0.1551  loss_ce_4: 0.1053  loss_mask_4: 0.2649  loss_dice_4: 0.1533  loss_ce_5: 0.1052  loss_mask_5: 0.2584  loss_dice_5: 0.154  loss_ce_6: 0.1052  loss_mask_6: 0.255  loss_dice_6: 0.1491  loss_ce_7: 0.1051  loss_mask_7: 0.248  loss_dice_7: 0.1563  loss_ce_8: 0.1051  loss_mask_8: 0.254  loss_dice_8: 0.1537  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:29] d2.utils.events INFO:  eta: 1:19:17  iter: 12459  total_loss: 5.142  loss_ce: 0.1028  loss_mask: 0.2358  loss_dice: 0.1469  loss_ce_0: 0.05456  loss_mask_0: 0.237  loss_dice_0: 0.1466  loss_ce_1: 0.1027  loss_mask_1: 0.2399  loss_dice_1: 0.151  loss_ce_2: 0.103  loss_mask_2: 0.2441  loss_dice_2: 0.1504  loss_ce_3: 0.103  loss_mask_3: 0.2371  loss_dice_3: 0.1505  loss_ce_4: 0.1029  loss_mask_4: 0.2347  loss_dice_4: 0.143  loss_ce_5: 0.1028  loss_mask_5: 0.2477  loss_dice_5: 0.1487  loss_ce_6: 0.1028  loss_mask_6: 0.2415  loss_dice_6: 0.1467  loss_ce_7: 0.1028  loss_mask_7: 0.2383  loss_dice_7: 0.1476  loss_ce_8: 0.103  loss_mask_8: 0.2416  loss_dice_8: 0.1501  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:07:32] d2.utils.events INFO:  eta: 1:19:15  iter: 12479  total_loss: 5.148  loss_ce: 0.09452  loss_mask: 0.2105  loss_dice: 0.1747  loss_ce_0: 0.05205  loss_mask_0: 0.2103  loss_dice_0: 0.1812  loss_ce_1: 0.09446  loss_mask_1: 0.2087  loss_dice_1: 0.1818  loss_ce_2: 0.09458  loss_mask_2: 0.2134  loss_dice_2: 0.1782  loss_ce_3: 0.0945  loss_mask_3: 0.207  loss_dice_3: 0.1821  loss_ce_4: 0.0945  loss_mask_4: 0.2133  loss_dice_4: 0.1779  loss_ce_5: 0.09458  loss_mask_5: 0.2074  loss_dice_5: 0.1844  loss_ce_6: 0.09429  loss_mask_6: 0.2156  loss_dice_6: 0.1816  loss_ce_7: 0.0944  loss_mask_7: 0.2027  loss_dice_7: 0.1804  loss_ce_8: 0.0945  loss_mask_8: 0.1981  loss_dice_8: 0.1743  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:34] d2.utils.events INFO:  eta: 1:19:12  iter: 12499  total_loss: 4.662  loss_ce: 0.09539  loss_mask: 0.1659  loss_dice: 0.131  loss_ce_0: 0.05215  loss_mask_0: 0.162  loss_dice_0: 0.1293  loss_ce_1: 0.09533  loss_mask_1: 0.1684  loss_dice_1: 0.1291  loss_ce_2: 0.09545  loss_mask_2: 0.1627  loss_dice_2: 0.1327  loss_ce_3: 0.09541  loss_mask_3: 0.1713  loss_dice_3: 0.1328  loss_ce_4: 0.0955  loss_mask_4: 0.1629  loss_dice_4: 0.1302  loss_ce_5: 0.09557  loss_mask_5: 0.1631  loss_dice_5: 0.128  loss_ce_6: 0.09527  loss_mask_6: 0.1631  loss_dice_6: 0.1297  loss_ce_7: 0.0953  loss_mask_7: 0.1741  loss_dice_7: 0.1353  loss_ce_8: 0.09534  loss_mask_8: 0.1697  loss_dice_8: 0.1294  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:37] d2.utils.events INFO:  eta: 1:19:08  iter: 12519  total_loss: 5.167  loss_ce: 0.1307  loss_mask: 0.199  loss_dice: 0.146  loss_ce_0: 0.06486  loss_mask_0: 0.2035  loss_dice_0: 0.1567  loss_ce_1: 0.1308  loss_mask_1: 0.1888  loss_dice_1: 0.1533  loss_ce_2: 0.1309  loss_mask_2: 0.2017  loss_dice_2: 0.1581  loss_ce_3: 0.1307  loss_mask_3: 0.2005  loss_dice_3: 0.1544  loss_ce_4: 0.1307  loss_mask_4: 0.1957  loss_dice_4: 0.153  loss_ce_5: 0.1307  loss_mask_5: 0.1991  loss_dice_5: 0.1541  loss_ce_6: 0.1308  loss_mask_6: 0.1941  loss_dice_6: 0.1576  loss_ce_7: 0.1307  loss_mask_7: 0.1877  loss_dice_7: 0.1495  loss_ce_8: 0.1307  loss_mask_8: 0.1932  loss_dice_8: 0.15  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:39] d2.utils.events INFO:  eta: 1:19:02  iter: 12539  total_loss: 4.985  loss_ce: 0.09898  loss_mask: 0.2269  loss_dice: 0.1316  loss_ce_0: 0.0524  loss_mask_0: 0.2287  loss_dice_0: 0.1327  loss_ce_1: 0.0992  loss_mask_1: 0.2293  loss_dice_1: 0.1346  loss_ce_2: 0.09935  loss_mask_2: 0.2354  loss_dice_2: 0.1332  loss_ce_3: 0.09921  loss_mask_3: 0.2292  loss_dice_3: 0.1386  loss_ce_4: 0.09935  loss_mask_4: 0.2341  loss_dice_4: 0.1428  loss_ce_5: 0.09935  loss_mask_5: 0.2322  loss_dice_5: 0.1351  loss_ce_6: 0.09939  loss_mask_6: 0.2333  loss_dice_6: 0.1354  loss_ce_7: 0.09936  loss_mask_7: 0.2281  loss_dice_7: 0.1429  loss_ce_8: 0.09906  loss_mask_8: 0.2326  loss_dice_8: 0.1386  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:42] d2.utils.events INFO:  eta: 1:19:00  iter: 12559  total_loss: 4.747  loss_ce: 0.1395  loss_mask: 0.2  loss_dice: 0.1218  loss_ce_0: 0.07158  loss_mask_0: 0.1988  loss_dice_0: 0.1244  loss_ce_1: 0.1379  loss_mask_1: 0.1923  loss_dice_1: 0.1197  loss_ce_2: 0.1379  loss_mask_2: 0.1994  loss_dice_2: 0.1293  loss_ce_3: 0.1377  loss_mask_3: 0.193  loss_dice_3: 0.1256  loss_ce_4: 0.1368  loss_mask_4: 0.2009  loss_dice_4: 0.1207  loss_ce_5: 0.1377  loss_mask_5: 0.2003  loss_dice_5: 0.1206  loss_ce_6: 0.1372  loss_mask_6: 0.1979  loss_dice_6: 0.1233  loss_ce_7: 0.138  loss_mask_7: 0.1982  loss_dice_7: 0.1236  loss_ce_8: 0.1389  loss_mask_8: 0.1927  loss_dice_8: 0.1271  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:44] d2.utils.events INFO:  eta: 1:18:58  iter: 12579  total_loss: 5.021  loss_ce: 0.1194  loss_mask: 0.2347  loss_dice: 0.1319  loss_ce_0: 0.06456  loss_mask_0: 0.2249  loss_dice_0: 0.1351  loss_ce_1: 0.1168  loss_mask_1: 0.2298  loss_dice_1: 0.1333  loss_ce_2: 0.1168  loss_mask_2: 0.228  loss_dice_2: 0.1372  loss_ce_3: 0.1162  loss_mask_3: 0.2282  loss_dice_3: 0.135  loss_ce_4: 0.115  loss_mask_4: 0.2226  loss_dice_4: 0.1359  loss_ce_5: 0.1167  loss_mask_5: 0.2241  loss_dice_5: 0.1329  loss_ce_6: 0.116  loss_mask_6: 0.2154  loss_dice_6: 0.126  loss_ce_7: 0.1173  loss_mask_7: 0.2229  loss_dice_7: 0.1319  loss_ce_8: 0.1185  loss_mask_8: 0.2202  loss_dice_8: 0.1316  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:47] d2.utils.events INFO:  eta: 1:18:59  iter: 12599  total_loss: 4.798  loss_ce: 0.1528  loss_mask: 0.2047  loss_dice: 0.1245  loss_ce_0: 0.06844  loss_mask_0: 0.2007  loss_dice_0: 0.1331  loss_ce_1: 0.1517  loss_mask_1: 0.1979  loss_dice_1: 0.1336  loss_ce_2: 0.1516  loss_mask_2: 0.2157  loss_dice_2: 0.1315  loss_ce_3: 0.1517  loss_mask_3: 0.2132  loss_dice_3: 0.1392  loss_ce_4: 0.1521  loss_mask_4: 0.2129  loss_dice_4: 0.1324  loss_ce_5: 0.1528  loss_mask_5: 0.1946  loss_dice_5: 0.1298  loss_ce_6: 0.155  loss_mask_6: 0.2072  loss_dice_6: 0.1347  loss_ce_7: 0.1543  loss_mask_7: 0.193  loss_dice_7: 0.13  loss_ce_8: 0.1543  loss_mask_8: 0.2046  loss_dice_8: 0.134  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:49] d2.utils.events INFO:  eta: 1:18:53  iter: 12619  total_loss: 4.942  loss_ce: 0.1499  loss_mask: 0.2125  loss_dice: 0.1638  loss_ce_0: 0.06919  loss_mask_0: 0.2218  loss_dice_0: 0.1666  loss_ce_1: 0.1498  loss_mask_1: 0.2294  loss_dice_1: 0.1682  loss_ce_2: 0.1496  loss_mask_2: 0.2145  loss_dice_2: 0.1659  loss_ce_3: 0.1499  loss_mask_3: 0.2207  loss_dice_3: 0.1656  loss_ce_4: 0.1497  loss_mask_4: 0.2273  loss_dice_4: 0.1682  loss_ce_5: 0.15  loss_mask_5: 0.208  loss_dice_5: 0.1699  loss_ce_6: 0.1511  loss_mask_6: 0.2283  loss_dice_6: 0.1688  loss_ce_7: 0.1505  loss_mask_7: 0.2192  loss_dice_7: 0.1633  loss_ce_8: 0.1507  loss_mask_8: 0.2205  loss_dice_8: 0.1665  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:52] d2.utils.events INFO:  eta: 1:18:48  iter: 12639  total_loss: 5.1  loss_ce: 0.1354  loss_mask: 0.1985  loss_dice: 0.1582  loss_ce_0: 0.0665  loss_mask_0: 0.1938  loss_dice_0: 0.1691  loss_ce_1: 0.1344  loss_mask_1: 0.1992  loss_dice_1: 0.1648  loss_ce_2: 0.1339  loss_mask_2: 0.1927  loss_dice_2: 0.1688  loss_ce_3: 0.1336  loss_mask_3: 0.199  loss_dice_3: 0.1752  loss_ce_4: 0.1327  loss_mask_4: 0.1989  loss_dice_4: 0.1737  loss_ce_5: 0.134  loss_mask_5: 0.1945  loss_dice_5: 0.1736  loss_ce_6: 0.1348  loss_mask_6: 0.1979  loss_dice_6: 0.1706  loss_ce_7: 0.1352  loss_mask_7: 0.2074  loss_dice_7: 0.1742  loss_ce_8: 0.1359  loss_mask_8: 0.1993  loss_dice_8: 0.1728  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:07:54] d2.utils.events INFO:  eta: 1:18:46  iter: 12659  total_loss: 5.091  loss_ce: 0.1313  loss_mask: 0.2157  loss_dice: 0.1683  loss_ce_0: 0.06497  loss_mask_0: 0.2106  loss_dice_0: 0.1659  loss_ce_1: 0.132  loss_mask_1: 0.2084  loss_dice_1: 0.1606  loss_ce_2: 0.1325  loss_mask_2: 0.2171  loss_dice_2: 0.1705  loss_ce_3: 0.1328  loss_mask_3: 0.2075  loss_dice_3: 0.1658  loss_ce_4: 0.1335  loss_mask_4: 0.2118  loss_dice_4: 0.1628  loss_ce_5: 0.1327  loss_mask_5: 0.2113  loss_dice_5: 0.174  loss_ce_6: 0.1327  loss_mask_6: 0.21  loss_dice_6: 0.1655  loss_ce_7: 0.1319  loss_mask_7: 0.2153  loss_dice_7: 0.1658  loss_ce_8: 0.1311  loss_mask_8: 0.2173  loss_dice_8: 0.1615  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:07:57] d2.utils.events INFO:  eta: 1:18:43  iter: 12679  total_loss: 4.942  loss_ce: 0.1273  loss_mask: 0.2398  loss_dice: 0.127  loss_ce_0: 0.06413  loss_mask_0: 0.2249  loss_dice_0: 0.1243  loss_ce_1: 0.1278  loss_mask_1: 0.2337  loss_dice_1: 0.1235  loss_ce_2: 0.1279  loss_mask_2: 0.2354  loss_dice_2: 0.1247  loss_ce_3: 0.1278  loss_mask_3: 0.2401  loss_dice_3: 0.1293  loss_ce_4: 0.1279  loss_mask_4: 0.2339  loss_dice_4: 0.1294  loss_ce_5: 0.1278  loss_mask_5: 0.2328  loss_dice_5: 0.1292  loss_ce_6: 0.1275  loss_mask_6: 0.2271  loss_dice_6: 0.127  loss_ce_7: 0.1274  loss_mask_7: 0.2238  loss_dice_7: 0.1228  loss_ce_8: 0.1272  loss_mask_8: 0.2225  loss_dice_8: 0.1291  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:07:59] d2.utils.events INFO:  eta: 1:18:41  iter: 12699  total_loss: 4.737  loss_ce: 0.1274  loss_mask: 0.1858  loss_dice: 0.163  loss_ce_0: 0.06417  loss_mask_0: 0.1857  loss_dice_0: 0.1589  loss_ce_1: 0.1276  loss_mask_1: 0.1868  loss_dice_1: 0.1623  loss_ce_2: 0.1275  loss_mask_2: 0.1975  loss_dice_2: 0.1637  loss_ce_3: 0.1275  loss_mask_3: 0.1864  loss_dice_3: 0.1567  loss_ce_4: 0.1275  loss_mask_4: 0.1853  loss_dice_4: 0.164  loss_ce_5: 0.1275  loss_mask_5: 0.1854  loss_dice_5: 0.1629  loss_ce_6: 0.1276  loss_mask_6: 0.1931  loss_dice_6: 0.1544  loss_ce_7: 0.1276  loss_mask_7: 0.1879  loss_dice_7: 0.1568  loss_ce_8: 0.1274  loss_mask_8: 0.1876  loss_dice_8: 0.1557  time: 0.1261  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:08:02] d2.utils.events INFO:  eta: 1:18:39  iter: 12719  total_loss: 4.961  loss_ce: 0.1279  loss_mask: 0.1737  loss_dice: 0.1392  loss_ce_0: 0.06421  loss_mask_0: 0.1708  loss_dice_0: 0.1345  loss_ce_1: 0.1285  loss_mask_1: 0.1835  loss_dice_1: 0.1408  loss_ce_2: 0.1281  loss_mask_2: 0.1744  loss_dice_2: 0.1407  loss_ce_3: 0.1284  loss_mask_3: 0.1845  loss_dice_3: 0.141  loss_ce_4: 0.1283  loss_mask_4: 0.1804  loss_dice_4: 0.1373  loss_ce_5: 0.1283  loss_mask_5: 0.173  loss_dice_5: 0.141  loss_ce_6: 0.1282  loss_mask_6: 0.1815  loss_dice_6: 0.1443  loss_ce_7: 0.1281  loss_mask_7: 0.18  loss_dice_7: 0.1391  loss_ce_8: 0.1278  loss_mask_8: 0.1861  loss_dice_8: 0.1398  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:05] d2.utils.events INFO:  eta: 1:18:35  iter: 12739  total_loss: 4.703  loss_ce: 0.1274  loss_mask: 0.2197  loss_dice: 0.1303  loss_ce_0: 0.06409  loss_mask_0: 0.2148  loss_dice_0: 0.1289  loss_ce_1: 0.1275  loss_mask_1: 0.2158  loss_dice_1: 0.1362  loss_ce_2: 0.1276  loss_mask_2: 0.2181  loss_dice_2: 0.1326  loss_ce_3: 0.1275  loss_mask_3: 0.2153  loss_dice_3: 0.1298  loss_ce_4: 0.1275  loss_mask_4: 0.2137  loss_dice_4: 0.1308  loss_ce_5: 0.1275  loss_mask_5: 0.2176  loss_dice_5: 0.1315  loss_ce_6: 0.1276  loss_mask_6: 0.2079  loss_dice_6: 0.1319  loss_ce_7: 0.1276  loss_mask_7: 0.219  loss_dice_7: 0.1264  loss_ce_8: 0.1275  loss_mask_8: 0.2138  loss_dice_8: 0.133  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:07] d2.utils.events INFO:  eta: 1:18:29  iter: 12759  total_loss: 4.786  loss_ce: 0.1277  loss_mask: 0.1962  loss_dice: 0.1398  loss_ce_0: 0.06409  loss_mask_0: 0.1822  loss_dice_0: 0.1346  loss_ce_1: 0.1278  loss_mask_1: 0.1867  loss_dice_1: 0.1375  loss_ce_2: 0.1279  loss_mask_2: 0.1869  loss_dice_2: 0.1325  loss_ce_3: 0.1278  loss_mask_3: 0.1782  loss_dice_3: 0.1401  loss_ce_4: 0.1278  loss_mask_4: 0.1899  loss_dice_4: 0.1382  loss_ce_5: 0.1278  loss_mask_5: 0.1855  loss_dice_5: 0.1414  loss_ce_6: 0.1278  loss_mask_6: 0.1873  loss_dice_6: 0.1377  loss_ce_7: 0.1278  loss_mask_7: 0.1835  loss_dice_7: 0.14  loss_ce_8: 0.1277  loss_mask_8: 0.1827  loss_dice_8: 0.1436  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:10] d2.utils.events INFO:  eta: 1:18:26  iter: 12779  total_loss: 4.78  loss_ce: 0.127  loss_mask: 0.204  loss_dice: 0.1431  loss_ce_0: 0.06385  loss_mask_0: 0.2018  loss_dice_0: 0.1405  loss_ce_1: 0.1276  loss_mask_1: 0.1894  loss_dice_1: 0.1347  loss_ce_2: 0.128  loss_mask_2: 0.2152  loss_dice_2: 0.1377  loss_ce_3: 0.1285  loss_mask_3: 0.1987  loss_dice_3: 0.1401  loss_ce_4: 0.1289  loss_mask_4: 0.1985  loss_dice_4: 0.1374  loss_ce_5: 0.1283  loss_mask_5: 0.2024  loss_dice_5: 0.1359  loss_ce_6: 0.1277  loss_mask_6: 0.1962  loss_dice_6: 0.1344  loss_ce_7: 0.1274  loss_mask_7: 0.1984  loss_dice_7: 0.1414  loss_ce_8: 0.1272  loss_mask_8: 0.1937  loss_dice_8: 0.1354  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:08:12] d2.utils.events INFO:  eta: 1:18:26  iter: 12799  total_loss: 4.946  loss_ce: 0.1186  loss_mask: 0.1918  loss_dice: 0.1417  loss_ce_0: 0.06122  loss_mask_0: 0.2056  loss_dice_0: 0.1386  loss_ce_1: 0.1176  loss_mask_1: 0.2119  loss_dice_1: 0.1444  loss_ce_2: 0.1177  loss_mask_2: 0.2059  loss_dice_2: 0.14  loss_ce_3: 0.1175  loss_mask_3: 0.2088  loss_dice_3: 0.143  loss_ce_4: 0.117  loss_mask_4: 0.2037  loss_dice_4: 0.1386  loss_ce_5: 0.1176  loss_mask_5: 0.2075  loss_dice_5: 0.1401  loss_ce_6: 0.1179  loss_mask_6: 0.1954  loss_dice_6: 0.1446  loss_ce_7: 0.1182  loss_mask_7: 0.2137  loss_dice_7: 0.1376  loss_ce_8: 0.1188  loss_mask_8: 0.2034  loss_dice_8: 0.1396  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:15] d2.utils.events INFO:  eta: 1:18:25  iter: 12819  total_loss: 5.108  loss_ce: 0.1098  loss_mask: 0.2152  loss_dice: 0.1718  loss_ce_0: 0.05872  loss_mask_0: 0.2142  loss_dice_0: 0.1689  loss_ce_1: 0.1091  loss_mask_1: 0.2163  loss_dice_1: 0.1689  loss_ce_2: 0.109  loss_mask_2: 0.2122  loss_dice_2: 0.1616  loss_ce_3: 0.1086  loss_mask_3: 0.2078  loss_dice_3: 0.1677  loss_ce_4: 0.1081  loss_mask_4: 0.214  loss_dice_4: 0.1627  loss_ce_5: 0.1086  loss_mask_5: 0.2219  loss_dice_5: 0.1711  loss_ce_6: 0.1084  loss_mask_6: 0.2115  loss_dice_6: 0.1604  loss_ce_7: 0.1089  loss_mask_7: 0.2162  loss_dice_7: 0.1641  loss_ce_8: 0.1097  loss_mask_8: 0.2178  loss_dice_8: 0.1658  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:17] d2.utils.events INFO:  eta: 1:18:20  iter: 12839  total_loss: 4.826  loss_ce: 0.101  loss_mask: 0.2038  loss_dice: 0.1543  loss_ce_0: 0.05595  loss_mask_0: 0.2069  loss_dice_0: 0.1534  loss_ce_1: 0.1004  loss_mask_1: 0.2032  loss_dice_1: 0.1543  loss_ce_2: 0.1004  loss_mask_2: 0.2103  loss_dice_2: 0.1603  loss_ce_3: 0.1003  loss_mask_3: 0.2039  loss_dice_3: 0.1596  loss_ce_4: 0.09996  loss_mask_4: 0.2037  loss_dice_4: 0.1589  loss_ce_5: 0.1002  loss_mask_5: 0.2039  loss_dice_5: 0.1561  loss_ce_6: 0.09955  loss_mask_6: 0.2015  loss_dice_6: 0.1565  loss_ce_7: 0.09996  loss_mask_7: 0.2063  loss_dice_7: 0.1526  loss_ce_8: 0.1006  loss_mask_8: 0.2048  loss_dice_8: 0.1539  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:20] d2.utils.events INFO:  eta: 1:18:17  iter: 12859  total_loss: 4.719  loss_ce: 0.1642  loss_mask: 0.1823  loss_dice: 0.1327  loss_ce_0: 0.07539  loss_mask_0: 0.1833  loss_dice_0: 0.1334  loss_ce_1: 0.1644  loss_mask_1: 0.1847  loss_dice_1: 0.1302  loss_ce_2: 0.1645  loss_mask_2: 0.1768  loss_dice_2: 0.1271  loss_ce_3: 0.1646  loss_mask_3: 0.1896  loss_dice_3: 0.1273  loss_ce_4: 0.1645  loss_mask_4: 0.1844  loss_dice_4: 0.1283  loss_ce_5: 0.1642  loss_mask_5: 0.1842  loss_dice_5: 0.1329  loss_ce_6: 0.1648  loss_mask_6: 0.1788  loss_dice_6: 0.1368  loss_ce_7: 0.1644  loss_mask_7: 0.1754  loss_dice_7: 0.1362  loss_ce_8: 0.1641  loss_mask_8: 0.182  loss_dice_8: 0.129  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:22] d2.utils.events INFO:  eta: 1:18:16  iter: 12879  total_loss: 4.956  loss_ce: 0.102  loss_mask: 0.2058  loss_dice: 0.1666  loss_ce_0: 0.05457  loss_mask_0: 0.2015  loss_dice_0: 0.1657  loss_ce_1: 0.1022  loss_mask_1: 0.2064  loss_dice_1: 0.1708  loss_ce_2: 0.1027  loss_mask_2: 0.2141  loss_dice_2: 0.1653  loss_ce_3: 0.1025  loss_mask_3: 0.2049  loss_dice_3: 0.1678  loss_ce_4: 0.1028  loss_mask_4: 0.2169  loss_dice_4: 0.1613  loss_ce_5: 0.1028  loss_mask_5: 0.2004  loss_dice_5: 0.1667  loss_ce_6: 0.1026  loss_mask_6: 0.2125  loss_dice_6: 0.1635  loss_ce_7: 0.1027  loss_mask_7: 0.2083  loss_dice_7: 0.164  loss_ce_8: 0.1022  loss_mask_8: 0.214  loss_dice_8: 0.1639  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:25] d2.utils.events INFO:  eta: 1:18:15  iter: 12899  total_loss: 5.001  loss_ce: 0.1095  loss_mask: 0.2139  loss_dice: 0.1537  loss_ce_0: 0.05645  loss_mask_0: 0.2201  loss_dice_0: 0.16  loss_ce_1: 0.1102  loss_mask_1: 0.2247  loss_dice_1: 0.1529  loss_ce_2: 0.1108  loss_mask_2: 0.2295  loss_dice_2: 0.1614  loss_ce_3: 0.1109  loss_mask_3: 0.2289  loss_dice_3: 0.1479  loss_ce_4: 0.1113  loss_mask_4: 0.2265  loss_dice_4: 0.1564  loss_ce_5: 0.1108  loss_mask_5: 0.2186  loss_dice_5: 0.1564  loss_ce_6: 0.1111  loss_mask_6: 0.2216  loss_dice_6: 0.1576  loss_ce_7: 0.1109  loss_mask_7: 0.2169  loss_dice_7: 0.1569  loss_ce_8: 0.1101  loss_mask_8: 0.2265  loss_dice_8: 0.1565  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:27] d2.utils.events INFO:  eta: 1:18:13  iter: 12919  total_loss: 4.999  loss_ce: 0.1079  loss_mask: 0.2055  loss_dice: 0.1487  loss_ce_0: 0.05602  loss_mask_0: 0.2092  loss_dice_0: 0.1477  loss_ce_1: 0.108  loss_mask_1: 0.212  loss_dice_1: 0.1438  loss_ce_2: 0.1082  loss_mask_2: 0.2067  loss_dice_2: 0.1436  loss_ce_3: 0.1079  loss_mask_3: 0.2087  loss_dice_3: 0.1451  loss_ce_4: 0.1079  loss_mask_4: 0.2099  loss_dice_4: 0.148  loss_ce_5: 0.1079  loss_mask_5: 0.2111  loss_dice_5: 0.1446  loss_ce_6: 0.1082  loss_mask_6: 0.2141  loss_dice_6: 0.1493  loss_ce_7: 0.1083  loss_mask_7: 0.207  loss_dice_7: 0.1506  loss_ce_8: 0.1082  loss_mask_8: 0.2052  loss_dice_8: 0.1447  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:30] d2.utils.events INFO:  eta: 1:18:12  iter: 12939  total_loss: 4.856  loss_ce: 0.1069  loss_mask: 0.2105  loss_dice: 0.1481  loss_ce_0: 0.05587  loss_mask_0: 0.1965  loss_dice_0: 0.1525  loss_ce_1: 0.1071  loss_mask_1: 0.2044  loss_dice_1: 0.1556  loss_ce_2: 0.1071  loss_mask_2: 0.2048  loss_dice_2: 0.1496  loss_ce_3: 0.1071  loss_mask_3: 0.1983  loss_dice_3: 0.1576  loss_ce_4: 0.1071  loss_mask_4: 0.2017  loss_dice_4: 0.1572  loss_ce_5: 0.1069  loss_mask_5: 0.1954  loss_dice_5: 0.1484  loss_ce_6: 0.107  loss_mask_6: 0.2017  loss_dice_6: 0.1541  loss_ce_7: 0.1071  loss_mask_7: 0.2009  loss_dice_7: 0.1526  loss_ce_8: 0.107  loss_mask_8: 0.1992  loss_dice_8: 0.1531  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:08:33] d2.utils.events INFO:  eta: 1:18:12  iter: 12959  total_loss: 4.243  loss_ce: 0.1084  loss_mask: 0.1719  loss_dice: 0.1314  loss_ce_0: 0.05632  loss_mask_0: 0.1706  loss_dice_0: 0.1371  loss_ce_1: 0.1085  loss_mask_1: 0.1649  loss_dice_1: 0.1357  loss_ce_2: 0.1087  loss_mask_2: 0.1697  loss_dice_2: 0.133  loss_ce_3: 0.1088  loss_mask_3: 0.169  loss_dice_3: 0.136  loss_ce_4: 0.1089  loss_mask_4: 0.1777  loss_dice_4: 0.1303  loss_ce_5: 0.1087  loss_mask_5: 0.165  loss_dice_5: 0.1348  loss_ce_6: 0.1087  loss_mask_6: 0.173  loss_dice_6: 0.1402  loss_ce_7: 0.1086  loss_mask_7: 0.1745  loss_dice_7: 0.1272  loss_ce_8: 0.1086  loss_mask_8: 0.166  loss_dice_8: 0.1322  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:35] d2.utils.events INFO:  eta: 1:18:07  iter: 12979  total_loss: 5.272  loss_ce: 0.1308  loss_mask: 0.194  loss_dice: 0.1609  loss_ce_0: 0.06494  loss_mask_0: 0.1943  loss_dice_0: 0.1593  loss_ce_1: 0.1308  loss_mask_1: 0.1946  loss_dice_1: 0.1646  loss_ce_2: 0.1308  loss_mask_2: 0.1961  loss_dice_2: 0.1611  loss_ce_3: 0.1308  loss_mask_3: 0.1994  loss_dice_3: 0.1633  loss_ce_4: 0.1308  loss_mask_4: 0.1917  loss_dice_4: 0.1616  loss_ce_5: 0.1308  loss_mask_5: 0.1946  loss_dice_5: 0.1615  loss_ce_6: 0.1308  loss_mask_6: 0.193  loss_dice_6: 0.1612  loss_ce_7: 0.1308  loss_mask_7: 0.1962  loss_dice_7: 0.1583  loss_ce_8: 0.1308  loss_mask_8: 0.1915  loss_dice_8: 0.1561  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:38] d2.utils.events INFO:  eta: 1:18:08  iter: 12999  total_loss: 5.213  loss_ce: 0.1036  loss_mask: 0.2076  loss_dice: 0.1468  loss_ce_0: 0.0549  loss_mask_0: 0.2251  loss_dice_0: 0.1515  loss_ce_1: 0.1035  loss_mask_1: 0.2082  loss_dice_1: 0.1499  loss_ce_2: 0.1037  loss_mask_2: 0.2141  loss_dice_2: 0.1502  loss_ce_3: 0.1037  loss_mask_3: 0.214  loss_dice_3: 0.1504  loss_ce_4: 0.1038  loss_mask_4: 0.2093  loss_dice_4: 0.153  loss_ce_5: 0.1038  loss_mask_5: 0.2159  loss_dice_5: 0.1505  loss_ce_6: 0.1037  loss_mask_6: 0.2139  loss_dice_6: 0.1493  loss_ce_7: 0.1037  loss_mask_7: 0.2105  loss_dice_7: 0.1518  loss_ce_8: 0.1036  loss_mask_8: 0.2262  loss_dice_8: 0.1484  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:40] d2.utils.events INFO:  eta: 1:18:06  iter: 13019  total_loss: 5.148  loss_ce: 0.1071  loss_mask: 0.2174  loss_dice: 0.169  loss_ce_0: 0.05573  loss_mask_0: 0.2183  loss_dice_0: 0.1703  loss_ce_1: 0.1074  loss_mask_1: 0.2015  loss_dice_1: 0.1637  loss_ce_2: 0.1075  loss_mask_2: 0.2153  loss_dice_2: 0.1544  loss_ce_3: 0.1075  loss_mask_3: 0.2106  loss_dice_3: 0.1668  loss_ce_4: 0.1079  loss_mask_4: 0.2135  loss_dice_4: 0.1556  loss_ce_5: 0.1076  loss_mask_5: 0.2133  loss_dice_5: 0.1589  loss_ce_6: 0.1078  loss_mask_6: 0.2116  loss_dice_6: 0.1637  loss_ce_7: 0.1077  loss_mask_7: 0.2215  loss_dice_7: 0.1598  loss_ce_8: 0.1074  loss_mask_8: 0.2194  loss_dice_8: 0.1652  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:43] d2.utils.events INFO:  eta: 1:18:02  iter: 13039  total_loss: 4.853  loss_ce: 0.1042  loss_mask: 0.2126  loss_dice: 0.1412  loss_ce_0: 0.0549  loss_mask_0: 0.2116  loss_dice_0: 0.1333  loss_ce_1: 0.104  loss_mask_1: 0.2185  loss_dice_1: 0.137  loss_ce_2: 0.1041  loss_mask_2: 0.2163  loss_dice_2: 0.1429  loss_ce_3: 0.104  loss_mask_3: 0.2195  loss_dice_3: 0.133  loss_ce_4: 0.104  loss_mask_4: 0.219  loss_dice_4: 0.1374  loss_ce_5: 0.1041  loss_mask_5: 0.2203  loss_dice_5: 0.139  loss_ce_6: 0.1041  loss_mask_6: 0.2124  loss_dice_6: 0.1426  loss_ce_7: 0.1042  loss_mask_7: 0.2196  loss_dice_7: 0.1381  loss_ce_8: 0.1042  loss_mask_8: 0.2222  loss_dice_8: 0.1387  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:45] d2.utils.events INFO:  eta: 1:17:58  iter: 13059  total_loss: 5.208  loss_ce: 0.1491  loss_mask: 0.2016  loss_dice: 0.1688  loss_ce_0: 0.07275  loss_mask_0: 0.2052  loss_dice_0: 0.1593  loss_ce_1: 0.1486  loss_mask_1: 0.1978  loss_dice_1: 0.1643  loss_ce_2: 0.1484  loss_mask_2: 0.2064  loss_dice_2: 0.156  loss_ce_3: 0.1483  loss_mask_3: 0.2085  loss_dice_3: 0.1645  loss_ce_4: 0.1479  loss_mask_4: 0.2055  loss_dice_4: 0.1653  loss_ce_5: 0.1483  loss_mask_5: 0.2064  loss_dice_5: 0.1563  loss_ce_6: 0.1481  loss_mask_6: 0.2052  loss_dice_6: 0.1597  loss_ce_7: 0.1483  loss_mask_7: 0.196  loss_dice_7: 0.1609  loss_ce_8: 0.1486  loss_mask_8: 0.207  loss_dice_8: 0.1583  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:08:48] d2.utils.events INFO:  eta: 1:17:51  iter: 13079  total_loss: 4.788  loss_ce: 0.1272  loss_mask: 0.2177  loss_dice: 0.1357  loss_ce_0: 0.06144  loss_mask_0: 0.2158  loss_dice_0: 0.138  loss_ce_1: 0.1285  loss_mask_1: 0.2254  loss_dice_1: 0.1381  loss_ce_2: 0.1284  loss_mask_2: 0.215  loss_dice_2: 0.1362  loss_ce_3: 0.1289  loss_mask_3: 0.2184  loss_dice_3: 0.1408  loss_ce_4: 0.1291  loss_mask_4: 0.2127  loss_dice_4: 0.1395  loss_ce_5: 0.1288  loss_mask_5: 0.221  loss_dice_5: 0.1419  loss_ce_6: 0.1291  loss_mask_6: 0.2175  loss_dice_6: 0.1358  loss_ce_7: 0.1286  loss_mask_7: 0.2201  loss_dice_7: 0.1367  loss_ce_8: 0.1279  loss_mask_8: 0.2188  loss_dice_8: 0.1376  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:50] d2.utils.events INFO:  eta: 1:17:53  iter: 13099  total_loss: 4.964  loss_ce: 0.1269  loss_mask: 0.1848  loss_dice: 0.1713  loss_ce_0: 0.06391  loss_mask_0: 0.1787  loss_dice_0: 0.1726  loss_ce_1: 0.1273  loss_mask_1: 0.1703  loss_dice_1: 0.1712  loss_ce_2: 0.1268  loss_mask_2: 0.1716  loss_dice_2: 0.1703  loss_ce_3: 0.1268  loss_mask_3: 0.1794  loss_dice_3: 0.1803  loss_ce_4: 0.1269  loss_mask_4: 0.1749  loss_dice_4: 0.1779  loss_ce_5: 0.1268  loss_mask_5: 0.1753  loss_dice_5: 0.1641  loss_ce_6: 0.1269  loss_mask_6: 0.1692  loss_dice_6: 0.1662  loss_ce_7: 0.1271  loss_mask_7: 0.1657  loss_dice_7: 0.178  loss_ce_8: 0.1269  loss_mask_8: 0.1712  loss_dice_8: 0.1658  time: 0.1261  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:08:53] d2.utils.events INFO:  eta: 1:17:50  iter: 13119  total_loss: 5.316  loss_ce: 0.1277  loss_mask: 0.1426  loss_dice: 0.1701  loss_ce_0: 0.06413  loss_mask_0: 0.1485  loss_dice_0: 0.1622  loss_ce_1: 0.1278  loss_mask_1: 0.1448  loss_dice_1: 0.1677  loss_ce_2: 0.1277  loss_mask_2: 0.1523  loss_dice_2: 0.1606  loss_ce_3: 0.1278  loss_mask_3: 0.1535  loss_dice_3: 0.1687  loss_ce_4: 0.1278  loss_mask_4: 0.1464  loss_dice_4: 0.1592  loss_ce_5: 0.1277  loss_mask_5: 0.1439  loss_dice_5: 0.1696  loss_ce_6: 0.1277  loss_mask_6: 0.1515  loss_dice_6: 0.1582  loss_ce_7: 0.1277  loss_mask_7: 0.1458  loss_dice_7: 0.161  loss_ce_8: 0.1277  loss_mask_8: 0.1483  loss_dice_8: 0.1679  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:55] d2.utils.events INFO:  eta: 1:17:51  iter: 13139  total_loss: 4.748  loss_ce: 0.1169  loss_mask: 0.1654  loss_dice: 0.1308  loss_ce_0: 0.05957  loss_mask_0: 0.1587  loss_dice_0: 0.137  loss_ce_1: 0.1169  loss_mask_1: 0.1515  loss_dice_1: 0.1273  loss_ce_2: 0.117  loss_mask_2: 0.1704  loss_dice_2: 0.1275  loss_ce_3: 0.1171  loss_mask_3: 0.1663  loss_dice_3: 0.1327  loss_ce_4: 0.117  loss_mask_4: 0.1616  loss_dice_4: 0.129  loss_ce_5: 0.1169  loss_mask_5: 0.1587  loss_dice_5: 0.128  loss_ce_6: 0.1169  loss_mask_6: 0.1633  loss_dice_6: 0.1318  loss_ce_7: 0.1167  loss_mask_7: 0.1624  loss_dice_7: 0.1345  loss_ce_8: 0.1169  loss_mask_8: 0.1648  loss_dice_8: 0.1301  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:08:58] d2.utils.events INFO:  eta: 1:17:52  iter: 13159  total_loss: 5.128  loss_ce: 0.1332  loss_mask: 0.1932  loss_dice: 0.1605  loss_ce_0: 0.06678  loss_mask_0: 0.1929  loss_dice_0: 0.1659  loss_ce_1: 0.133  loss_mask_1: 0.1984  loss_dice_1: 0.1644  loss_ce_2: 0.1325  loss_mask_2: 0.1899  loss_dice_2: 0.1638  loss_ce_3: 0.1324  loss_mask_3: 0.2014  loss_dice_3: 0.1633  loss_ce_4: 0.1322  loss_mask_4: 0.1977  loss_dice_4: 0.1572  loss_ce_5: 0.1326  loss_mask_5: 0.1998  loss_dice_5: 0.1663  loss_ce_6: 0.1327  loss_mask_6: 0.2004  loss_dice_6: 0.1631  loss_ce_7: 0.133  loss_mask_7: 0.1982  loss_dice_7: 0.16  loss_ce_8: 0.1329  loss_mask_8: 0.1945  loss_dice_8: 0.1607  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:09:00] d2.utils.events INFO:  eta: 1:17:52  iter: 13179  total_loss: 4.627  loss_ce: 0.1274  loss_mask: 0.2237  loss_dice: 0.1292  loss_ce_0: 0.06279  loss_mask_0: 0.2102  loss_dice_0: 0.1264  loss_ce_1: 0.1273  loss_mask_1: 0.2129  loss_dice_1: 0.1304  loss_ce_2: 0.1275  loss_mask_2: 0.2078  loss_dice_2: 0.1359  loss_ce_3: 0.1276  loss_mask_3: 0.2238  loss_dice_3: 0.1327  loss_ce_4: 0.1275  loss_mask_4: 0.2128  loss_dice_4: 0.1258  loss_ce_5: 0.1275  loss_mask_5: 0.2218  loss_dice_5: 0.1297  loss_ce_6: 0.1275  loss_mask_6: 0.2261  loss_dice_6: 0.1328  loss_ce_7: 0.1274  loss_mask_7: 0.2218  loss_dice_7: 0.1327  loss_ce_8: 0.1274  loss_mask_8: 0.2219  loss_dice_8: 0.1294  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:09:03] d2.utils.events INFO:  eta: 1:17:47  iter: 13199  total_loss: 4.976  loss_ce: 0.123  loss_mask: 0.1772  loss_dice: 0.2041  loss_ce_0: 0.06164  loss_mask_0: 0.1752  loss_dice_0: 0.1954  loss_ce_1: 0.1226  loss_mask_1: 0.1751  loss_dice_1: 0.1983  loss_ce_2: 0.1227  loss_mask_2: 0.1801  loss_dice_2: 0.2097  loss_ce_3: 0.1228  loss_mask_3: 0.1677  loss_dice_3: 0.1959  loss_ce_4: 0.1227  loss_mask_4: 0.1739  loss_dice_4: 0.2018  loss_ce_5: 0.1228  loss_mask_5: 0.1775  loss_dice_5: 0.199  loss_ce_6: 0.1232  loss_mask_6: 0.1718  loss_dice_6: 0.1986  loss_ce_7: 0.1231  loss_mask_7: 0.1762  loss_dice_7: 0.2059  loss_ce_8: 0.1233  loss_mask_8: 0.1797  loss_dice_8: 0.2007  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:09:06] d2.utils.events INFO:  eta: 1:17:44  iter: 13219  total_loss: 5.106  loss_ce: 0.1144  loss_mask: 0.2141  loss_dice: 0.1397  loss_ce_0: 0.05933  loss_mask_0: 0.2146  loss_dice_0: 0.1368  loss_ce_1: 0.1142  loss_mask_1: 0.2061  loss_dice_1: 0.1355  loss_ce_2: 0.1144  loss_mask_2: 0.201  loss_dice_2: 0.1397  loss_ce_3: 0.114  loss_mask_3: 0.2173  loss_dice_3: 0.1373  loss_ce_4: 0.1137  loss_mask_4: 0.2137  loss_dice_4: 0.1358  loss_ce_5: 0.114  loss_mask_5: 0.207  loss_dice_5: 0.1413  loss_ce_6: 0.1139  loss_mask_6: 0.2192  loss_dice_6: 0.1416  loss_ce_7: 0.1141  loss_mask_7: 0.203  loss_dice_7: 0.1383  loss_ce_8: 0.1143  loss_mask_8: 0.2159  loss_dice_8: 0.1325  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:09:08] d2.utils.events INFO:  eta: 1:17:39  iter: 13239  total_loss: 5.253  loss_ce: 0.1533  loss_mask: 0.1517  loss_dice: 0.1724  loss_ce_0: 0.0718  loss_mask_0: 0.1549  loss_dice_0: 0.172  loss_ce_1: 0.1541  loss_mask_1: 0.1509  loss_dice_1: 0.1699  loss_ce_2: 0.1537  loss_mask_2: 0.1507  loss_dice_2: 0.1702  loss_ce_3: 0.1543  loss_mask_3: 0.1485  loss_dice_3: 0.1686  loss_ce_4: 0.1549  loss_mask_4: 0.1441  loss_dice_4: 0.1696  loss_ce_5: 0.1542  loss_mask_5: 0.1456  loss_dice_5: 0.1648  loss_ce_6: 0.1546  loss_mask_6: 0.1482  loss_dice_6: 0.1678  loss_ce_7: 0.1543  loss_mask_7: 0.1523  loss_dice_7: 0.1719  loss_ce_8: 0.1537  loss_mask_8: 0.1442  loss_dice_8: 0.1631  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:11] d2.utils.events INFO:  eta: 1:17:34  iter: 13259  total_loss: 5.024  loss_ce: 0.1076  loss_mask: 0.2194  loss_dice: 0.1645  loss_ce_0: 0.05695  loss_mask_0: 0.2116  loss_dice_0: 0.1722  loss_ce_1: 0.1077  loss_mask_1: 0.2177  loss_dice_1: 0.1737  loss_ce_2: 0.108  loss_mask_2: 0.2244  loss_dice_2: 0.1711  loss_ce_3: 0.108  loss_mask_3: 0.2171  loss_dice_3: 0.173  loss_ce_4: 0.1082  loss_mask_4: 0.227  loss_dice_4: 0.1655  loss_ce_5: 0.108  loss_mask_5: 0.2274  loss_dice_5: 0.1761  loss_ce_6: 0.1079  loss_mask_6: 0.2197  loss_dice_6: 0.1665  loss_ce_7: 0.108  loss_mask_7: 0.2199  loss_dice_7: 0.1707  loss_ce_8: 0.1078  loss_mask_8: 0.222  loss_dice_8: 0.1705  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:13] d2.utils.events INFO:  eta: 1:17:27  iter: 13279  total_loss: 5.45  loss_ce: 0.1029  loss_mask: 0.1979  loss_dice: 0.1631  loss_ce_0: 0.05533  loss_mask_0: 0.2055  loss_dice_0: 0.1621  loss_ce_1: 0.1024  loss_mask_1: 0.1976  loss_dice_1: 0.1637  loss_ce_2: 0.1027  loss_mask_2: 0.2158  loss_dice_2: 0.1662  loss_ce_3: 0.1025  loss_mask_3: 0.2139  loss_dice_3: 0.1721  loss_ce_4: 0.1024  loss_mask_4: 0.1917  loss_dice_4: 0.1662  loss_ce_5: 0.1025  loss_mask_5: 0.198  loss_dice_5: 0.17  loss_ce_6: 0.1024  loss_mask_6: 0.2021  loss_dice_6: 0.164  loss_ce_7: 0.1025  loss_mask_7: 0.199  loss_dice_7: 0.1672  loss_ce_8: 0.1029  loss_mask_8: 0.2041  loss_dice_8: 0.1627  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:16] d2.utils.events INFO:  eta: 1:17:19  iter: 13299  total_loss: 4.885  loss_ce: 0.1002  loss_mask: 0.1585  loss_dice: 0.1496  loss_ce_0: 0.05427  loss_mask_0: 0.1621  loss_dice_0: 0.1524  loss_ce_1: 0.1001  loss_mask_1: 0.1608  loss_dice_1: 0.1506  loss_ce_2: 0.1003  loss_mask_2: 0.1615  loss_dice_2: 0.148  loss_ce_3: 0.1002  loss_mask_3: 0.1661  loss_dice_3: 0.1549  loss_ce_4: 0.1002  loss_mask_4: 0.1621  loss_dice_4: 0.1566  loss_ce_5: 0.1002  loss_mask_5: 0.1584  loss_dice_5: 0.1526  loss_ce_6: 0.09992  loss_mask_6: 0.1625  loss_dice_6: 0.153  loss_ce_7: 0.09999  loss_mask_7: 0.1672  loss_dice_7: 0.1592  loss_ce_8: 0.1003  loss_mask_8: 0.1747  loss_dice_8: 0.1585  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:18] d2.utils.events INFO:  eta: 1:17:17  iter: 13319  total_loss: 4.967  loss_ce: 0.1535  loss_mask: 0.213  loss_dice: 0.1417  loss_ce_0: 0.07362  loss_mask_0: 0.2162  loss_dice_0: 0.1443  loss_ce_1: 0.1531  loss_mask_1: 0.21  loss_dice_1: 0.1444  loss_ce_2: 0.1531  loss_mask_2: 0.225  loss_dice_2: 0.1469  loss_ce_3: 0.1532  loss_mask_3: 0.2159  loss_dice_3: 0.1438  loss_ce_4: 0.1528  loss_mask_4: 0.22  loss_dice_4: 0.1426  loss_ce_5: 0.153  loss_mask_5: 0.2219  loss_dice_5: 0.142  loss_ce_6: 0.153  loss_mask_6: 0.2176  loss_dice_6: 0.1426  loss_ce_7: 0.1529  loss_mask_7: 0.2203  loss_dice_7: 0.1404  loss_ce_8: 0.1532  loss_mask_8: 0.2142  loss_dice_8: 0.1449  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:09:21] d2.utils.events INFO:  eta: 1:17:08  iter: 13339  total_loss: 4.575  loss_ce: 0.1353  loss_mask: 0.2288  loss_dice: 0.1203  loss_ce_0: 0.06858  loss_mask_0: 0.223  loss_dice_0: 0.1226  loss_ce_1: 0.1343  loss_mask_1: 0.2259  loss_dice_1: 0.1213  loss_ce_2: 0.1345  loss_mask_2: 0.2258  loss_dice_2: 0.1205  loss_ce_3: 0.1342  loss_mask_3: 0.2214  loss_dice_3: 0.1229  loss_ce_4: 0.1338  loss_mask_4: 0.2193  loss_dice_4: 0.1222  loss_ce_5: 0.1343  loss_mask_5: 0.2318  loss_dice_5: 0.1218  loss_ce_6: 0.1339  loss_mask_6: 0.2223  loss_dice_6: 0.1167  loss_ce_7: 0.1344  loss_mask_7: 0.2188  loss_dice_7: 0.1207  loss_ce_8: 0.1349  loss_mask_8: 0.2219  loss_dice_8: 0.1217  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:23] d2.utils.events INFO:  eta: 1:17:03  iter: 13359  total_loss: 5.101  loss_ce: 0.1236  loss_mask: 0.2277  loss_dice: 0.1652  loss_ce_0: 0.06056  loss_mask_0: 0.2226  loss_dice_0: 0.1587  loss_ce_1: 0.1245  loss_mask_1: 0.2306  loss_dice_1: 0.1657  loss_ce_2: 0.1244  loss_mask_2: 0.2281  loss_dice_2: 0.1668  loss_ce_3: 0.1247  loss_mask_3: 0.2337  loss_dice_3: 0.1659  loss_ce_4: 0.125  loss_mask_4: 0.236  loss_dice_4: 0.1641  loss_ce_5: 0.1243  loss_mask_5: 0.2318  loss_dice_5: 0.1613  loss_ce_6: 0.1249  loss_mask_6: 0.2286  loss_dice_6: 0.1661  loss_ce_7: 0.1243  loss_mask_7: 0.2183  loss_dice_7: 0.1618  loss_ce_8: 0.124  loss_mask_8: 0.2157  loss_dice_8: 0.169  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:26] d2.utils.events INFO:  eta: 1:16:54  iter: 13379  total_loss: 5.067  loss_ce: 0.1258  loss_mask: 0.1898  loss_dice: 0.1464  loss_ce_0: 0.06175  loss_mask_0: 0.1954  loss_dice_0: 0.1479  loss_ce_1: 0.1261  loss_mask_1: 0.1997  loss_dice_1: 0.148  loss_ce_2: 0.126  loss_mask_2: 0.199  loss_dice_2: 0.1479  loss_ce_3: 0.1264  loss_mask_3: 0.2016  loss_dice_3: 0.1523  loss_ce_4: 0.1263  loss_mask_4: 0.1956  loss_dice_4: 0.1466  loss_ce_5: 0.1259  loss_mask_5: 0.1918  loss_dice_5: 0.1433  loss_ce_6: 0.1263  loss_mask_6: 0.1999  loss_dice_6: 0.1485  loss_ce_7: 0.1259  loss_mask_7: 0.2044  loss_dice_7: 0.147  loss_ce_8: 0.1259  loss_mask_8: 0.1868  loss_dice_8: 0.1416  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:28] d2.utils.events INFO:  eta: 1:16:48  iter: 13399  total_loss: 4.76  loss_ce: 0.1237  loss_mask: 0.2126  loss_dice: 0.1475  loss_ce_0: 0.06128  loss_mask_0: 0.2061  loss_dice_0: 0.1369  loss_ce_1: 0.1237  loss_mask_1: 0.2136  loss_dice_1: 0.1455  loss_ce_2: 0.1237  loss_mask_2: 0.2193  loss_dice_2: 0.1404  loss_ce_3: 0.1239  loss_mask_3: 0.217  loss_dice_3: 0.1439  loss_ce_4: 0.1237  loss_mask_4: 0.2083  loss_dice_4: 0.1399  loss_ce_5: 0.1235  loss_mask_5: 0.213  loss_dice_5: 0.1389  loss_ce_6: 0.1239  loss_mask_6: 0.2149  loss_dice_6: 0.147  loss_ce_7: 0.1238  loss_mask_7: 0.2092  loss_dice_7: 0.1439  loss_ce_8: 0.1238  loss_mask_8: 0.2154  loss_dice_8: 0.1455  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:31] d2.utils.events INFO:  eta: 1:16:38  iter: 13419  total_loss: 5.067  loss_ce: 0.1117  loss_mask: 0.2028  loss_dice: 0.1647  loss_ce_0: 0.05816  loss_mask_0: 0.1964  loss_dice_0: 0.1687  loss_ce_1: 0.1109  loss_mask_1: 0.1949  loss_dice_1: 0.1702  loss_ce_2: 0.1109  loss_mask_2: 0.1993  loss_dice_2: 0.1707  loss_ce_3: 0.1108  loss_mask_3: 0.2014  loss_dice_3: 0.1727  loss_ce_4: 0.1104  loss_mask_4: 0.2051  loss_dice_4: 0.1707  loss_ce_5: 0.111  loss_mask_5: 0.2006  loss_dice_5: 0.1644  loss_ce_6: 0.1111  loss_mask_6: 0.1942  loss_dice_6: 0.1698  loss_ce_7: 0.1114  loss_mask_7: 0.1977  loss_dice_7: 0.1634  loss_ce_8: 0.1117  loss_mask_8: 0.1928  loss_dice_8: 0.1717  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:33] d2.utils.events INFO:  eta: 1:16:28  iter: 13439  total_loss: 4.942  loss_ce: 0.1496  loss_mask: 0.1737  loss_dice: 0.1557  loss_ce_0: 0.07134  loss_mask_0: 0.1646  loss_dice_0: 0.1577  loss_ce_1: 0.1492  loss_mask_1: 0.167  loss_dice_1: 0.1548  loss_ce_2: 0.1493  loss_mask_2: 0.1548  loss_dice_2: 0.1505  loss_ce_3: 0.1493  loss_mask_3: 0.1653  loss_dice_3: 0.1561  loss_ce_4: 0.149  loss_mask_4: 0.1719  loss_dice_4: 0.1537  loss_ce_5: 0.1493  loss_mask_5: 0.1633  loss_dice_5: 0.1548  loss_ce_6: 0.1497  loss_mask_6: 0.1615  loss_dice_6: 0.1559  loss_ce_7: 0.1496  loss_mask_7: 0.1639  loss_dice_7: 0.1546  loss_ce_8: 0.1496  loss_mask_8: 0.1614  loss_dice_8: 0.1493  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:36] d2.utils.events INFO:  eta: 1:16:23  iter: 13459  total_loss: 4.877  loss_ce: 0.1279  loss_mask: 0.1837  loss_dice: 0.1511  loss_ce_0: 0.06401  loss_mask_0: 0.1948  loss_dice_0: 0.1515  loss_ce_1: 0.1279  loss_mask_1: 0.1936  loss_dice_1: 0.1564  loss_ce_2: 0.1278  loss_mask_2: 0.185  loss_dice_2: 0.152  loss_ce_3: 0.1279  loss_mask_3: 0.1817  loss_dice_3: 0.1547  loss_ce_4: 0.1278  loss_mask_4: 0.1837  loss_dice_4: 0.1555  loss_ce_5: 0.1278  loss_mask_5: 0.1774  loss_dice_5: 0.1479  loss_ce_6: 0.1278  loss_mask_6: 0.1848  loss_dice_6: 0.1539  loss_ce_7: 0.1278  loss_mask_7: 0.1885  loss_dice_7: 0.1437  loss_ce_8: 0.1278  loss_mask_8: 0.1875  loss_dice_8: 0.1562  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:38] d2.utils.events INFO:  eta: 1:16:16  iter: 13479  total_loss: 4.716  loss_ce: 0.1331  loss_mask: 0.2006  loss_dice: 0.1316  loss_ce_0: 0.06674  loss_mask_0: 0.2025  loss_dice_0: 0.1259  loss_ce_1: 0.1329  loss_mask_1: 0.1999  loss_dice_1: 0.1258  loss_ce_2: 0.1329  loss_mask_2: 0.1979  loss_dice_2: 0.1259  loss_ce_3: 0.1327  loss_mask_3: 0.1966  loss_dice_3: 0.1284  loss_ce_4: 0.1324  loss_mask_4: 0.2017  loss_dice_4: 0.1267  loss_ce_5: 0.133  loss_mask_5: 0.2064  loss_dice_5: 0.128  loss_ce_6: 0.1327  loss_mask_6: 0.1975  loss_dice_6: 0.1249  loss_ce_7: 0.133  loss_mask_7: 0.2007  loss_dice_7: 0.126  loss_ce_8: 0.1329  loss_mask_8: 0.205  loss_dice_8: 0.1252  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:41] d2.utils.events INFO:  eta: 1:16:16  iter: 13499  total_loss: 5.234  loss_ce: 0.1252  loss_mask: 0.1833  loss_dice: 0.1705  loss_ce_0: 0.0619  loss_mask_0: 0.1832  loss_dice_0: 0.1664  loss_ce_1: 0.1258  loss_mask_1: 0.1756  loss_dice_1: 0.1747  loss_ce_2: 0.1258  loss_mask_2: 0.1736  loss_dice_2: 0.1813  loss_ce_3: 0.126  loss_mask_3: 0.1816  loss_dice_3: 0.1764  loss_ce_4: 0.1264  loss_mask_4: 0.1848  loss_dice_4: 0.1759  loss_ce_5: 0.1257  loss_mask_5: 0.1831  loss_dice_5: 0.1618  loss_ce_6: 0.1259  loss_mask_6: 0.1806  loss_dice_6: 0.1717  loss_ce_7: 0.1255  loss_mask_7: 0.1747  loss_dice_7: 0.175  loss_ce_8: 0.1254  loss_mask_8: 0.1802  loss_dice_8: 0.175  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:43] d2.utils.events INFO:  eta: 1:16:19  iter: 13519  total_loss: 4.975  loss_ce: 0.1156  loss_mask: 0.2004  loss_dice: 0.146  loss_ce_0: 0.05926  loss_mask_0: 0.1978  loss_dice_0: 0.1494  loss_ce_1: 0.1153  loss_mask_1: 0.2125  loss_dice_1: 0.1542  loss_ce_2: 0.1154  loss_mask_2: 0.2032  loss_dice_2: 0.1608  loss_ce_3: 0.1153  loss_mask_3: 0.2026  loss_dice_3: 0.1488  loss_ce_4: 0.1151  loss_mask_4: 0.1934  loss_dice_4: 0.1514  loss_ce_5: 0.1153  loss_mask_5: 0.2042  loss_dice_5: 0.1534  loss_ce_6: 0.1155  loss_mask_6: 0.2024  loss_dice_6: 0.1516  loss_ce_7: 0.1156  loss_mask_7: 0.2054  loss_dice_7: 0.157  loss_ce_8: 0.1157  loss_mask_8: 0.2087  loss_dice_8: 0.1513  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:09:46] d2.utils.events INFO:  eta: 1:16:19  iter: 13539  total_loss: 5.279  loss_ce: 0.1438  loss_mask: 0.1424  loss_dice: 0.1423  loss_ce_0: 0.06948  loss_mask_0: 0.1427  loss_dice_0: 0.1461  loss_ce_1: 0.1439  loss_mask_1: 0.1484  loss_dice_1: 0.1447  loss_ce_2: 0.1436  loss_mask_2: 0.1373  loss_dice_2: 0.1473  loss_ce_3: 0.1439  loss_mask_3: 0.1391  loss_dice_3: 0.1446  loss_ce_4: 0.144  loss_mask_4: 0.1465  loss_dice_4: 0.1486  loss_ce_5: 0.144  loss_mask_5: 0.1428  loss_dice_5: 0.1435  loss_ce_6: 0.1441  loss_mask_6: 0.1409  loss_dice_6: 0.145  loss_ce_7: 0.144  loss_mask_7: 0.1403  loss_dice_7: 0.1484  loss_ce_8: 0.1438  loss_mask_8: 0.136  loss_dice_8: 0.1415  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:09:48] d2.utils.events INFO:  eta: 1:16:21  iter: 13559  total_loss: 4.852  loss_ce: 0.1365  loss_mask: 0.1679  loss_dice: 0.1519  loss_ce_0: 0.06744  loss_mask_0: 0.1714  loss_dice_0: 0.1546  loss_ce_1: 0.1361  loss_mask_1: 0.1849  loss_dice_1: 0.1499  loss_ce_2: 0.136  loss_mask_2: 0.1806  loss_dice_2: 0.1462  loss_ce_3: 0.1357  loss_mask_3: 0.1716  loss_dice_3: 0.1487  loss_ce_4: 0.1356  loss_mask_4: 0.1825  loss_dice_4: 0.1552  loss_ce_5: 0.1359  loss_mask_5: 0.1735  loss_dice_5: 0.1545  loss_ce_6: 0.136  loss_mask_6: 0.1794  loss_dice_6: 0.1476  loss_ce_7: 0.1361  loss_mask_7: 0.1744  loss_dice_7: 0.1568  loss_ce_8: 0.1363  loss_mask_8: 0.1637  loss_dice_8: 0.1502  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:51] d2.utils.events INFO:  eta: 1:16:18  iter: 13579  total_loss: 4.885  loss_ce: 0.1292  loss_mask: 0.1931  loss_dice: 0.1424  loss_ce_0: 0.06414  loss_mask_0: 0.203  loss_dice_0: 0.1473  loss_ce_1: 0.1288  loss_mask_1: 0.1968  loss_dice_1: 0.1486  loss_ce_2: 0.1287  loss_mask_2: 0.1994  loss_dice_2: 0.14  loss_ce_3: 0.1283  loss_mask_3: 0.1975  loss_dice_3: 0.1498  loss_ce_4: 0.1282  loss_mask_4: 0.1971  loss_dice_4: 0.1475  loss_ce_5: 0.1285  loss_mask_5: 0.1984  loss_dice_5: 0.145  loss_ce_6: 0.1283  loss_mask_6: 0.2021  loss_dice_6: 0.1423  loss_ce_7: 0.1287  loss_mask_7: 0.1918  loss_dice_7: 0.1467  loss_ce_8: 0.1291  loss_mask_8: 0.2049  loss_dice_8: 0.143  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:53] d2.utils.events INFO:  eta: 1:16:16  iter: 13599  total_loss: 4.978  loss_ce: 0.1284  loss_mask: 0.2084  loss_dice: 0.15  loss_ce_0: 0.0639  loss_mask_0: 0.2111  loss_dice_0: 0.1533  loss_ce_1: 0.1282  loss_mask_1: 0.2158  loss_dice_1: 0.1564  loss_ce_2: 0.1282  loss_mask_2: 0.206  loss_dice_2: 0.1539  loss_ce_3: 0.1283  loss_mask_3: 0.2099  loss_dice_3: 0.1589  loss_ce_4: 0.1283  loss_mask_4: 0.1996  loss_dice_4: 0.1541  loss_ce_5: 0.1282  loss_mask_5: 0.2072  loss_dice_5: 0.1541  loss_ce_6: 0.1287  loss_mask_6: 0.2073  loss_dice_6: 0.1478  loss_ce_7: 0.1285  loss_mask_7: 0.2026  loss_dice_7: 0.1533  loss_ce_8: 0.1288  loss_mask_8: 0.2005  loss_dice_8: 0.1588  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:56] d2.utils.events INFO:  eta: 1:16:15  iter: 13619  total_loss: 4.812  loss_ce: 0.126  loss_mask: 0.1881  loss_dice: 0.1663  loss_ce_0: 0.06265  loss_mask_0: 0.1861  loss_dice_0: 0.1651  loss_ce_1: 0.1256  loss_mask_1: 0.1893  loss_dice_1: 0.1609  loss_ce_2: 0.1257  loss_mask_2: 0.1918  loss_dice_2: 0.1658  loss_ce_3: 0.1255  loss_mask_3: 0.1916  loss_dice_3: 0.1656  loss_ce_4: 0.1252  loss_mask_4: 0.192  loss_dice_4: 0.1599  loss_ce_5: 0.1256  loss_mask_5: 0.1875  loss_dice_5: 0.1633  loss_ce_6: 0.1262  loss_mask_6: 0.2016  loss_dice_6: 0.163  loss_ce_7: 0.126  loss_mask_7: 0.198  loss_dice_7: 0.1625  loss_ce_8: 0.1262  loss_mask_8: 0.1938  loss_dice_8: 0.1578  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:09:58] d2.utils.events INFO:  eta: 1:16:15  iter: 13639  total_loss: 4.766  loss_ce: 0.1371  loss_mask: 0.2176  loss_dice: 0.1316  loss_ce_0: 0.06691  loss_mask_0: 0.221  loss_dice_0: 0.1303  loss_ce_1: 0.1367  loss_mask_1: 0.2231  loss_dice_1: 0.1326  loss_ce_2: 0.1364  loss_mask_2: 0.2161  loss_dice_2: 0.1268  loss_ce_3: 0.1366  loss_mask_3: 0.215  loss_dice_3: 0.1337  loss_ce_4: 0.1368  loss_mask_4: 0.2148  loss_dice_4: 0.1301  loss_ce_5: 0.1367  loss_mask_5: 0.2229  loss_dice_5: 0.1329  loss_ce_6: 0.1371  loss_mask_6: 0.2161  loss_dice_6: 0.1342  loss_ce_7: 0.1371  loss_mask_7: 0.2139  loss_dice_7: 0.1357  loss_ce_8: 0.1369  loss_mask_8: 0.2148  loss_dice_8: 0.1378  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:01] d2.utils.events INFO:  eta: 1:16:11  iter: 13659  total_loss: 5.114  loss_ce: 0.127  loss_mask: 0.2029  loss_dice: 0.1442  loss_ce_0: 0.06311  loss_mask_0: 0.2035  loss_dice_0: 0.1493  loss_ce_1: 0.1275  loss_mask_1: 0.1956  loss_dice_1: 0.1484  loss_ce_2: 0.1277  loss_mask_2: 0.2  loss_dice_2: 0.1407  loss_ce_3: 0.128  loss_mask_3: 0.1979  loss_dice_3: 0.1442  loss_ce_4: 0.1283  loss_mask_4: 0.197  loss_dice_4: 0.1493  loss_ce_5: 0.1279  loss_mask_5: 0.204  loss_dice_5: 0.1394  loss_ce_6: 0.1276  loss_mask_6: 0.1995  loss_dice_6: 0.1467  loss_ce_7: 0.1274  loss_mask_7: 0.2114  loss_dice_7: 0.1467  loss_ce_8: 0.127  loss_mask_8: 0.1937  loss_dice_8: 0.1466  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:10:03] d2.utils.events INFO:  eta: 1:16:10  iter: 13679  total_loss: 4.792  loss_ce: 0.1269  loss_mask: 0.1704  loss_dice: 0.1769  loss_ce_0: 0.06297  loss_mask_0: 0.1679  loss_dice_0: 0.1759  loss_ce_1: 0.1266  loss_mask_1: 0.1676  loss_dice_1: 0.1728  loss_ce_2: 0.1268  loss_mask_2: 0.1804  loss_dice_2: 0.1709  loss_ce_3: 0.127  loss_mask_3: 0.1641  loss_dice_3: 0.1805  loss_ce_4: 0.127  loss_mask_4: 0.1626  loss_dice_4: 0.1786  loss_ce_5: 0.127  loss_mask_5: 0.1644  loss_dice_5: 0.171  loss_ce_6: 0.127  loss_mask_6: 0.1664  loss_dice_6: 0.1716  loss_ce_7: 0.127  loss_mask_7: 0.1682  loss_dice_7: 0.1777  loss_ce_8: 0.1269  loss_mask_8: 0.1702  loss_dice_8: 0.1701  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:10:06] d2.utils.events INFO:  eta: 1:16:06  iter: 13699  total_loss: 5.516  loss_ce: 0.1215  loss_mask: 0.166  loss_dice: 0.2168  loss_ce_0: 0.06151  loss_mask_0: 0.1658  loss_dice_0: 0.2037  loss_ce_1: 0.1211  loss_mask_1: 0.174  loss_dice_1: 0.2187  loss_ce_2: 0.1213  loss_mask_2: 0.1766  loss_dice_2: 0.2227  loss_ce_3: 0.1212  loss_mask_3: 0.1751  loss_dice_3: 0.2064  loss_ce_4: 0.121  loss_mask_4: 0.1732  loss_dice_4: 0.2043  loss_ce_5: 0.1212  loss_mask_5: 0.1737  loss_dice_5: 0.2232  loss_ce_6: 0.1213  loss_mask_6: 0.1777  loss_dice_6: 0.1991  loss_ce_7: 0.1215  loss_mask_7: 0.1627  loss_dice_7: 0.2101  loss_ce_8: 0.1216  loss_mask_8: 0.1729  loss_dice_8: 0.2109  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:09] d2.utils.events INFO:  eta: 1:15:58  iter: 13719  total_loss: 4.731  loss_ce: 0.1081  loss_mask: 0.2147  loss_dice: 0.1412  loss_ce_0: 0.05749  loss_mask_0: 0.2096  loss_dice_0: 0.1442  loss_ce_1: 0.1077  loss_mask_1: 0.2069  loss_dice_1: 0.1441  loss_ce_2: 0.1081  loss_mask_2: 0.2072  loss_dice_2: 0.1457  loss_ce_3: 0.1079  loss_mask_3: 0.202  loss_dice_3: 0.1423  loss_ce_4: 0.1075  loss_mask_4: 0.2069  loss_dice_4: 0.1411  loss_ce_5: 0.1078  loss_mask_5: 0.2082  loss_dice_5: 0.1398  loss_ce_6: 0.1076  loss_mask_6: 0.2073  loss_dice_6: 0.1428  loss_ce_7: 0.1079  loss_mask_7: 0.2075  loss_dice_7: 0.1376  loss_ce_8: 0.1082  loss_mask_8: 0.2091  loss_dice_8: 0.1369  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:10:11] d2.utils.events INFO:  eta: 1:15:58  iter: 13739  total_loss: 4.823  loss_ce: 0.1036  loss_mask: 0.1859  loss_dice: 0.1461  loss_ce_0: 0.05571  loss_mask_0: 0.1875  loss_dice_0: 0.1415  loss_ce_1: 0.1036  loss_mask_1: 0.1876  loss_dice_1: 0.1411  loss_ce_2: 0.1038  loss_mask_2: 0.1859  loss_dice_2: 0.1419  loss_ce_3: 0.1036  loss_mask_3: 0.1909  loss_dice_3: 0.1465  loss_ce_4: 0.1037  loss_mask_4: 0.1815  loss_dice_4: 0.1467  loss_ce_5: 0.1037  loss_mask_5: 0.1772  loss_dice_5: 0.1384  loss_ce_6: 0.1033  loss_mask_6: 0.1869  loss_dice_6: 0.1472  loss_ce_7: 0.1036  loss_mask_7: 0.1842  loss_dice_7: 0.1409  loss_ce_8: 0.1036  loss_mask_8: 0.1834  loss_dice_8: 0.1408  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:14] d2.utils.events INFO:  eta: 1:15:55  iter: 13759  total_loss: 5.081  loss_ce: 0.09777  loss_mask: 0.2333  loss_dice: 0.1367  loss_ce_0: 0.05384  loss_mask_0: 0.224  loss_dice_0: 0.1405  loss_ce_1: 0.0972  loss_mask_1: 0.2277  loss_dice_1: 0.1418  loss_ce_2: 0.09731  loss_mask_2: 0.2424  loss_dice_2: 0.1383  loss_ce_3: 0.0972  loss_mask_3: 0.2398  loss_dice_3: 0.1413  loss_ce_4: 0.09701  loss_mask_4: 0.2421  loss_dice_4: 0.1419  loss_ce_5: 0.09716  loss_mask_5: 0.2366  loss_dice_5: 0.139  loss_ce_6: 0.09683  loss_mask_6: 0.2216  loss_dice_6: 0.1398  loss_ce_7: 0.09727  loss_mask_7: 0.2427  loss_dice_7: 0.1428  loss_ce_8: 0.09764  loss_mask_8: 0.2333  loss_dice_8: 0.1422  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:16] d2.utils.events INFO:  eta: 1:15:53  iter: 13779  total_loss: 4.692  loss_ce: 0.1765  loss_mask: 0.1768  loss_dice: 0.1358  loss_ce_0: 0.0792  loss_mask_0: 0.1813  loss_dice_0: 0.1266  loss_ce_1: 0.1771  loss_mask_1: 0.1856  loss_dice_1: 0.1364  loss_ce_2: 0.1766  loss_mask_2: 0.1821  loss_dice_2: 0.1331  loss_ce_3: 0.177  loss_mask_3: 0.1843  loss_dice_3: 0.14  loss_ce_4: 0.177  loss_mask_4: 0.1776  loss_dice_4: 0.1318  loss_ce_5: 0.1769  loss_mask_5: 0.1785  loss_dice_5: 0.1352  loss_ce_6: 0.1777  loss_mask_6: 0.1789  loss_dice_6: 0.1296  loss_ce_7: 0.1772  loss_mask_7: 0.1799  loss_dice_7: 0.1314  loss_ce_8: 0.1768  loss_mask_8: 0.1815  loss_dice_8: 0.1349  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:10:19] d2.utils.events INFO:  eta: 1:15:50  iter: 13799  total_loss: 5.031  loss_ce: 0.1307  loss_mask: 0.2063  loss_dice: 0.147  loss_ce_0: 0.06477  loss_mask_0: 0.2105  loss_dice_0: 0.1516  loss_ce_1: 0.1308  loss_mask_1: 0.2198  loss_dice_1: 0.1533  loss_ce_2: 0.1309  loss_mask_2: 0.2184  loss_dice_2: 0.1519  loss_ce_3: 0.1308  loss_mask_3: 0.2178  loss_dice_3: 0.1581  loss_ce_4: 0.1307  loss_mask_4: 0.2139  loss_dice_4: 0.1515  loss_ce_5: 0.1307  loss_mask_5: 0.2102  loss_dice_5: 0.1568  loss_ce_6: 0.1307  loss_mask_6: 0.2002  loss_dice_6: 0.1549  loss_ce_7: 0.1307  loss_mask_7: 0.2069  loss_dice_7: 0.1595  loss_ce_8: 0.1307  loss_mask_8: 0.2152  loss_dice_8: 0.1477  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:10:21] d2.utils.events INFO:  eta: 1:15:48  iter: 13819  total_loss: 4.898  loss_ce: 0.09617  loss_mask: 0.177  loss_dice: 0.1696  loss_ce_0: 0.05188  loss_mask_0: 0.1821  loss_dice_0: 0.1669  loss_ce_1: 0.09668  loss_mask_1: 0.1762  loss_dice_1: 0.1667  loss_ce_2: 0.09661  loss_mask_2: 0.1807  loss_dice_2: 0.1698  loss_ce_3: 0.09624  loss_mask_3: 0.1802  loss_dice_3: 0.1686  loss_ce_4: 0.0969  loss_mask_4: 0.1817  loss_dice_4: 0.1616  loss_ce_5: 0.09663  loss_mask_5: 0.1835  loss_dice_5: 0.1701  loss_ce_6: 0.09643  loss_mask_6: 0.1787  loss_dice_6: 0.1676  loss_ce_7: 0.09661  loss_mask_7: 0.1794  loss_dice_7: 0.1625  loss_ce_8: 0.09635  loss_mask_8: 0.1771  loss_dice_8: 0.1585  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:24] d2.utils.events INFO:  eta: 1:15:42  iter: 13839  total_loss: 4.982  loss_ce: 0.1306  loss_mask: 0.232  loss_dice: 0.1498  loss_ce_0: 0.06475  loss_mask_0: 0.2237  loss_dice_0: 0.1438  loss_ce_1: 0.1306  loss_mask_1: 0.2268  loss_dice_1: 0.1454  loss_ce_2: 0.1307  loss_mask_2: 0.2362  loss_dice_2: 0.1443  loss_ce_3: 0.1307  loss_mask_3: 0.228  loss_dice_3: 0.1439  loss_ce_4: 0.1305  loss_mask_4: 0.2361  loss_dice_4: 0.1466  loss_ce_5: 0.1306  loss_mask_5: 0.2291  loss_dice_5: 0.1472  loss_ce_6: 0.1306  loss_mask_6: 0.2326  loss_dice_6: 0.1504  loss_ce_7: 0.1307  loss_mask_7: 0.226  loss_dice_7: 0.1419  loss_ce_8: 0.1305  loss_mask_8: 0.2366  loss_dice_8: 0.1433  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:26] d2.utils.events INFO:  eta: 1:15:45  iter: 13859  total_loss: 5.074  loss_ce: 0.1294  loss_mask: 0.2012  loss_dice: 0.1451  loss_ce_0: 0.06436  loss_mask_0: 0.1983  loss_dice_0: 0.1397  loss_ce_1: 0.1294  loss_mask_1: 0.1929  loss_dice_1: 0.1427  loss_ce_2: 0.1294  loss_mask_2: 0.1955  loss_dice_2: 0.1383  loss_ce_3: 0.1294  loss_mask_3: 0.1902  loss_dice_3: 0.1448  loss_ce_4: 0.1294  loss_mask_4: 0.1978  loss_dice_4: 0.1507  loss_ce_5: 0.1294  loss_mask_5: 0.1887  loss_dice_5: 0.1422  loss_ce_6: 0.1294  loss_mask_6: 0.2019  loss_dice_6: 0.1434  loss_ce_7: 0.1293  loss_mask_7: 0.1991  loss_dice_7: 0.1459  loss_ce_8: 0.1293  loss_mask_8: 0.2043  loss_dice_8: 0.1458  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:29] d2.utils.events INFO:  eta: 1:15:44  iter: 13879  total_loss: 5.269  loss_ce: 0.1017  loss_mask: 0.2173  loss_dice: 0.1761  loss_ce_0: 0.05328  loss_mask_0: 0.2168  loss_dice_0: 0.1743  loss_ce_1: 0.102  loss_mask_1: 0.2258  loss_dice_1: 0.1735  loss_ce_2: 0.1019  loss_mask_2: 0.2217  loss_dice_2: 0.1728  loss_ce_3: 0.1016  loss_mask_3: 0.2184  loss_dice_3: 0.1791  loss_ce_4: 0.1024  loss_mask_4: 0.2152  loss_dice_4: 0.1804  loss_ce_5: 0.1019  loss_mask_5: 0.2308  loss_dice_5: 0.1695  loss_ce_6: 0.1019  loss_mask_6: 0.22  loss_dice_6: 0.1775  loss_ce_7: 0.102  loss_mask_7: 0.2254  loss_dice_7: 0.1667  loss_ce_8: 0.1018  loss_mask_8: 0.2214  loss_dice_8: 0.1731  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:31] d2.utils.events INFO:  eta: 1:15:39  iter: 13899  total_loss: 4.403  loss_ce: 0.09793  loss_mask: 0.1774  loss_dice: 0.1625  loss_ce_0: 0.05234  loss_mask_0: 0.1726  loss_dice_0: 0.1607  loss_ce_1: 0.09782  loss_mask_1: 0.1751  loss_dice_1: 0.1552  loss_ce_2: 0.09779  loss_mask_2: 0.1712  loss_dice_2: 0.161  loss_ce_3: 0.09782  loss_mask_3: 0.1689  loss_dice_3: 0.165  loss_ce_4: 0.09771  loss_mask_4: 0.1758  loss_dice_4: 0.1498  loss_ce_5: 0.09773  loss_mask_5: 0.1565  loss_dice_5: 0.1539  loss_ce_6: 0.09771  loss_mask_6: 0.1662  loss_dice_6: 0.1621  loss_ce_7: 0.09771  loss_mask_7: 0.1694  loss_dice_7: 0.1548  loss_ce_8: 0.09782  loss_mask_8: 0.165  loss_dice_8: 0.1585  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:34] d2.utils.events INFO:  eta: 1:15:35  iter: 13919  total_loss: 5.132  loss_ce: 0.1301  loss_mask: 0.1864  loss_dice: 0.1451  loss_ce_0: 0.0645  loss_mask_0: 0.1877  loss_dice_0: 0.1409  loss_ce_1: 0.1302  loss_mask_1: 0.1943  loss_dice_1: 0.1435  loss_ce_2: 0.1301  loss_mask_2: 0.1958  loss_dice_2: 0.1452  loss_ce_3: 0.13  loss_mask_3: 0.1914  loss_dice_3: 0.1447  loss_ce_4: 0.13  loss_mask_4: 0.194  loss_dice_4: 0.1435  loss_ce_5: 0.1301  loss_mask_5: 0.181  loss_dice_5: 0.1374  loss_ce_6: 0.1301  loss_mask_6: 0.1948  loss_dice_6: 0.1409  loss_ce_7: 0.1301  loss_mask_7: 0.1819  loss_dice_7: 0.1458  loss_ce_8: 0.13  loss_mask_8: 0.1875  loss_dice_8: 0.1399  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:36] d2.utils.events INFO:  eta: 1:15:30  iter: 13939  total_loss: 4.774  loss_ce: 0.1476  loss_mask: 0.1851  loss_dice: 0.136  loss_ce_0: 0.07259  loss_mask_0: 0.186  loss_dice_0: 0.1364  loss_ce_1: 0.1469  loss_mask_1: 0.1875  loss_dice_1: 0.1343  loss_ce_2: 0.1471  loss_mask_2: 0.1901  loss_dice_2: 0.1387  loss_ce_3: 0.147  loss_mask_3: 0.193  loss_dice_3: 0.1382  loss_ce_4: 0.1467  loss_mask_4: 0.1973  loss_dice_4: 0.1385  loss_ce_5: 0.1471  loss_mask_5: 0.1911  loss_dice_5: 0.1358  loss_ce_6: 0.1468  loss_mask_6: 0.1833  loss_dice_6: 0.1362  loss_ce_7: 0.1469  loss_mask_7: 0.1965  loss_dice_7: 0.1385  loss_ce_8: 0.1472  loss_mask_8: 0.187  loss_dice_8: 0.1399  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:39] d2.utils.events INFO:  eta: 1:15:30  iter: 13959  total_loss: 5.11  loss_ce: 0.1319  loss_mask: 0.2278  loss_dice: 0.1267  loss_ce_0: 0.06764  loss_mask_0: 0.2339  loss_dice_0: 0.1309  loss_ce_1: 0.1312  loss_mask_1: 0.2397  loss_dice_1: 0.1288  loss_ce_2: 0.1313  loss_mask_2: 0.2278  loss_dice_2: 0.1264  loss_ce_3: 0.1308  loss_mask_3: 0.2394  loss_dice_3: 0.1305  loss_ce_4: 0.1305  loss_mask_4: 0.2281  loss_dice_4: 0.1293  loss_ce_5: 0.1313  loss_mask_5: 0.2226  loss_dice_5: 0.1258  loss_ce_6: 0.1307  loss_mask_6: 0.2385  loss_dice_6: 0.1251  loss_ce_7: 0.131  loss_mask_7: 0.2319  loss_dice_7: 0.1261  loss_ce_8: 0.1312  loss_mask_8: 0.2368  loss_dice_8: 0.13  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:42] d2.utils.events INFO:  eta: 1:15:27  iter: 13979  total_loss: 4.547  loss_ce: 0.1277  loss_mask: 0.1793  loss_dice: 0.1378  loss_ce_0: 0.06417  loss_mask_0: 0.177  loss_dice_0: 0.1304  loss_ce_1: 0.1278  loss_mask_1: 0.182  loss_dice_1: 0.1324  loss_ce_2: 0.1278  loss_mask_2: 0.187  loss_dice_2: 0.133  loss_ce_3: 0.1278  loss_mask_3: 0.1917  loss_dice_3: 0.1366  loss_ce_4: 0.1278  loss_mask_4: 0.1858  loss_dice_4: 0.1301  loss_ce_5: 0.1277  loss_mask_5: 0.1878  loss_dice_5: 0.1357  loss_ce_6: 0.1278  loss_mask_6: 0.1842  loss_dice_6: 0.1294  loss_ce_7: 0.1278  loss_mask_7: 0.1798  loss_dice_7: 0.1252  loss_ce_8: 0.1277  loss_mask_8: 0.1927  loss_dice_8: 0.1306  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:44] d2.utils.events INFO:  eta: 1:15:21  iter: 13999  total_loss: 5.121  loss_ce: 0.1273  loss_mask: 0.1966  loss_dice: 0.1409  loss_ce_0: 0.06341  loss_mask_0: 0.2028  loss_dice_0: 0.1353  loss_ce_1: 0.1276  loss_mask_1: 0.2019  loss_dice_1: 0.1347  loss_ce_2: 0.1274  loss_mask_2: 0.2048  loss_dice_2: 0.129  loss_ce_3: 0.1275  loss_mask_3: 0.2064  loss_dice_3: 0.1317  loss_ce_4: 0.1274  loss_mask_4: 0.2032  loss_dice_4: 0.1351  loss_ce_5: 0.1273  loss_mask_5: 0.205  loss_dice_5: 0.1344  loss_ce_6: 0.1274  loss_mask_6: 0.1973  loss_dice_6: 0.1332  loss_ce_7: 0.1274  loss_mask_7: 0.2054  loss_dice_7: 0.1376  loss_ce_8: 0.1274  loss_mask_8: 0.2125  loss_dice_8: 0.1445  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:47] d2.utils.events INFO:  eta: 1:15:18  iter: 14019  total_loss: 4.69  loss_ce: 0.1356  loss_mask: 0.1865  loss_dice: 0.1494  loss_ce_0: 0.06489  loss_mask_0: 0.1845  loss_dice_0: 0.1494  loss_ce_1: 0.1355  loss_mask_1: 0.1904  loss_dice_1: 0.1479  loss_ce_2: 0.1355  loss_mask_2: 0.1762  loss_dice_2: 0.1439  loss_ce_3: 0.1358  loss_mask_3: 0.181  loss_dice_3: 0.1485  loss_ce_4: 0.1357  loss_mask_4: 0.1845  loss_dice_4: 0.1447  loss_ce_5: 0.1356  loss_mask_5: 0.1796  loss_dice_5: 0.1452  loss_ce_6: 0.1361  loss_mask_6: 0.1815  loss_dice_6: 0.1449  loss_ce_7: 0.1356  loss_mask_7: 0.1877  loss_dice_7: 0.1484  loss_ce_8: 0.1357  loss_mask_8: 0.184  loss_dice_8: 0.1481  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:49] d2.utils.events INFO:  eta: 1:15:17  iter: 14039  total_loss: 4.336  loss_ce: 0.1196  loss_mask: 0.2164  loss_dice: 0.1183  loss_ce_0: 0.06176  loss_mask_0: 0.2127  loss_dice_0: 0.1202  loss_ce_1: 0.1195  loss_mask_1: 0.2083  loss_dice_1: 0.1158  loss_ce_2: 0.1197  loss_mask_2: 0.2138  loss_dice_2: 0.1187  loss_ce_3: 0.1195  loss_mask_3: 0.2131  loss_dice_3: 0.1142  loss_ce_4: 0.1195  loss_mask_4: 0.2045  loss_dice_4: 0.1132  loss_ce_5: 0.1195  loss_mask_5: 0.206  loss_dice_5: 0.1171  loss_ce_6: 0.1193  loss_mask_6: 0.2051  loss_dice_6: 0.1098  loss_ce_7: 0.1197  loss_mask_7: 0.2087  loss_dice_7: 0.1178  loss_ce_8: 0.1196  loss_mask_8: 0.2088  loss_dice_8: 0.1162  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:52] d2.utils.events INFO:  eta: 1:15:18  iter: 14059  total_loss: 4.863  loss_ce: 0.1097  loss_mask: 0.2124  loss_dice: 0.1508  loss_ce_0: 0.05856  loss_mask_0: 0.2061  loss_dice_0: 0.153  loss_ce_1: 0.1091  loss_mask_1: 0.2005  loss_dice_1: 0.1514  loss_ce_2: 0.1091  loss_mask_2: 0.2204  loss_dice_2: 0.1571  loss_ce_3: 0.1087  loss_mask_3: 0.2146  loss_dice_3: 0.1493  loss_ce_4: 0.1086  loss_mask_4: 0.2053  loss_dice_4: 0.1506  loss_ce_5: 0.1097  loss_mask_5: 0.213  loss_dice_5: 0.147  loss_ce_6: 0.1094  loss_mask_6: 0.2183  loss_dice_6: 0.1491  loss_ce_7: 0.1098  loss_mask_7: 0.2115  loss_dice_7: 0.1447  loss_ce_8: 0.1094  loss_mask_8: 0.2107  loss_dice_8: 0.1564  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:10:54] d2.utils.events INFO:  eta: 1:15:16  iter: 14079  total_loss: 5.097  loss_ce: 0.1507  loss_mask: 0.2365  loss_dice: 0.1434  loss_ce_0: 0.07068  loss_mask_0: 0.2286  loss_dice_0: 0.1419  loss_ce_1: 0.1504  loss_mask_1: 0.2439  loss_dice_1: 0.1442  loss_ce_2: 0.1504  loss_mask_2: 0.2335  loss_dice_2: 0.1467  loss_ce_3: 0.1506  loss_mask_3: 0.2235  loss_dice_3: 0.1442  loss_ce_4: 0.1507  loss_mask_4: 0.2358  loss_dice_4: 0.145  loss_ce_5: 0.1507  loss_mask_5: 0.2312  loss_dice_5: 0.1441  loss_ce_6: 0.1512  loss_mask_6: 0.2396  loss_dice_6: 0.145  loss_ce_7: 0.1507  loss_mask_7: 0.231  loss_dice_7: 0.1423  loss_ce_8: 0.1508  loss_mask_8: 0.2327  loss_dice_8: 0.143  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:10:57] d2.utils.events INFO:  eta: 1:15:13  iter: 14099  total_loss: 4.868  loss_ce: 0.1131  loss_mask: 0.1907  loss_dice: 0.1538  loss_ce_0: 0.05806  loss_mask_0: 0.195  loss_dice_0: 0.1454  loss_ce_1: 0.1134  loss_mask_1: 0.1961  loss_dice_1: 0.1496  loss_ce_2: 0.1135  loss_mask_2: 0.1994  loss_dice_2: 0.1548  loss_ce_3: 0.1138  loss_mask_3: 0.1917  loss_dice_3: 0.1538  loss_ce_4: 0.1136  loss_mask_4: 0.1937  loss_dice_4: 0.1513  loss_ce_5: 0.1127  loss_mask_5: 0.204  loss_dice_5: 0.1476  loss_ce_6: 0.1125  loss_mask_6: 0.1858  loss_dice_6: 0.1479  loss_ce_7: 0.113  loss_mask_7: 0.2  loss_dice_7: 0.1481  loss_ce_8: 0.1134  loss_mask_8: 0.1928  loss_dice_8: 0.1454  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:10:59] d2.utils.events INFO:  eta: 1:15:14  iter: 14119  total_loss: 5.271  loss_ce: 0.144  loss_mask: 0.233  loss_dice: 0.1499  loss_ce_0: 0.0699  loss_mask_0: 0.2268  loss_dice_0: 0.145  loss_ce_1: 0.1439  loss_mask_1: 0.2327  loss_dice_1: 0.1452  loss_ce_2: 0.144  loss_mask_2: 0.2245  loss_dice_2: 0.1541  loss_ce_3: 0.1439  loss_mask_3: 0.2209  loss_dice_3: 0.1508  loss_ce_4: 0.144  loss_mask_4: 0.2225  loss_dice_4: 0.1489  loss_ce_5: 0.1446  loss_mask_5: 0.2249  loss_dice_5: 0.153  loss_ce_6: 0.1446  loss_mask_6: 0.2328  loss_dice_6: 0.1501  loss_ce_7: 0.1446  loss_mask_7: 0.2356  loss_dice_7: 0.1484  loss_ce_8: 0.1441  loss_mask_8: 0.2275  loss_dice_8: 0.1524  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:02] d2.utils.events INFO:  eta: 1:15:11  iter: 14139  total_loss: 5.027  loss_ce: 0.1308  loss_mask: 0.2123  loss_dice: 0.1511  loss_ce_0: 0.066  loss_mask_0: 0.209  loss_dice_0: 0.1486  loss_ce_1: 0.1309  loss_mask_1: 0.2218  loss_dice_1: 0.1514  loss_ce_2: 0.1309  loss_mask_2: 0.2049  loss_dice_2: 0.1579  loss_ce_3: 0.1311  loss_mask_3: 0.2188  loss_dice_3: 0.1525  loss_ce_4: 0.1309  loss_mask_4: 0.2061  loss_dice_4: 0.1537  loss_ce_5: 0.131  loss_mask_5: 0.2218  loss_dice_5: 0.1515  loss_ce_6: 0.1311  loss_mask_6: 0.2173  loss_dice_6: 0.1523  loss_ce_7: 0.131  loss_mask_7: 0.2093  loss_dice_7: 0.1524  loss_ce_8: 0.1309  loss_mask_8: 0.2127  loss_dice_8: 0.1491  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:04] d2.utils.events INFO:  eta: 1:15:09  iter: 14159  total_loss: 4.795  loss_ce: 0.1212  loss_mask: 0.2394  loss_dice: 0.1225  loss_ce_0: 0.0635  loss_mask_0: 0.2458  loss_dice_0: 0.127  loss_ce_1: 0.1205  loss_mask_1: 0.2463  loss_dice_1: 0.1259  loss_ce_2: 0.1204  loss_mask_2: 0.2384  loss_dice_2: 0.1251  loss_ce_3: 0.1196  loss_mask_3: 0.2374  loss_dice_3: 0.1279  loss_ce_4: 0.1191  loss_mask_4: 0.2393  loss_dice_4: 0.129  loss_ce_5: 0.1209  loss_mask_5: 0.2362  loss_dice_5: 0.1227  loss_ce_6: 0.1208  loss_mask_6: 0.2442  loss_dice_6: 0.1324  loss_ce_7: 0.121  loss_mask_7: 0.2404  loss_dice_7: 0.1242  loss_ce_8: 0.1208  loss_mask_8: 0.2385  loss_dice_8: 0.1256  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:07] d2.utils.events INFO:  eta: 1:15:06  iter: 14179  total_loss: 4.993  loss_ce: 0.1283  loss_mask: 0.2075  loss_dice: 0.1503  loss_ce_0: 0.06364  loss_mask_0: 0.2125  loss_dice_0: 0.1499  loss_ce_1: 0.1284  loss_mask_1: 0.2073  loss_dice_1: 0.1478  loss_ce_2: 0.1283  loss_mask_2: 0.2075  loss_dice_2: 0.1442  loss_ce_3: 0.1283  loss_mask_3: 0.2081  loss_dice_3: 0.1498  loss_ce_4: 0.1284  loss_mask_4: 0.2166  loss_dice_4: 0.1508  loss_ce_5: 0.1283  loss_mask_5: 0.2035  loss_dice_5: 0.1494  loss_ce_6: 0.1284  loss_mask_6: 0.2092  loss_dice_6: 0.1469  loss_ce_7: 0.1283  loss_mask_7: 0.2101  loss_dice_7: 0.1477  loss_ce_8: 0.1284  loss_mask_8: 0.2026  loss_dice_8: 0.1427  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:11:10] d2.utils.events INFO:  eta: 1:15:04  iter: 14199  total_loss: 5.001  loss_ce: 0.121  loss_mask: 0.2072  loss_dice: 0.1577  loss_ce_0: 0.06165  loss_mask_0: 0.2124  loss_dice_0: 0.1612  loss_ce_1: 0.1214  loss_mask_1: 0.2074  loss_dice_1: 0.1636  loss_ce_2: 0.1217  loss_mask_2: 0.2112  loss_dice_2: 0.1656  loss_ce_3: 0.1218  loss_mask_3: 0.2056  loss_dice_3: 0.1587  loss_ce_4: 0.1222  loss_mask_4: 0.208  loss_dice_4: 0.1713  loss_ce_5: 0.1218  loss_mask_5: 0.2144  loss_dice_5: 0.1691  loss_ce_6: 0.1216  loss_mask_6: 0.209  loss_dice_6: 0.1652  loss_ce_7: 0.1217  loss_mask_7: 0.21  loss_dice_7: 0.1641  loss_ce_8: 0.1213  loss_mask_8: 0.212  loss_dice_8: 0.1641  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:11:12] d2.utils.events INFO:  eta: 1:15:01  iter: 14219  total_loss: 4.831  loss_ce: 0.1283  loss_mask: 0.2294  loss_dice: 0.142  loss_ce_0: 0.06397  loss_mask_0: 0.2234  loss_dice_0: 0.1417  loss_ce_1: 0.1288  loss_mask_1: 0.2272  loss_dice_1: 0.1408  loss_ce_2: 0.1286  loss_mask_2: 0.2249  loss_dice_2: 0.1454  loss_ce_3: 0.1286  loss_mask_3: 0.2199  loss_dice_3: 0.1392  loss_ce_4: 0.1288  loss_mask_4: 0.2161  loss_dice_4: 0.1418  loss_ce_5: 0.1287  loss_mask_5: 0.2192  loss_dice_5: 0.1368  loss_ce_6: 0.1286  loss_mask_6: 0.2122  loss_dice_6: 0.1377  loss_ce_7: 0.1286  loss_mask_7: 0.2145  loss_dice_7: 0.1347  loss_ce_8: 0.1284  loss_mask_8: 0.2215  loss_dice_8: 0.1415  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:15] d2.utils.events INFO:  eta: 1:15:00  iter: 14239  total_loss: 5.723  loss_ce: 0.1217  loss_mask: 0.2032  loss_dice: 0.1904  loss_ce_0: 0.0614  loss_mask_0: 0.1982  loss_dice_0: 0.1805  loss_ce_1: 0.122  loss_mask_1: 0.1944  loss_dice_1: 0.1843  loss_ce_2: 0.1221  loss_mask_2: 0.1966  loss_dice_2: 0.1765  loss_ce_3: 0.1221  loss_mask_3: 0.1984  loss_dice_3: 0.1841  loss_ce_4: 0.1222  loss_mask_4: 0.1956  loss_dice_4: 0.1794  loss_ce_5: 0.122  loss_mask_5: 0.2051  loss_dice_5: 0.1786  loss_ce_6: 0.122  loss_mask_6: 0.1971  loss_dice_6: 0.1856  loss_ce_7: 0.122  loss_mask_7: 0.1987  loss_dice_7: 0.196  loss_ce_8: 0.1218  loss_mask_8: 0.198  loss_dice_8: 0.1846  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:17] d2.utils.events INFO:  eta: 1:15:00  iter: 14259  total_loss: 5.517  loss_ce: 0.1279  loss_mask: 0.1847  loss_dice: 0.1681  loss_ce_0: 0.06367  loss_mask_0: 0.1806  loss_dice_0: 0.159  loss_ce_1: 0.1277  loss_mask_1: 0.1831  loss_dice_1: 0.1693  loss_ce_2: 0.1279  loss_mask_2: 0.187  loss_dice_2: 0.1585  loss_ce_3: 0.1279  loss_mask_3: 0.1806  loss_dice_3: 0.165  loss_ce_4: 0.1278  loss_mask_4: 0.184  loss_dice_4: 0.1587  loss_ce_5: 0.1279  loss_mask_5: 0.186  loss_dice_5: 0.1681  loss_ce_6: 0.1279  loss_mask_6: 0.1891  loss_dice_6: 0.1655  loss_ce_7: 0.1276  loss_mask_7: 0.1889  loss_dice_7: 0.1658  loss_ce_8: 0.1279  loss_mask_8: 0.1849  loss_dice_8: 0.1618  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:11:20] d2.utils.events INFO:  eta: 1:14:58  iter: 14279  total_loss: 5.91  loss_ce: 0.1287  loss_mask: 0.1729  loss_dice: 0.2174  loss_ce_0: 0.06393  loss_mask_0: 0.1836  loss_dice_0: 0.2275  loss_ce_1: 0.1286  loss_mask_1: 0.186  loss_dice_1: 0.2283  loss_ce_2: 0.1286  loss_mask_2: 0.1841  loss_dice_2: 0.2208  loss_ce_3: 0.1285  loss_mask_3: 0.1785  loss_dice_3: 0.2247  loss_ce_4: 0.1282  loss_mask_4: 0.1867  loss_dice_4: 0.2282  loss_ce_5: 0.1286  loss_mask_5: 0.1829  loss_dice_5: 0.2282  loss_ce_6: 0.1286  loss_mask_6: 0.1979  loss_dice_6: 0.2292  loss_ce_7: 0.1288  loss_mask_7: 0.1833  loss_dice_7: 0.2199  loss_ce_8: 0.1287  loss_mask_8: 0.1776  loss_dice_8: 0.2306  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:11:22] d2.utils.events INFO:  eta: 1:15:01  iter: 14299  total_loss: 4.882  loss_ce: 0.1363  loss_mask: 0.2153  loss_dice: 0.127  loss_ce_0: 0.06537  loss_mask_0: 0.2151  loss_dice_0: 0.1283  loss_ce_1: 0.1364  loss_mask_1: 0.2139  loss_dice_1: 0.131  loss_ce_2: 0.1365  loss_mask_2: 0.2217  loss_dice_2: 0.1279  loss_ce_3: 0.1367  loss_mask_3: 0.2258  loss_dice_3: 0.1301  loss_ce_4: 0.1368  loss_mask_4: 0.2184  loss_dice_4: 0.1266  loss_ce_5: 0.1364  loss_mask_5: 0.2178  loss_dice_5: 0.1269  loss_ce_6: 0.1369  loss_mask_6: 0.2169  loss_dice_6: 0.1323  loss_ce_7: 0.1367  loss_mask_7: 0.2196  loss_dice_7: 0.1341  loss_ce_8: 0.1367  loss_mask_8: 0.224  loss_dice_8: 0.1308  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:11:25] d2.utils.events INFO:  eta: 1:14:59  iter: 14319  total_loss: 5.138  loss_ce: 0.1297  loss_mask: 0.2402  loss_dice: 0.1407  loss_ce_0: 0.06399  loss_mask_0: 0.2438  loss_dice_0: 0.1364  loss_ce_1: 0.1289  loss_mask_1: 0.2473  loss_dice_1: 0.1406  loss_ce_2: 0.129  loss_mask_2: 0.2424  loss_dice_2: 0.1397  loss_ce_3: 0.1288  loss_mask_3: 0.2479  loss_dice_3: 0.1393  loss_ce_4: 0.1284  loss_mask_4: 0.2395  loss_dice_4: 0.1334  loss_ce_5: 0.1289  loss_mask_5: 0.2458  loss_dice_5: 0.1376  loss_ce_6: 0.1295  loss_mask_6: 0.2349  loss_dice_6: 0.1387  loss_ce_7: 0.1294  loss_mask_7: 0.2401  loss_dice_7: 0.1356  loss_ce_8: 0.1298  loss_mask_8: 0.2457  loss_dice_8: 0.1361  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:27] d2.utils.events INFO:  eta: 1:14:58  iter: 14339  total_loss: 5.269  loss_ce: 0.1166  loss_mask: 0.2123  loss_dice: 0.1506  loss_ce_0: 0.06033  loss_mask_0: 0.2086  loss_dice_0: 0.1618  loss_ce_1: 0.1153  loss_mask_1: 0.2106  loss_dice_1: 0.1577  loss_ce_2: 0.1156  loss_mask_2: 0.2105  loss_dice_2: 0.1537  loss_ce_3: 0.1148  loss_mask_3: 0.2094  loss_dice_3: 0.159  loss_ce_4: 0.1141  loss_mask_4: 0.2112  loss_dice_4: 0.1572  loss_ce_5: 0.1154  loss_mask_5: 0.202  loss_dice_5: 0.1622  loss_ce_6: 0.1156  loss_mask_6: 0.2144  loss_dice_6: 0.1682  loss_ce_7: 0.1156  loss_mask_7: 0.2052  loss_dice_7: 0.1556  loss_ce_8: 0.1164  loss_mask_8: 0.2169  loss_dice_8: 0.1602  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:30] d2.utils.events INFO:  eta: 1:14:57  iter: 14359  total_loss: 5.474  loss_ce: 0.149  loss_mask: 0.2303  loss_dice: 0.1759  loss_ce_0: 0.06999  loss_mask_0: 0.2284  loss_dice_0: 0.175  loss_ce_1: 0.1485  loss_mask_1: 0.2277  loss_dice_1: 0.1847  loss_ce_2: 0.1483  loss_mask_2: 0.2305  loss_dice_2: 0.1794  loss_ce_3: 0.1484  loss_mask_3: 0.2348  loss_dice_3: 0.1833  loss_ce_4: 0.1479  loss_mask_4: 0.2215  loss_dice_4: 0.1797  loss_ce_5: 0.1478  loss_mask_5: 0.223  loss_dice_5: 0.1744  loss_ce_6: 0.1485  loss_mask_6: 0.2238  loss_dice_6: 0.1781  loss_ce_7: 0.148  loss_mask_7: 0.2288  loss_dice_7: 0.1784  loss_ce_8: 0.1487  loss_mask_8: 0.2259  loss_dice_8: 0.1822  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:32] d2.utils.events INFO:  eta: 1:14:56  iter: 14379  total_loss: 6.046  loss_ce: 0.1336  loss_mask: 0.1485  loss_dice: 0.3357  loss_ce_0: 0.06602  loss_mask_0: 0.145  loss_dice_0: 0.3045  loss_ce_1: 0.1319  loss_mask_1: 0.1543  loss_dice_1: 0.3168  loss_ce_2: 0.1316  loss_mask_2: 0.1568  loss_dice_2: 0.3397  loss_ce_3: 0.1312  loss_mask_3: 0.149  loss_dice_3: 0.3237  loss_ce_4: 0.1295  loss_mask_4: 0.1565  loss_dice_4: 0.3483  loss_ce_5: 0.1307  loss_mask_5: 0.1529  loss_dice_5: 0.3325  loss_ce_6: 0.131  loss_mask_6: 0.1442  loss_dice_6: 0.3314  loss_ce_7: 0.1303  loss_mask_7: 0.1558  loss_dice_7: 0.3461  loss_ce_8: 0.1324  loss_mask_8: 0.1513  loss_dice_8: 0.3111  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:35] d2.utils.events INFO:  eta: 1:14:53  iter: 14399  total_loss: 5.294  loss_ce: 0.1268  loss_mask: 0.2145  loss_dice: 0.1466  loss_ce_0: 0.0626  loss_mask_0: 0.2111  loss_dice_0: 0.1515  loss_ce_1: 0.1274  loss_mask_1: 0.2168  loss_dice_1: 0.1485  loss_ce_2: 0.1276  loss_mask_2: 0.2073  loss_dice_2: 0.1577  loss_ce_3: 0.1276  loss_mask_3: 0.2101  loss_dice_3: 0.1519  loss_ce_4: 0.1273  loss_mask_4: 0.2225  loss_dice_4: 0.1519  loss_ce_5: 0.1274  loss_mask_5: 0.2159  loss_dice_5: 0.151  loss_ce_6: 0.1274  loss_mask_6: 0.2169  loss_dice_6: 0.1509  loss_ce_7: 0.1276  loss_mask_7: 0.2184  loss_dice_7: 0.1582  loss_ce_8: 0.1274  loss_mask_8: 0.2112  loss_dice_8: 0.1539  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:37] d2.utils.events INFO:  eta: 1:14:52  iter: 14419  total_loss: 5.652  loss_ce: 0.1274  loss_mask: 0.2095  loss_dice: 0.196  loss_ce_0: 0.06346  loss_mask_0: 0.2134  loss_dice_0: 0.1944  loss_ce_1: 0.1279  loss_mask_1: 0.2086  loss_dice_1: 0.2025  loss_ce_2: 0.1281  loss_mask_2: 0.2003  loss_dice_2: 0.1937  loss_ce_3: 0.128  loss_mask_3: 0.2169  loss_dice_3: 0.1983  loss_ce_4: 0.1283  loss_mask_4: 0.2042  loss_dice_4: 0.2002  loss_ce_5: 0.128  loss_mask_5: 0.2144  loss_dice_5: 0.2013  loss_ce_6: 0.1281  loss_mask_6: 0.2083  loss_dice_6: 0.1978  loss_ce_7: 0.128  loss_mask_7: 0.2148  loss_dice_7: 0.1975  loss_ce_8: 0.1277  loss_mask_8: 0.2067  loss_dice_8: 0.1977  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:11:40] d2.utils.events INFO:  eta: 1:14:51  iter: 14439  total_loss: 5.48  loss_ce: 0.1271  loss_mask: 0.2031  loss_dice: 0.1854  loss_ce_0: 0.06277  loss_mask_0: 0.1949  loss_dice_0: 0.1794  loss_ce_1: 0.1269  loss_mask_1: 0.1994  loss_dice_1: 0.1843  loss_ce_2: 0.1271  loss_mask_2: 0.2028  loss_dice_2: 0.1832  loss_ce_3: 0.1269  loss_mask_3: 0.1815  loss_dice_3: 0.1794  loss_ce_4: 0.1268  loss_mask_4: 0.1952  loss_dice_4: 0.1738  loss_ce_5: 0.1267  loss_mask_5: 0.2104  loss_dice_5: 0.1845  loss_ce_6: 0.127  loss_mask_6: 0.2022  loss_dice_6: 0.1801  loss_ce_7: 0.127  loss_mask_7: 0.1925  loss_dice_7: 0.1705  loss_ce_8: 0.1274  loss_mask_8: 0.2044  loss_dice_8: 0.1835  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:11:42] d2.utils.events INFO:  eta: 1:14:51  iter: 14459  total_loss: 4.802  loss_ce: 0.1386  loss_mask: 0.1965  loss_dice: 0.1537  loss_ce_0: 0.06759  loss_mask_0: 0.2012  loss_dice_0: 0.1475  loss_ce_1: 0.1383  loss_mask_1: 0.1988  loss_dice_1: 0.158  loss_ce_2: 0.1381  loss_mask_2: 0.1999  loss_dice_2: 0.1553  loss_ce_3: 0.1384  loss_mask_3: 0.1942  loss_dice_3: 0.1518  loss_ce_4: 0.1386  loss_mask_4: 0.2057  loss_dice_4: 0.1532  loss_ce_5: 0.1383  loss_mask_5: 0.2026  loss_dice_5: 0.1535  loss_ce_6: 0.1385  loss_mask_6: 0.2025  loss_dice_6: 0.1551  loss_ce_7: 0.1389  loss_mask_7: 0.1988  loss_dice_7: 0.1484  loss_ce_8: 0.1389  loss_mask_8: 0.2057  loss_dice_8: 0.1526  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:45] d2.utils.events INFO:  eta: 1:14:57  iter: 14479  total_loss: 5.892  loss_ce: 0.1354  loss_mask: 0.1894  loss_dice: 0.1665  loss_ce_0: 0.06656  loss_mask_0: 0.1874  loss_dice_0: 0.1699  loss_ce_1: 0.1349  loss_mask_1: 0.1959  loss_dice_1: 0.1704  loss_ce_2: 0.1348  loss_mask_2: 0.1946  loss_dice_2: 0.1724  loss_ce_3: 0.1345  loss_mask_3: 0.1912  loss_dice_3: 0.1672  loss_ce_4: 0.1344  loss_mask_4: 0.1921  loss_dice_4: 0.1621  loss_ce_5: 0.1345  loss_mask_5: 0.1927  loss_dice_5: 0.1647  loss_ce_6: 0.1348  loss_mask_6: 0.1892  loss_dice_6: 0.1762  loss_ce_7: 0.1349  loss_mask_7: 0.196  loss_dice_7: 0.1678  loss_ce_8: 0.1352  loss_mask_8: 0.1961  loss_dice_8: 0.1716  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:47] d2.utils.events INFO:  eta: 1:14:49  iter: 14499  total_loss: 5.158  loss_ce: 0.1218  loss_mask: 0.2135  loss_dice: 0.1343  loss_ce_0: 0.06137  loss_mask_0: 0.2082  loss_dice_0: 0.138  loss_ce_1: 0.1222  loss_mask_1: 0.2117  loss_dice_1: 0.1394  loss_ce_2: 0.1223  loss_mask_2: 0.2071  loss_dice_2: 0.1368  loss_ce_3: 0.1227  loss_mask_3: 0.2113  loss_dice_3: 0.1356  loss_ce_4: 0.1228  loss_mask_4: 0.2122  loss_dice_4: 0.1356  loss_ce_5: 0.1226  loss_mask_5: 0.2136  loss_dice_5: 0.1401  loss_ce_6: 0.1224  loss_mask_6: 0.2159  loss_dice_6: 0.1376  loss_ce_7: 0.1224  loss_mask_7: 0.2155  loss_dice_7: 0.1387  loss_ce_8: 0.122  loss_mask_8: 0.2085  loss_dice_8: 0.143  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:50] d2.utils.events INFO:  eta: 1:14:43  iter: 14519  total_loss: 4.781  loss_ce: 0.1209  loss_mask: 0.1713  loss_dice: 0.1838  loss_ce_0: 0.06097  loss_mask_0: 0.1639  loss_dice_0: 0.1807  loss_ce_1: 0.1206  loss_mask_1: 0.159  loss_dice_1: 0.1804  loss_ce_2: 0.1207  loss_mask_2: 0.1639  loss_dice_2: 0.1748  loss_ce_3: 0.1208  loss_mask_3: 0.1636  loss_dice_3: 0.178  loss_ce_4: 0.1207  loss_mask_4: 0.1669  loss_dice_4: 0.1796  loss_ce_5: 0.1208  loss_mask_5: 0.1594  loss_dice_5: 0.1759  loss_ce_6: 0.1208  loss_mask_6: 0.1671  loss_dice_6: 0.1823  loss_ce_7: 0.1208  loss_mask_7: 0.1683  loss_dice_7: 0.184  loss_ce_8: 0.1209  loss_mask_8: 0.1634  loss_dice_8: 0.1751  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:11:53] d2.utils.events INFO:  eta: 1:14:41  iter: 14539  total_loss: 4.991  loss_ce: 0.1278  loss_mask: 0.2379  loss_dice: 0.1345  loss_ce_0: 0.06365  loss_mask_0: 0.2396  loss_dice_0: 0.1395  loss_ce_1: 0.1278  loss_mask_1: 0.2328  loss_dice_1: 0.1403  loss_ce_2: 0.1278  loss_mask_2: 0.2427  loss_dice_2: 0.1448  loss_ce_3: 0.1279  loss_mask_3: 0.2307  loss_dice_3: 0.141  loss_ce_4: 0.1278  loss_mask_4: 0.2308  loss_dice_4: 0.1447  loss_ce_5: 0.1279  loss_mask_5: 0.2436  loss_dice_5: 0.1417  loss_ce_6: 0.1279  loss_mask_6: 0.2394  loss_dice_6: 0.1376  loss_ce_7: 0.1278  loss_mask_7: 0.2297  loss_dice_7: 0.1448  loss_ce_8: 0.1278  loss_mask_8: 0.236  loss_dice_8: 0.1378  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:11:55] d2.utils.events INFO:  eta: 1:14:37  iter: 14559  total_loss: 5.006  loss_ce: 0.1212  loss_mask: 0.2125  loss_dice: 0.1292  loss_ce_0: 0.06104  loss_mask_0: 0.2229  loss_dice_0: 0.1269  loss_ce_1: 0.1215  loss_mask_1: 0.2243  loss_dice_1: 0.1319  loss_ce_2: 0.1215  loss_mask_2: 0.2289  loss_dice_2: 0.1319  loss_ce_3: 0.1218  loss_mask_3: 0.2119  loss_dice_3: 0.1299  loss_ce_4: 0.1218  loss_mask_4: 0.2063  loss_dice_4: 0.1266  loss_ce_5: 0.1217  loss_mask_5: 0.2145  loss_dice_5: 0.1265  loss_ce_6: 0.1216  loss_mask_6: 0.2108  loss_dice_6: 0.1313  loss_ce_7: 0.1215  loss_mask_7: 0.2224  loss_dice_7: 0.1324  loss_ce_8: 0.1213  loss_mask_8: 0.2165  loss_dice_8: 0.1319  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:11:58] d2.utils.events INFO:  eta: 1:14:33  iter: 14579  total_loss: 4.588  loss_ce: 0.1295  loss_mask: 0.1955  loss_dice: 0.1474  loss_ce_0: 0.06467  loss_mask_0: 0.1951  loss_dice_0: 0.1456  loss_ce_1: 0.1289  loss_mask_1: 0.1897  loss_dice_1: 0.1435  loss_ce_2: 0.1292  loss_mask_2: 0.1909  loss_dice_2: 0.1496  loss_ce_3: 0.1289  loss_mask_3: 0.1955  loss_dice_3: 0.1458  loss_ce_4: 0.1288  loss_mask_4: 0.1894  loss_dice_4: 0.1508  loss_ce_5: 0.1291  loss_mask_5: 0.187  loss_dice_5: 0.1498  loss_ce_6: 0.1291  loss_mask_6: 0.1932  loss_dice_6: 0.1507  loss_ce_7: 0.1291  loss_mask_7: 0.1885  loss_dice_7: 0.1472  loss_ce_8: 0.1294  loss_mask_8: 0.2003  loss_dice_8: 0.1541  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:00] d2.utils.events INFO:  eta: 1:14:29  iter: 14599  total_loss: 4.943  loss_ce: 0.1262  loss_mask: 0.1988  loss_dice: 0.1407  loss_ce_0: 0.06253  loss_mask_0: 0.2058  loss_dice_0: 0.1385  loss_ce_1: 0.1265  loss_mask_1: 0.2151  loss_dice_1: 0.1374  loss_ce_2: 0.1265  loss_mask_2: 0.2087  loss_dice_2: 0.1378  loss_ce_3: 0.1266  loss_mask_3: 0.2026  loss_dice_3: 0.137  loss_ce_4: 0.1267  loss_mask_4: 0.2058  loss_dice_4: 0.138  loss_ce_5: 0.1264  loss_mask_5: 0.2039  loss_dice_5: 0.1382  loss_ce_6: 0.1265  loss_mask_6: 0.2061  loss_dice_6: 0.136  loss_ce_7: 0.1265  loss_mask_7: 0.2078  loss_dice_7: 0.134  loss_ce_8: 0.1263  loss_mask_8: 0.202  loss_dice_8: 0.1395  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:03] d2.utils.events INFO:  eta: 1:14:26  iter: 14619  total_loss: 4.688  loss_ce: 0.1378  loss_mask: 0.2094  loss_dice: 0.1225  loss_ce_0: 0.06715  loss_mask_0: 0.1878  loss_dice_0: 0.1283  loss_ce_1: 0.1383  loss_mask_1: 0.1981  loss_dice_1: 0.1235  loss_ce_2: 0.138  loss_mask_2: 0.2025  loss_dice_2: 0.1278  loss_ce_3: 0.1381  loss_mask_3: 0.2052  loss_dice_3: 0.1277  loss_ce_4: 0.1385  loss_mask_4: 0.1981  loss_dice_4: 0.1318  loss_ce_5: 0.1383  loss_mask_5: 0.1928  loss_dice_5: 0.1276  loss_ce_6: 0.1381  loss_mask_6: 0.1965  loss_dice_6: 0.1203  loss_ce_7: 0.1381  loss_mask_7: 0.2002  loss_dice_7: 0.1225  loss_ce_8: 0.1379  loss_mask_8: 0.1969  loss_dice_8: 0.1275  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:05] d2.utils.events INFO:  eta: 1:14:21  iter: 14639  total_loss: 4.694  loss_ce: 0.123  loss_mask: 0.1974  loss_dice: 0.1626  loss_ce_0: 0.06168  loss_mask_0: 0.1872  loss_dice_0: 0.1617  loss_ce_1: 0.1235  loss_mask_1: 0.1938  loss_dice_1: 0.1612  loss_ce_2: 0.1236  loss_mask_2: 0.191  loss_dice_2: 0.1615  loss_ce_3: 0.1238  loss_mask_3: 0.1929  loss_dice_3: 0.1655  loss_ce_4: 0.1242  loss_mask_4: 0.197  loss_dice_4: 0.1612  loss_ce_5: 0.1238  loss_mask_5: 0.1924  loss_dice_5: 0.1642  loss_ce_6: 0.1238  loss_mask_6: 0.1861  loss_dice_6: 0.158  loss_ce_7: 0.1238  loss_mask_7: 0.1945  loss_dice_7: 0.153  loss_ce_8: 0.1233  loss_mask_8: 0.1893  loss_dice_8: 0.1579  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:08] d2.utils.events INFO:  eta: 1:14:19  iter: 14659  total_loss: 4.69  loss_ce: 0.1275  loss_mask: 0.1831  loss_dice: 0.1515  loss_ce_0: 0.06352  loss_mask_0: 0.168  loss_dice_0: 0.1501  loss_ce_1: 0.1275  loss_mask_1: 0.1746  loss_dice_1: 0.1443  loss_ce_2: 0.1275  loss_mask_2: 0.1713  loss_dice_2: 0.1525  loss_ce_3: 0.1276  loss_mask_3: 0.1722  loss_dice_3: 0.1502  loss_ce_4: 0.1276  loss_mask_4: 0.1697  loss_dice_4: 0.1567  loss_ce_5: 0.1275  loss_mask_5: 0.172  loss_dice_5: 0.1548  loss_ce_6: 0.1275  loss_mask_6: 0.1665  loss_dice_6: 0.1493  loss_ce_7: 0.1275  loss_mask_7: 0.1723  loss_dice_7: 0.1548  loss_ce_8: 0.1275  loss_mask_8: 0.1689  loss_dice_8: 0.1511  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:10] d2.utils.events INFO:  eta: 1:14:16  iter: 14679  total_loss: 5.357  loss_ce: 0.1246  loss_mask: 0.2361  loss_dice: 0.1396  loss_ce_0: 0.06211  loss_mask_0: 0.2359  loss_dice_0: 0.1329  loss_ce_1: 0.125  loss_mask_1: 0.2286  loss_dice_1: 0.1353  loss_ce_2: 0.125  loss_mask_2: 0.2346  loss_dice_2: 0.1326  loss_ce_3: 0.1249  loss_mask_3: 0.2363  loss_dice_3: 0.1292  loss_ce_4: 0.1251  loss_mask_4: 0.2276  loss_dice_4: 0.1345  loss_ce_5: 0.1248  loss_mask_5: 0.2322  loss_dice_5: 0.1348  loss_ce_6: 0.1248  loss_mask_6: 0.2297  loss_dice_6: 0.1339  loss_ce_7: 0.1247  loss_mask_7: 0.2227  loss_dice_7: 0.1354  loss_ce_8: 0.1247  loss_mask_8: 0.2364  loss_dice_8: 0.1362  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:13] d2.utils.events INFO:  eta: 1:14:16  iter: 14699  total_loss: 5.156  loss_ce: 0.1379  loss_mask: 0.1261  loss_dice: 0.1606  loss_ce_0: 0.06705  loss_mask_0: 0.133  loss_dice_0: 0.1584  loss_ce_1: 0.1378  loss_mask_1: 0.1347  loss_dice_1: 0.1602  loss_ce_2: 0.1379  loss_mask_2: 0.1262  loss_dice_2: 0.1549  loss_ce_3: 0.1379  loss_mask_3: 0.1231  loss_dice_3: 0.1583  loss_ce_4: 0.1378  loss_mask_4: 0.1403  loss_dice_4: 0.1582  loss_ce_5: 0.1379  loss_mask_5: 0.1319  loss_dice_5: 0.1631  loss_ce_6: 0.1382  loss_mask_6: 0.1297  loss_dice_6: 0.1587  loss_ce_7: 0.1381  loss_mask_7: 0.1409  loss_dice_7: 0.1618  loss_ce_8: 0.1378  loss_mask_8: 0.1299  loss_dice_8: 0.1629  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:15] d2.utils.events INFO:  eta: 1:14:16  iter: 14719  total_loss: 4.837  loss_ce: 0.1272  loss_mask: 0.1868  loss_dice: 0.1447  loss_ce_0: 0.06364  loss_mask_0: 0.1864  loss_dice_0: 0.1374  loss_ce_1: 0.1281  loss_mask_1: 0.1847  loss_dice_1: 0.1505  loss_ce_2: 0.1278  loss_mask_2: 0.1864  loss_dice_2: 0.1443  loss_ce_3: 0.1281  loss_mask_3: 0.1934  loss_dice_3: 0.1413  loss_ce_4: 0.1284  loss_mask_4: 0.1818  loss_dice_4: 0.1498  loss_ce_5: 0.128  loss_mask_5: 0.1797  loss_dice_5: 0.1501  loss_ce_6: 0.1279  loss_mask_6: 0.1854  loss_dice_6: 0.1489  loss_ce_7: 0.1279  loss_mask_7: 0.1888  loss_dice_7: 0.1489  loss_ce_8: 0.1274  loss_mask_8: 0.1891  loss_dice_8: 0.1502  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:18] d2.utils.events INFO:  eta: 1:14:14  iter: 14739  total_loss: 4.83  loss_ce: 0.1338  loss_mask: 0.2035  loss_dice: 0.1519  loss_ce_0: 0.0649  loss_mask_0: 0.2003  loss_dice_0: 0.1512  loss_ce_1: 0.1344  loss_mask_1: 0.1954  loss_dice_1: 0.1553  loss_ce_2: 0.1341  loss_mask_2: 0.2009  loss_dice_2: 0.1607  loss_ce_3: 0.1343  loss_mask_3: 0.2009  loss_dice_3: 0.1681  loss_ce_4: 0.1348  loss_mask_4: 0.1993  loss_dice_4: 0.1566  loss_ce_5: 0.1342  loss_mask_5: 0.1951  loss_dice_5: 0.1499  loss_ce_6: 0.1345  loss_mask_6: 0.1958  loss_dice_6: 0.1612  loss_ce_7: 0.1344  loss_mask_7: 0.1988  loss_dice_7: 0.1565  loss_ce_8: 0.1341  loss_mask_8: 0.2038  loss_dice_8: 0.1622  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:12:20] d2.utils.events INFO:  eta: 1:14:15  iter: 14759  total_loss: 5.176  loss_ce: 0.1238  loss_mask: 0.1968  loss_dice: 0.1472  loss_ce_0: 0.06246  loss_mask_0: 0.2057  loss_dice_0: 0.1456  loss_ce_1: 0.1245  loss_mask_1: 0.2031  loss_dice_1: 0.1413  loss_ce_2: 0.1241  loss_mask_2: 0.2054  loss_dice_2: 0.1439  loss_ce_3: 0.124  loss_mask_3: 0.2012  loss_dice_3: 0.1458  loss_ce_4: 0.1241  loss_mask_4: 0.2026  loss_dice_4: 0.1475  loss_ce_5: 0.1242  loss_mask_5: 0.2014  loss_dice_5: 0.1432  loss_ce_6: 0.1238  loss_mask_6: 0.2027  loss_dice_6: 0.1485  loss_ce_7: 0.124  loss_mask_7: 0.1955  loss_dice_7: 0.1428  loss_ce_8: 0.1237  loss_mask_8: 0.2061  loss_dice_8: 0.1474  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:12:23] d2.utils.events INFO:  eta: 1:14:09  iter: 14779  total_loss: 5.271  loss_ce: 0.1287  loss_mask: 0.2064  loss_dice: 0.1577  loss_ce_0: 0.06381  loss_mask_0: 0.1996  loss_dice_0: 0.1576  loss_ce_1: 0.1287  loss_mask_1: 0.2085  loss_dice_1: 0.1662  loss_ce_2: 0.1287  loss_mask_2: 0.2116  loss_dice_2: 0.1563  loss_ce_3: 0.1287  loss_mask_3: 0.1979  loss_dice_3: 0.1621  loss_ce_4: 0.1288  loss_mask_4: 0.2036  loss_dice_4: 0.1638  loss_ce_5: 0.1287  loss_mask_5: 0.1884  loss_dice_5: 0.1583  loss_ce_6: 0.1287  loss_mask_6: 0.2001  loss_dice_6: 0.157  loss_ce_7: 0.1286  loss_mask_7: 0.1916  loss_dice_7: 0.1597  loss_ce_8: 0.1288  loss_mask_8: 0.1953  loss_dice_8: 0.1631  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:25] d2.utils.events INFO:  eta: 1:14:07  iter: 14799  total_loss: 5.827  loss_ce: 0.1352  loss_mask: 0.1776  loss_dice: 0.2396  loss_ce_0: 0.06576  loss_mask_0: 0.1879  loss_dice_0: 0.2272  loss_ce_1: 0.1356  loss_mask_1: 0.1987  loss_dice_1: 0.2419  loss_ce_2: 0.1354  loss_mask_2: 0.1948  loss_dice_2: 0.2267  loss_ce_3: 0.1356  loss_mask_3: 0.185  loss_dice_3: 0.2242  loss_ce_4: 0.1359  loss_mask_4: 0.1921  loss_dice_4: 0.2442  loss_ce_5: 0.1354  loss_mask_5: 0.1941  loss_dice_5: 0.2385  loss_ce_6: 0.1355  loss_mask_6: 0.1874  loss_dice_6: 0.2349  loss_ce_7: 0.1355  loss_mask_7: 0.2001  loss_dice_7: 0.2511  loss_ce_8: 0.1351  loss_mask_8: 0.1979  loss_dice_8: 0.2319  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:28] d2.utils.events INFO:  eta: 1:14:03  iter: 14819  total_loss: 5.527  loss_ce: 0.1316  loss_mask: 0.2143  loss_dice: 0.1372  loss_ce_0: 0.06469  loss_mask_0: 0.2136  loss_dice_0: 0.1504  loss_ce_1: 0.1308  loss_mask_1: 0.2211  loss_dice_1: 0.1454  loss_ce_2: 0.1307  loss_mask_2: 0.2184  loss_dice_2: 0.1462  loss_ce_3: 0.1307  loss_mask_3: 0.2166  loss_dice_3: 0.1456  loss_ce_4: 0.1303  loss_mask_4: 0.207  loss_dice_4: 0.1436  loss_ce_5: 0.1306  loss_mask_5: 0.2118  loss_dice_5: 0.1448  loss_ce_6: 0.1309  loss_mask_6: 0.2207  loss_dice_6: 0.1485  loss_ce_7: 0.1309  loss_mask_7: 0.2084  loss_dice_7: 0.1432  loss_ce_8: 0.1312  loss_mask_8: 0.2117  loss_dice_8: 0.1429  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:30] d2.utils.events INFO:  eta: 1:14:00  iter: 14839  total_loss: 5.17  loss_ce: 0.1221  loss_mask: 0.2291  loss_dice: 0.146  loss_ce_0: 0.06156  loss_mask_0: 0.2257  loss_dice_0: 0.1499  loss_ce_1: 0.1222  loss_mask_1: 0.2355  loss_dice_1: 0.1546  loss_ce_2: 0.1222  loss_mask_2: 0.2237  loss_dice_2: 0.1527  loss_ce_3: 0.1223  loss_mask_3: 0.2289  loss_dice_3: 0.1488  loss_ce_4: 0.1223  loss_mask_4: 0.2188  loss_dice_4: 0.1476  loss_ce_5: 0.1222  loss_mask_5: 0.2372  loss_dice_5: 0.1494  loss_ce_6: 0.1222  loss_mask_6: 0.2328  loss_dice_6: 0.1481  loss_ce_7: 0.1222  loss_mask_7: 0.2344  loss_dice_7: 0.1528  loss_ce_8: 0.1224  loss_mask_8: 0.2328  loss_dice_8: 0.15  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:12:33] d2.utils.events INFO:  eta: 1:13:54  iter: 14859  total_loss: 5.576  loss_ce: 0.1198  loss_mask: 0.172  loss_dice: 0.243  loss_ce_0: 0.0608  loss_mask_0: 0.178  loss_dice_0: 0.2454  loss_ce_1: 0.1194  loss_mask_1: 0.1763  loss_dice_1: 0.2262  loss_ce_2: 0.1198  loss_mask_2: 0.1856  loss_dice_2: 0.2328  loss_ce_3: 0.1198  loss_mask_3: 0.1867  loss_dice_3: 0.2387  loss_ce_4: 0.1196  loss_mask_4: 0.167  loss_dice_4: 0.2359  loss_ce_5: 0.1196  loss_mask_5: 0.1847  loss_dice_5: 0.2395  loss_ce_6: 0.1196  loss_mask_6: 0.1825  loss_dice_6: 0.2274  loss_ce_7: 0.1196  loss_mask_7: 0.1831  loss_dice_7: 0.2296  loss_ce_8: 0.1199  loss_mask_8: 0.173  loss_dice_8: 0.2277  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:12:36] d2.utils.events INFO:  eta: 1:13:52  iter: 14879  total_loss: 4.64  loss_ce: 0.1288  loss_mask: 0.2045  loss_dice: 0.1383  loss_ce_0: 0.06369  loss_mask_0: 0.2056  loss_dice_0: 0.1332  loss_ce_1: 0.1289  loss_mask_1: 0.196  loss_dice_1: 0.1307  loss_ce_2: 0.1289  loss_mask_2: 0.2061  loss_dice_2: 0.1353  loss_ce_3: 0.129  loss_mask_3: 0.2036  loss_dice_3: 0.131  loss_ce_4: 0.129  loss_mask_4: 0.1991  loss_dice_4: 0.1318  loss_ce_5: 0.129  loss_mask_5: 0.2004  loss_dice_5: 0.1364  loss_ce_6: 0.129  loss_mask_6: 0.2008  loss_dice_6: 0.1349  loss_ce_7: 0.129  loss_mask_7: 0.207  loss_dice_7: 0.1386  loss_ce_8: 0.1289  loss_mask_8: 0.2028  loss_dice_8: 0.1392  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:38] d2.utils.events INFO:  eta: 1:13:50  iter: 14899  total_loss: 4.954  loss_ce: 0.1458  loss_mask: 0.2411  loss_dice: 0.1318  loss_ce_0: 0.0699  loss_mask_0: 0.2311  loss_dice_0: 0.1285  loss_ce_1: 0.1456  loss_mask_1: 0.2338  loss_dice_1: 0.1322  loss_ce_2: 0.1454  loss_mask_2: 0.2337  loss_dice_2: 0.1363  loss_ce_3: 0.1457  loss_mask_3: 0.2288  loss_dice_3: 0.136  loss_ce_4: 0.1453  loss_mask_4: 0.2287  loss_dice_4: 0.1331  loss_ce_5: 0.1453  loss_mask_5: 0.2401  loss_dice_5: 0.1346  loss_ce_6: 0.1455  loss_mask_6: 0.2335  loss_dice_6: 0.1288  loss_ce_7: 0.1453  loss_mask_7: 0.234  loss_dice_7: 0.1312  loss_ce_8: 0.1456  loss_mask_8: 0.2356  loss_dice_8: 0.1336  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:12:41] d2.utils.events INFO:  eta: 1:13:47  iter: 14919  total_loss: 5.38  loss_ce: 0.1273  loss_mask: 0.2133  loss_dice: 0.1795  loss_ce_0: 0.06341  loss_mask_0: 0.2234  loss_dice_0: 0.1775  loss_ce_1: 0.1273  loss_mask_1: 0.209  loss_dice_1: 0.1863  loss_ce_2: 0.1273  loss_mask_2: 0.2059  loss_dice_2: 0.188  loss_ce_3: 0.1272  loss_mask_3: 0.2188  loss_dice_3: 0.1859  loss_ce_4: 0.1274  loss_mask_4: 0.2215  loss_dice_4: 0.1801  loss_ce_5: 0.1274  loss_mask_5: 0.2133  loss_dice_5: 0.1879  loss_ce_6: 0.1274  loss_mask_6: 0.2151  loss_dice_6: 0.1807  loss_ce_7: 0.1274  loss_mask_7: 0.2254  loss_dice_7: 0.1878  loss_ce_8: 0.1273  loss_mask_8: 0.2068  loss_dice_8: 0.1847  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:43] d2.utils.events INFO:  eta: 1:13:46  iter: 14939  total_loss: 5.309  loss_ce: 0.1191  loss_mask: 0.2179  loss_dice: 0.1592  loss_ce_0: 0.05975  loss_mask_0: 0.2263  loss_dice_0: 0.169  loss_ce_1: 0.1195  loss_mask_1: 0.2207  loss_dice_1: 0.1592  loss_ce_2: 0.1195  loss_mask_2: 0.2189  loss_dice_2: 0.1649  loss_ce_3: 0.1195  loss_mask_3: 0.2144  loss_dice_3: 0.1646  loss_ce_4: 0.1195  loss_mask_4: 0.2238  loss_dice_4: 0.1695  loss_ce_5: 0.1194  loss_mask_5: 0.221  loss_dice_5: 0.1706  loss_ce_6: 0.1195  loss_mask_6: 0.2187  loss_dice_6: 0.1654  loss_ce_7: 0.1194  loss_mask_7: 0.2237  loss_dice_7: 0.1722  loss_ce_8: 0.1192  loss_mask_8: 0.2276  loss_dice_8: 0.1673  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:46] d2.utils.events INFO:  eta: 1:13:43  iter: 14959  total_loss: 5.09  loss_ce: 0.1126  loss_mask: 0.2161  loss_dice: 0.1418  loss_ce_0: 0.05774  loss_mask_0: 0.2196  loss_dice_0: 0.1458  loss_ce_1: 0.1121  loss_mask_1: 0.2014  loss_dice_1: 0.1492  loss_ce_2: 0.1122  loss_mask_2: 0.2032  loss_dice_2: 0.1456  loss_ce_3: 0.112  loss_mask_3: 0.2062  loss_dice_3: 0.1459  loss_ce_4: 0.1117  loss_mask_4: 0.2024  loss_dice_4: 0.1435  loss_ce_5: 0.1118  loss_mask_5: 0.2031  loss_dice_5: 0.1465  loss_ce_6: 0.112  loss_mask_6: 0.204  loss_dice_6: 0.144  loss_ce_7: 0.1121  loss_mask_7: 0.2135  loss_dice_7: 0.1484  loss_ce_8: 0.1125  loss_mask_8: 0.1999  loss_dice_8: 0.144  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:48] d2.utils.events INFO:  eta: 1:13:40  iter: 14979  total_loss: 5.232  loss_ce: 0.1035  loss_mask: 0.2218  loss_dice: 0.1672  loss_ce_0: 0.05497  loss_mask_0: 0.2065  loss_dice_0: 0.1596  loss_ce_1: 0.1028  loss_mask_1: 0.2162  loss_dice_1: 0.1615  loss_ce_2: 0.1028  loss_mask_2: 0.2114  loss_dice_2: 0.157  loss_ce_3: 0.1028  loss_mask_3: 0.2169  loss_dice_3: 0.1574  loss_ce_4: 0.1025  loss_mask_4: 0.215  loss_dice_4: 0.1669  loss_ce_5: 0.1028  loss_mask_5: 0.2097  loss_dice_5: 0.1622  loss_ce_6: 0.1026  loss_mask_6: 0.212  loss_dice_6: 0.1658  loss_ce_7: 0.1029  loss_mask_7: 0.216  loss_dice_7: 0.1604  loss_ce_8: 0.1033  loss_mask_8: 0.2028  loss_dice_8: 0.1634  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:12:51] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0014999.pth
[04/13 15:12:51] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 15:12:51] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 15:12:51] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 15:12:51] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 15:12:51] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 15:12:54] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0527 s/iter. Eval: 0.2321 s/iter. Total: 0.2854 s/iter. ETA=0:04:01
[04/13 15:12:59] d2.evaluation.evaluator INFO: Inference done 29/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2317 s/iter. Total: 0.2852 s/iter. ETA=0:03:55
[04/13 15:13:05] d2.evaluation.evaluator INFO: Inference done 47/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2335 s/iter. Total: 0.2870 s/iter. ETA=0:03:52
[04/13 15:13:10] d2.evaluation.evaluator INFO: Inference done 65/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2330 s/iter. Total: 0.2866 s/iter. ETA=0:03:46
[04/13 15:13:15] d2.evaluation.evaluator INFO: Inference done 83/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2329 s/iter. Total: 0.2865 s/iter. ETA=0:03:41
[04/13 15:13:20] d2.evaluation.evaluator INFO: Inference done 101/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2329 s/iter. Total: 0.2865 s/iter. ETA=0:03:36
[04/13 15:13:25] d2.evaluation.evaluator INFO: Inference done 119/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2329 s/iter. Total: 0.2865 s/iter. ETA=0:03:31
[04/13 15:13:30] d2.evaluation.evaluator INFO: Inference done 137/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2331 s/iter. Total: 0.2867 s/iter. ETA=0:03:26
[04/13 15:13:36] d2.evaluation.evaluator INFO: Inference done 155/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2328 s/iter. Total: 0.2864 s/iter. ETA=0:03:20
[04/13 15:13:41] d2.evaluation.evaluator INFO: Inference done 173/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2327 s/iter. Total: 0.2863 s/iter. ETA=0:03:15
[04/13 15:13:46] d2.evaluation.evaluator INFO: Inference done 191/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2326 s/iter. Total: 0.2862 s/iter. ETA=0:03:10
[04/13 15:13:51] d2.evaluation.evaluator INFO: Inference done 209/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2324 s/iter. Total: 0.2860 s/iter. ETA=0:03:05
[04/13 15:13:56] d2.evaluation.evaluator INFO: Inference done 227/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2323 s/iter. Total: 0.2859 s/iter. ETA=0:02:59
[04/13 15:14:01] d2.evaluation.evaluator INFO: Inference done 245/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2325 s/iter. Total: 0.2861 s/iter. ETA=0:02:54
[04/13 15:14:07] d2.evaluation.evaluator INFO: Inference done 263/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2329 s/iter. Total: 0.2865 s/iter. ETA=0:02:49
[04/13 15:14:12] d2.evaluation.evaluator INFO: Inference done 281/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2329 s/iter. Total: 0.2865 s/iter. ETA=0:02:44
[04/13 15:14:17] d2.evaluation.evaluator INFO: Inference done 299/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2329 s/iter. Total: 0.2866 s/iter. ETA=0:02:39
[04/13 15:14:22] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2330 s/iter. Total: 0.2866 s/iter. ETA=0:02:34
[04/13 15:14:27] d2.evaluation.evaluator INFO: Inference done 335/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2330 s/iter. Total: 0.2867 s/iter. ETA=0:02:29
[04/13 15:14:32] d2.evaluation.evaluator INFO: Inference done 353/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2330 s/iter. Total: 0.2867 s/iter. ETA=0:02:24
[04/13 15:14:38] d2.evaluation.evaluator INFO: Inference done 371/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2332 s/iter. Total: 0.2869 s/iter. ETA=0:02:19
[04/13 15:14:43] d2.evaluation.evaluator INFO: Inference done 389/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2332 s/iter. Total: 0.2869 s/iter. ETA=0:02:13
[04/13 15:14:48] d2.evaluation.evaluator INFO: Inference done 407/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2332 s/iter. Total: 0.2869 s/iter. ETA=0:02:08
[04/13 15:14:53] d2.evaluation.evaluator INFO: Inference done 425/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2333 s/iter. Total: 0.2870 s/iter. ETA=0:02:03
[04/13 15:14:58] d2.evaluation.evaluator INFO: Inference done 443/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2333 s/iter. Total: 0.2870 s/iter. ETA=0:01:58
[04/13 15:15:03] d2.evaluation.evaluator INFO: Inference done 461/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2333 s/iter. Total: 0.2870 s/iter. ETA=0:01:53
[04/13 15:15:09] d2.evaluation.evaluator INFO: Inference done 479/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2333 s/iter. Total: 0.2870 s/iter. ETA=0:01:48
[04/13 15:15:14] d2.evaluation.evaluator INFO: Inference done 496/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:01:43
[04/13 15:15:19] d2.evaluation.evaluator INFO: Inference done 514/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:01:38
[04/13 15:15:24] d2.evaluation.evaluator INFO: Inference done 532/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:01:33
[04/13 15:15:29] d2.evaluation.evaluator INFO: Inference done 550/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:01:27
[04/13 15:15:34] d2.evaluation.evaluator INFO: Inference done 568/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:01:22
[04/13 15:15:40] d2.evaluation.evaluator INFO: Inference done 586/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:01:17
[04/13 15:15:45] d2.evaluation.evaluator INFO: Inference done 604/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:01:12
[04/13 15:15:50] d2.evaluation.evaluator INFO: Inference done 622/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:01:07
[04/13 15:15:55] d2.evaluation.evaluator INFO: Inference done 640/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2336 s/iter. Total: 0.2873 s/iter. ETA=0:01:02
[04/13 15:16:00] d2.evaluation.evaluator INFO: Inference done 658/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2337 s/iter. Total: 0.2874 s/iter. ETA=0:00:56
[04/13 15:16:05] d2.evaluation.evaluator INFO: Inference done 676/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2337 s/iter. Total: 0.2874 s/iter. ETA=0:00:51
[04/13 15:16:11] d2.evaluation.evaluator INFO: Inference done 694/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2337 s/iter. Total: 0.2874 s/iter. ETA=0:00:46
[04/13 15:16:16] d2.evaluation.evaluator INFO: Inference done 712/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2337 s/iter. Total: 0.2874 s/iter. ETA=0:00:41
[04/13 15:16:21] d2.evaluation.evaluator INFO: Inference done 729/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2339 s/iter. Total: 0.2876 s/iter. ETA=0:00:36
[04/13 15:16:26] d2.evaluation.evaluator INFO: Inference done 747/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2339 s/iter. Total: 0.2876 s/iter. ETA=0:00:31
[04/13 15:16:31] d2.evaluation.evaluator INFO: Inference done 765/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2339 s/iter. Total: 0.2876 s/iter. ETA=0:00:26
[04/13 15:16:36] d2.evaluation.evaluator INFO: Inference done 783/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2339 s/iter. Total: 0.2877 s/iter. ETA=0:00:21
[04/13 15:16:42] d2.evaluation.evaluator INFO: Inference done 801/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2339 s/iter. Total: 0.2877 s/iter. ETA=0:00:15
[04/13 15:16:47] d2.evaluation.evaluator INFO: Inference done 819/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2341 s/iter. Total: 0.2878 s/iter. ETA=0:00:10
[04/13 15:16:52] d2.evaluation.evaluator INFO: Inference done 836/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2343 s/iter. Total: 0.2881 s/iter. ETA=0:00:05
[04/13 15:16:57] d2.evaluation.evaluator INFO: Inference done 853/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2345 s/iter. Total: 0.2883 s/iter. ETA=0:00:00
[04/13 15:16:58] d2.evaluation.evaluator INFO: Total inference time: 0:04:05.456952 (0.288434 s / iter per device, on 1 devices)
[04/13 15:16:58] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:45 (0.052880 s / iter per device, on 1 devices)
[04/13 15:16:59] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 15:16:59] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 15:17:01] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 15:17:01] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 15:17:01] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.28 seconds.
[04/13 15:17:01] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 15:17:01] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 15:17:01] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 15:17:01] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 15:17:01] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 15:17:05] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 15:17:07] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 2.03 seconds.
[04/13 15:17:07] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 15:17:07] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 15:17:07] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 60.484 | 60.484 | 60.484 |  nan  |  nan  | 60.484 |
[04/13 15:17:07] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 15:17:07] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 68.515 | defect     | 52.453 |
[04/13 15:17:07] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 15:17:07] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 15:17:07] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 15:17:07] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 15:17:07] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 15:17:07] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 15:17:07] d2.evaluation.testing INFO: copypaste: 60.4841,60.4841,60.4841,nan,nan,60.4841
[04/13 15:17:07] d2.utils.events INFO:  eta: 1:13:36  iter: 14999  total_loss: 5.184  loss_ce: 0.1004  loss_mask: 0.2134  loss_dice: 0.1713  loss_ce_0: 0.05387  loss_mask_0: 0.2222  loss_dice_0: 0.1617  loss_ce_1: 0.1002  loss_mask_1: 0.214  loss_dice_1: 0.1669  loss_ce_2: 0.1004  loss_mask_2: 0.2081  loss_dice_2: 0.1694  loss_ce_3: 0.1003  loss_mask_3: 0.2191  loss_dice_3: 0.1587  loss_ce_4: 0.1003  loss_mask_4: 0.2167  loss_dice_4: 0.1679  loss_ce_5: 0.1003  loss_mask_5: 0.2115  loss_dice_5: 0.1707  loss_ce_6: 0.09987  loss_mask_6: 0.211  loss_dice_6: 0.1687  loss_ce_7: 0.1001  loss_mask_7: 0.2216  loss_dice_7: 0.1615  loss_ce_8: 0.1002  loss_mask_8: 0.2226  loss_dice_8: 0.1595  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:17:10] d2.utils.events INFO:  eta: 1:13:34  iter: 15019  total_loss: 5.286  loss_ce: 0.09854  loss_mask: 0.2081  loss_dice: 0.1801  loss_ce_0: 0.05298  loss_mask_0: 0.2031  loss_dice_0: 0.1714  loss_ce_1: 0.09841  loss_mask_1: 0.2206  loss_dice_1: 0.1782  loss_ce_2: 0.09849  loss_mask_2: 0.2003  loss_dice_2: 0.1817  loss_ce_3: 0.09845  loss_mask_3: 0.2124  loss_dice_3: 0.1733  loss_ce_4: 0.09849  loss_mask_4: 0.1983  loss_dice_4: 0.1646  loss_ce_5: 0.09856  loss_mask_5: 0.2065  loss_dice_5: 0.177  loss_ce_6: 0.09834  loss_mask_6: 0.2106  loss_dice_6: 0.1821  loss_ce_7: 0.09849  loss_mask_7: 0.208  loss_dice_7: 0.1784  loss_ce_8: 0.09856  loss_mask_8: 0.2029  loss_dice_8: 0.1789  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:12] d2.utils.events INFO:  eta: 1:13:30  iter: 15039  total_loss: 4.971  loss_ce: 0.1599  loss_mask: 0.1743  loss_dice: 0.1551  loss_ce_0: 0.07521  loss_mask_0: 0.1831  loss_dice_0: 0.1581  loss_ce_1: 0.1601  loss_mask_1: 0.1816  loss_dice_1: 0.1692  loss_ce_2: 0.16  loss_mask_2: 0.1789  loss_dice_2: 0.1662  loss_ce_3: 0.1604  loss_mask_3: 0.1859  loss_dice_3: 0.1632  loss_ce_4: 0.16  loss_mask_4: 0.1772  loss_dice_4: 0.1602  loss_ce_5: 0.1599  loss_mask_5: 0.1805  loss_dice_5: 0.1645  loss_ce_6: 0.1599  loss_mask_6: 0.1848  loss_dice_6: 0.1645  loss_ce_7: 0.1597  loss_mask_7: 0.1735  loss_dice_7: 0.1634  loss_ce_8: 0.1597  loss_mask_8: 0.1778  loss_dice_8: 0.1679  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:15] d2.utils.events INFO:  eta: 1:13:24  iter: 15059  total_loss: 5.099  loss_ce: 0.1459  loss_mask: 0.1966  loss_dice: 0.1574  loss_ce_0: 0.07133  loss_mask_0: 0.2067  loss_dice_0: 0.1579  loss_ce_1: 0.145  loss_mask_1: 0.2077  loss_dice_1: 0.1672  loss_ce_2: 0.1454  loss_mask_2: 0.196  loss_dice_2: 0.1613  loss_ce_3: 0.1452  loss_mask_3: 0.2037  loss_dice_3: 0.16  loss_ce_4: 0.1445  loss_mask_4: 0.1969  loss_dice_4: 0.1622  loss_ce_5: 0.1445  loss_mask_5: 0.2061  loss_dice_5: 0.1629  loss_ce_6: 0.1443  loss_mask_6: 0.1947  loss_dice_6: 0.1611  loss_ce_7: 0.1443  loss_mask_7: 0.1997  loss_dice_7: 0.1604  loss_ce_8: 0.1453  loss_mask_8: 0.209  loss_dice_8: 0.1591  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:17] d2.utils.events INFO:  eta: 1:13:23  iter: 15079  total_loss: 5.338  loss_ce: 0.1205  loss_mask: 0.2488  loss_dice: 0.1617  loss_ce_0: 0.05936  loss_mask_0: 0.2353  loss_dice_0: 0.1685  loss_ce_1: 0.1218  loss_mask_1: 0.2451  loss_dice_1: 0.1576  loss_ce_2: 0.1214  loss_mask_2: 0.2413  loss_dice_2: 0.1619  loss_ce_3: 0.1218  loss_mask_3: 0.2452  loss_dice_3: 0.1612  loss_ce_4: 0.1225  loss_mask_4: 0.2618  loss_dice_4: 0.1673  loss_ce_5: 0.1224  loss_mask_5: 0.2372  loss_dice_5: 0.1618  loss_ce_6: 0.1228  loss_mask_6: 0.2398  loss_dice_6: 0.163  loss_ce_7: 0.1224  loss_mask_7: 0.2473  loss_dice_7: 0.1645  loss_ce_8: 0.1214  loss_mask_8: 0.2504  loss_dice_8: 0.1641  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:20] d2.utils.events INFO:  eta: 1:13:20  iter: 15099  total_loss: 5.033  loss_ce: 0.1276  loss_mask: 0.2065  loss_dice: 0.1577  loss_ce_0: 0.06352  loss_mask_0: 0.2087  loss_dice_0: 0.1599  loss_ce_1: 0.1276  loss_mask_1: 0.2062  loss_dice_1: 0.1595  loss_ce_2: 0.1275  loss_mask_2: 0.2139  loss_dice_2: 0.1666  loss_ce_3: 0.1276  loss_mask_3: 0.2047  loss_dice_3: 0.1632  loss_ce_4: 0.1276  loss_mask_4: 0.2182  loss_dice_4: 0.1598  loss_ce_5: 0.1275  loss_mask_5: 0.2009  loss_dice_5: 0.1683  loss_ce_6: 0.1274  loss_mask_6: 0.2098  loss_dice_6: 0.1722  loss_ce_7: 0.1275  loss_mask_7: 0.2099  loss_dice_7: 0.1632  loss_ce_8: 0.1276  loss_mask_8: 0.2145  loss_dice_8: 0.1641  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:22] d2.utils.events INFO:  eta: 1:13:16  iter: 15119  total_loss: 5.031  loss_ce: 0.1258  loss_mask: 0.2151  loss_dice: 0.1351  loss_ce_0: 0.06174  loss_mask_0: 0.2185  loss_dice_0: 0.1342  loss_ce_1: 0.1265  loss_mask_1: 0.2085  loss_dice_1: 0.1359  loss_ce_2: 0.1263  loss_mask_2: 0.2241  loss_dice_2: 0.1323  loss_ce_3: 0.1271  loss_mask_3: 0.2131  loss_dice_3: 0.1356  loss_ce_4: 0.1274  loss_mask_4: 0.2135  loss_dice_4: 0.1345  loss_ce_5: 0.1267  loss_mask_5: 0.2043  loss_dice_5: 0.1332  loss_ce_6: 0.1269  loss_mask_6: 0.212  loss_dice_6: 0.1342  loss_ce_7: 0.1265  loss_mask_7: 0.2102  loss_dice_7: 0.1369  loss_ce_8: 0.1261  loss_mask_8: 0.2038  loss_dice_8: 0.1377  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:25] d2.utils.events INFO:  eta: 1:13:13  iter: 15139  total_loss: 5.296  loss_ce: 0.1238  loss_mask: 0.2283  loss_dice: 0.1396  loss_ce_0: 0.06117  loss_mask_0: 0.2351  loss_dice_0: 0.1456  loss_ce_1: 0.1238  loss_mask_1: 0.233  loss_dice_1: 0.134  loss_ce_2: 0.1238  loss_mask_2: 0.2222  loss_dice_2: 0.1425  loss_ce_3: 0.1243  loss_mask_3: 0.2269  loss_dice_3: 0.1426  loss_ce_4: 0.1245  loss_mask_4: 0.2474  loss_dice_4: 0.1474  loss_ce_5: 0.1242  loss_mask_5: 0.2255  loss_dice_5: 0.1458  loss_ce_6: 0.1246  loss_mask_6: 0.2348  loss_dice_6: 0.1375  loss_ce_7: 0.1242  loss_mask_7: 0.2388  loss_dice_7: 0.1441  loss_ce_8: 0.1242  loss_mask_8: 0.2348  loss_dice_8: 0.1425  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:17:27] d2.utils.events INFO:  eta: 1:13:10  iter: 15159  total_loss: 4.548  loss_ce: 0.1274  loss_mask: 0.198  loss_dice: 0.1225  loss_ce_0: 0.06341  loss_mask_0: 0.2006  loss_dice_0: 0.1206  loss_ce_1: 0.1274  loss_mask_1: 0.2014  loss_dice_1: 0.1193  loss_ce_2: 0.1274  loss_mask_2: 0.1964  loss_dice_2: 0.1268  loss_ce_3: 0.1274  loss_mask_3: 0.2016  loss_dice_3: 0.121  loss_ce_4: 0.1274  loss_mask_4: 0.2022  loss_dice_4: 0.1227  loss_ce_5: 0.1274  loss_mask_5: 0.1959  loss_dice_5: 0.1229  loss_ce_6: 0.1274  loss_mask_6: 0.1995  loss_dice_6: 0.1236  loss_ce_7: 0.1274  loss_mask_7: 0.207  loss_dice_7: 0.1209  loss_ce_8: 0.1274  loss_mask_8: 0.2028  loss_dice_8: 0.12  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:17:30] d2.utils.events INFO:  eta: 1:13:07  iter: 15179  total_loss: 4.566  loss_ce: 0.1276  loss_mask: 0.1675  loss_dice: 0.1432  loss_ce_0: 0.06346  loss_mask_0: 0.1709  loss_dice_0: 0.1464  loss_ce_1: 0.1275  loss_mask_1: 0.1534  loss_dice_1: 0.1418  loss_ce_2: 0.1276  loss_mask_2: 0.1562  loss_dice_2: 0.1519  loss_ce_3: 0.1276  loss_mask_3: 0.1602  loss_dice_3: 0.1482  loss_ce_4: 0.1276  loss_mask_4: 0.1646  loss_dice_4: 0.1513  loss_ce_5: 0.1276  loss_mask_5: 0.1594  loss_dice_5: 0.144  loss_ce_6: 0.1276  loss_mask_6: 0.1608  loss_dice_6: 0.1468  loss_ce_7: 0.1276  loss_mask_7: 0.1702  loss_dice_7: 0.1447  loss_ce_8: 0.1276  loss_mask_8: 0.167  loss_dice_8: 0.1481  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:17:32] d2.utils.events INFO:  eta: 1:13:01  iter: 15199  total_loss: 4.912  loss_ce: 0.1314  loss_mask: 0.1881  loss_dice: 0.1219  loss_ce_0: 0.0644  loss_mask_0: 0.179  loss_dice_0: 0.1333  loss_ce_1: 0.1297  loss_mask_1: 0.1865  loss_dice_1: 0.1315  loss_ce_2: 0.1299  loss_mask_2: 0.1824  loss_dice_2: 0.1333  loss_ce_3: 0.1295  loss_mask_3: 0.1773  loss_dice_3: 0.1275  loss_ce_4: 0.1307  loss_mask_4: 0.1846  loss_dice_4: 0.1293  loss_ce_5: 0.1304  loss_mask_5: 0.1766  loss_dice_5: 0.1313  loss_ce_6: 0.1298  loss_mask_6: 0.1761  loss_dice_6: 0.133  loss_ce_7: 0.1285  loss_mask_7: 0.1837  loss_dice_7: 0.1349  loss_ce_8: 0.128  loss_mask_8: 0.1781  loss_dice_8: 0.1287  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:35] d2.utils.events INFO:  eta: 1:12:56  iter: 15219  total_loss: 4.472  loss_ce: 0.1279  loss_mask: 0.1697  loss_dice: 0.1388  loss_ce_0: 0.06389  loss_mask_0: 0.1711  loss_dice_0: 0.143  loss_ce_1: 0.1277  loss_mask_1: 0.1693  loss_dice_1: 0.1369  loss_ce_2: 0.1281  loss_mask_2: 0.1764  loss_dice_2: 0.1436  loss_ce_3: 0.128  loss_mask_3: 0.1827  loss_dice_3: 0.1431  loss_ce_4: 0.1279  loss_mask_4: 0.1769  loss_dice_4: 0.146  loss_ce_5: 0.1279  loss_mask_5: 0.1759  loss_dice_5: 0.1396  loss_ce_6: 0.128  loss_mask_6: 0.1687  loss_dice_6: 0.1377  loss_ce_7: 0.1278  loss_mask_7: 0.1737  loss_dice_7: 0.1436  loss_ce_8: 0.128  loss_mask_8: 0.1682  loss_dice_8: 0.1419  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:17:37] d2.utils.events INFO:  eta: 1:12:47  iter: 15239  total_loss: 4.872  loss_ce: 0.1195  loss_mask: 0.2036  loss_dice: 0.1838  loss_ce_0: 0.06048  loss_mask_0: 0.1862  loss_dice_0: 0.1777  loss_ce_1: 0.1194  loss_mask_1: 0.1782  loss_dice_1: 0.1735  loss_ce_2: 0.1147  loss_mask_2: 0.2001  loss_dice_2: 0.1806  loss_ce_3: 0.1156  loss_mask_3: 0.1999  loss_dice_3: 0.1804  loss_ce_4: 0.1146  loss_mask_4: 0.1944  loss_dice_4: 0.1761  loss_ce_5: 0.1148  loss_mask_5: 0.1906  loss_dice_5: 0.1724  loss_ce_6: 0.1148  loss_mask_6: 0.2028  loss_dice_6: 0.1776  loss_ce_7: 0.1162  loss_mask_7: 0.1966  loss_dice_7: 0.1735  loss_ce_8: 0.1154  loss_mask_8: 0.1913  loss_dice_8: 0.1754  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:40] d2.utils.events INFO:  eta: 1:12:43  iter: 15259  total_loss: 5.114  loss_ce: 0.1465  loss_mask: 0.2331  loss_dice: 0.1425  loss_ce_0: 0.06933  loss_mask_0: 0.2362  loss_dice_0: 0.1425  loss_ce_1: 0.1461  loss_mask_1: 0.2358  loss_dice_1: 0.1379  loss_ce_2: 0.1489  loss_mask_2: 0.2292  loss_dice_2: 0.1367  loss_ce_3: 0.148  loss_mask_3: 0.2278  loss_dice_3: 0.1403  loss_ce_4: 0.1487  loss_mask_4: 0.235  loss_dice_4: 0.1367  loss_ce_5: 0.1488  loss_mask_5: 0.2319  loss_dice_5: 0.1352  loss_ce_6: 0.149  loss_mask_6: 0.2412  loss_dice_6: 0.1397  loss_ce_7: 0.1479  loss_mask_7: 0.2272  loss_dice_7: 0.1368  loss_ce_8: 0.1499  loss_mask_8: 0.2226  loss_dice_8: 0.1371  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:17:42] d2.utils.events INFO:  eta: 1:12:41  iter: 15279  total_loss: 5.101  loss_ce: 0.1314  loss_mask: 0.2218  loss_dice: 0.1372  loss_ce_0: 0.06553  loss_mask_0: 0.2005  loss_dice_0: 0.1365  loss_ce_1: 0.1318  loss_mask_1: 0.2087  loss_dice_1: 0.1414  loss_ce_2: 0.1283  loss_mask_2: 0.2018  loss_dice_2: 0.1388  loss_ce_3: 0.1283  loss_mask_3: 0.1988  loss_dice_3: 0.1351  loss_ce_4: 0.1277  loss_mask_4: 0.2018  loss_dice_4: 0.1392  loss_ce_5: 0.1284  loss_mask_5: 0.2076  loss_dice_5: 0.1327  loss_ce_6: 0.1283  loss_mask_6: 0.2131  loss_dice_6: 0.1471  loss_ce_7: 0.129  loss_mask_7: 0.2002  loss_dice_7: 0.1428  loss_ce_8: 0.1298  loss_mask_8: 0.1988  loss_dice_8: 0.1326  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:17:45] d2.utils.events INFO:  eta: 1:12:33  iter: 15299  total_loss: 4.714  loss_ce: 0.1292  loss_mask: 0.1494  loss_dice: 0.1603  loss_ce_0: 0.06356  loss_mask_0: 0.158  loss_dice_0: 0.1662  loss_ce_1: 0.1297  loss_mask_1: 0.1557  loss_dice_1: 0.1707  loss_ce_2: 0.1294  loss_mask_2: 0.1598  loss_dice_2: 0.1696  loss_ce_3: 0.1294  loss_mask_3: 0.1545  loss_dice_3: 0.1718  loss_ce_4: 0.1291  loss_mask_4: 0.1573  loss_dice_4: 0.1734  loss_ce_5: 0.1294  loss_mask_5: 0.1541  loss_dice_5: 0.1693  loss_ce_6: 0.1296  loss_mask_6: 0.1603  loss_dice_6: 0.1709  loss_ce_7: 0.1298  loss_mask_7: 0.1617  loss_dice_7: 0.1791  loss_ce_8: 0.1295  loss_mask_8: 0.1573  loss_dice_8: 0.166  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:47] d2.utils.events INFO:  eta: 1:12:26  iter: 15319  total_loss: 4.904  loss_ce: 0.1285  loss_mask: 0.193  loss_dice: 0.1525  loss_ce_0: 0.06253  loss_mask_0: 0.1955  loss_dice_0: 0.1445  loss_ce_1: 0.1276  loss_mask_1: 0.1946  loss_dice_1: 0.1441  loss_ce_2: 0.1289  loss_mask_2: 0.2076  loss_dice_2: 0.1443  loss_ce_3: 0.1279  loss_mask_3: 0.1968  loss_dice_3: 0.1469  loss_ce_4: 0.1277  loss_mask_4: 0.1981  loss_dice_4: 0.1438  loss_ce_5: 0.1282  loss_mask_5: 0.1911  loss_dice_5: 0.147  loss_ce_6: 0.1287  loss_mask_6: 0.1902  loss_dice_6: 0.1478  loss_ce_7: 0.1286  loss_mask_7: 0.1893  loss_dice_7: 0.1468  loss_ce_8: 0.1288  loss_mask_8: 0.1927  loss_dice_8: 0.1402  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:50] d2.utils.events INFO:  eta: 1:12:23  iter: 15339  total_loss: 4.382  loss_ce: 0.1125  loss_mask: 0.1865  loss_dice: 0.131  loss_ce_0: 0.05747  loss_mask_0: 0.1765  loss_dice_0: 0.1209  loss_ce_1: 0.1109  loss_mask_1: 0.1879  loss_dice_1: 0.1267  loss_ce_2: 0.1088  loss_mask_2: 0.1827  loss_dice_2: 0.1317  loss_ce_3: 0.1067  loss_mask_3: 0.1775  loss_dice_3: 0.1254  loss_ce_4: 0.1051  loss_mask_4: 0.1782  loss_dice_4: 0.1299  loss_ce_5: 0.1067  loss_mask_5: 0.1881  loss_dice_5: 0.1278  loss_ce_6: 0.1067  loss_mask_6: 0.1852  loss_dice_6: 0.1294  loss_ce_7: 0.1082  loss_mask_7: 0.1839  loss_dice_7: 0.1264  loss_ce_8: 0.1109  loss_mask_8: 0.1806  loss_dice_8: 0.1295  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:17:52] d2.utils.events INFO:  eta: 1:12:20  iter: 15359  total_loss: 5.299  loss_ce: 0.1293  loss_mask: 0.1536  loss_dice: 0.1518  loss_ce_0: 0.06376  loss_mask_0: 0.144  loss_dice_0: 0.1456  loss_ce_1: 0.1295  loss_mask_1: 0.16  loss_dice_1: 0.1557  loss_ce_2: 0.1299  loss_mask_2: 0.1476  loss_dice_2: 0.1468  loss_ce_3: 0.1305  loss_mask_3: 0.1567  loss_dice_3: 0.1484  loss_ce_4: 0.1306  loss_mask_4: 0.1453  loss_dice_4: 0.1459  loss_ce_5: 0.1305  loss_mask_5: 0.1537  loss_dice_5: 0.1586  loss_ce_6: 0.1306  loss_mask_6: 0.1523  loss_dice_6: 0.1514  loss_ce_7: 0.13  loss_mask_7: 0.1477  loss_dice_7: 0.1532  loss_ce_8: 0.1297  loss_mask_8: 0.1513  loss_dice_8: 0.1485  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:17:55] d2.utils.events INFO:  eta: 1:12:18  iter: 15379  total_loss: 5.391  loss_ce: 0.1553  loss_mask: 0.2042  loss_dice: 0.1417  loss_ce_0: 0.07274  loss_mask_0: 0.2138  loss_dice_0: 0.1438  loss_ce_1: 0.1545  loss_mask_1: 0.2123  loss_dice_1: 0.1442  loss_ce_2: 0.1547  loss_mask_2: 0.2021  loss_dice_2: 0.1447  loss_ce_3: 0.1539  loss_mask_3: 0.2109  loss_dice_3: 0.1459  loss_ce_4: 0.1523  loss_mask_4: 0.2089  loss_dice_4: 0.1473  loss_ce_5: 0.1532  loss_mask_5: 0.1984  loss_dice_5: 0.1516  loss_ce_6: 0.1543  loss_mask_6: 0.2064  loss_dice_6: 0.1451  loss_ce_7: 0.1546  loss_mask_7: 0.2106  loss_dice_7: 0.1385  loss_ce_8: 0.1563  loss_mask_8: 0.2074  loss_dice_8: 0.1455  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:17:57] d2.utils.events INFO:  eta: 1:12:15  iter: 15399  total_loss: 5.534  loss_ce: 0.1364  loss_mask: 0.1793  loss_dice: 0.1879  loss_ce_0: 0.0672  loss_mask_0: 0.1759  loss_dice_0: 0.1931  loss_ce_1: 0.135  loss_mask_1: 0.1726  loss_dice_1: 0.1954  loss_ce_2: 0.1323  loss_mask_2: 0.1745  loss_dice_2: 0.1964  loss_ce_3: 0.1313  loss_mask_3: 0.1748  loss_dice_3: 0.1969  loss_ce_4: 0.1316  loss_mask_4: 0.1713  loss_dice_4: 0.1953  loss_ce_5: 0.1313  loss_mask_5: 0.1713  loss_dice_5: 0.1879  loss_ce_6: 0.1314  loss_mask_6: 0.1732  loss_dice_6: 0.1973  loss_ce_7: 0.1324  loss_mask_7: 0.1764  loss_dice_7: 0.2068  loss_ce_8: 0.1354  loss_mask_8: 0.1673  loss_dice_8: 0.1942  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:00] d2.utils.events INFO:  eta: 1:12:13  iter: 15419  total_loss: 5.23  loss_ce: 0.127  loss_mask: 0.2318  loss_dice: 0.1673  loss_ce_0: 0.06353  loss_mask_0: 0.2127  loss_dice_0: 0.1599  loss_ce_1: 0.1261  loss_mask_1: 0.2129  loss_dice_1: 0.1608  loss_ce_2: 0.123  loss_mask_2: 0.2366  loss_dice_2: 0.159  loss_ce_3: 0.121  loss_mask_3: 0.2229  loss_dice_3: 0.1662  loss_ce_4: 0.1179  loss_mask_4: 0.2136  loss_dice_4: 0.1528  loss_ce_5: 0.1205  loss_mask_5: 0.2246  loss_dice_5: 0.1646  loss_ce_6: 0.1207  loss_mask_6: 0.2115  loss_dice_6: 0.1595  loss_ce_7: 0.1227  loss_mask_7: 0.2316  loss_dice_7: 0.1619  loss_ce_8: 0.1265  loss_mask_8: 0.223  loss_dice_8: 0.1643  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:18:02] d2.utils.events INFO:  eta: 1:12:13  iter: 15439  total_loss: 4.708  loss_ce: 0.1223  loss_mask: 0.1833  loss_dice: 0.1541  loss_ce_0: 0.0631  loss_mask_0: 0.1775  loss_dice_0: 0.1591  loss_ce_1: 0.1233  loss_mask_1: 0.1807  loss_dice_1: 0.1554  loss_ce_2: 0.1223  loss_mask_2: 0.1909  loss_dice_2: 0.1546  loss_ce_3: 0.1235  loss_mask_3: 0.1828  loss_dice_3: 0.1591  loss_ce_4: 0.1241  loss_mask_4: 0.1822  loss_dice_4: 0.1502  loss_ce_5: 0.1239  loss_mask_5: 0.1855  loss_dice_5: 0.156  loss_ce_6: 0.1226  loss_mask_6: 0.1863  loss_dice_6: 0.1542  loss_ce_7: 0.1228  loss_mask_7: 0.185  loss_dice_7: 0.1525  loss_ce_8: 0.1212  loss_mask_8: 0.1787  loss_dice_8: 0.1511  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:05] d2.utils.events INFO:  eta: 1:12:09  iter: 15459  total_loss: 5.416  loss_ce: 0.1206  loss_mask: 0.2223  loss_dice: 0.1688  loss_ce_0: 0.06213  loss_mask_0: 0.2169  loss_dice_0: 0.1723  loss_ce_1: 0.1206  loss_mask_1: 0.2236  loss_dice_1: 0.1677  loss_ce_2: 0.1198  loss_mask_2: 0.2218  loss_dice_2: 0.1665  loss_ce_3: 0.1206  loss_mask_3: 0.2219  loss_dice_3: 0.1671  loss_ce_4: 0.1208  loss_mask_4: 0.2242  loss_dice_4: 0.1651  loss_ce_5: 0.1209  loss_mask_5: 0.2275  loss_dice_5: 0.1732  loss_ce_6: 0.1203  loss_mask_6: 0.2268  loss_dice_6: 0.1701  loss_ce_7: 0.1206  loss_mask_7: 0.2283  loss_dice_7: 0.1674  loss_ce_8: 0.1197  loss_mask_8: 0.2302  loss_dice_8: 0.1652  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:07] d2.utils.events INFO:  eta: 1:12:04  iter: 15479  total_loss: 5.503  loss_ce: 0.111  loss_mask: 0.1827  loss_dice: 0.1977  loss_ce_0: 0.05816  loss_mask_0: 0.189  loss_dice_0: 0.2033  loss_ce_1: 0.1094  loss_mask_1: 0.1876  loss_dice_1: 0.1855  loss_ce_2: 0.1076  loss_mask_2: 0.1905  loss_dice_2: 0.2126  loss_ce_3: 0.1071  loss_mask_3: 0.1915  loss_dice_3: 0.2021  loss_ce_4: 0.1054  loss_mask_4: 0.1824  loss_dice_4: 0.1898  loss_ce_5: 0.1067  loss_mask_5: 0.1827  loss_dice_5: 0.1979  loss_ce_6: 0.1067  loss_mask_6: 0.1826  loss_dice_6: 0.198  loss_ce_7: 0.1078  loss_mask_7: 0.1834  loss_dice_7: 0.1979  loss_ce_8: 0.1099  loss_mask_8: 0.1931  loss_dice_8: 0.2019  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:10] d2.utils.events INFO:  eta: 1:12:03  iter: 15499  total_loss: 5.964  loss_ce: 0.1285  loss_mask: 0.1433  loss_dice: 0.3221  loss_ce_0: 0.06373  loss_mask_0: 0.142  loss_dice_0: 0.3067  loss_ce_1: 0.1285  loss_mask_1: 0.1488  loss_dice_1: 0.3098  loss_ce_2: 0.1287  loss_mask_2: 0.1457  loss_dice_2: 0.3063  loss_ce_3: 0.1285  loss_mask_3: 0.1527  loss_dice_3: 0.3222  loss_ce_4: 0.1285  loss_mask_4: 0.1403  loss_dice_4: 0.3133  loss_ce_5: 0.1286  loss_mask_5: 0.1379  loss_dice_5: 0.3002  loss_ce_6: 0.1285  loss_mask_6: 0.1458  loss_dice_6: 0.3096  loss_ce_7: 0.1285  loss_mask_7: 0.1403  loss_dice_7: 0.2908  loss_ce_8: 0.1287  loss_mask_8: 0.1419  loss_dice_8: 0.2994  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:13] d2.utils.events INFO:  eta: 1:12:05  iter: 15519  total_loss: 5.752  loss_ce: 0.1431  loss_mask: 0.1478  loss_dice: 0.2858  loss_ce_0: 0.06862  loss_mask_0: 0.1467  loss_dice_0: 0.304  loss_ce_1: 0.1423  loss_mask_1: 0.1465  loss_dice_1: 0.3117  loss_ce_2: 0.142  loss_mask_2: 0.1295  loss_dice_2: 0.2781  loss_ce_3: 0.1408  loss_mask_3: 0.1511  loss_dice_3: 0.3062  loss_ce_4: 0.1395  loss_mask_4: 0.1461  loss_dice_4: 0.3043  loss_ce_5: 0.1403  loss_mask_5: 0.1439  loss_dice_5: 0.3073  loss_ce_6: 0.1405  loss_mask_6: 0.1541  loss_dice_6: 0.3223  loss_ce_7: 0.1408  loss_mask_7: 0.1483  loss_dice_7: 0.2994  loss_ce_8: 0.143  loss_mask_8: 0.139  loss_dice_8: 0.3055  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:15] d2.utils.events INFO:  eta: 1:12:03  iter: 15539  total_loss: 5.086  loss_ce: 0.1331  loss_mask: 0.2085  loss_dice: 0.1399  loss_ce_0: 0.06545  loss_mask_0: 0.2056  loss_dice_0: 0.135  loss_ce_1: 0.1335  loss_mask_1: 0.1943  loss_dice_1: 0.133  loss_ce_2: 0.1332  loss_mask_2: 0.2203  loss_dice_2: 0.137  loss_ce_3: 0.1331  loss_mask_3: 0.2028  loss_dice_3: 0.1345  loss_ce_4: 0.1312  loss_mask_4: 0.2083  loss_dice_4: 0.1341  loss_ce_5: 0.1325  loss_mask_5: 0.2035  loss_dice_5: 0.1311  loss_ce_6: 0.1325  loss_mask_6: 0.2052  loss_dice_6: 0.1341  loss_ce_7: 0.1332  loss_mask_7: 0.2018  loss_dice_7: 0.1325  loss_ce_8: 0.1332  loss_mask_8: 0.2091  loss_dice_8: 0.1299  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:18] d2.utils.events INFO:  eta: 1:12:05  iter: 15559  total_loss: 4.874  loss_ce: 0.1357  loss_mask: 0.2165  loss_dice: 0.1271  loss_ce_0: 0.06608  loss_mask_0: 0.2082  loss_dice_0: 0.1309  loss_ce_1: 0.1366  loss_mask_1: 0.2211  loss_dice_1: 0.1385  loss_ce_2: 0.1378  loss_mask_2: 0.2167  loss_dice_2: 0.1389  loss_ce_3: 0.1382  loss_mask_3: 0.2107  loss_dice_3: 0.1362  loss_ce_4: 0.1388  loss_mask_4: 0.2194  loss_dice_4: 0.1352  loss_ce_5: 0.1386  loss_mask_5: 0.216  loss_dice_5: 0.1366  loss_ce_6: 0.1403  loss_mask_6: 0.2202  loss_dice_6: 0.132  loss_ce_7: 0.1394  loss_mask_7: 0.217  loss_dice_7: 0.1354  loss_ce_8: 0.1373  loss_mask_8: 0.2144  loss_dice_8: 0.1341  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:18:20] d2.utils.events INFO:  eta: 1:12:04  iter: 15579  total_loss: 5.181  loss_ce: 0.1279  loss_mask: 0.2276  loss_dice: 0.1461  loss_ce_0: 0.0637  loss_mask_0: 0.2124  loss_dice_0: 0.1424  loss_ce_1: 0.1279  loss_mask_1: 0.2081  loss_dice_1: 0.1498  loss_ce_2: 0.128  loss_mask_2: 0.2176  loss_dice_2: 0.1486  loss_ce_3: 0.128  loss_mask_3: 0.208  loss_dice_3: 0.149  loss_ce_4: 0.128  loss_mask_4: 0.2091  loss_dice_4: 0.1508  loss_ce_5: 0.128  loss_mask_5: 0.2125  loss_dice_5: 0.1505  loss_ce_6: 0.128  loss_mask_6: 0.211  loss_dice_6: 0.1467  loss_ce_7: 0.128  loss_mask_7: 0.2186  loss_dice_7: 0.1484  loss_ce_8: 0.1279  loss_mask_8: 0.2082  loss_dice_8: 0.1512  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:23] d2.utils.events INFO:  eta: 1:12:03  iter: 15599  total_loss: 5.211  loss_ce: 0.1403  loss_mask: 0.1988  loss_dice: 0.1692  loss_ce_0: 0.06735  loss_mask_0: 0.1942  loss_dice_0: 0.1666  loss_ce_1: 0.1394  loss_mask_1: 0.1934  loss_dice_1: 0.1669  loss_ce_2: 0.1394  loss_mask_2: 0.1981  loss_dice_2: 0.1699  loss_ce_3: 0.1388  loss_mask_3: 0.1954  loss_dice_3: 0.1685  loss_ce_4: 0.1379  loss_mask_4: 0.1898  loss_dice_4: 0.1645  loss_ce_5: 0.1385  loss_mask_5: 0.2063  loss_dice_5: 0.1608  loss_ce_6: 0.1394  loss_mask_6: 0.1922  loss_dice_6: 0.1684  loss_ce_7: 0.1395  loss_mask_7: 0.2022  loss_dice_7: 0.1603  loss_ce_8: 0.1406  loss_mask_8: 0.2015  loss_dice_8: 0.1653  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:25] d2.utils.events INFO:  eta: 1:12:03  iter: 15619  total_loss: 5.18  loss_ce: 0.1276  loss_mask: 0.2069  loss_dice: 0.1388  loss_ce_0: 0.0636  loss_mask_0: 0.2072  loss_dice_0: 0.1338  loss_ce_1: 0.1276  loss_mask_1: 0.2027  loss_dice_1: 0.1324  loss_ce_2: 0.1276  loss_mask_2: 0.2072  loss_dice_2: 0.1309  loss_ce_3: 0.1276  loss_mask_3: 0.2104  loss_dice_3: 0.1414  loss_ce_4: 0.1275  loss_mask_4: 0.2114  loss_dice_4: 0.1319  loss_ce_5: 0.1276  loss_mask_5: 0.2043  loss_dice_5: 0.129  loss_ce_6: 0.1276  loss_mask_6: 0.2089  loss_dice_6: 0.1338  loss_ce_7: 0.1276  loss_mask_7: 0.2165  loss_dice_7: 0.1319  loss_ce_8: 0.1276  loss_mask_8: 0.2093  loss_dice_8: 0.1347  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:28] d2.utils.events INFO:  eta: 1:12:01  iter: 15639  total_loss: 4.549  loss_ce: 0.1352  loss_mask: 0.1937  loss_dice: 0.1317  loss_ce_0: 0.06613  loss_mask_0: 0.1987  loss_dice_0: 0.1364  loss_ce_1: 0.1345  loss_mask_1: 0.1871  loss_dice_1: 0.1389  loss_ce_2: 0.1339  loss_mask_2: 0.1925  loss_dice_2: 0.133  loss_ce_3: 0.1334  loss_mask_3: 0.199  loss_dice_3: 0.1307  loss_ce_4: 0.1333  loss_mask_4: 0.195  loss_dice_4: 0.1333  loss_ce_5: 0.1339  loss_mask_5: 0.1966  loss_dice_5: 0.1289  loss_ce_6: 0.1338  loss_mask_6: 0.1936  loss_dice_6: 0.1329  loss_ce_7: 0.1342  loss_mask_7: 0.1972  loss_dice_7: 0.1363  loss_ce_8: 0.1348  loss_mask_8: 0.1924  loss_dice_8: 0.128  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:18:30] d2.utils.events INFO:  eta: 1:12:02  iter: 15659  total_loss: 4.921  loss_ce: 0.134  loss_mask: 0.2108  loss_dice: 0.1594  loss_ce_0: 0.06583  loss_mask_0: 0.2062  loss_dice_0: 0.1583  loss_ce_1: 0.1338  loss_mask_1: 0.213  loss_dice_1: 0.1598  loss_ce_2: 0.1333  loss_mask_2: 0.2064  loss_dice_2: 0.1557  loss_ce_3: 0.1328  loss_mask_3: 0.1954  loss_dice_3: 0.1589  loss_ce_4: 0.1326  loss_mask_4: 0.2071  loss_dice_4: 0.1572  loss_ce_5: 0.1332  loss_mask_5: 0.2039  loss_dice_5: 0.1635  loss_ce_6: 0.133  loss_mask_6: 0.1974  loss_dice_6: 0.1586  loss_ce_7: 0.1334  loss_mask_7: 0.2024  loss_dice_7: 0.1573  loss_ce_8: 0.1337  loss_mask_8: 0.2021  loss_dice_8: 0.1553  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:33] d2.utils.events INFO:  eta: 1:11:59  iter: 15679  total_loss: 4.627  loss_ce: 0.1281  loss_mask: 0.2169  loss_dice: 0.138  loss_ce_0: 0.06367  loss_mask_0: 0.2102  loss_dice_0: 0.1451  loss_ce_1: 0.1281  loss_mask_1: 0.2166  loss_dice_1: 0.1423  loss_ce_2: 0.1283  loss_mask_2: 0.2116  loss_dice_2: 0.1359  loss_ce_3: 0.1284  loss_mask_3: 0.2124  loss_dice_3: 0.1377  loss_ce_4: 0.1287  loss_mask_4: 0.2042  loss_dice_4: 0.1371  loss_ce_5: 0.1287  loss_mask_5: 0.2149  loss_dice_5: 0.1367  loss_ce_6: 0.1287  loss_mask_6: 0.2169  loss_dice_6: 0.1388  loss_ce_7: 0.1286  loss_mask_7: 0.2118  loss_dice_7: 0.1359  loss_ce_8: 0.128  loss_mask_8: 0.2169  loss_dice_8: 0.1369  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:18:35] d2.utils.events INFO:  eta: 1:11:55  iter: 15699  total_loss: 4.92  loss_ce: 0.1253  loss_mask: 0.1878  loss_dice: 0.1374  loss_ce_0: 0.06288  loss_mask_0: 0.1931  loss_dice_0: 0.1459  loss_ce_1: 0.1255  loss_mask_1: 0.1983  loss_dice_1: 0.1426  loss_ce_2: 0.1263  loss_mask_2: 0.1959  loss_dice_2: 0.1436  loss_ce_3: 0.1261  loss_mask_3: 0.1948  loss_dice_3: 0.1422  loss_ce_4: 0.1264  loss_mask_4: 0.1938  loss_dice_4: 0.1419  loss_ce_5: 0.1266  loss_mask_5: 0.1915  loss_dice_5: 0.1457  loss_ce_6: 0.1268  loss_mask_6: 0.1975  loss_dice_6: 0.1447  loss_ce_7: 0.1264  loss_mask_7: 0.194  loss_dice_7: 0.1443  loss_ce_8: 0.1262  loss_mask_8: 0.1918  loss_dice_8: 0.1405  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:18:38] d2.utils.events INFO:  eta: 1:11:51  iter: 15719  total_loss: 5.062  loss_ce: 0.1277  loss_mask: 0.1831  loss_dice: 0.1677  loss_ce_0: 0.06347  loss_mask_0: 0.1799  loss_dice_0: 0.1668  loss_ce_1: 0.1278  loss_mask_1: 0.1762  loss_dice_1: 0.1643  loss_ce_2: 0.1278  loss_mask_2: 0.1718  loss_dice_2: 0.1704  loss_ce_3: 0.1277  loss_mask_3: 0.173  loss_dice_3: 0.1728  loss_ce_4: 0.128  loss_mask_4: 0.1684  loss_dice_4: 0.171  loss_ce_5: 0.1278  loss_mask_5: 0.1812  loss_dice_5: 0.1674  loss_ce_6: 0.1278  loss_mask_6: 0.1793  loss_dice_6: 0.1713  loss_ce_7: 0.1278  loss_mask_7: 0.1863  loss_dice_7: 0.1686  loss_ce_8: 0.1277  loss_mask_8: 0.1835  loss_dice_8: 0.1623  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:18:40] d2.utils.events INFO:  eta: 1:11:51  iter: 15739  total_loss: 4.991  loss_ce: 0.1268  loss_mask: 0.164  loss_dice: 0.1495  loss_ce_0: 0.06339  loss_mask_0: 0.1648  loss_dice_0: 0.1502  loss_ce_1: 0.1275  loss_mask_1: 0.163  loss_dice_1: 0.1543  loss_ce_2: 0.1278  loss_mask_2: 0.1646  loss_dice_2: 0.1504  loss_ce_3: 0.1278  loss_mask_3: 0.1506  loss_dice_3: 0.1459  loss_ce_4: 0.1274  loss_mask_4: 0.1652  loss_dice_4: 0.1498  loss_ce_5: 0.1276  loss_mask_5: 0.1634  loss_dice_5: 0.1511  loss_ce_6: 0.1276  loss_mask_6: 0.1622  loss_dice_6: 0.1484  loss_ce_7: 0.1279  loss_mask_7: 0.161  loss_dice_7: 0.156  loss_ce_8: 0.1276  loss_mask_8: 0.1637  loss_dice_8: 0.1492  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:18:43] d2.utils.events INFO:  eta: 1:11:48  iter: 15759  total_loss: 5.035  loss_ce: 0.1275  loss_mask: 0.1866  loss_dice: 0.2062  loss_ce_0: 0.06333  loss_mask_0: 0.1771  loss_dice_0: 0.2061  loss_ce_1: 0.1275  loss_mask_1: 0.1804  loss_dice_1: 0.2011  loss_ce_2: 0.1274  loss_mask_2: 0.1785  loss_dice_2: 0.1945  loss_ce_3: 0.1274  loss_mask_3: 0.1858  loss_dice_3: 0.2017  loss_ce_4: 0.1273  loss_mask_4: 0.1789  loss_dice_4: 0.201  loss_ce_5: 0.1273  loss_mask_5: 0.186  loss_dice_5: 0.1916  loss_ce_6: 0.1274  loss_mask_6: 0.1838  loss_dice_6: 0.1929  loss_ce_7: 0.1274  loss_mask_7: 0.1832  loss_dice_7: 0.1929  loss_ce_8: 0.1274  loss_mask_8: 0.1824  loss_dice_8: 0.2007  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:18:46] d2.utils.events INFO:  eta: 1:11:47  iter: 15779  total_loss: 5.054  loss_ce: 0.13  loss_mask: 0.2403  loss_dice: 0.1425  loss_ce_0: 0.06443  loss_mask_0: 0.2534  loss_dice_0: 0.1421  loss_ce_1: 0.1308  loss_mask_1: 0.2474  loss_dice_1: 0.139  loss_ce_2: 0.1314  loss_mask_2: 0.2439  loss_dice_2: 0.1452  loss_ce_3: 0.1319  loss_mask_3: 0.2402  loss_dice_3: 0.1412  loss_ce_4: 0.1322  loss_mask_4: 0.2405  loss_dice_4: 0.1414  loss_ce_5: 0.1318  loss_mask_5: 0.2363  loss_dice_5: 0.1384  loss_ce_6: 0.1317  loss_mask_6: 0.2424  loss_dice_6: 0.137  loss_ce_7: 0.1316  loss_mask_7: 0.2344  loss_dice_7: 0.1388  loss_ce_8: 0.1309  loss_mask_8: 0.2483  loss_dice_8: 0.138  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:48] d2.utils.events INFO:  eta: 1:11:44  iter: 15799  total_loss: 5.497  loss_ce: 0.1279  loss_mask: 0.1839  loss_dice: 0.1741  loss_ce_0: 0.06344  loss_mask_0: 0.1804  loss_dice_0: 0.1743  loss_ce_1: 0.1276  loss_mask_1: 0.181  loss_dice_1: 0.1776  loss_ce_2: 0.1282  loss_mask_2: 0.1799  loss_dice_2: 0.1788  loss_ce_3: 0.1279  loss_mask_3: 0.1843  loss_dice_3: 0.1795  loss_ce_4: 0.1281  loss_mask_4: 0.1837  loss_dice_4: 0.1742  loss_ce_5: 0.1282  loss_mask_5: 0.1804  loss_dice_5: 0.1784  loss_ce_6: 0.1283  loss_mask_6: 0.189  loss_dice_6: 0.1793  loss_ce_7: 0.1281  loss_mask_7: 0.1825  loss_dice_7: 0.1805  loss_ce_8: 0.1285  loss_mask_8: 0.1795  loss_dice_8: 0.1808  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:51] d2.utils.events INFO:  eta: 1:11:39  iter: 15819  total_loss: 5.439  loss_ce: 0.1273  loss_mask: 0.2273  loss_dice: 0.1844  loss_ce_0: 0.06327  loss_mask_0: 0.2177  loss_dice_0: 0.1792  loss_ce_1: 0.1271  loss_mask_1: 0.2239  loss_dice_1: 0.1868  loss_ce_2: 0.1268  loss_mask_2: 0.223  loss_dice_2: 0.184  loss_ce_3: 0.1271  loss_mask_3: 0.2279  loss_dice_3: 0.1881  loss_ce_4: 0.1271  loss_mask_4: 0.2114  loss_dice_4: 0.1826  loss_ce_5: 0.1267  loss_mask_5: 0.2153  loss_dice_5: 0.1773  loss_ce_6: 0.1271  loss_mask_6: 0.2289  loss_dice_6: 0.1815  loss_ce_7: 0.1272  loss_mask_7: 0.2271  loss_dice_7: 0.179  loss_ce_8: 0.1271  loss_mask_8: 0.2343  loss_dice_8: 0.1848  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:53] d2.utils.events INFO:  eta: 1:11:37  iter: 15839  total_loss: 4.737  loss_ce: 0.1195  loss_mask: 0.1856  loss_dice: 0.1408  loss_ce_0: 0.05989  loss_mask_0: 0.1795  loss_dice_0: 0.1446  loss_ce_1: 0.117  loss_mask_1: 0.1847  loss_dice_1: 0.1452  loss_ce_2: 0.1152  loss_mask_2: 0.1851  loss_dice_2: 0.1409  loss_ce_3: 0.1147  loss_mask_3: 0.1854  loss_dice_3: 0.1427  loss_ce_4: 0.1131  loss_mask_4: 0.1835  loss_dice_4: 0.1427  loss_ce_5: 0.1139  loss_mask_5: 0.1813  loss_dice_5: 0.1434  loss_ce_6: 0.1141  loss_mask_6: 0.1752  loss_dice_6: 0.1382  loss_ce_7: 0.1153  loss_mask_7: 0.1744  loss_dice_7: 0.1353  loss_ce_8: 0.1167  loss_mask_8: 0.1831  loss_dice_8: 0.1366  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:56] d2.utils.events INFO:  eta: 1:11:34  iter: 15859  total_loss: 4.96  loss_ce: 0.1076  loss_mask: 0.2014  loss_dice: 0.1556  loss_ce_0: 0.05638  loss_mask_0: 0.2129  loss_dice_0: 0.1491  loss_ce_1: 0.1064  loss_mask_1: 0.2118  loss_dice_1: 0.1513  loss_ce_2: 0.1049  loss_mask_2: 0.1959  loss_dice_2: 0.1489  loss_ce_3: 0.1051  loss_mask_3: 0.2065  loss_dice_3: 0.1475  loss_ce_4: 0.1046  loss_mask_4: 0.2043  loss_dice_4: 0.1498  loss_ce_5: 0.1052  loss_mask_5: 0.2077  loss_dice_5: 0.1522  loss_ce_6: 0.1044  loss_mask_6: 0.2032  loss_dice_6: 0.1467  loss_ce_7: 0.1057  loss_mask_7: 0.2014  loss_dice_7: 0.1495  loss_ce_8: 0.1064  loss_mask_8: 0.215  loss_dice_8: 0.1449  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:18:58] d2.utils.events INFO:  eta: 1:11:29  iter: 15879  total_loss: 5.641  loss_ce: 0.1016  loss_mask: 0.1791  loss_dice: 0.2011  loss_ce_0: 0.05451  loss_mask_0: 0.1813  loss_dice_0: 0.1836  loss_ce_1: 0.1011  loss_mask_1: 0.186  loss_dice_1: 0.1978  loss_ce_2: 0.09963  loss_mask_2: 0.1828  loss_dice_2: 0.1927  loss_ce_3: 0.09971  loss_mask_3: 0.1789  loss_dice_3: 0.1968  loss_ce_4: 0.0989  loss_mask_4: 0.1775  loss_dice_4: 0.1928  loss_ce_5: 0.09845  loss_mask_5: 0.1835  loss_dice_5: 0.1954  loss_ce_6: 0.09743  loss_mask_6: 0.1745  loss_dice_6: 0.1927  loss_ce_7: 0.09675  loss_mask_7: 0.1766  loss_dice_7: 0.189  loss_ce_8: 0.09957  loss_mask_8: 0.1856  loss_dice_8: 0.2041  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:01] d2.utils.events INFO:  eta: 1:11:25  iter: 15899  total_loss: 4.77  loss_ce: 0.1601  loss_mask: 0.1901  loss_dice: 0.1359  loss_ce_0: 0.07467  loss_mask_0: 0.1884  loss_dice_0: 0.1336  loss_ce_1: 0.1619  loss_mask_1: 0.1871  loss_dice_1: 0.1335  loss_ce_2: 0.1628  loss_mask_2: 0.1842  loss_dice_2: 0.1333  loss_ce_3: 0.1615  loss_mask_3: 0.1872  loss_dice_3: 0.1344  loss_ce_4: 0.1605  loss_mask_4: 0.1883  loss_dice_4: 0.1328  loss_ce_5: 0.1594  loss_mask_5: 0.1941  loss_dice_5: 0.1388  loss_ce_6: 0.1593  loss_mask_6: 0.1895  loss_dice_6: 0.1323  loss_ce_7: 0.1569  loss_mask_7: 0.1887  loss_dice_7: 0.1378  loss_ce_8: 0.1601  loss_mask_8: 0.1862  loss_dice_8: 0.1327  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:03] d2.utils.events INFO:  eta: 1:11:24  iter: 15919  total_loss: 4.758  loss_ce: 0.1475  loss_mask: 0.1884  loss_dice: 0.1316  loss_ce_0: 0.07102  loss_mask_0: 0.188  loss_dice_0: 0.1265  loss_ce_1: 0.1472  loss_mask_1: 0.19  loss_dice_1: 0.1281  loss_ce_2: 0.1462  loss_mask_2: 0.1958  loss_dice_2: 0.1316  loss_ce_3: 0.1438  loss_mask_3: 0.1939  loss_dice_3: 0.1319  loss_ce_4: 0.1416  loss_mask_4: 0.1999  loss_dice_4: 0.1325  loss_ce_5: 0.1401  loss_mask_5: 0.1962  loss_dice_5: 0.1323  loss_ce_6: 0.1398  loss_mask_6: 0.1852  loss_dice_6: 0.1326  loss_ce_7: 0.1396  loss_mask_7: 0.1907  loss_dice_7: 0.1306  loss_ce_8: 0.1451  loss_mask_8: 0.1786  loss_dice_8: 0.1317  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:06] d2.utils.events INFO:  eta: 1:11:21  iter: 15939  total_loss: 5.053  loss_ce: 0.1142  loss_mask: 0.2003  loss_dice: 0.1486  loss_ce_0: 0.058  loss_mask_0: 0.2005  loss_dice_0: 0.1519  loss_ce_1: 0.1157  loss_mask_1: 0.2081  loss_dice_1: 0.153  loss_ce_2: 0.1167  loss_mask_2: 0.2063  loss_dice_2: 0.1698  loss_ce_3: 0.1181  loss_mask_3: 0.199  loss_dice_3: 0.1625  loss_ce_4: 0.1192  loss_mask_4: 0.1975  loss_dice_4: 0.1577  loss_ce_5: 0.1202  loss_mask_5: 0.1957  loss_dice_5: 0.1541  loss_ce_6: 0.1201  loss_mask_6: 0.2051  loss_dice_6: 0.1545  loss_ce_7: 0.1198  loss_mask_7: 0.1976  loss_dice_7: 0.1595  loss_ce_8: 0.1163  loss_mask_8: 0.1942  loss_dice_8: 0.1514  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:08] d2.utils.events INFO:  eta: 1:11:20  iter: 15959  total_loss: 5.217  loss_ce: 0.1059  loss_mask: 0.1826  loss_dice: 0.1647  loss_ce_0: 0.05473  loss_mask_0: 0.1775  loss_dice_0: 0.1587  loss_ce_1: 0.107  loss_mask_1: 0.1746  loss_dice_1: 0.165  loss_ce_2: 0.1064  loss_mask_2: 0.1757  loss_dice_2: 0.1678  loss_ce_3: 0.1054  loss_mask_3: 0.1799  loss_dice_3: 0.1701  loss_ce_4: 0.1034  loss_mask_4: 0.1824  loss_dice_4: 0.1725  loss_ce_5: 0.1033  loss_mask_5: 0.1759  loss_dice_5: 0.1651  loss_ce_6: 0.1034  loss_mask_6: 0.1701  loss_dice_6: 0.1642  loss_ce_7: 0.104  loss_mask_7: 0.1721  loss_dice_7: 0.1644  loss_ce_8: 0.1058  loss_mask_8: 0.1812  loss_dice_8: 0.1644  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:11] d2.utils.events INFO:  eta: 1:11:19  iter: 15979  total_loss: 4.839  loss_ce: 0.1296  loss_mask: 0.2012  loss_dice: 0.1856  loss_ce_0: 0.06419  loss_mask_0: 0.1833  loss_dice_0: 0.1928  loss_ce_1: 0.1297  loss_mask_1: 0.1988  loss_dice_1: 0.1999  loss_ce_2: 0.1297  loss_mask_2: 0.1915  loss_dice_2: 0.1971  loss_ce_3: 0.13  loss_mask_3: 0.1979  loss_dice_3: 0.196  loss_ce_4: 0.1302  loss_mask_4: 0.1868  loss_dice_4: 0.1824  loss_ce_5: 0.1305  loss_mask_5: 0.2063  loss_dice_5: 0.1961  loss_ce_6: 0.1303  loss_mask_6: 0.1904  loss_dice_6: 0.1917  loss_ce_7: 0.1301  loss_mask_7: 0.186  loss_dice_7: 0.1948  loss_ce_8: 0.1298  loss_mask_8: 0.1913  loss_dice_8: 0.196  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:13] d2.utils.events INFO:  eta: 1:11:19  iter: 15999  total_loss: 5.03  loss_ce: 0.1011  loss_mask: 0.2388  loss_dice: 0.1432  loss_ce_0: 0.05346  loss_mask_0: 0.2286  loss_dice_0: 0.1432  loss_ce_1: 0.1006  loss_mask_1: 0.2371  loss_dice_1: 0.1445  loss_ce_2: 0.1004  loss_mask_2: 0.2381  loss_dice_2: 0.1432  loss_ce_3: 0.1005  loss_mask_3: 0.237  loss_dice_3: 0.1452  loss_ce_4: 0.1006  loss_mask_4: 0.2465  loss_dice_4: 0.1438  loss_ce_5: 0.1008  loss_mask_5: 0.2373  loss_dice_5: 0.1456  loss_ce_6: 0.1005  loss_mask_6: 0.2262  loss_dice_6: 0.1404  loss_ce_7: 0.1006  loss_mask_7: 0.2256  loss_dice_7: 0.1424  loss_ce_8: 0.1006  loss_mask_8: 0.2321  loss_dice_8: 0.1419  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:16] d2.utils.events INFO:  eta: 1:11:16  iter: 16019  total_loss: 5.129  loss_ce: 0.1548  loss_mask: 0.2174  loss_dice: 0.122  loss_ce_0: 0.07371  loss_mask_0: 0.2151  loss_dice_0: 0.1221  loss_ce_1: 0.1548  loss_mask_1: 0.2246  loss_dice_1: 0.1233  loss_ce_2: 0.1547  loss_mask_2: 0.2124  loss_dice_2: 0.1241  loss_ce_3: 0.1539  loss_mask_3: 0.2217  loss_dice_3: 0.1266  loss_ce_4: 0.1531  loss_mask_4: 0.2213  loss_dice_4: 0.1229  loss_ce_5: 0.1531  loss_mask_5: 0.2206  loss_dice_5: 0.1187  loss_ce_6: 0.1531  loss_mask_6: 0.2154  loss_dice_6: 0.1193  loss_ce_7: 0.1535  loss_mask_7: 0.2127  loss_dice_7: 0.1188  loss_ce_8: 0.1546  loss_mask_8: 0.2218  loss_dice_8: 0.1215  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:18] d2.utils.events INFO:  eta: 1:11:14  iter: 16039  total_loss: 5.714  loss_ce: 0.107  loss_mask: 0.2268  loss_dice: 0.167  loss_ce_0: 0.05543  loss_mask_0: 0.2293  loss_dice_0: 0.1632  loss_ce_1: 0.107  loss_mask_1: 0.2279  loss_dice_1: 0.1651  loss_ce_2: 0.1072  loss_mask_2: 0.2278  loss_dice_2: 0.171  loss_ce_3: 0.1081  loss_mask_3: 0.2205  loss_dice_3: 0.1706  loss_ce_4: 0.1091  loss_mask_4: 0.2347  loss_dice_4: 0.1726  loss_ce_5: 0.1092  loss_mask_5: 0.2263  loss_dice_5: 0.1739  loss_ce_6: 0.1094  loss_mask_6: 0.2254  loss_dice_6: 0.1645  loss_ce_7: 0.1088  loss_mask_7: 0.2299  loss_dice_7: 0.171  loss_ce_8: 0.1075  loss_mask_8: 0.2284  loss_dice_8: 0.1716  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:21] d2.utils.events INFO:  eta: 1:11:14  iter: 16059  total_loss: 5.626  loss_ce: 0.1022  loss_mask: 0.1816  loss_dice: 0.2812  loss_ce_0: 0.05378  loss_mask_0: 0.1711  loss_dice_0: 0.2451  loss_ce_1: 0.1023  loss_mask_1: 0.1708  loss_dice_1: 0.2543  loss_ce_2: 0.1022  loss_mask_2: 0.1752  loss_dice_2: 0.2611  loss_ce_3: 0.1019  loss_mask_3: 0.1787  loss_dice_3: 0.2801  loss_ce_4: 0.1013  loss_mask_4: 0.178  loss_dice_4: 0.2788  loss_ce_5: 0.1013  loss_mask_5: 0.1759  loss_dice_5: 0.2651  loss_ce_6: 0.1013  loss_mask_6: 0.1817  loss_dice_6: 0.2676  loss_ce_7: 0.1015  loss_mask_7: 0.1796  loss_dice_7: 0.2762  loss_ce_8: 0.1021  loss_mask_8: 0.1752  loss_dice_8: 0.2482  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:24] d2.utils.events INFO:  eta: 1:11:13  iter: 16079  total_loss: 5.334  loss_ce: 0.09923  loss_mask: 0.2152  loss_dice: 0.181  loss_ce_0: 0.05286  loss_mask_0: 0.2147  loss_dice_0: 0.1691  loss_ce_1: 0.09922  loss_mask_1: 0.2118  loss_dice_1: 0.1769  loss_ce_2: 0.09895  loss_mask_2: 0.2163  loss_dice_2: 0.1686  loss_ce_3: 0.09858  loss_mask_3: 0.208  loss_dice_3: 0.1827  loss_ce_4: 0.09761  loss_mask_4: 0.2209  loss_dice_4: 0.1741  loss_ce_5: 0.09757  loss_mask_5: 0.2105  loss_dice_5: 0.1754  loss_ce_6: 0.09759  loss_mask_6: 0.2191  loss_dice_6: 0.1776  loss_ce_7: 0.09774  loss_mask_7: 0.2111  loss_dice_7: 0.166  loss_ce_8: 0.09881  loss_mask_8: 0.2169  loss_dice_8: 0.1753  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:26] d2.utils.events INFO:  eta: 1:11:11  iter: 16099  total_loss: 4.718  loss_ce: 0.09625  loss_mask: 0.1981  loss_dice: 0.1501  loss_ce_0: 0.05182  loss_mask_0: 0.2049  loss_dice_0: 0.1487  loss_ce_1: 0.09582  loss_mask_1: 0.2016  loss_dice_1: 0.1453  loss_ce_2: 0.09567  loss_mask_2: 0.2072  loss_dice_2: 0.1461  loss_ce_3: 0.09553  loss_mask_3: 0.2143  loss_dice_3: 0.1471  loss_ce_4: 0.09556  loss_mask_4: 0.2034  loss_dice_4: 0.1559  loss_ce_5: 0.09559  loss_mask_5: 0.212  loss_dice_5: 0.1604  loss_ce_6: 0.09532  loss_mask_6: 0.2011  loss_dice_6: 0.1448  loss_ce_7: 0.09554  loss_mask_7: 0.2087  loss_dice_7: 0.1617  loss_ce_8: 0.09598  loss_mask_8: 0.2086  loss_dice_8: 0.1531  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:29] d2.utils.events INFO:  eta: 1:11:08  iter: 16119  total_loss: 5.155  loss_ce: 0.08576  loss_mask: 0.1629  loss_dice: 0.1739  loss_ce_0: 0.0488  loss_mask_0: 0.1704  loss_dice_0: 0.1726  loss_ce_1: 0.08538  loss_mask_1: 0.1665  loss_dice_1: 0.1805  loss_ce_2: 0.08511  loss_mask_2: 0.1696  loss_dice_2: 0.1771  loss_ce_3: 0.08468  loss_mask_3: 0.1711  loss_dice_3: 0.1745  loss_ce_4: 0.08369  loss_mask_4: 0.1628  loss_dice_4: 0.1742  loss_ce_5: 0.08351  loss_mask_5: 0.1598  loss_dice_5: 0.1753  loss_ce_6: 0.08292  loss_mask_6: 0.1672  loss_dice_6: 0.1713  loss_ce_7: 0.08328  loss_mask_7: 0.1641  loss_dice_7: 0.1745  loss_ce_8: 0.08496  loss_mask_8: 0.1681  loss_dice_8: 0.1738  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:31] d2.utils.events INFO:  eta: 1:11:06  iter: 16139  total_loss: 5.274  loss_ce: 0.07129  loss_mask: 0.1639  loss_dice: 0.1611  loss_ce_0: 0.04497  loss_mask_0: 0.1681  loss_dice_0: 0.1605  loss_ce_1: 0.0713  loss_mask_1: 0.1675  loss_dice_1: 0.1596  loss_ce_2: 0.0702  loss_mask_2: 0.1672  loss_dice_2: 0.1615  loss_ce_3: 0.0696  loss_mask_3: 0.1666  loss_dice_3: 0.1691  loss_ce_4: 0.06909  loss_mask_4: 0.1712  loss_dice_4: 0.1591  loss_ce_5: 0.06827  loss_mask_5: 0.1699  loss_dice_5: 0.1622  loss_ce_6: 0.06694  loss_mask_6: 0.1623  loss_dice_6: 0.1493  loss_ce_7: 0.06816  loss_mask_7: 0.1668  loss_dice_7: 0.1583  loss_ce_8: 0.06998  loss_mask_8: 0.1679  loss_dice_8: 0.1612  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:34] d2.utils.events INFO:  eta: 1:11:02  iter: 16159  total_loss: 6.106  loss_ce: 0.1383  loss_mask: 0.2018  loss_dice: 0.2022  loss_ce_0: 0.06688  loss_mask_0: 0.1996  loss_dice_0: 0.203  loss_ce_1: 0.1389  loss_mask_1: 0.2021  loss_dice_1: 0.2221  loss_ce_2: 0.1386  loss_mask_2: 0.2001  loss_dice_2: 0.2087  loss_ce_3: 0.1388  loss_mask_3: 0.2055  loss_dice_3: 0.2183  loss_ce_4: 0.1383  loss_mask_4: 0.2004  loss_dice_4: 0.2134  loss_ce_5: 0.1382  loss_mask_5: 0.2019  loss_dice_5: 0.2211  loss_ce_6: 0.1381  loss_mask_6: 0.2075  loss_dice_6: 0.2153  loss_ce_7: 0.1381  loss_mask_7: 0.2032  loss_dice_7: 0.2103  loss_ce_8: 0.1384  loss_mask_8: 0.2022  loss_dice_8: 0.2095  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:36] d2.utils.events INFO:  eta: 1:10:57  iter: 16179  total_loss: 5.289  loss_ce: 0.08057  loss_mask: 0.1732  loss_dice: 0.2047  loss_ce_0: 0.0445  loss_mask_0: 0.1879  loss_dice_0: 0.2236  loss_ce_1: 0.08008  loss_mask_1: 0.1778  loss_dice_1: 0.2193  loss_ce_2: 0.08085  loss_mask_2: 0.1844  loss_dice_2: 0.2113  loss_ce_3: 0.08056  loss_mask_3: 0.1751  loss_dice_3: 0.2133  loss_ce_4: 0.08192  loss_mask_4: 0.18  loss_dice_4: 0.2233  loss_ce_5: 0.08198  loss_mask_5: 0.1735  loss_dice_5: 0.2105  loss_ce_6: 0.08202  loss_mask_6: 0.1805  loss_dice_6: 0.2171  loss_ce_7: 0.08223  loss_mask_7: 0.1841  loss_dice_7: 0.2057  loss_ce_8: 0.08087  loss_mask_8: 0.178  loss_dice_8: 0.2054  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:19:39] d2.utils.events INFO:  eta: 1:10:55  iter: 16199  total_loss: 4.81  loss_ce: 0.136  loss_mask: 0.2024  loss_dice: 0.1392  loss_ce_0: 0.06654  loss_mask_0: 0.1921  loss_dice_0: 0.1269  loss_ce_1: 0.1357  loss_mask_1: 0.205  loss_dice_1: 0.1298  loss_ce_2: 0.1357  loss_mask_2: 0.1954  loss_dice_2: 0.1323  loss_ce_3: 0.1359  loss_mask_3: 0.2067  loss_dice_3: 0.1346  loss_ce_4: 0.1358  loss_mask_4: 0.2044  loss_dice_4: 0.1329  loss_ce_5: 0.1359  loss_mask_5: 0.2  loss_dice_5: 0.1294  loss_ce_6: 0.1359  loss_mask_6: 0.1976  loss_dice_6: 0.131  loss_ce_7: 0.1359  loss_mask_7: 0.2046  loss_dice_7: 0.1314  loss_ce_8: 0.136  loss_mask_8: 0.1927  loss_dice_8: 0.1314  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:41] d2.utils.events INFO:  eta: 1:10:55  iter: 16219  total_loss: 5.157  loss_ce: 0.1739  loss_mask: 0.1577  loss_dice: 0.1801  loss_ce_0: 0.08356  loss_mask_0: 0.1617  loss_dice_0: 0.1762  loss_ce_1: 0.1741  loss_mask_1: 0.1625  loss_dice_1: 0.1799  loss_ce_2: 0.1729  loss_mask_2: 0.1637  loss_dice_2: 0.1811  loss_ce_3: 0.1727  loss_mask_3: 0.1573  loss_dice_3: 0.1719  loss_ce_4: 0.1712  loss_mask_4: 0.1643  loss_dice_4: 0.1824  loss_ce_5: 0.1713  loss_mask_5: 0.1608  loss_dice_5: 0.1766  loss_ce_6: 0.1708  loss_mask_6: 0.1606  loss_dice_6: 0.186  loss_ce_7: 0.1709  loss_mask_7: 0.16  loss_dice_7: 0.1772  loss_ce_8: 0.1734  loss_mask_8: 0.1515  loss_dice_8: 0.1803  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:44] d2.utils.events INFO:  eta: 1:10:57  iter: 16239  total_loss: 5.016  loss_ce: 0.1294  loss_mask: 0.1816  loss_dice: 0.1677  loss_ce_0: 0.06461  loss_mask_0: 0.1816  loss_dice_0: 0.161  loss_ce_1: 0.1293  loss_mask_1: 0.1818  loss_dice_1: 0.1623  loss_ce_2: 0.129  loss_mask_2: 0.1872  loss_dice_2: 0.1586  loss_ce_3: 0.1291  loss_mask_3: 0.1765  loss_dice_3: 0.1623  loss_ce_4: 0.1289  loss_mask_4: 0.1809  loss_dice_4: 0.1644  loss_ce_5: 0.129  loss_mask_5: 0.1879  loss_dice_5: 0.1646  loss_ce_6: 0.1289  loss_mask_6: 0.1776  loss_dice_6: 0.1649  loss_ce_7: 0.1289  loss_mask_7: 0.1773  loss_dice_7: 0.1639  loss_ce_8: 0.1293  loss_mask_8: 0.1849  loss_dice_8: 0.1649  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:46] d2.utils.events INFO:  eta: 1:10:56  iter: 16259  total_loss: 4.977  loss_ce: 0.1051  loss_mask: 0.1831  loss_dice: 0.176  loss_ce_0: 0.05258  loss_mask_0: 0.1879  loss_dice_0: 0.1736  loss_ce_1: 0.1049  loss_mask_1: 0.1914  loss_dice_1: 0.1702  loss_ce_2: 0.1066  loss_mask_2: 0.1777  loss_dice_2: 0.1646  loss_ce_3: 0.1066  loss_mask_3: 0.1833  loss_dice_3: 0.1645  loss_ce_4: 0.1076  loss_mask_4: 0.1828  loss_dice_4: 0.1726  loss_ce_5: 0.1069  loss_mask_5: 0.1818  loss_dice_5: 0.1669  loss_ce_6: 0.1071  loss_mask_6: 0.1779  loss_dice_6: 0.1689  loss_ce_7: 0.1071  loss_mask_7: 0.1787  loss_dice_7: 0.1734  loss_ce_8: 0.1055  loss_mask_8: 0.1853  loss_dice_8: 0.1756  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:49] d2.utils.events INFO:  eta: 1:10:53  iter: 16279  total_loss: 5.13  loss_ce: 0.1054  loss_mask: 0.2437  loss_dice: 0.1497  loss_ce_0: 0.05283  loss_mask_0: 0.2426  loss_dice_0: 0.1427  loss_ce_1: 0.1052  loss_mask_1: 0.2494  loss_dice_1: 0.1462  loss_ce_2: 0.1067  loss_mask_2: 0.2473  loss_dice_2: 0.143  loss_ce_3: 0.1065  loss_mask_3: 0.2469  loss_dice_3: 0.1477  loss_ce_4: 0.1073  loss_mask_4: 0.2534  loss_dice_4: 0.1427  loss_ce_5: 0.1068  loss_mask_5: 0.238  loss_dice_5: 0.1433  loss_ce_6: 0.107  loss_mask_6: 0.253  loss_dice_6: 0.141  loss_ce_7: 0.107  loss_mask_7: 0.2444  loss_dice_7: 0.1485  loss_ce_8: 0.1057  loss_mask_8: 0.2547  loss_dice_8: 0.1456  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:51] d2.utils.events INFO:  eta: 1:10:55  iter: 16299  total_loss: 5.713  loss_ce: 0.1526  loss_mask: 0.1885  loss_dice: 0.2745  loss_ce_0: 0.07467  loss_mask_0: 0.1862  loss_dice_0: 0.2742  loss_ce_1: 0.1527  loss_mask_1: 0.1978  loss_dice_1: 0.2624  loss_ce_2: 0.1523  loss_mask_2: 0.1968  loss_dice_2: 0.2781  loss_ce_3: 0.1521  loss_mask_3: 0.1868  loss_dice_3: 0.2688  loss_ce_4: 0.1518  loss_mask_4: 0.1928  loss_dice_4: 0.2683  loss_ce_5: 0.1518  loss_mask_5: 0.201  loss_dice_5: 0.2765  loss_ce_6: 0.1515  loss_mask_6: 0.1955  loss_dice_6: 0.2619  loss_ce_7: 0.1517  loss_mask_7: 0.1923  loss_dice_7: 0.2632  loss_ce_8: 0.1524  loss_mask_8: 0.1955  loss_dice_8: 0.2631  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:54] d2.utils.events INFO:  eta: 1:10:54  iter: 16319  total_loss: 5.385  loss_ce: 0.1325  loss_mask: 0.1925  loss_dice: 0.1456  loss_ce_0: 0.06751  loss_mask_0: 0.1861  loss_dice_0: 0.1427  loss_ce_1: 0.1321  loss_mask_1: 0.1883  loss_dice_1: 0.1465  loss_ce_2: 0.1334  loss_mask_2: 0.1839  loss_dice_2: 0.1493  loss_ce_3: 0.1329  loss_mask_3: 0.1811  loss_dice_3: 0.1469  loss_ce_4: 0.1343  loss_mask_4: 0.1874  loss_dice_4: 0.1498  loss_ce_5: 0.1335  loss_mask_5: 0.1941  loss_dice_5: 0.1454  loss_ce_6: 0.1323  loss_mask_6: 0.1917  loss_dice_6: 0.1484  loss_ce_7: 0.1327  loss_mask_7: 0.1917  loss_dice_7: 0.1428  loss_ce_8: 0.1325  loss_mask_8: 0.1854  loss_dice_8: 0.1523  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:19:56] d2.utils.events INFO:  eta: 1:10:54  iter: 16339  total_loss: 5.539  loss_ce: 0.1386  loss_mask: 0.1896  loss_dice: 0.1814  loss_ce_0: 0.06353  loss_mask_0: 0.2034  loss_dice_0: 0.1752  loss_ce_1: 0.1365  loss_mask_1: 0.206  loss_dice_1: 0.1893  loss_ce_2: 0.1413  loss_mask_2: 0.1941  loss_dice_2: 0.1853  loss_ce_3: 0.14  loss_mask_3: 0.2041  loss_dice_3: 0.1845  loss_ce_4: 0.1417  loss_mask_4: 0.2007  loss_dice_4: 0.1739  loss_ce_5: 0.1411  loss_mask_5: 0.2015  loss_dice_5: 0.1892  loss_ce_6: 0.1404  loss_mask_6: 0.2008  loss_dice_6: 0.1887  loss_ce_7: 0.1418  loss_mask_7: 0.1979  loss_dice_7: 0.1909  loss_ce_8: 0.1394  loss_mask_8: 0.208  loss_dice_8: 0.1839  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:19:59] d2.utils.events INFO:  eta: 1:10:54  iter: 16359  total_loss: 5.216  loss_ce: 0.1281  loss_mask: 0.1969  loss_dice: 0.1677  loss_ce_0: 0.06269  loss_mask_0: 0.2028  loss_dice_0: 0.1689  loss_ce_1: 0.1278  loss_mask_1: 0.1945  loss_dice_1: 0.1685  loss_ce_2: 0.1274  loss_mask_2: 0.1997  loss_dice_2: 0.1663  loss_ce_3: 0.1273  loss_mask_3: 0.2  loss_dice_3: 0.1706  loss_ce_4: 0.1272  loss_mask_4: 0.1954  loss_dice_4: 0.1656  loss_ce_5: 0.1278  loss_mask_5: 0.1918  loss_dice_5: 0.1672  loss_ce_6: 0.1283  loss_mask_6: 0.1924  loss_dice_6: 0.175  loss_ce_7: 0.1282  loss_mask_7: 0.1994  loss_dice_7: 0.166  loss_ce_8: 0.1281  loss_mask_8: 0.1913  loss_dice_8: 0.1697  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:02] d2.utils.events INFO:  eta: 1:10:53  iter: 16379  total_loss: 5.178  loss_ce: 0.1277  loss_mask: 0.1974  loss_dice: 0.1847  loss_ce_0: 0.06363  loss_mask_0: 0.2097  loss_dice_0: 0.1765  loss_ce_1: 0.1276  loss_mask_1: 0.2051  loss_dice_1: 0.1792  loss_ce_2: 0.1277  loss_mask_2: 0.1951  loss_dice_2: 0.1766  loss_ce_3: 0.1276  loss_mask_3: 0.2105  loss_dice_3: 0.1842  loss_ce_4: 0.1279  loss_mask_4: 0.2117  loss_dice_4: 0.1806  loss_ce_5: 0.1277  loss_mask_5: 0.2098  loss_dice_5: 0.1754  loss_ce_6: 0.1277  loss_mask_6: 0.2116  loss_dice_6: 0.1814  loss_ce_7: 0.1277  loss_mask_7: 0.2076  loss_dice_7: 0.1832  loss_ce_8: 0.1276  loss_mask_8: 0.2117  loss_dice_8: 0.1756  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:04] d2.utils.events INFO:  eta: 1:10:51  iter: 16399  total_loss: 5.056  loss_ce: 0.1162  loss_mask: 0.1795  loss_dice: 0.1493  loss_ce_0: 0.05957  loss_mask_0: 0.1866  loss_dice_0: 0.1508  loss_ce_1: 0.1177  loss_mask_1: 0.1861  loss_dice_1: 0.1501  loss_ce_2: 0.1162  loss_mask_2: 0.1823  loss_dice_2: 0.1532  loss_ce_3: 0.1173  loss_mask_3: 0.1867  loss_dice_3: 0.1541  loss_ce_4: 0.1166  loss_mask_4: 0.1812  loss_dice_4: 0.1566  loss_ce_5: 0.1169  loss_mask_5: 0.1891  loss_dice_5: 0.153  loss_ce_6: 0.1167  loss_mask_6: 0.1788  loss_dice_6: 0.1523  loss_ce_7: 0.1162  loss_mask_7: 0.1907  loss_dice_7: 0.1468  loss_ce_8: 0.1161  loss_mask_8: 0.1865  loss_dice_8: 0.1539  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:07] d2.utils.events INFO:  eta: 1:10:48  iter: 16419  total_loss: 5.002  loss_ce: 0.11  loss_mask: 0.2219  loss_dice: 0.1279  loss_ce_0: 0.05749  loss_mask_0: 0.2285  loss_dice_0: 0.1311  loss_ce_1: 0.1101  loss_mask_1: 0.2219  loss_dice_1: 0.1325  loss_ce_2: 0.1084  loss_mask_2: 0.2184  loss_dice_2: 0.1367  loss_ce_3: 0.1091  loss_mask_3: 0.2218  loss_dice_3: 0.1292  loss_ce_4: 0.1077  loss_mask_4: 0.2149  loss_dice_4: 0.1331  loss_ce_5: 0.1088  loss_mask_5: 0.2191  loss_dice_5: 0.1296  loss_ce_6: 0.1091  loss_mask_6: 0.2129  loss_dice_6: 0.1323  loss_ce_7: 0.1087  loss_mask_7: 0.2069  loss_dice_7: 0.1257  loss_ce_8: 0.1098  loss_mask_8: 0.2194  loss_dice_8: 0.1329  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:20:09] d2.utils.events INFO:  eta: 1:10:45  iter: 16439  total_loss: 5.399  loss_ce: 0.1073  loss_mask: 0.2184  loss_dice: 0.174  loss_ce_0: 0.05654  loss_mask_0: 0.2198  loss_dice_0: 0.1647  loss_ce_1: 0.1077  loss_mask_1: 0.2263  loss_dice_1: 0.1671  loss_ce_2: 0.1063  loss_mask_2: 0.2182  loss_dice_2: 0.164  loss_ce_3: 0.1069  loss_mask_3: 0.2246  loss_dice_3: 0.1609  loss_ce_4: 0.106  loss_mask_4: 0.2198  loss_dice_4: 0.1685  loss_ce_5: 0.1066  loss_mask_5: 0.219  loss_dice_5: 0.1687  loss_ce_6: 0.1067  loss_mask_6: 0.2216  loss_dice_6: 0.1604  loss_ce_7: 0.1066  loss_mask_7: 0.2258  loss_dice_7: 0.1654  loss_ce_8: 0.1073  loss_mask_8: 0.2267  loss_dice_8: 0.1686  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:20:12] d2.utils.events INFO:  eta: 1:10:45  iter: 16459  total_loss: 5.055  loss_ce: 0.144  loss_mask: 0.2032  loss_dice: 0.1597  loss_ce_0: 0.06944  loss_mask_0: 0.2053  loss_dice_0: 0.1615  loss_ce_1: 0.1433  loss_mask_1: 0.2015  loss_dice_1: 0.1656  loss_ce_2: 0.1431  loss_mask_2: 0.2126  loss_dice_2: 0.1625  loss_ce_3: 0.142  loss_mask_3: 0.2038  loss_dice_3: 0.1675  loss_ce_4: 0.1408  loss_mask_4: 0.1976  loss_dice_4: 0.1683  loss_ce_5: 0.1417  loss_mask_5: 0.2069  loss_dice_5: 0.173  loss_ce_6: 0.1423  loss_mask_6: 0.2001  loss_dice_6: 0.1604  loss_ce_7: 0.1424  loss_mask_7: 0.202  loss_dice_7: 0.1651  loss_ce_8: 0.1439  loss_mask_8: 0.2003  loss_dice_8: 0.1629  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:14] d2.utils.events INFO:  eta: 1:10:45  iter: 16479  total_loss: 4.98  loss_ce: 0.1374  loss_mask: 0.1938  loss_dice: 0.1774  loss_ce_0: 0.0677  loss_mask_0: 0.1865  loss_dice_0: 0.1779  loss_ce_1: 0.137  loss_mask_1: 0.1835  loss_dice_1: 0.1693  loss_ce_2: 0.1366  loss_mask_2: 0.1816  loss_dice_2: 0.178  loss_ce_3: 0.1356  loss_mask_3: 0.1813  loss_dice_3: 0.1862  loss_ce_4: 0.1347  loss_mask_4: 0.1928  loss_dice_4: 0.18  loss_ce_5: 0.1354  loss_mask_5: 0.1822  loss_dice_5: 0.172  loss_ce_6: 0.1355  loss_mask_6: 0.1918  loss_dice_6: 0.1755  loss_ce_7: 0.1356  loss_mask_7: 0.1901  loss_dice_7: 0.1767  loss_ce_8: 0.1371  loss_mask_8: 0.1863  loss_dice_8: 0.1807  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:20:17] d2.utils.events INFO:  eta: 1:10:43  iter: 16499  total_loss: 4.858  loss_ce: 0.1234  loss_mask: 0.2084  loss_dice: 0.1535  loss_ce_0: 0.06126  loss_mask_0: 0.1954  loss_dice_0: 0.1503  loss_ce_1: 0.1236  loss_mask_1: 0.212  loss_dice_1: 0.1524  loss_ce_2: 0.1242  loss_mask_2: 0.2012  loss_dice_2: 0.1515  loss_ce_3: 0.1247  loss_mask_3: 0.2008  loss_dice_3: 0.1514  loss_ce_4: 0.1252  loss_mask_4: 0.2064  loss_dice_4: 0.1499  loss_ce_5: 0.1246  loss_mask_5: 0.2118  loss_dice_5: 0.1566  loss_ce_6: 0.1245  loss_mask_6: 0.2091  loss_dice_6: 0.1553  loss_ce_7: 0.1247  loss_mask_7: 0.2015  loss_dice_7: 0.1471  loss_ce_8: 0.1238  loss_mask_8: 0.2055  loss_dice_8: 0.1502  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:19] d2.utils.events INFO:  eta: 1:10:36  iter: 16519  total_loss: 5.461  loss_ce: 0.1326  loss_mask: 0.1545  loss_dice: 0.1895  loss_ce_0: 0.06608  loss_mask_0: 0.1441  loss_dice_0: 0.1916  loss_ce_1: 0.1325  loss_mask_1: 0.1504  loss_dice_1: 0.1876  loss_ce_2: 0.1322  loss_mask_2: 0.1547  loss_dice_2: 0.1946  loss_ce_3: 0.1321  loss_mask_3: 0.1538  loss_dice_3: 0.1919  loss_ce_4: 0.1319  loss_mask_4: 0.151  loss_dice_4: 0.1827  loss_ce_5: 0.1322  loss_mask_5: 0.1518  loss_dice_5: 0.1897  loss_ce_6: 0.1322  loss_mask_6: 0.156  loss_dice_6: 0.197  loss_ce_7: 0.132  loss_mask_7: 0.1536  loss_dice_7: 0.2023  loss_ce_8: 0.1323  loss_mask_8: 0.1513  loss_dice_8: 0.1879  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:22] d2.utils.events INFO:  eta: 1:10:33  iter: 16539  total_loss: 4.746  loss_ce: 0.1275  loss_mask: 0.2322  loss_dice: 0.1264  loss_ce_0: 0.06347  loss_mask_0: 0.2291  loss_dice_0: 0.1303  loss_ce_1: 0.1275  loss_mask_1: 0.2295  loss_dice_1: 0.1366  loss_ce_2: 0.1274  loss_mask_2: 0.2244  loss_dice_2: 0.1331  loss_ce_3: 0.1274  loss_mask_3: 0.2336  loss_dice_3: 0.1291  loss_ce_4: 0.1275  loss_mask_4: 0.2174  loss_dice_4: 0.1331  loss_ce_5: 0.1274  loss_mask_5: 0.2303  loss_dice_5: 0.1345  loss_ce_6: 0.1273  loss_mask_6: 0.2218  loss_dice_6: 0.1312  loss_ce_7: 0.1274  loss_mask_7: 0.235  loss_dice_7: 0.1347  loss_ce_8: 0.1274  loss_mask_8: 0.2299  loss_dice_8: 0.135  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:20:24] d2.utils.events INFO:  eta: 1:10:30  iter: 16559  total_loss: 4.801  loss_ce: 0.1276  loss_mask: 0.226  loss_dice: 0.1467  loss_ce_0: 0.06343  loss_mask_0: 0.2237  loss_dice_0: 0.1441  loss_ce_1: 0.1277  loss_mask_1: 0.2169  loss_dice_1: 0.1368  loss_ce_2: 0.1279  loss_mask_2: 0.2174  loss_dice_2: 0.1391  loss_ce_3: 0.1279  loss_mask_3: 0.2176  loss_dice_3: 0.141  loss_ce_4: 0.1278  loss_mask_4: 0.226  loss_dice_4: 0.1387  loss_ce_5: 0.1278  loss_mask_5: 0.2245  loss_dice_5: 0.1433  loss_ce_6: 0.1278  loss_mask_6: 0.2203  loss_dice_6: 0.1444  loss_ce_7: 0.1279  loss_mask_7: 0.2197  loss_dice_7: 0.1432  loss_ce_8: 0.1276  loss_mask_8: 0.2257  loss_dice_8: 0.1449  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:27] d2.utils.events INFO:  eta: 1:10:26  iter: 16579  total_loss: 5.211  loss_ce: 0.1284  loss_mask: 0.2191  loss_dice: 0.1735  loss_ce_0: 0.0639  loss_mask_0: 0.2083  loss_dice_0: 0.1714  loss_ce_1: 0.1286  loss_mask_1: 0.2182  loss_dice_1: 0.1773  loss_ce_2: 0.1285  loss_mask_2: 0.2161  loss_dice_2: 0.1723  loss_ce_3: 0.1287  loss_mask_3: 0.2127  loss_dice_3: 0.17  loss_ce_4: 0.1286  loss_mask_4: 0.2176  loss_dice_4: 0.1724  loss_ce_5: 0.1286  loss_mask_5: 0.2107  loss_dice_5: 0.1716  loss_ce_6: 0.1285  loss_mask_6: 0.2115  loss_dice_6: 0.1714  loss_ce_7: 0.1285  loss_mask_7: 0.2143  loss_dice_7: 0.17  loss_ce_8: 0.1283  loss_mask_8: 0.223  loss_dice_8: 0.1733  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:29] d2.utils.events INFO:  eta: 1:10:23  iter: 16599  total_loss: 5.451  loss_ce: 0.1242  loss_mask: 0.2118  loss_dice: 0.2114  loss_ce_0: 0.06272  loss_mask_0: 0.2114  loss_dice_0: 0.2093  loss_ce_1: 0.124  loss_mask_1: 0.2171  loss_dice_1: 0.2111  loss_ce_2: 0.1236  loss_mask_2: 0.2147  loss_dice_2: 0.2257  loss_ce_3: 0.1233  loss_mask_3: 0.2071  loss_dice_3: 0.218  loss_ce_4: 0.1229  loss_mask_4: 0.2078  loss_dice_4: 0.2223  loss_ce_5: 0.1234  loss_mask_5: 0.2135  loss_dice_5: 0.214  loss_ce_6: 0.1238  loss_mask_6: 0.2086  loss_dice_6: 0.2138  loss_ce_7: 0.1236  loss_mask_7: 0.2088  loss_dice_7: 0.2209  loss_ce_8: 0.1241  loss_mask_8: 0.2186  loss_dice_8: 0.2225  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:32] d2.utils.events INFO:  eta: 1:10:18  iter: 16619  total_loss: 4.904  loss_ce: 0.1345  loss_mask: 0.2255  loss_dice: 0.1585  loss_ce_0: 0.06535  loss_mask_0: 0.2257  loss_dice_0: 0.1617  loss_ce_1: 0.1337  loss_mask_1: 0.2299  loss_dice_1: 0.1603  loss_ce_2: 0.1342  loss_mask_2: 0.2248  loss_dice_2: 0.1606  loss_ce_3: 0.134  loss_mask_3: 0.2251  loss_dice_3: 0.1556  loss_ce_4: 0.134  loss_mask_4: 0.2296  loss_dice_4: 0.162  loss_ce_5: 0.1341  loss_mask_5: 0.2228  loss_dice_5: 0.1596  loss_ce_6: 0.1344  loss_mask_6: 0.2299  loss_dice_6: 0.1514  loss_ce_7: 0.1344  loss_mask_7: 0.2286  loss_dice_7: 0.1652  loss_ce_8: 0.1345  loss_mask_8: 0.2272  loss_dice_8: 0.1658  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:34] d2.utils.events INFO:  eta: 1:10:13  iter: 16639  total_loss: 5.296  loss_ce: 0.1338  loss_mask: 0.2041  loss_dice: 0.1933  loss_ce_0: 0.06522  loss_mask_0: 0.1987  loss_dice_0: 0.1898  loss_ce_1: 0.1336  loss_mask_1: 0.208  loss_dice_1: 0.1924  loss_ce_2: 0.1329  loss_mask_2: 0.1951  loss_dice_2: 0.1896  loss_ce_3: 0.1333  loss_mask_3: 0.1978  loss_dice_3: 0.1986  loss_ce_4: 0.1331  loss_mask_4: 0.2104  loss_dice_4: 0.1981  loss_ce_5: 0.1332  loss_mask_5: 0.2046  loss_dice_5: 0.2086  loss_ce_6: 0.1335  loss_mask_6: 0.1975  loss_dice_6: 0.1881  loss_ce_7: 0.1338  loss_mask_7: 0.198  loss_dice_7: 0.1967  loss_ce_8: 0.1337  loss_mask_8: 0.2009  loss_dice_8: 0.2084  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:20:37] d2.utils.events INFO:  eta: 1:10:05  iter: 16659  total_loss: 4.447  loss_ce: 0.1311  loss_mask: 0.2041  loss_dice: 0.1301  loss_ce_0: 0.06454  loss_mask_0: 0.2035  loss_dice_0: 0.1325  loss_ce_1: 0.1315  loss_mask_1: 0.2109  loss_dice_1: 0.1235  loss_ce_2: 0.1315  loss_mask_2: 0.2067  loss_dice_2: 0.1209  loss_ce_3: 0.1312  loss_mask_3: 0.2056  loss_dice_3: 0.1315  loss_ce_4: 0.1311  loss_mask_4: 0.2081  loss_dice_4: 0.1298  loss_ce_5: 0.1309  loss_mask_5: 0.2064  loss_dice_5: 0.1236  loss_ce_6: 0.1313  loss_mask_6: 0.2143  loss_dice_6: 0.1284  loss_ce_7: 0.1314  loss_mask_7: 0.2082  loss_dice_7: 0.1269  loss_ce_8: 0.1311  loss_mask_8: 0.2063  loss_dice_8: 0.1305  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:39] d2.utils.events INFO:  eta: 1:10:01  iter: 16679  total_loss: 5.114  loss_ce: 0.1276  loss_mask: 0.2407  loss_dice: 0.1356  loss_ce_0: 0.06355  loss_mask_0: 0.2514  loss_dice_0: 0.1344  loss_ce_1: 0.128  loss_mask_1: 0.2475  loss_dice_1: 0.1372  loss_ce_2: 0.1276  loss_mask_2: 0.2359  loss_dice_2: 0.1425  loss_ce_3: 0.128  loss_mask_3: 0.239  loss_dice_3: 0.1361  loss_ce_4: 0.128  loss_mask_4: 0.2447  loss_dice_4: 0.1323  loss_ce_5: 0.128  loss_mask_5: 0.2505  loss_dice_5: 0.1367  loss_ce_6: 0.128  loss_mask_6: 0.2481  loss_dice_6: 0.1333  loss_ce_7: 0.1279  loss_mask_7: 0.2402  loss_dice_7: 0.1367  loss_ce_8: 0.1276  loss_mask_8: 0.2433  loss_dice_8: 0.133  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:42] d2.utils.events INFO:  eta: 1:09:59  iter: 16699  total_loss: 5.137  loss_ce: 0.1287  loss_mask: 0.2324  loss_dice: 0.176  loss_ce_0: 0.06403  loss_mask_0: 0.2331  loss_dice_0: 0.1687  loss_ce_1: 0.1291  loss_mask_1: 0.2337  loss_dice_1: 0.1645  loss_ce_2: 0.1294  loss_mask_2: 0.2409  loss_dice_2: 0.1644  loss_ce_3: 0.1297  loss_mask_3: 0.2277  loss_dice_3: 0.1731  loss_ce_4: 0.13  loss_mask_4: 0.2361  loss_dice_4: 0.1706  loss_ce_5: 0.1297  loss_mask_5: 0.2384  loss_dice_5: 0.1709  loss_ce_6: 0.1294  loss_mask_6: 0.2334  loss_dice_6: 0.1691  loss_ce_7: 0.1291  loss_mask_7: 0.2375  loss_dice_7: 0.1684  loss_ce_8: 0.1286  loss_mask_8: 0.2391  loss_dice_8: 0.1688  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:20:44] d2.utils.events INFO:  eta: 1:09:56  iter: 16719  total_loss: 4.992  loss_ce: 0.1247  loss_mask: 0.2014  loss_dice: 0.174  loss_ce_0: 0.06253  loss_mask_0: 0.1905  loss_dice_0: 0.1759  loss_ce_1: 0.1242  loss_mask_1: 0.2114  loss_dice_1: 0.1831  loss_ce_2: 0.1242  loss_mask_2: 0.2019  loss_dice_2: 0.1756  loss_ce_3: 0.124  loss_mask_3: 0.2032  loss_dice_3: 0.173  loss_ce_4: 0.1234  loss_mask_4: 0.1991  loss_dice_4: 0.1698  loss_ce_5: 0.124  loss_mask_5: 0.2027  loss_dice_5: 0.1739  loss_ce_6: 0.1242  loss_mask_6: 0.1949  loss_dice_6: 0.1699  loss_ce_7: 0.1244  loss_mask_7: 0.1927  loss_dice_7: 0.1685  loss_ce_8: 0.1248  loss_mask_8: 0.1984  loss_dice_8: 0.1759  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:47] d2.utils.events INFO:  eta: 1:09:55  iter: 16739  total_loss: 4.83  loss_ce: 0.1374  loss_mask: 0.2209  loss_dice: 0.1319  loss_ce_0: 0.06651  loss_mask_0: 0.2157  loss_dice_0: 0.1276  loss_ce_1: 0.1369  loss_mask_1: 0.2204  loss_dice_1: 0.1263  loss_ce_2: 0.1371  loss_mask_2: 0.2206  loss_dice_2: 0.1324  loss_ce_3: 0.1371  loss_mask_3: 0.2188  loss_dice_3: 0.131  loss_ce_4: 0.1374  loss_mask_4: 0.2224  loss_dice_4: 0.1313  loss_ce_5: 0.1372  loss_mask_5: 0.2248  loss_dice_5: 0.1329  loss_ce_6: 0.1374  loss_mask_6: 0.2177  loss_dice_6: 0.1324  loss_ce_7: 0.1374  loss_mask_7: 0.2219  loss_dice_7: 0.1291  loss_ce_8: 0.1374  loss_mask_8: 0.2133  loss_dice_8: 0.1266  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:50] d2.utils.events INFO:  eta: 1:09:52  iter: 16759  total_loss: 5.232  loss_ce: 0.1293  loss_mask: 0.1966  loss_dice: 0.1984  loss_ce_0: 0.064  loss_mask_0: 0.196  loss_dice_0: 0.2029  loss_ce_1: 0.1285  loss_mask_1: 0.1929  loss_dice_1: 0.1955  loss_ce_2: 0.1284  loss_mask_2: 0.1968  loss_dice_2: 0.1966  loss_ce_3: 0.1281  loss_mask_3: 0.1967  loss_dice_3: 0.1944  loss_ce_4: 0.1277  loss_mask_4: 0.1939  loss_dice_4: 0.1977  loss_ce_5: 0.1278  loss_mask_5: 0.1963  loss_dice_5: 0.1982  loss_ce_6: 0.1281  loss_mask_6: 0.1938  loss_dice_6: 0.1998  loss_ce_7: 0.1282  loss_mask_7: 0.1994  loss_dice_7: 0.1997  loss_ce_8: 0.129  loss_mask_8: 0.1876  loss_dice_8: 0.189  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:52] d2.utils.events INFO:  eta: 1:09:50  iter: 16779  total_loss: 5.089  loss_ce: 0.1268  loss_mask: 0.1863  loss_dice: 0.1536  loss_ce_0: 0.06321  loss_mask_0: 0.1895  loss_dice_0: 0.1585  loss_ce_1: 0.1261  loss_mask_1: 0.1853  loss_dice_1: 0.154  loss_ce_2: 0.1259  loss_mask_2: 0.1941  loss_dice_2: 0.1546  loss_ce_3: 0.1254  loss_mask_3: 0.1865  loss_dice_3: 0.1599  loss_ce_4: 0.1248  loss_mask_4: 0.1783  loss_dice_4: 0.1514  loss_ce_5: 0.1252  loss_mask_5: 0.1825  loss_dice_5: 0.1526  loss_ce_6: 0.1255  loss_mask_6: 0.1898  loss_dice_6: 0.1515  loss_ce_7: 0.1256  loss_mask_7: 0.1872  loss_dice_7: 0.1553  loss_ce_8: 0.1265  loss_mask_8: 0.1808  loss_dice_8: 0.1511  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:55] d2.utils.events INFO:  eta: 1:09:54  iter: 16799  total_loss: 4.92  loss_ce: 0.1367  loss_mask: 0.167  loss_dice: 0.19  loss_ce_0: 0.06621  loss_mask_0: 0.1618  loss_dice_0: 0.187  loss_ce_1: 0.1358  loss_mask_1: 0.1688  loss_dice_1: 0.1895  loss_ce_2: 0.1362  loss_mask_2: 0.1563  loss_dice_2: 0.1874  loss_ce_3: 0.1359  loss_mask_3: 0.1664  loss_dice_3: 0.1963  loss_ce_4: 0.1359  loss_mask_4: 0.1666  loss_dice_4: 0.2054  loss_ce_5: 0.1364  loss_mask_5: 0.1666  loss_dice_5: 0.1973  loss_ce_6: 0.1371  loss_mask_6: 0.1698  loss_dice_6: 0.1971  loss_ce_7: 0.1371  loss_mask_7: 0.167  loss_dice_7: 0.202  loss_ce_8: 0.137  loss_mask_8: 0.1667  loss_dice_8: 0.1866  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:20:57] d2.utils.events INFO:  eta: 1:09:56  iter: 16819  total_loss: 5.653  loss_ce: 0.1268  loss_mask: 0.2274  loss_dice: 0.1938  loss_ce_0: 0.06327  loss_mask_0: 0.2287  loss_dice_0: 0.1874  loss_ce_1: 0.1266  loss_mask_1: 0.2209  loss_dice_1: 0.1926  loss_ce_2: 0.1266  loss_mask_2: 0.2177  loss_dice_2: 0.1908  loss_ce_3: 0.1261  loss_mask_3: 0.2237  loss_dice_3: 0.1981  loss_ce_4: 0.1254  loss_mask_4: 0.2111  loss_dice_4: 0.1959  loss_ce_5: 0.1261  loss_mask_5: 0.2204  loss_dice_5: 0.1897  loss_ce_6: 0.1267  loss_mask_6: 0.2276  loss_dice_6: 0.203  loss_ce_7: 0.1267  loss_mask_7: 0.2248  loss_dice_7: 0.1895  loss_ce_8: 0.1272  loss_mask_8: 0.2153  loss_dice_8: 0.1961  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:00] d2.utils.events INFO:  eta: 1:10:00  iter: 16839  total_loss: 5.302  loss_ce: 0.1312  loss_mask: 0.2165  loss_dice: 0.1905  loss_ce_0: 0.06456  loss_mask_0: 0.2224  loss_dice_0: 0.1896  loss_ce_1: 0.1307  loss_mask_1: 0.2144  loss_dice_1: 0.1881  loss_ce_2: 0.1307  loss_mask_2: 0.2172  loss_dice_2: 0.186  loss_ce_3: 0.1307  loss_mask_3: 0.2162  loss_dice_3: 0.1946  loss_ce_4: 0.1309  loss_mask_4: 0.2168  loss_dice_4: 0.1874  loss_ce_5: 0.131  loss_mask_5: 0.2104  loss_dice_5: 0.1854  loss_ce_6: 0.1313  loss_mask_6: 0.216  loss_dice_6: 0.1928  loss_ce_7: 0.1314  loss_mask_7: 0.219  loss_dice_7: 0.1928  loss_ce_8: 0.1312  loss_mask_8: 0.2166  loss_dice_8: 0.1906  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:03] d2.utils.events INFO:  eta: 1:10:01  iter: 16859  total_loss: 5.336  loss_ce: 0.1275  loss_mask: 0.1873  loss_dice: 0.1684  loss_ce_0: 0.06353  loss_mask_0: 0.1909  loss_dice_0: 0.1711  loss_ce_1: 0.1277  loss_mask_1: 0.1836  loss_dice_1: 0.1678  loss_ce_2: 0.1282  loss_mask_2: 0.1906  loss_dice_2: 0.1738  loss_ce_3: 0.1274  loss_mask_3: 0.1948  loss_dice_3: 0.1759  loss_ce_4: 0.1273  loss_mask_4: 0.1905  loss_dice_4: 0.1703  loss_ce_5: 0.1276  loss_mask_5: 0.1888  loss_dice_5: 0.1701  loss_ce_6: 0.1278  loss_mask_6: 0.1943  loss_dice_6: 0.1777  loss_ce_7: 0.1279  loss_mask_7: 0.1931  loss_dice_7: 0.1775  loss_ce_8: 0.1276  loss_mask_8: 0.1913  loss_dice_8: 0.1787  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:05] d2.utils.events INFO:  eta: 1:10:00  iter: 16879  total_loss: 4.782  loss_ce: 0.1328  loss_mask: 0.229  loss_dice: 0.1507  loss_ce_0: 0.06519  loss_mask_0: 0.2175  loss_dice_0: 0.1494  loss_ce_1: 0.1319  loss_mask_1: 0.217  loss_dice_1: 0.1491  loss_ce_2: 0.1328  loss_mask_2: 0.2139  loss_dice_2: 0.1513  loss_ce_3: 0.133  loss_mask_3: 0.2191  loss_dice_3: 0.155  loss_ce_4: 0.1329  loss_mask_4: 0.2178  loss_dice_4: 0.151  loss_ce_5: 0.133  loss_mask_5: 0.2139  loss_dice_5: 0.1535  loss_ce_6: 0.1331  loss_mask_6: 0.2173  loss_dice_6: 0.1524  loss_ce_7: 0.1326  loss_mask_7: 0.2155  loss_dice_7: 0.1542  loss_ce_8: 0.1329  loss_mask_8: 0.2255  loss_dice_8: 0.1454  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:21:08] d2.utils.events INFO:  eta: 1:09:58  iter: 16899  total_loss: 4.914  loss_ce: 0.1282  loss_mask: 0.2065  loss_dice: 0.1686  loss_ce_0: 0.06373  loss_mask_0: 0.2035  loss_dice_0: 0.169  loss_ce_1: 0.1273  loss_mask_1: 0.202  loss_dice_1: 0.1732  loss_ce_2: 0.128  loss_mask_2: 0.1968  loss_dice_2: 0.1684  loss_ce_3: 0.1279  loss_mask_3: 0.1987  loss_dice_3: 0.1648  loss_ce_4: 0.1272  loss_mask_4: 0.2114  loss_dice_4: 0.1728  loss_ce_5: 0.1276  loss_mask_5: 0.1999  loss_dice_5: 0.169  loss_ce_6: 0.1278  loss_mask_6: 0.2028  loss_dice_6: 0.1651  loss_ce_7: 0.1273  loss_mask_7: 0.2017  loss_dice_7: 0.1696  loss_ce_8: 0.1282  loss_mask_8: 0.204  loss_dice_8: 0.1715  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:21:10] d2.utils.events INFO:  eta: 1:09:54  iter: 16919  total_loss: 5  loss_ce: 0.1275  loss_mask: 0.1766  loss_dice: 0.169  loss_ce_0: 0.06336  loss_mask_0: 0.1811  loss_dice_0: 0.1693  loss_ce_1: 0.1276  loss_mask_1: 0.181  loss_dice_1: 0.1629  loss_ce_2: 0.1275  loss_mask_2: 0.1796  loss_dice_2: 0.1595  loss_ce_3: 0.1275  loss_mask_3: 0.1696  loss_dice_3: 0.1684  loss_ce_4: 0.1276  loss_mask_4: 0.1782  loss_dice_4: 0.1598  loss_ce_5: 0.1276  loss_mask_5: 0.1848  loss_dice_5: 0.1682  loss_ce_6: 0.1276  loss_mask_6: 0.1776  loss_dice_6: 0.169  loss_ce_7: 0.1276  loss_mask_7: 0.1772  loss_dice_7: 0.166  loss_ce_8: 0.1276  loss_mask_8: 0.179  loss_dice_8: 0.1658  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:21:13] d2.utils.events INFO:  eta: 1:09:52  iter: 16939  total_loss: 4.739  loss_ce: 0.1287  loss_mask: 0.2049  loss_dice: 0.1567  loss_ce_0: 0.06387  loss_mask_0: 0.2083  loss_dice_0: 0.154  loss_ce_1: 0.1286  loss_mask_1: 0.2111  loss_dice_1: 0.1624  loss_ce_2: 0.1285  loss_mask_2: 0.2004  loss_dice_2: 0.153  loss_ce_3: 0.1286  loss_mask_3: 0.1979  loss_dice_3: 0.1564  loss_ce_4: 0.1284  loss_mask_4: 0.2057  loss_dice_4: 0.1575  loss_ce_5: 0.1285  loss_mask_5: 0.2019  loss_dice_5: 0.1526  loss_ce_6: 0.1286  loss_mask_6: 0.2038  loss_dice_6: 0.1587  loss_ce_7: 0.1287  loss_mask_7: 0.2011  loss_dice_7: 0.1606  loss_ce_8: 0.1288  loss_mask_8: 0.2003  loss_dice_8: 0.1561  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:15] d2.utils.events INFO:  eta: 1:09:50  iter: 16959  total_loss: 5.13  loss_ce: 0.1189  loss_mask: 0.1621  loss_dice: 0.1839  loss_ce_0: 0.06047  loss_mask_0: 0.154  loss_dice_0: 0.1862  loss_ce_1: 0.1176  loss_mask_1: 0.1607  loss_dice_1: 0.1898  loss_ce_2: 0.1175  loss_mask_2: 0.1656  loss_dice_2: 0.1823  loss_ce_3: 0.1172  loss_mask_3: 0.1696  loss_dice_3: 0.1896  loss_ce_4: 0.1163  loss_mask_4: 0.1634  loss_dice_4: 0.1809  loss_ce_5: 0.117  loss_mask_5: 0.1568  loss_dice_5: 0.1845  loss_ce_6: 0.1177  loss_mask_6: 0.1622  loss_dice_6: 0.195  loss_ce_7: 0.1177  loss_mask_7: 0.1732  loss_dice_7: 0.1896  loss_ce_8: 0.119  loss_mask_8: 0.1699  loss_dice_8: 0.1873  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:18] d2.utils.events INFO:  eta: 1:09:46  iter: 16979  total_loss: 5.283  loss_ce: 0.1275  loss_mask: 0.1832  loss_dice: 0.1552  loss_ce_0: 0.06329  loss_mask_0: 0.1836  loss_dice_0: 0.1561  loss_ce_1: 0.1277  loss_mask_1: 0.186  loss_dice_1: 0.1551  loss_ce_2: 0.1278  loss_mask_2: 0.1842  loss_dice_2: 0.1594  loss_ce_3: 0.1278  loss_mask_3: 0.1798  loss_dice_3: 0.1599  loss_ce_4: 0.1281  loss_mask_4: 0.1751  loss_dice_4: 0.1531  loss_ce_5: 0.1279  loss_mask_5: 0.1752  loss_dice_5: 0.156  loss_ce_6: 0.1278  loss_mask_6: 0.1802  loss_dice_6: 0.1586  loss_ce_7: 0.1278  loss_mask_7: 0.1735  loss_dice_7: 0.1624  loss_ce_8: 0.1275  loss_mask_8: 0.1743  loss_dice_8: 0.1606  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:21:20] d2.utils.events INFO:  eta: 1:09:44  iter: 16999  total_loss: 4.971  loss_ce: 0.1513  loss_mask: 0.1872  loss_dice: 0.1456  loss_ce_0: 0.07127  loss_mask_0: 0.187  loss_dice_0: 0.1442  loss_ce_1: 0.1506  loss_mask_1: 0.1955  loss_dice_1: 0.1487  loss_ce_2: 0.1507  loss_mask_2: 0.1902  loss_dice_2: 0.1484  loss_ce_3: 0.1505  loss_mask_3: 0.1906  loss_dice_3: 0.1471  loss_ce_4: 0.1503  loss_mask_4: 0.19  loss_dice_4: 0.147  loss_ce_5: 0.1501  loss_mask_5: 0.1906  loss_dice_5: 0.1357  loss_ce_6: 0.1507  loss_mask_6: 0.1967  loss_dice_6: 0.1433  loss_ce_7: 0.1509  loss_mask_7: 0.1829  loss_dice_7: 0.1468  loss_ce_8: 0.151  loss_mask_8: 0.1873  loss_dice_8: 0.1422  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:21:23] d2.utils.events INFO:  eta: 1:09:41  iter: 17019  total_loss: 6.095  loss_ce: 0.1116  loss_mask: 0.1625  loss_dice: 0.2267  loss_ce_0: 0.05783  loss_mask_0: 0.1575  loss_dice_0: 0.2251  loss_ce_1: 0.1122  loss_mask_1: 0.1539  loss_dice_1: 0.2297  loss_ce_2: 0.1125  loss_mask_2: 0.158  loss_dice_2: 0.2331  loss_ce_3: 0.1126  loss_mask_3: 0.1631  loss_dice_3: 0.2269  loss_ce_4: 0.1132  loss_mask_4: 0.1743  loss_dice_4: 0.2339  loss_ce_5: 0.1133  loss_mask_5: 0.1566  loss_dice_5: 0.2223  loss_ce_6: 0.1129  loss_mask_6: 0.1645  loss_dice_6: 0.2308  loss_ce_7: 0.1127  loss_mask_7: 0.1688  loss_dice_7: 0.2218  loss_ce_8: 0.1119  loss_mask_8: 0.1691  loss_dice_8: 0.2278  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:25] d2.utils.events INFO:  eta: 1:09:40  iter: 17039  total_loss: 4.779  loss_ce: 0.1281  loss_mask: 0.2028  loss_dice: 0.1398  loss_ce_0: 0.06366  loss_mask_0: 0.2094  loss_dice_0: 0.1438  loss_ce_1: 0.1282  loss_mask_1: 0.2117  loss_dice_1: 0.1413  loss_ce_2: 0.1282  loss_mask_2: 0.2104  loss_dice_2: 0.1354  loss_ce_3: 0.1282  loss_mask_3: 0.2112  loss_dice_3: 0.1456  loss_ce_4: 0.1281  loss_mask_4: 0.2087  loss_dice_4: 0.139  loss_ce_5: 0.1281  loss_mask_5: 0.2016  loss_dice_5: 0.1412  loss_ce_6: 0.1281  loss_mask_6: 0.21  loss_dice_6: 0.1454  loss_ce_7: 0.1281  loss_mask_7: 0.2119  loss_dice_7: 0.1393  loss_ce_8: 0.1281  loss_mask_8: 0.2014  loss_dice_8: 0.1393  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:21:28] d2.utils.events INFO:  eta: 1:09:38  iter: 17059  total_loss: 4.864  loss_ce: 0.1091  loss_mask: 0.2229  loss_dice: 0.1488  loss_ce_0: 0.05662  loss_mask_0: 0.2118  loss_dice_0: 0.1508  loss_ce_1: 0.1092  loss_mask_1: 0.2231  loss_dice_1: 0.1508  loss_ce_2: 0.1091  loss_mask_2: 0.2233  loss_dice_2: 0.1496  loss_ce_3: 0.109  loss_mask_3: 0.2176  loss_dice_3: 0.1524  loss_ce_4: 0.109  loss_mask_4: 0.2244  loss_dice_4: 0.1539  loss_ce_5: 0.1089  loss_mask_5: 0.2251  loss_dice_5: 0.1489  loss_ce_6: 0.1088  loss_mask_6: 0.2225  loss_dice_6: 0.1474  loss_ce_7: 0.1089  loss_mask_7: 0.2313  loss_dice_7: 0.1543  loss_ce_8: 0.109  loss_mask_8: 0.2231  loss_dice_8: 0.1458  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:31] d2.utils.events INFO:  eta: 1:09:33  iter: 17079  total_loss: 5.157  loss_ce: 0.1082  loss_mask: 0.2138  loss_dice: 0.174  loss_ce_0: 0.05628  loss_mask_0: 0.2103  loss_dice_0: 0.1653  loss_ce_1: 0.1082  loss_mask_1: 0.2169  loss_dice_1: 0.1773  loss_ce_2: 0.1081  loss_mask_2: 0.2147  loss_dice_2: 0.1725  loss_ce_3: 0.1082  loss_mask_3: 0.2154  loss_dice_3: 0.1794  loss_ce_4: 0.1082  loss_mask_4: 0.2093  loss_dice_4: 0.1727  loss_ce_5: 0.1081  loss_mask_5: 0.2136  loss_dice_5: 0.1714  loss_ce_6: 0.1079  loss_mask_6: 0.2178  loss_dice_6: 0.1702  loss_ce_7: 0.108  loss_mask_7: 0.208  loss_dice_7: 0.1658  loss_ce_8: 0.1081  loss_mask_8: 0.2218  loss_dice_8: 0.1745  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:33] d2.utils.events INFO:  eta: 1:09:31  iter: 17099  total_loss: 5.304  loss_ce: 0.1495  loss_mask: 0.163  loss_dice: 0.2332  loss_ce_0: 0.07143  loss_mask_0: 0.1515  loss_dice_0: 0.2269  loss_ce_1: 0.1493  loss_mask_1: 0.1542  loss_dice_1: 0.2362  loss_ce_2: 0.1495  loss_mask_2: 0.1602  loss_dice_2: 0.2269  loss_ce_3: 0.1495  loss_mask_3: 0.1472  loss_dice_3: 0.2255  loss_ce_4: 0.1494  loss_mask_4: 0.156  loss_dice_4: 0.2309  loss_ce_5: 0.1494  loss_mask_5: 0.154  loss_dice_5: 0.235  loss_ce_6: 0.1495  loss_mask_6: 0.1529  loss_dice_6: 0.2349  loss_ce_7: 0.1494  loss_mask_7: 0.1591  loss_dice_7: 0.2223  loss_ce_8: 0.1495  loss_mask_8: 0.1516  loss_dice_8: 0.2222  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:36] d2.utils.events INFO:  eta: 1:09:30  iter: 17119  total_loss: 4.879  loss_ce: 0.1144  loss_mask: 0.2277  loss_dice: 0.1368  loss_ce_0: 0.05823  loss_mask_0: 0.2306  loss_dice_0: 0.134  loss_ce_1: 0.1152  loss_mask_1: 0.2209  loss_dice_1: 0.1337  loss_ce_2: 0.1151  loss_mask_2: 0.2229  loss_dice_2: 0.1364  loss_ce_3: 0.1151  loss_mask_3: 0.2166  loss_dice_3: 0.14  loss_ce_4: 0.1155  loss_mask_4: 0.2171  loss_dice_4: 0.1396  loss_ce_5: 0.1154  loss_mask_5: 0.221  loss_dice_5: 0.1392  loss_ce_6: 0.1154  loss_mask_6: 0.2291  loss_dice_6: 0.1367  loss_ce_7: 0.1154  loss_mask_7: 0.209  loss_dice_7: 0.1398  loss_ce_8: 0.1146  loss_mask_8: 0.2174  loss_dice_8: 0.1373  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:21:38] d2.utils.events INFO:  eta: 1:09:28  iter: 17139  total_loss: 5.26  loss_ce: 0.1464  loss_mask: 0.1343  loss_dice: 0.2356  loss_ce_0: 0.07069  loss_mask_0: 0.1354  loss_dice_0: 0.2316  loss_ce_1: 0.1464  loss_mask_1: 0.1321  loss_dice_1: 0.2244  loss_ce_2: 0.1466  loss_mask_2: 0.1337  loss_dice_2: 0.2251  loss_ce_3: 0.1468  loss_mask_3: 0.1317  loss_dice_3: 0.2265  loss_ce_4: 0.1469  loss_mask_4: 0.1317  loss_dice_4: 0.2405  loss_ce_5: 0.1468  loss_mask_5: 0.1283  loss_dice_5: 0.2374  loss_ce_6: 0.1467  loss_mask_6: 0.129  loss_dice_6: 0.233  loss_ce_7: 0.1466  loss_mask_7: 0.131  loss_dice_7: 0.2157  loss_ce_8: 0.1465  loss_mask_8: 0.1357  loss_dice_8: 0.232  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:21:41] d2.utils.events INFO:  eta: 1:09:25  iter: 17159  total_loss: 5.471  loss_ce: 0.1138  loss_mask: 0.1851  loss_dice: 0.1892  loss_ce_0: 0.05799  loss_mask_0: 0.1968  loss_dice_0: 0.1867  loss_ce_1: 0.1143  loss_mask_1: 0.1878  loss_dice_1: 0.1841  loss_ce_2: 0.1143  loss_mask_2: 0.189  loss_dice_2: 0.1877  loss_ce_3: 0.1143  loss_mask_3: 0.1929  loss_dice_3: 0.1912  loss_ce_4: 0.1144  loss_mask_4: 0.197  loss_dice_4: 0.1909  loss_ce_5: 0.1143  loss_mask_5: 0.1993  loss_dice_5: 0.194  loss_ce_6: 0.1143  loss_mask_6: 0.2004  loss_dice_6: 0.1881  loss_ce_7: 0.1143  loss_mask_7: 0.1818  loss_dice_7: 0.1885  loss_ce_8: 0.1137  loss_mask_8: 0.1821  loss_dice_8: 0.1942  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:43] d2.utils.events INFO:  eta: 1:09:25  iter: 17179  total_loss: 5.695  loss_ce: 0.1082  loss_mask: 0.1583  loss_dice: 0.2326  loss_ce_0: 0.05608  loss_mask_0: 0.168  loss_dice_0: 0.2333  loss_ce_1: 0.1079  loss_mask_1: 0.1683  loss_dice_1: 0.2378  loss_ce_2: 0.1079  loss_mask_2: 0.1688  loss_dice_2: 0.2313  loss_ce_3: 0.1077  loss_mask_3: 0.1724  loss_dice_3: 0.2283  loss_ce_4: 0.1075  loss_mask_4: 0.1723  loss_dice_4: 0.2444  loss_ce_5: 0.1077  loss_mask_5: 0.1663  loss_dice_5: 0.2231  loss_ce_6: 0.1076  loss_mask_6: 0.1736  loss_dice_6: 0.2386  loss_ce_7: 0.1077  loss_mask_7: 0.1747  loss_dice_7: 0.2379  loss_ce_8: 0.108  loss_mask_8: 0.1804  loss_dice_8: 0.2311  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:21:46] d2.utils.events INFO:  eta: 1:09:23  iter: 17199  total_loss: 5.192  loss_ce: 0.1555  loss_mask: 0.1897  loss_dice: 0.1911  loss_ce_0: 0.07346  loss_mask_0: 0.1917  loss_dice_0: 0.2022  loss_ce_1: 0.1558  loss_mask_1: 0.1841  loss_dice_1: 0.1988  loss_ce_2: 0.1556  loss_mask_2: 0.1902  loss_dice_2: 0.1977  loss_ce_3: 0.1558  loss_mask_3: 0.1821  loss_dice_3: 0.1968  loss_ce_4: 0.156  loss_mask_4: 0.1817  loss_dice_4: 0.1965  loss_ce_5: 0.1559  loss_mask_5: 0.1991  loss_dice_5: 0.2064  loss_ce_6: 0.1561  loss_mask_6: 0.19  loss_dice_6: 0.2032  loss_ce_7: 0.156  loss_mask_7: 0.1878  loss_dice_7: 0.1933  loss_ce_8: 0.1555  loss_mask_8: 0.1846  loss_dice_8: 0.2087  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:48] d2.utils.events INFO:  eta: 1:09:20  iter: 17219  total_loss: 5.361  loss_ce: 0.1069  loss_mask: 0.2553  loss_dice: 0.1654  loss_ce_0: 0.05555  loss_mask_0: 0.2566  loss_dice_0: 0.161  loss_ce_1: 0.107  loss_mask_1: 0.2608  loss_dice_1: 0.1644  loss_ce_2: 0.1072  loss_mask_2: 0.2598  loss_dice_2: 0.1691  loss_ce_3: 0.1072  loss_mask_3: 0.2496  loss_dice_3: 0.1622  loss_ce_4: 0.1073  loss_mask_4: 0.2617  loss_dice_4: 0.1622  loss_ce_5: 0.1074  loss_mask_5: 0.2602  loss_dice_5: 0.16  loss_ce_6: 0.1073  loss_mask_6: 0.2605  loss_dice_6: 0.169  loss_ce_7: 0.1073  loss_mask_7: 0.2523  loss_dice_7: 0.172  loss_ce_8: 0.1069  loss_mask_8: 0.2662  loss_dice_8: 0.1668  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:21:51] d2.utils.events INFO:  eta: 1:09:16  iter: 17239  total_loss: 5.369  loss_ce: 0.1051  loss_mask: 0.2571  loss_dice: 0.1681  loss_ce_0: 0.05492  loss_mask_0: 0.2518  loss_dice_0: 0.1688  loss_ce_1: 0.1048  loss_mask_1: 0.2511  loss_dice_1: 0.1727  loss_ce_2: 0.1051  loss_mask_2: 0.2518  loss_dice_2: 0.1667  loss_ce_3: 0.1051  loss_mask_3: 0.251  loss_dice_3: 0.1707  loss_ce_4: 0.105  loss_mask_4: 0.2389  loss_dice_4: 0.1654  loss_ce_5: 0.1051  loss_mask_5: 0.255  loss_dice_5: 0.1726  loss_ce_6: 0.105  loss_mask_6: 0.2573  loss_dice_6: 0.1726  loss_ce_7: 0.105  loss_mask_7: 0.2526  loss_dice_7: 0.1744  loss_ce_8: 0.1051  loss_mask_8: 0.2479  loss_dice_8: 0.1781  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:54] d2.utils.events INFO:  eta: 1:09:13  iter: 17259  total_loss: 4.856  loss_ce: 0.1296  loss_mask: 0.1939  loss_dice: 0.1835  loss_ce_0: 0.064  loss_mask_0: 0.1882  loss_dice_0: 0.1861  loss_ce_1: 0.1296  loss_mask_1: 0.1889  loss_dice_1: 0.1884  loss_ce_2: 0.1296  loss_mask_2: 0.1856  loss_dice_2: 0.1828  loss_ce_3: 0.1296  loss_mask_3: 0.1816  loss_dice_3: 0.1843  loss_ce_4: 0.1296  loss_mask_4: 0.178  loss_dice_4: 0.1804  loss_ce_5: 0.1296  loss_mask_5: 0.181  loss_dice_5: 0.1858  loss_ce_6: 0.1296  loss_mask_6: 0.1742  loss_dice_6: 0.1801  loss_ce_7: 0.1296  loss_mask_7: 0.1846  loss_dice_7: 0.1853  loss_ce_8: 0.1295  loss_mask_8: 0.1798  loss_dice_8: 0.1935  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:56] d2.utils.events INFO:  eta: 1:09:13  iter: 17279  total_loss: 4.943  loss_ce: 0.1514  loss_mask: 0.2188  loss_dice: 0.1429  loss_ce_0: 0.07245  loss_mask_0: 0.2216  loss_dice_0: 0.1434  loss_ce_1: 0.151  loss_mask_1: 0.2183  loss_dice_1: 0.1389  loss_ce_2: 0.1513  loss_mask_2: 0.2171  loss_dice_2: 0.1349  loss_ce_3: 0.1512  loss_mask_3: 0.2085  loss_dice_3: 0.1319  loss_ce_4: 0.1509  loss_mask_4: 0.2162  loss_dice_4: 0.1403  loss_ce_5: 0.1509  loss_mask_5: 0.2103  loss_dice_5: 0.1382  loss_ce_6: 0.1509  loss_mask_6: 0.2249  loss_dice_6: 0.1429  loss_ce_7: 0.1509  loss_mask_7: 0.2129  loss_dice_7: 0.1368  loss_ce_8: 0.1514  loss_mask_8: 0.2175  loss_dice_8: 0.1412  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:21:59] d2.utils.events INFO:  eta: 1:09:11  iter: 17299  total_loss: 5.301  loss_ce: 0.1069  loss_mask: 0.21  loss_dice: 0.2069  loss_ce_0: 0.05541  loss_mask_0: 0.2015  loss_dice_0: 0.2164  loss_ce_1: 0.1073  loss_mask_1: 0.2038  loss_dice_1: 0.2078  loss_ce_2: 0.1072  loss_mask_2: 0.209  loss_dice_2: 0.2132  loss_ce_3: 0.1072  loss_mask_3: 0.2157  loss_dice_3: 0.2158  loss_ce_4: 0.1075  loss_mask_4: 0.2047  loss_dice_4: 0.2158  loss_ce_5: 0.1074  loss_mask_5: 0.2097  loss_dice_5: 0.2232  loss_ce_6: 0.1074  loss_mask_6: 0.2149  loss_dice_6: 0.2198  loss_ce_7: 0.1073  loss_mask_7: 0.2084  loss_dice_7: 0.214  loss_ce_8: 0.107  loss_mask_8: 0.2087  loss_dice_8: 0.2132  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:01] d2.utils.events INFO:  eta: 1:09:08  iter: 17319  total_loss: 4.729  loss_ce: 0.1279  loss_mask: 0.1955  loss_dice: 0.1369  loss_ce_0: 0.06352  loss_mask_0: 0.1986  loss_dice_0: 0.138  loss_ce_1: 0.1279  loss_mask_1: 0.1998  loss_dice_1: 0.1348  loss_ce_2: 0.1279  loss_mask_2: 0.197  loss_dice_2: 0.1372  loss_ce_3: 0.1279  loss_mask_3: 0.191  loss_dice_3: 0.1338  loss_ce_4: 0.1278  loss_mask_4: 0.198  loss_dice_4: 0.1359  loss_ce_5: 0.1278  loss_mask_5: 0.1933  loss_dice_5: 0.1333  loss_ce_6: 0.1278  loss_mask_6: 0.197  loss_dice_6: 0.1375  loss_ce_7: 0.1279  loss_mask_7: 0.1958  loss_dice_7: 0.1422  loss_ce_8: 0.1279  loss_mask_8: 0.1974  loss_dice_8: 0.1334  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:04] d2.utils.events INFO:  eta: 1:09:06  iter: 17339  total_loss: 5.084  loss_ce: 0.1146  loss_mask: 0.2034  loss_dice: 0.1469  loss_ce_0: 0.05792  loss_mask_0: 0.199  loss_dice_0: 0.1513  loss_ce_1: 0.1151  loss_mask_1: 0.2032  loss_dice_1: 0.1511  loss_ce_2: 0.1149  loss_mask_2: 0.2006  loss_dice_2: 0.1468  loss_ce_3: 0.1151  loss_mask_3: 0.1973  loss_dice_3: 0.1472  loss_ce_4: 0.1154  loss_mask_4: 0.196  loss_dice_4: 0.151  loss_ce_5: 0.1152  loss_mask_5: 0.2017  loss_dice_5: 0.1562  loss_ce_6: 0.1155  loss_mask_6: 0.1894  loss_dice_6: 0.1443  loss_ce_7: 0.1154  loss_mask_7: 0.1911  loss_dice_7: 0.145  loss_ce_8: 0.1148  loss_mask_8: 0.1992  loss_dice_8: 0.148  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:06] d2.utils.events INFO:  eta: 1:09:02  iter: 17359  total_loss: 5.068  loss_ce: 0.1388  loss_mask: 0.2145  loss_dice: 0.1538  loss_ce_0: 0.06805  loss_mask_0: 0.2207  loss_dice_0: 0.1522  loss_ce_1: 0.1385  loss_mask_1: 0.2184  loss_dice_1: 0.1584  loss_ce_2: 0.1386  loss_mask_2: 0.2266  loss_dice_2: 0.1574  loss_ce_3: 0.1383  loss_mask_3: 0.2176  loss_dice_3: 0.1567  loss_ce_4: 0.1383  loss_mask_4: 0.2181  loss_dice_4: 0.1645  loss_ce_5: 0.1386  loss_mask_5: 0.2225  loss_dice_5: 0.1549  loss_ce_6: 0.1385  loss_mask_6: 0.2271  loss_dice_6: 0.1545  loss_ce_7: 0.1385  loss_mask_7: 0.2235  loss_dice_7: 0.1572  loss_ce_8: 0.1386  loss_mask_8: 0.2202  loss_dice_8: 0.1586  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:09] d2.utils.events INFO:  eta: 1:09:00  iter: 17379  total_loss: 5.106  loss_ce: 0.1176  loss_mask: 0.2143  loss_dice: 0.1628  loss_ce_0: 0.05911  loss_mask_0: 0.2159  loss_dice_0: 0.1546  loss_ce_1: 0.118  loss_mask_1: 0.2038  loss_dice_1: 0.1571  loss_ce_2: 0.1178  loss_mask_2: 0.2028  loss_dice_2: 0.1524  loss_ce_3: 0.1181  loss_mask_3: 0.2123  loss_dice_3: 0.1583  loss_ce_4: 0.1182  loss_mask_4: 0.2129  loss_dice_4: 0.1601  loss_ce_5: 0.1179  loss_mask_5: 0.2131  loss_dice_5: 0.165  loss_ce_6: 0.118  loss_mask_6: 0.2057  loss_dice_6: 0.1614  loss_ce_7: 0.1179  loss_mask_7: 0.2036  loss_dice_7: 0.1558  loss_ce_8: 0.1177  loss_mask_8: 0.2099  loss_dice_8: 0.1634  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:11] d2.utils.events INFO:  eta: 1:08:59  iter: 17399  total_loss: 4.837  loss_ce: 0.1107  loss_mask: 0.2406  loss_dice: 0.1398  loss_ce_0: 0.05696  loss_mask_0: 0.2405  loss_dice_0: 0.1358  loss_ce_1: 0.1104  loss_mask_1: 0.2424  loss_dice_1: 0.1437  loss_ce_2: 0.1105  loss_mask_2: 0.2377  loss_dice_2: 0.1373  loss_ce_3: 0.1103  loss_mask_3: 0.2308  loss_dice_3: 0.1375  loss_ce_4: 0.1101  loss_mask_4: 0.2223  loss_dice_4: 0.1388  loss_ce_5: 0.1102  loss_mask_5: 0.2446  loss_dice_5: 0.1377  loss_ce_6: 0.1103  loss_mask_6: 0.2413  loss_dice_6: 0.1426  loss_ce_7: 0.1103  loss_mask_7: 0.2343  loss_dice_7: 0.1405  loss_ce_8: 0.1106  loss_mask_8: 0.221  loss_dice_8: 0.1387  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:14] d2.utils.events INFO:  eta: 1:08:57  iter: 17419  total_loss: 5.087  loss_ce: 0.103  loss_mask: 0.2201  loss_dice: 0.1632  loss_ce_0: 0.05461  loss_mask_0: 0.2238  loss_dice_0: 0.1567  loss_ce_1: 0.1022  loss_mask_1: 0.2283  loss_dice_1: 0.1512  loss_ce_2: 0.1027  loss_mask_2: 0.2261  loss_dice_2: 0.159  loss_ce_3: 0.1025  loss_mask_3: 0.2338  loss_dice_3: 0.1557  loss_ce_4: 0.1021  loss_mask_4: 0.2129  loss_dice_4: 0.1613  loss_ce_5: 0.1022  loss_mask_5: 0.223  loss_dice_5: 0.1608  loss_ce_6: 0.1022  loss_mask_6: 0.2243  loss_dice_6: 0.1608  loss_ce_7: 0.1021  loss_mask_7: 0.2288  loss_dice_7: 0.1599  loss_ce_8: 0.103  loss_mask_8: 0.2201  loss_dice_8: 0.1561  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:16] d2.utils.events INFO:  eta: 1:08:53  iter: 17439  total_loss: 5.305  loss_ce: 0.13  loss_mask: 0.2505  loss_dice: 0.1608  loss_ce_0: 0.06408  loss_mask_0: 0.2499  loss_dice_0: 0.1686  loss_ce_1: 0.1301  loss_mask_1: 0.2363  loss_dice_1: 0.1664  loss_ce_2: 0.1299  loss_mask_2: 0.2439  loss_dice_2: 0.1642  loss_ce_3: 0.1299  loss_mask_3: 0.2508  loss_dice_3: 0.1665  loss_ce_4: 0.1299  loss_mask_4: 0.251  loss_dice_4: 0.1655  loss_ce_5: 0.1299  loss_mask_5: 0.253  loss_dice_5: 0.1709  loss_ce_6: 0.13  loss_mask_6: 0.2349  loss_dice_6: 0.1667  loss_ce_7: 0.13  loss_mask_7: 0.2458  loss_dice_7: 0.1612  loss_ce_8: 0.13  loss_mask_8: 0.2398  loss_dice_8: 0.1624  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:19] d2.utils.events INFO:  eta: 1:08:46  iter: 17459  total_loss: 5.506  loss_ce: 0.09932  loss_mask: 0.2039  loss_dice: 0.1969  loss_ce_0: 0.05305  loss_mask_0: 0.215  loss_dice_0: 0.1915  loss_ce_1: 0.09917  loss_mask_1: 0.2074  loss_dice_1: 0.1866  loss_ce_2: 0.09936  loss_mask_2: 0.2082  loss_dice_2: 0.1879  loss_ce_3: 0.09925  loss_mask_3: 0.2123  loss_dice_3: 0.1905  loss_ce_4: 0.09927  loss_mask_4: 0.2134  loss_dice_4: 0.1902  loss_ce_5: 0.09934  loss_mask_5: 0.2056  loss_dice_5: 0.1866  loss_ce_6: 0.09913  loss_mask_6: 0.2053  loss_dice_6: 0.1873  loss_ce_7: 0.09917  loss_mask_7: 0.2169  loss_dice_7: 0.185  loss_ce_8: 0.09928  loss_mask_8: 0.2101  loss_dice_8: 0.1851  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:21] d2.utils.events INFO:  eta: 1:08:41  iter: 17479  total_loss: 4.763  loss_ce: 0.0937  loss_mask: 0.2093  loss_dice: 0.1556  loss_ce_0: 0.05129  loss_mask_0: 0.2062  loss_dice_0: 0.1588  loss_ce_1: 0.09328  loss_mask_1: 0.2091  loss_dice_1: 0.156  loss_ce_2: 0.09365  loss_mask_2: 0.2094  loss_dice_2: 0.1492  loss_ce_3: 0.09357  loss_mask_3: 0.2074  loss_dice_3: 0.1593  loss_ce_4: 0.0935  loss_mask_4: 0.206  loss_dice_4: 0.1534  loss_ce_5: 0.09339  loss_mask_5: 0.2105  loss_dice_5: 0.1574  loss_ce_6: 0.09318  loss_mask_6: 0.2051  loss_dice_6: 0.1632  loss_ce_7: 0.09318  loss_mask_7: 0.2125  loss_dice_7: 0.1478  loss_ce_8: 0.09357  loss_mask_8: 0.2128  loss_dice_8: 0.1524  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:24] d2.utils.events INFO:  eta: 1:08:33  iter: 17499  total_loss: 5.163  loss_ce: 0.08322  loss_mask: 0.2157  loss_dice: 0.1586  loss_ce_0: 0.04836  loss_mask_0: 0.2195  loss_dice_0: 0.1608  loss_ce_1: 0.08258  loss_mask_1: 0.2189  loss_dice_1: 0.1651  loss_ce_2: 0.08307  loss_mask_2: 0.2161  loss_dice_2: 0.1623  loss_ce_3: 0.08317  loss_mask_3: 0.2143  loss_dice_3: 0.1659  loss_ce_4: 0.08302  loss_mask_4: 0.2088  loss_dice_4: 0.1567  loss_ce_5: 0.08271  loss_mask_5: 0.2204  loss_dice_5: 0.1597  loss_ce_6: 0.08216  loss_mask_6: 0.2171  loss_dice_6: 0.1644  loss_ce_7: 0.08206  loss_mask_7: 0.2106  loss_dice_7: 0.1693  loss_ce_8: 0.08297  loss_mask_8: 0.2185  loss_dice_8: 0.1602  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:26] d2.utils.events INFO:  eta: 1:08:31  iter: 17519  total_loss: 5.452  loss_ce: 0.1355  loss_mask: 0.1634  loss_dice: 0.2194  loss_ce_0: 0.06535  loss_mask_0: 0.1706  loss_dice_0: 0.225  loss_ce_1: 0.1355  loss_mask_1: 0.1684  loss_dice_1: 0.2201  loss_ce_2: 0.1358  loss_mask_2: 0.1668  loss_dice_2: 0.2133  loss_ce_3: 0.1357  loss_mask_3: 0.1541  loss_dice_3: 0.2163  loss_ce_4: 0.1356  loss_mask_4: 0.1681  loss_dice_4: 0.2207  loss_ce_5: 0.1357  loss_mask_5: 0.1629  loss_dice_5: 0.2198  loss_ce_6: 0.1357  loss_mask_6: 0.1572  loss_dice_6: 0.2107  loss_ce_7: 0.1355  loss_mask_7: 0.1619  loss_dice_7: 0.2263  loss_ce_8: 0.1355  loss_mask_8: 0.1564  loss_dice_8: 0.224  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:29] d2.utils.events INFO:  eta: 1:08:31  iter: 17539  total_loss: 5.698  loss_ce: 0.1764  loss_mask: 0.2341  loss_dice: 0.1999  loss_ce_0: 0.08227  loss_mask_0: 0.2355  loss_dice_0: 0.1962  loss_ce_1: 0.1768  loss_mask_1: 0.2364  loss_dice_1: 0.1944  loss_ce_2: 0.1769  loss_mask_2: 0.2341  loss_dice_2: 0.196  loss_ce_3: 0.1777  loss_mask_3: 0.2361  loss_dice_3: 0.1952  loss_ce_4: 0.1773  loss_mask_4: 0.2376  loss_dice_4: 0.2048  loss_ce_5: 0.1765  loss_mask_5: 0.2405  loss_dice_5: 0.2039  loss_ce_6: 0.1761  loss_mask_6: 0.2344  loss_dice_6: 0.1945  loss_ce_7: 0.1757  loss_mask_7: 0.2325  loss_dice_7: 0.1944  loss_ce_8: 0.1759  loss_mask_8: 0.2355  loss_dice_8: 0.2002  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:31] d2.utils.events INFO:  eta: 1:08:30  iter: 17559  total_loss: 5.324  loss_ce: 0.1279  loss_mask: 0.1645  loss_dice: 0.1825  loss_ce_0: 0.06378  loss_mask_0: 0.1575  loss_dice_0: 0.1752  loss_ce_1: 0.1275  loss_mask_1: 0.1564  loss_dice_1: 0.1814  loss_ce_2: 0.1277  loss_mask_2: 0.162  loss_dice_2: 0.1797  loss_ce_3: 0.1277  loss_mask_3: 0.1641  loss_dice_3: 0.1844  loss_ce_4: 0.1276  loss_mask_4: 0.1569  loss_dice_4: 0.184  loss_ce_5: 0.1277  loss_mask_5: 0.1547  loss_dice_5: 0.1868  loss_ce_6: 0.1276  loss_mask_6: 0.1628  loss_dice_6: 0.1791  loss_ce_7: 0.1277  loss_mask_7: 0.1576  loss_dice_7: 0.1801  loss_ce_8: 0.1279  loss_mask_8: 0.1665  loss_dice_8: 0.1755  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:34] d2.utils.events INFO:  eta: 1:08:30  iter: 17579  total_loss: 4.866  loss_ce: 0.1381  loss_mask: 0.1637  loss_dice: 0.1726  loss_ce_0: 0.06964  loss_mask_0: 0.1683  loss_dice_0: 0.1612  loss_ce_1: 0.1357  loss_mask_1: 0.1655  loss_dice_1: 0.1708  loss_ce_2: 0.1366  loss_mask_2: 0.1727  loss_dice_2: 0.1769  loss_ce_3: 0.1363  loss_mask_3: 0.1649  loss_dice_3: 0.1658  loss_ce_4: 0.1354  loss_mask_4: 0.1579  loss_dice_4: 0.1625  loss_ce_5: 0.1359  loss_mask_5: 0.168  loss_dice_5: 0.1775  loss_ce_6: 0.1354  loss_mask_6: 0.1601  loss_dice_6: 0.1642  loss_ce_7: 0.1353  loss_mask_7: 0.1669  loss_dice_7: 0.1658  loss_ce_8: 0.1373  loss_mask_8: 0.1597  loss_dice_8: 0.1678  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:37] d2.utils.events INFO:  eta: 1:08:29  iter: 17599  total_loss: 5.502  loss_ce: 0.1263  loss_mask: 0.1956  loss_dice: 0.1751  loss_ce_0: 0.06407  loss_mask_0: 0.1979  loss_dice_0: 0.1717  loss_ce_1: 0.1248  loss_mask_1: 0.1939  loss_dice_1: 0.1718  loss_ce_2: 0.1259  loss_mask_2: 0.1969  loss_dice_2: 0.1754  loss_ce_3: 0.125  loss_mask_3: 0.1975  loss_dice_3: 0.1739  loss_ce_4: 0.124  loss_mask_4: 0.1998  loss_dice_4: 0.1727  loss_ce_5: 0.125  loss_mask_5: 0.1985  loss_dice_5: 0.167  loss_ce_6: 0.1245  loss_mask_6: 0.1974  loss_dice_6: 0.1721  loss_ce_7: 0.1246  loss_mask_7: 0.1935  loss_dice_7: 0.1654  loss_ce_8: 0.126  loss_mask_8: 0.1889  loss_dice_8: 0.1693  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:39] d2.utils.events INFO:  eta: 1:08:31  iter: 17619  total_loss: 5.085  loss_ce: 0.141  loss_mask: 0.2149  loss_dice: 0.143  loss_ce_0: 0.06598  loss_mask_0: 0.2265  loss_dice_0: 0.1408  loss_ce_1: 0.1415  loss_mask_1: 0.2246  loss_dice_1: 0.142  loss_ce_2: 0.1408  loss_mask_2: 0.2213  loss_dice_2: 0.139  loss_ce_3: 0.1416  loss_mask_3: 0.2177  loss_dice_3: 0.1388  loss_ce_4: 0.1417  loss_mask_4: 0.2208  loss_dice_4: 0.1442  loss_ce_5: 0.1414  loss_mask_5: 0.217  loss_dice_5: 0.1392  loss_ce_6: 0.1426  loss_mask_6: 0.2301  loss_dice_6: 0.1391  loss_ce_7: 0.1423  loss_mask_7: 0.2248  loss_dice_7: 0.142  loss_ce_8: 0.1419  loss_mask_8: 0.2204  loss_dice_8: 0.1417  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:42] d2.utils.events INFO:  eta: 1:08:31  iter: 17639  total_loss: 5.076  loss_ce: 0.117  loss_mask: 0.2138  loss_dice: 0.1394  loss_ce_0: 0.06111  loss_mask_0: 0.2172  loss_dice_0: 0.1374  loss_ce_1: 0.1175  loss_mask_1: 0.2212  loss_dice_1: 0.1366  loss_ce_2: 0.1179  loss_mask_2: 0.2227  loss_dice_2: 0.1354  loss_ce_3: 0.1176  loss_mask_3: 0.2249  loss_dice_3: 0.1306  loss_ce_4: 0.1179  loss_mask_4: 0.2199  loss_dice_4: 0.1312  loss_ce_5: 0.1176  loss_mask_5: 0.2192  loss_dice_5: 0.134  loss_ce_6: 0.1164  loss_mask_6: 0.2198  loss_dice_6: 0.1362  loss_ce_7: 0.1169  loss_mask_7: 0.2168  loss_dice_7: 0.136  loss_ce_8: 0.1168  loss_mask_8: 0.2188  loss_dice_8: 0.1411  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:44] d2.utils.events INFO:  eta: 1:08:30  iter: 17659  total_loss: 5.745  loss_ce: 0.1335  loss_mask: 0.1819  loss_dice: 0.2197  loss_ce_0: 0.06498  loss_mask_0: 0.1855  loss_dice_0: 0.1951  loss_ce_1: 0.1326  loss_mask_1: 0.1807  loss_dice_1: 0.2181  loss_ce_2: 0.1327  loss_mask_2: 0.1845  loss_dice_2: 0.2087  loss_ce_3: 0.1328  loss_mask_3: 0.1848  loss_dice_3: 0.2249  loss_ce_4: 0.1322  loss_mask_4: 0.1803  loss_dice_4: 0.2046  loss_ce_5: 0.1328  loss_mask_5: 0.1806  loss_dice_5: 0.1994  loss_ce_6: 0.1332  loss_mask_6: 0.1788  loss_dice_6: 0.2111  loss_ce_7: 0.1326  loss_mask_7: 0.1854  loss_dice_7: 0.2133  loss_ce_8: 0.1332  loss_mask_8: 0.1874  loss_dice_8: 0.2193  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:47] d2.utils.events INFO:  eta: 1:08:27  iter: 17679  total_loss: 5.645  loss_ce: 0.1265  loss_mask: 0.1652  loss_dice: 0.1971  loss_ce_0: 0.06299  loss_mask_0: 0.1683  loss_dice_0: 0.1933  loss_ce_1: 0.1257  loss_mask_1: 0.1768  loss_dice_1: 0.2069  loss_ce_2: 0.1262  loss_mask_2: 0.1698  loss_dice_2: 0.2008  loss_ce_3: 0.1258  loss_mask_3: 0.1662  loss_dice_3: 0.1916  loss_ce_4: 0.1251  loss_mask_4: 0.1609  loss_dice_4: 0.1881  loss_ce_5: 0.1259  loss_mask_5: 0.169  loss_dice_5: 0.1965  loss_ce_6: 0.1263  loss_mask_6: 0.1705  loss_dice_6: 0.1977  loss_ce_7: 0.1258  loss_mask_7: 0.1651  loss_dice_7: 0.1981  loss_ce_8: 0.1262  loss_mask_8: 0.1723  loss_dice_8: 0.1941  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:22:49] d2.utils.events INFO:  eta: 1:08:26  iter: 17699  total_loss: 5.538  loss_ce: 0.1271  loss_mask: 0.1895  loss_dice: 0.2378  loss_ce_0: 0.06335  loss_mask_0: 0.1844  loss_dice_0: 0.2479  loss_ce_1: 0.1271  loss_mask_1: 0.1821  loss_dice_1: 0.2434  loss_ce_2: 0.1272  loss_mask_2: 0.1804  loss_dice_2: 0.2497  loss_ce_3: 0.1272  loss_mask_3: 0.1754  loss_dice_3: 0.2494  loss_ce_4: 0.1273  loss_mask_4: 0.1706  loss_dice_4: 0.2373  loss_ce_5: 0.1273  loss_mask_5: 0.1768  loss_dice_5: 0.2419  loss_ce_6: 0.1272  loss_mask_6: 0.186  loss_dice_6: 0.2469  loss_ce_7: 0.1272  loss_mask_7: 0.1709  loss_dice_7: 0.2408  loss_ce_8: 0.1271  loss_mask_8: 0.1787  loss_dice_8: 0.2477  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:52] d2.utils.events INFO:  eta: 1:08:24  iter: 17719  total_loss: 5.324  loss_ce: 0.1324  loss_mask: 0.225  loss_dice: 0.1891  loss_ce_0: 0.0649  loss_mask_0: 0.2171  loss_dice_0: 0.1844  loss_ce_1: 0.1315  loss_mask_1: 0.218  loss_dice_1: 0.1841  loss_ce_2: 0.1317  loss_mask_2: 0.2198  loss_dice_2: 0.191  loss_ce_3: 0.1315  loss_mask_3: 0.2249  loss_dice_3: 0.1932  loss_ce_4: 0.1313  loss_mask_4: 0.2174  loss_dice_4: 0.1947  loss_ce_5: 0.1313  loss_mask_5: 0.2254  loss_dice_5: 0.1815  loss_ce_6: 0.1317  loss_mask_6: 0.2183  loss_dice_6: 0.1878  loss_ce_7: 0.1316  loss_mask_7: 0.226  loss_dice_7: 0.1894  loss_ce_8: 0.1321  loss_mask_8: 0.2204  loss_dice_8: 0.198  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:54] d2.utils.events INFO:  eta: 1:08:21  iter: 17739  total_loss: 5.016  loss_ce: 0.127  loss_mask: 0.1732  loss_dice: 0.1656  loss_ce_0: 0.0632  loss_mask_0: 0.1744  loss_dice_0: 0.1657  loss_ce_1: 0.1271  loss_mask_1: 0.1679  loss_dice_1: 0.1656  loss_ce_2: 0.1271  loss_mask_2: 0.176  loss_dice_2: 0.1668  loss_ce_3: 0.1274  loss_mask_3: 0.1586  loss_dice_3: 0.1665  loss_ce_4: 0.1277  loss_mask_4: 0.1675  loss_dice_4: 0.1695  loss_ce_5: 0.1277  loss_mask_5: 0.1707  loss_dice_5: 0.1671  loss_ce_6: 0.1275  loss_mask_6: 0.1792  loss_dice_6: 0.1646  loss_ce_7: 0.1274  loss_mask_7: 0.1762  loss_dice_7: 0.1696  loss_ce_8: 0.127  loss_mask_8: 0.1703  loss_dice_8: 0.171  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:22:57] d2.utils.events INFO:  eta: 1:08:19  iter: 17759  total_loss: 5.013  loss_ce: 0.1275  loss_mask: 0.2258  loss_dice: 0.1358  loss_ce_0: 0.06333  loss_mask_0: 0.2315  loss_dice_0: 0.135  loss_ce_1: 0.1269  loss_mask_1: 0.2178  loss_dice_1: 0.1372  loss_ce_2: 0.1276  loss_mask_2: 0.2272  loss_dice_2: 0.1349  loss_ce_3: 0.1277  loss_mask_3: 0.2176  loss_dice_3: 0.1284  loss_ce_4: 0.1274  loss_mask_4: 0.2184  loss_dice_4: 0.1362  loss_ce_5: 0.1276  loss_mask_5: 0.2224  loss_dice_5: 0.1386  loss_ce_6: 0.1277  loss_mask_6: 0.2248  loss_dice_6: 0.1304  loss_ce_7: 0.1271  loss_mask_7: 0.2229  loss_dice_7: 0.1318  loss_ce_8: 0.1274  loss_mask_8: 0.2147  loss_dice_8: 0.136  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:00] d2.utils.events INFO:  eta: 1:08:16  iter: 17779  total_loss: 4.641  loss_ce: 0.1232  loss_mask: 0.1954  loss_dice: 0.116  loss_ce_0: 0.06189  loss_mask_0: 0.1883  loss_dice_0: 0.1168  loss_ce_1: 0.1237  loss_mask_1: 0.1939  loss_dice_1: 0.117  loss_ce_2: 0.1235  loss_mask_2: 0.1901  loss_dice_2: 0.1142  loss_ce_3: 0.1236  loss_mask_3: 0.1828  loss_dice_3: 0.1198  loss_ce_4: 0.124  loss_mask_4: 0.1872  loss_dice_4: 0.1154  loss_ce_5: 0.1238  loss_mask_5: 0.1883  loss_dice_5: 0.1145  loss_ce_6: 0.1234  loss_mask_6: 0.1882  loss_dice_6: 0.1154  loss_ce_7: 0.1236  loss_mask_7: 0.1923  loss_dice_7: 0.112  loss_ce_8: 0.1231  loss_mask_8: 0.1879  loss_dice_8: 0.1169  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:02] d2.utils.events INFO:  eta: 1:08:09  iter: 17799  total_loss: 4.813  loss_ce: 0.1206  loss_mask: 0.1947  loss_dice: 0.1276  loss_ce_0: 0.06085  loss_mask_0: 0.1981  loss_dice_0: 0.1259  loss_ce_1: 0.1203  loss_mask_1: 0.1968  loss_dice_1: 0.1298  loss_ce_2: 0.1203  loss_mask_2: 0.1989  loss_dice_2: 0.1221  loss_ce_3: 0.1202  loss_mask_3: 0.2044  loss_dice_3: 0.1258  loss_ce_4: 0.1203  loss_mask_4: 0.1928  loss_dice_4: 0.128  loss_ce_5: 0.1204  loss_mask_5: 0.1887  loss_dice_5: 0.1299  loss_ce_6: 0.1203  loss_mask_6: 0.199  loss_dice_6: 0.1219  loss_ce_7: 0.1206  loss_mask_7: 0.1967  loss_dice_7: 0.1296  loss_ce_8: 0.1204  loss_mask_8: 0.2013  loss_dice_8: 0.1229  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:05] d2.utils.events INFO:  eta: 1:08:04  iter: 17819  total_loss: 5.439  loss_ce: 0.1492  loss_mask: 0.1834  loss_dice: 0.1662  loss_ce_0: 0.07065  loss_mask_0: 0.1794  loss_dice_0: 0.1585  loss_ce_1: 0.1494  loss_mask_1: 0.1864  loss_dice_1: 0.1583  loss_ce_2: 0.1493  loss_mask_2: 0.1794  loss_dice_2: 0.1575  loss_ce_3: 0.1495  loss_mask_3: 0.1874  loss_dice_3: 0.1594  loss_ce_4: 0.1499  loss_mask_4: 0.1883  loss_dice_4: 0.1617  loss_ce_5: 0.1498  loss_mask_5: 0.1805  loss_dice_5: 0.1573  loss_ce_6: 0.1505  loss_mask_6: 0.1832  loss_dice_6: 0.161  loss_ce_7: 0.1501  loss_mask_7: 0.1851  loss_dice_7: 0.1621  loss_ce_8: 0.1497  loss_mask_8: 0.1939  loss_dice_8: 0.1551  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:07] d2.utils.events INFO:  eta: 1:07:58  iter: 17839  total_loss: 5.503  loss_ce: 0.1444  loss_mask: 0.172  loss_dice: 0.2424  loss_ce_0: 0.06944  loss_mask_0: 0.1723  loss_dice_0: 0.2383  loss_ce_1: 0.1437  loss_mask_1: 0.173  loss_dice_1: 0.2223  loss_ce_2: 0.1439  loss_mask_2: 0.1712  loss_dice_2: 0.2261  loss_ce_3: 0.1439  loss_mask_3: 0.1748  loss_dice_3: 0.226  loss_ce_4: 0.1437  loss_mask_4: 0.1772  loss_dice_4: 0.238  loss_ce_5: 0.144  loss_mask_5: 0.1699  loss_dice_5: 0.2248  loss_ce_6: 0.1445  loss_mask_6: 0.1764  loss_dice_6: 0.2321  loss_ce_7: 0.1442  loss_mask_7: 0.172  loss_dice_7: 0.2403  loss_ce_8: 0.1444  loss_mask_8: 0.1793  loss_dice_8: 0.2247  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:10] d2.utils.events INFO:  eta: 1:07:53  iter: 17859  total_loss: 5.13  loss_ce: 0.1277  loss_mask: 0.1759  loss_dice: 0.1766  loss_ce_0: 0.06342  loss_mask_0: 0.1774  loss_dice_0: 0.1811  loss_ce_1: 0.1267  loss_mask_1: 0.182  loss_dice_1: 0.1765  loss_ce_2: 0.128  loss_mask_2: 0.168  loss_dice_2: 0.1789  loss_ce_3: 0.1277  loss_mask_3: 0.181  loss_dice_3: 0.1738  loss_ce_4: 0.1273  loss_mask_4: 0.1684  loss_dice_4: 0.1754  loss_ce_5: 0.1278  loss_mask_5: 0.1687  loss_dice_5: 0.172  loss_ce_6: 0.1276  loss_mask_6: 0.1762  loss_dice_6: 0.1665  loss_ce_7: 0.1271  loss_mask_7: 0.1714  loss_dice_7: 0.1836  loss_ce_8: 0.1279  loss_mask_8: 0.1744  loss_dice_8: 0.1766  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:12] d2.utils.events INFO:  eta: 1:07:53  iter: 17879  total_loss: 5.215  loss_ce: 0.1388  loss_mask: 0.2137  loss_dice: 0.1509  loss_ce_0: 0.06769  loss_mask_0: 0.2236  loss_dice_0: 0.1541  loss_ce_1: 0.1387  loss_mask_1: 0.2092  loss_dice_1: 0.1557  loss_ce_2: 0.1387  loss_mask_2: 0.2124  loss_dice_2: 0.1625  loss_ce_3: 0.1387  loss_mask_3: 0.2171  loss_dice_3: 0.1536  loss_ce_4: 0.1385  loss_mask_4: 0.2077  loss_dice_4: 0.1553  loss_ce_5: 0.1386  loss_mask_5: 0.2033  loss_dice_5: 0.1485  loss_ce_6: 0.1387  loss_mask_6: 0.2034  loss_dice_6: 0.147  loss_ce_7: 0.1386  loss_mask_7: 0.2063  loss_dice_7: 0.1545  loss_ce_8: 0.1389  loss_mask_8: 0.2133  loss_dice_8: 0.1581  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:15] d2.utils.events INFO:  eta: 1:07:51  iter: 17899  total_loss: 5.291  loss_ce: 0.1276  loss_mask: 0.1666  loss_dice: 0.1658  loss_ce_0: 0.06331  loss_mask_0: 0.1947  loss_dice_0: 0.1852  loss_ce_1: 0.128  loss_mask_1: 0.1817  loss_dice_1: 0.1836  loss_ce_2: 0.1277  loss_mask_2: 0.1911  loss_dice_2: 0.1828  loss_ce_3: 0.1276  loss_mask_3: 0.1956  loss_dice_3: 0.1931  loss_ce_4: 0.1277  loss_mask_4: 0.1855  loss_dice_4: 0.1849  loss_ce_5: 0.1276  loss_mask_5: 0.1798  loss_dice_5: 0.177  loss_ce_6: 0.1276  loss_mask_6: 0.1907  loss_dice_6: 0.1828  loss_ce_7: 0.1277  loss_mask_7: 0.1706  loss_dice_7: 0.1842  loss_ce_8: 0.1276  loss_mask_8: 0.1751  loss_dice_8: 0.179  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:23:17] d2.utils.events INFO:  eta: 1:07:48  iter: 17919  total_loss: 4.998  loss_ce: 0.125  loss_mask: 0.1765  loss_dice: 0.1395  loss_ce_0: 0.06191  loss_mask_0: 0.1787  loss_dice_0: 0.1439  loss_ce_1: 0.1259  loss_mask_1: 0.1798  loss_dice_1: 0.134  loss_ce_2: 0.1255  loss_mask_2: 0.1819  loss_dice_2: 0.14  loss_ce_3: 0.1257  loss_mask_3: 0.1835  loss_dice_3: 0.141  loss_ce_4: 0.1261  loss_mask_4: 0.1693  loss_dice_4: 0.1461  loss_ce_5: 0.1256  loss_mask_5: 0.1777  loss_dice_5: 0.1368  loss_ce_6: 0.1255  loss_mask_6: 0.1781  loss_dice_6: 0.1363  loss_ce_7: 0.1256  loss_mask_7: 0.1737  loss_dice_7: 0.1396  loss_ce_8: 0.1251  loss_mask_8: 0.1781  loss_dice_8: 0.1386  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:20] d2.utils.events INFO:  eta: 1:07:46  iter: 17939  total_loss: 5.206  loss_ce: 0.1294  loss_mask: 0.2374  loss_dice: 0.1484  loss_ce_0: 0.06431  loss_mask_0: 0.2456  loss_dice_0: 0.1531  loss_ce_1: 0.1285  loss_mask_1: 0.2373  loss_dice_1: 0.1548  loss_ce_2: 0.1291  loss_mask_2: 0.2339  loss_dice_2: 0.1575  loss_ce_3: 0.1291  loss_mask_3: 0.2435  loss_dice_3: 0.1548  loss_ce_4: 0.1288  loss_mask_4: 0.2376  loss_dice_4: 0.147  loss_ce_5: 0.129  loss_mask_5: 0.2382  loss_dice_5: 0.1527  loss_ce_6: 0.1291  loss_mask_6: 0.2347  loss_dice_6: 0.1544  loss_ce_7: 0.1289  loss_mask_7: 0.2395  loss_dice_7: 0.152  loss_ce_8: 0.1294  loss_mask_8: 0.2406  loss_dice_8: 0.1462  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:22] d2.utils.events INFO:  eta: 1:07:44  iter: 17959  total_loss: 5.031  loss_ce: 0.1237  loss_mask: 0.2048  loss_dice: 0.1626  loss_ce_0: 0.06229  loss_mask_0: 0.2014  loss_dice_0: 0.1605  loss_ce_1: 0.1222  loss_mask_1: 0.1976  loss_dice_1: 0.1654  loss_ce_2: 0.1227  loss_mask_2: 0.2102  loss_dice_2: 0.16  loss_ce_3: 0.1225  loss_mask_3: 0.1942  loss_dice_3: 0.1607  loss_ce_4: 0.1219  loss_mask_4: 0.2029  loss_dice_4: 0.1587  loss_ce_5: 0.1226  loss_mask_5: 0.2044  loss_dice_5: 0.1624  loss_ce_6: 0.1226  loss_mask_6: 0.2056  loss_dice_6: 0.1659  loss_ce_7: 0.1225  loss_mask_7: 0.196  loss_dice_7: 0.1651  loss_ce_8: 0.1235  loss_mask_8: 0.2  loss_dice_8: 0.1711  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:23:25] d2.utils.events INFO:  eta: 1:07:43  iter: 17979  total_loss: 4.949  loss_ce: 0.1437  loss_mask: 0.1964  loss_dice: 0.1344  loss_ce_0: 0.06831  loss_mask_0: 0.1951  loss_dice_0: 0.1335  loss_ce_1: 0.1453  loss_mask_1: 0.1937  loss_dice_1: 0.1332  loss_ce_2: 0.1448  loss_mask_2: 0.2006  loss_dice_2: 0.1347  loss_ce_3: 0.1449  loss_mask_3: 0.1936  loss_dice_3: 0.138  loss_ce_4: 0.1459  loss_mask_4: 0.1862  loss_dice_4: 0.1322  loss_ce_5: 0.1454  loss_mask_5: 0.2076  loss_dice_5: 0.136  loss_ce_6: 0.1459  loss_mask_6: 0.1984  loss_dice_6: 0.1324  loss_ce_7: 0.1459  loss_mask_7: 0.1952  loss_dice_7: 0.1324  loss_ce_8: 0.1442  loss_mask_8: 0.1927  loss_dice_8: 0.1383  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:23:28] d2.utils.events INFO:  eta: 1:07:43  iter: 17999  total_loss: 5.014  loss_ce: 0.1389  loss_mask: 0.1902  loss_dice: 0.159  loss_ce_0: 0.06708  loss_mask_0: 0.1978  loss_dice_0: 0.1553  loss_ce_1: 0.1386  loss_mask_1: 0.1924  loss_dice_1: 0.152  loss_ce_2: 0.1378  loss_mask_2: 0.194  loss_dice_2: 0.1598  loss_ce_3: 0.138  loss_mask_3: 0.1915  loss_dice_3: 0.1574  loss_ce_4: 0.1379  loss_mask_4: 0.2025  loss_dice_4: 0.1523  loss_ce_5: 0.1375  loss_mask_5: 0.195  loss_dice_5: 0.1592  loss_ce_6: 0.1378  loss_mask_6: 0.191  loss_dice_6: 0.1553  loss_ce_7: 0.138  loss_mask_7: 0.1909  loss_dice_7: 0.1515  loss_ce_8: 0.1385  loss_mask_8: 0.192  loss_dice_8: 0.1569  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:30] d2.utils.events INFO:  eta: 1:07:40  iter: 18019  total_loss: 4.998  loss_ce: 0.1316  loss_mask: 0.2044  loss_dice: 0.1469  loss_ce_0: 0.06482  loss_mask_0: 0.2283  loss_dice_0: 0.1483  loss_ce_1: 0.131  loss_mask_1: 0.2164  loss_dice_1: 0.1469  loss_ce_2: 0.1309  loss_mask_2: 0.2221  loss_dice_2: 0.1505  loss_ce_3: 0.131  loss_mask_3: 0.2153  loss_dice_3: 0.1492  loss_ce_4: 0.1312  loss_mask_4: 0.224  loss_dice_4: 0.1443  loss_ce_5: 0.1309  loss_mask_5: 0.218  loss_dice_5: 0.1436  loss_ce_6: 0.1309  loss_mask_6: 0.2171  loss_dice_6: 0.1451  loss_ce_7: 0.131  loss_mask_7: 0.2089  loss_dice_7: 0.1486  loss_ce_8: 0.1311  loss_mask_8: 0.2207  loss_dice_8: 0.142  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:33] d2.utils.events INFO:  eta: 1:07:38  iter: 18039  total_loss: 4.954  loss_ce: 0.1281  loss_mask: 0.1796  loss_dice: 0.1619  loss_ce_0: 0.06376  loss_mask_0: 0.1701  loss_dice_0: 0.1575  loss_ce_1: 0.1283  loss_mask_1: 0.1776  loss_dice_1: 0.1637  loss_ce_2: 0.1283  loss_mask_2: 0.1745  loss_dice_2: 0.1658  loss_ce_3: 0.1285  loss_mask_3: 0.1744  loss_dice_3: 0.161  loss_ce_4: 0.1286  loss_mask_4: 0.1793  loss_dice_4: 0.1639  loss_ce_5: 0.1283  loss_mask_5: 0.1664  loss_dice_5: 0.1615  loss_ce_6: 0.128  loss_mask_6: 0.1806  loss_dice_6: 0.1629  loss_ce_7: 0.1279  loss_mask_7: 0.1741  loss_dice_7: 0.1635  loss_ce_8: 0.128  loss_mask_8: 0.1751  loss_dice_8: 0.1672  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:23:35] d2.utils.events INFO:  eta: 1:07:35  iter: 18059  total_loss: 5.087  loss_ce: 0.1287  loss_mask: 0.2446  loss_dice: 0.1609  loss_ce_0: 0.06341  loss_mask_0: 0.235  loss_dice_0: 0.1613  loss_ce_1: 0.129  loss_mask_1: 0.2322  loss_dice_1: 0.1641  loss_ce_2: 0.129  loss_mask_2: 0.2346  loss_dice_2: 0.1642  loss_ce_3: 0.1288  loss_mask_3: 0.2436  loss_dice_3: 0.1654  loss_ce_4: 0.1289  loss_mask_4: 0.2372  loss_dice_4: 0.1616  loss_ce_5: 0.1289  loss_mask_5: 0.2347  loss_dice_5: 0.1628  loss_ce_6: 0.1294  loss_mask_6: 0.241  loss_dice_6: 0.1633  loss_ce_7: 0.1294  loss_mask_7: 0.2383  loss_dice_7: 0.1694  loss_ce_8: 0.1289  loss_mask_8: 0.2303  loss_dice_8: 0.1666  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:23:38] d2.utils.events INFO:  eta: 1:07:33  iter: 18079  total_loss: 5.011  loss_ce: 0.1324  loss_mask: 0.2262  loss_dice: 0.1279  loss_ce_0: 0.06479  loss_mask_0: 0.2255  loss_dice_0: 0.1313  loss_ce_1: 0.1322  loss_mask_1: 0.2254  loss_dice_1: 0.1304  loss_ce_2: 0.1326  loss_mask_2: 0.2205  loss_dice_2: 0.131  loss_ce_3: 0.1328  loss_mask_3: 0.2215  loss_dice_3: 0.1303  loss_ce_4: 0.1328  loss_mask_4: 0.2255  loss_dice_4: 0.13  loss_ce_5: 0.1326  loss_mask_5: 0.2261  loss_dice_5: 0.1351  loss_ce_6: 0.1328  loss_mask_6: 0.2265  loss_dice_6: 0.1353  loss_ce_7: 0.1324  loss_mask_7: 0.2147  loss_dice_7: 0.1318  loss_ce_8: 0.1324  loss_mask_8: 0.2239  loss_dice_8: 0.1342  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:23:40] d2.utils.events INFO:  eta: 1:07:30  iter: 18099  total_loss: 4.603  loss_ce: 0.1308  loss_mask: 0.2089  loss_dice: 0.1167  loss_ce_0: 0.06423  loss_mask_0: 0.2134  loss_dice_0: 0.1196  loss_ce_1: 0.1303  loss_mask_1: 0.2141  loss_dice_1: 0.1174  loss_ce_2: 0.1305  loss_mask_2: 0.2134  loss_dice_2: 0.1172  loss_ce_3: 0.1306  loss_mask_3: 0.2078  loss_dice_3: 0.1163  loss_ce_4: 0.1305  loss_mask_4: 0.2125  loss_dice_4: 0.1168  loss_ce_5: 0.1304  loss_mask_5: 0.217  loss_dice_5: 0.116  loss_ce_6: 0.1307  loss_mask_6: 0.2139  loss_dice_6: 0.1202  loss_ce_7: 0.1305  loss_mask_7: 0.2133  loss_dice_7: 0.1212  loss_ce_8: 0.1307  loss_mask_8: 0.2045  loss_dice_8: 0.1151  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:23:43] d2.utils.events INFO:  eta: 1:07:25  iter: 18119  total_loss: 5.555  loss_ce: 0.1267  loss_mask: 0.2355  loss_dice: 0.1972  loss_ce_0: 0.06295  loss_mask_0: 0.2218  loss_dice_0: 0.2006  loss_ce_1: 0.1256  loss_mask_1: 0.2338  loss_dice_1: 0.1921  loss_ce_2: 0.1262  loss_mask_2: 0.2336  loss_dice_2: 0.1878  loss_ce_3: 0.1261  loss_mask_3: 0.2237  loss_dice_3: 0.1902  loss_ce_4: 0.1258  loss_mask_4: 0.2326  loss_dice_4: 0.2029  loss_ce_5: 0.126  loss_mask_5: 0.2321  loss_dice_5: 0.1909  loss_ce_6: 0.1262  loss_mask_6: 0.2303  loss_dice_6: 0.1898  loss_ce_7: 0.126  loss_mask_7: 0.2287  loss_dice_7: 0.1927  loss_ce_8: 0.1265  loss_mask_8: 0.2209  loss_dice_8: 0.1854  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:23:45] d2.utils.events INFO:  eta: 1:07:22  iter: 18139  total_loss: 5.294  loss_ce: 0.1268  loss_mask: 0.1897  loss_dice: 0.1862  loss_ce_0: 0.06306  loss_mask_0: 0.193  loss_dice_0: 0.1903  loss_ce_1: 0.1263  loss_mask_1: 0.1942  loss_dice_1: 0.1974  loss_ce_2: 0.1267  loss_mask_2: 0.1978  loss_dice_2: 0.1834  loss_ce_3: 0.1267  loss_mask_3: 0.1911  loss_dice_3: 0.1902  loss_ce_4: 0.1267  loss_mask_4: 0.189  loss_dice_4: 0.188  loss_ce_5: 0.1268  loss_mask_5: 0.201  loss_dice_5: 0.1909  loss_ce_6: 0.1268  loss_mask_6: 0.2007  loss_dice_6: 0.1955  loss_ce_7: 0.1266  loss_mask_7: 0.2072  loss_dice_7: 0.1802  loss_ce_8: 0.1267  loss_mask_8: 0.2023  loss_dice_8: 0.1911  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:23:48] d2.utils.events INFO:  eta: 1:07:22  iter: 18159  total_loss: 5.187  loss_ce: 0.1108  loss_mask: 0.2099  loss_dice: 0.1669  loss_ce_0: 0.05755  loss_mask_0: 0.2028  loss_dice_0: 0.1584  loss_ce_1: 0.1098  loss_mask_1: 0.2004  loss_dice_1: 0.1714  loss_ce_2: 0.1105  loss_mask_2: 0.2081  loss_dice_2: 0.1721  loss_ce_3: 0.1102  loss_mask_3: 0.205  loss_dice_3: 0.1681  loss_ce_4: 0.1096  loss_mask_4: 0.2088  loss_dice_4: 0.1696  loss_ce_5: 0.1099  loss_mask_5: 0.2068  loss_dice_5: 0.1614  loss_ce_6: 0.1095  loss_mask_6: 0.2052  loss_dice_6: 0.1683  loss_ce_7: 0.1094  loss_mask_7: 0.2008  loss_dice_7: 0.1709  loss_ce_8: 0.1103  loss_mask_8: 0.1988  loss_dice_8: 0.167  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:50] d2.utils.events INFO:  eta: 1:07:20  iter: 18179  total_loss: 4.858  loss_ce: 0.1419  loss_mask: 0.1855  loss_dice: 0.1452  loss_ce_0: 0.06837  loss_mask_0: 0.1981  loss_dice_0: 0.1492  loss_ce_1: 0.1411  loss_mask_1: 0.2008  loss_dice_1: 0.1538  loss_ce_2: 0.141  loss_mask_2: 0.2019  loss_dice_2: 0.1461  loss_ce_3: 0.1413  loss_mask_3: 0.2012  loss_dice_3: 0.1505  loss_ce_4: 0.141  loss_mask_4: 0.1882  loss_dice_4: 0.1507  loss_ce_5: 0.1408  loss_mask_5: 0.1908  loss_dice_5: 0.1461  loss_ce_6: 0.141  loss_mask_6: 0.1898  loss_dice_6: 0.1517  loss_ce_7: 0.1407  loss_mask_7: 0.1985  loss_dice_7: 0.151  loss_ce_8: 0.1415  loss_mask_8: 0.1871  loss_dice_8: 0.1502  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:53] d2.utils.events INFO:  eta: 1:07:18  iter: 18199  total_loss: 5.542  loss_ce: 0.1285  loss_mask: 0.1709  loss_dice: 0.185  loss_ce_0: 0.06379  loss_mask_0: 0.1606  loss_dice_0: 0.1832  loss_ce_1: 0.1286  loss_mask_1: 0.1733  loss_dice_1: 0.1878  loss_ce_2: 0.1286  loss_mask_2: 0.1716  loss_dice_2: 0.1811  loss_ce_3: 0.1287  loss_mask_3: 0.1642  loss_dice_3: 0.1854  loss_ce_4: 0.1288  loss_mask_4: 0.173  loss_dice_4: 0.1796  loss_ce_5: 0.1287  loss_mask_5: 0.1596  loss_dice_5: 0.1845  loss_ce_6: 0.1287  loss_mask_6: 0.1706  loss_dice_6: 0.1759  loss_ce_7: 0.1286  loss_mask_7: 0.1736  loss_dice_7: 0.1906  loss_ce_8: 0.1285  loss_mask_8: 0.164  loss_dice_8: 0.1857  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:56] d2.utils.events INFO:  eta: 1:07:16  iter: 18219  total_loss: 4.948  loss_ce: 0.1275  loss_mask: 0.1856  loss_dice: 0.1532  loss_ce_0: 0.06333  loss_mask_0: 0.175  loss_dice_0: 0.1396  loss_ce_1: 0.1271  loss_mask_1: 0.1839  loss_dice_1: 0.1511  loss_ce_2: 0.128  loss_mask_2: 0.1801  loss_dice_2: 0.1523  loss_ce_3: 0.1282  loss_mask_3: 0.1808  loss_dice_3: 0.1538  loss_ce_4: 0.1285  loss_mask_4: 0.1748  loss_dice_4: 0.1455  loss_ce_5: 0.1283  loss_mask_5: 0.1792  loss_dice_5: 0.1457  loss_ce_6: 0.1283  loss_mask_6: 0.1835  loss_dice_6: 0.1513  loss_ce_7: 0.1278  loss_mask_7: 0.1711  loss_dice_7: 0.1457  loss_ce_8: 0.1279  loss_mask_8: 0.1748  loss_dice_8: 0.149  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:23:58] d2.utils.events INFO:  eta: 1:07:13  iter: 18239  total_loss: 5.592  loss_ce: 0.1294  loss_mask: 0.1758  loss_dice: 0.2055  loss_ce_0: 0.0638  loss_mask_0: 0.1913  loss_dice_0: 0.2128  loss_ce_1: 0.1295  loss_mask_1: 0.1863  loss_dice_1: 0.2029  loss_ce_2: 0.1295  loss_mask_2: 0.1882  loss_dice_2: 0.2095  loss_ce_3: 0.1295  loss_mask_3: 0.1781  loss_dice_3: 0.217  loss_ce_4: 0.1298  loss_mask_4: 0.185  loss_dice_4: 0.2086  loss_ce_5: 0.1296  loss_mask_5: 0.1699  loss_dice_5: 0.2119  loss_ce_6: 0.13  loss_mask_6: 0.1906  loss_dice_6: 0.2112  loss_ce_7: 0.13  loss_mask_7: 0.1849  loss_dice_7: 0.2127  loss_ce_8: 0.1298  loss_mask_8: 0.1877  loss_dice_8: 0.2093  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:24:01] d2.utils.events INFO:  eta: 1:07:10  iter: 18259  total_loss: 5.008  loss_ce: 0.1272  loss_mask: 0.1758  loss_dice: 0.1942  loss_ce_0: 0.06324  loss_mask_0: 0.1795  loss_dice_0: 0.1928  loss_ce_1: 0.1272  loss_mask_1: 0.1763  loss_dice_1: 0.1924  loss_ce_2: 0.1272  loss_mask_2: 0.1746  loss_dice_2: 0.1869  loss_ce_3: 0.1272  loss_mask_3: 0.185  loss_dice_3: 0.1942  loss_ce_4: 0.1272  loss_mask_4: 0.1769  loss_dice_4: 0.1869  loss_ce_5: 0.1272  loss_mask_5: 0.1771  loss_dice_5: 0.193  loss_ce_6: 0.1272  loss_mask_6: 0.179  loss_dice_6: 0.1935  loss_ce_7: 0.1273  loss_mask_7: 0.1706  loss_dice_7: 0.187  loss_ce_8: 0.1273  loss_mask_8: 0.1799  loss_dice_8: 0.1968  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:24:03] d2.utils.events INFO:  eta: 1:07:08  iter: 18279  total_loss: 5.502  loss_ce: 0.1284  loss_mask: 0.1868  loss_dice: 0.2141  loss_ce_0: 0.06373  loss_mask_0: 0.1721  loss_dice_0: 0.2042  loss_ce_1: 0.1276  loss_mask_1: 0.1876  loss_dice_1: 0.22  loss_ce_2: 0.128  loss_mask_2: 0.1811  loss_dice_2: 0.2083  loss_ce_3: 0.1276  loss_mask_3: 0.1797  loss_dice_3: 0.2114  loss_ce_4: 0.1275  loss_mask_4: 0.1861  loss_dice_4: 0.2064  loss_ce_5: 0.1279  loss_mask_5: 0.1813  loss_dice_5: 0.2028  loss_ce_6: 0.128  loss_mask_6: 0.1827  loss_dice_6: 0.2068  loss_ce_7: 0.1281  loss_mask_7: 0.1775  loss_dice_7: 0.2095  loss_ce_8: 0.1284  loss_mask_8: 0.1754  loss_dice_8: 0.1951  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:24:06] d2.utils.events INFO:  eta: 1:07:05  iter: 18299  total_loss: 5.627  loss_ce: 0.1259  loss_mask: 0.1425  loss_dice: 0.1938  loss_ce_0: 0.06274  loss_mask_0: 0.1474  loss_dice_0: 0.2002  loss_ce_1: 0.1247  loss_mask_1: 0.1586  loss_dice_1: 0.1911  loss_ce_2: 0.1252  loss_mask_2: 0.1486  loss_dice_2: 0.1964  loss_ce_3: 0.1247  loss_mask_3: 0.1461  loss_dice_3: 0.201  loss_ce_4: 0.1244  loss_mask_4: 0.1606  loss_dice_4: 0.1979  loss_ce_5: 0.125  loss_mask_5: 0.1603  loss_dice_5: 0.1943  loss_ce_6: 0.1252  loss_mask_6: 0.1569  loss_dice_6: 0.1897  loss_ce_7: 0.1252  loss_mask_7: 0.1568  loss_dice_7: 0.1889  loss_ce_8: 0.1258  loss_mask_8: 0.1516  loss_dice_8: 0.1923  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:24:08] d2.utils.events INFO:  eta: 1:07:03  iter: 18319  total_loss: 4.856  loss_ce: 0.113  loss_mask: 0.191  loss_dice: 0.1386  loss_ce_0: 0.05832  loss_mask_0: 0.1874  loss_dice_0: 0.1442  loss_ce_1: 0.1124  loss_mask_1: 0.1868  loss_dice_1: 0.1402  loss_ce_2: 0.1127  loss_mask_2: 0.1918  loss_dice_2: 0.14  loss_ce_3: 0.1121  loss_mask_3: 0.1937  loss_dice_3: 0.1373  loss_ce_4: 0.1118  loss_mask_4: 0.1984  loss_dice_4: 0.1433  loss_ce_5: 0.1123  loss_mask_5: 0.1888  loss_dice_5: 0.141  loss_ce_6: 0.112  loss_mask_6: 0.1886  loss_dice_6: 0.1397  loss_ce_7: 0.1123  loss_mask_7: 0.1929  loss_dice_7: 0.1433  loss_ce_8: 0.1128  loss_mask_8: 0.1867  loss_dice_8: 0.1424  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:24:11] d2.utils.events INFO:  eta: 1:07:02  iter: 18339  total_loss: 5.691  loss_ce: 0.1572  loss_mask: 0.2248  loss_dice: 0.1941  loss_ce_0: 0.0731  loss_mask_0: 0.2374  loss_dice_0: 0.1932  loss_ce_1: 0.1582  loss_mask_1: 0.2377  loss_dice_1: 0.1902  loss_ce_2: 0.1584  loss_mask_2: 0.234  loss_dice_2: 0.1874  loss_ce_3: 0.1599  loss_mask_3: 0.2318  loss_dice_3: 0.1895  loss_ce_4: 0.1597  loss_mask_4: 0.2331  loss_dice_4: 0.1935  loss_ce_5: 0.1579  loss_mask_5: 0.2251  loss_dice_5: 0.1887  loss_ce_6: 0.1578  loss_mask_6: 0.237  loss_dice_6: 0.1911  loss_ce_7: 0.1576  loss_mask_7: 0.2325  loss_dice_7: 0.1926  loss_ce_8: 0.1573  loss_mask_8: 0.2348  loss_dice_8: 0.1905  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:14] d2.utils.events INFO:  eta: 1:07:00  iter: 18359  total_loss: 5.093  loss_ce: 0.1522  loss_mask: 0.204  loss_dice: 0.1873  loss_ce_0: 0.07207  loss_mask_0: 0.2012  loss_dice_0: 0.1942  loss_ce_1: 0.149  loss_mask_1: 0.2137  loss_dice_1: 0.1974  loss_ce_2: 0.1511  loss_mask_2: 0.2034  loss_dice_2: 0.1968  loss_ce_3: 0.1513  loss_mask_3: 0.2147  loss_dice_3: 0.2045  loss_ce_4: 0.1504  loss_mask_4: 0.2059  loss_dice_4: 0.1958  loss_ce_5: 0.1521  loss_mask_5: 0.2148  loss_dice_5: 0.1953  loss_ce_6: 0.1528  loss_mask_6: 0.2083  loss_dice_6: 0.1893  loss_ce_7: 0.1513  loss_mask_7: 0.2151  loss_dice_7: 0.1901  loss_ce_8: 0.1523  loss_mask_8: 0.2131  loss_dice_8: 0.2042  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:16] d2.utils.events INFO:  eta: 1:06:58  iter: 18379  total_loss: 5.049  loss_ce: 0.111  loss_mask: 0.2012  loss_dice: 0.1453  loss_ce_0: 0.057  loss_mask_0: 0.2059  loss_dice_0: 0.1506  loss_ce_1: 0.1136  loss_mask_1: 0.2024  loss_dice_1: 0.1539  loss_ce_2: 0.1111  loss_mask_2: 0.2066  loss_dice_2: 0.1457  loss_ce_3: 0.1114  loss_mask_3: 0.2098  loss_dice_3: 0.1522  loss_ce_4: 0.1126  loss_mask_4: 0.2075  loss_dice_4: 0.1548  loss_ce_5: 0.111  loss_mask_5: 0.2094  loss_dice_5: 0.1506  loss_ce_6: 0.1109  loss_mask_6: 0.2081  loss_dice_6: 0.1523  loss_ce_7: 0.1121  loss_mask_7: 0.2145  loss_dice_7: 0.1511  loss_ce_8: 0.1108  loss_mask_8: 0.207  loss_dice_8: 0.1546  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:19] d2.utils.events INFO:  eta: 1:06:55  iter: 18399  total_loss: 5.345  loss_ce: 0.1368  loss_mask: 0.2483  loss_dice: 0.1485  loss_ce_0: 0.06706  loss_mask_0: 0.2461  loss_dice_0: 0.1443  loss_ce_1: 0.1346  loss_mask_1: 0.2424  loss_dice_1: 0.1461  loss_ce_2: 0.1358  loss_mask_2: 0.233  loss_dice_2: 0.1457  loss_ce_3: 0.1352  loss_mask_3: 0.2462  loss_dice_3: 0.1519  loss_ce_4: 0.1346  loss_mask_4: 0.2413  loss_dice_4: 0.1458  loss_ce_5: 0.136  loss_mask_5: 0.252  loss_dice_5: 0.1472  loss_ce_6: 0.1364  loss_mask_6: 0.2417  loss_dice_6: 0.1446  loss_ce_7: 0.136  loss_mask_7: 0.2517  loss_dice_7: 0.1452  loss_ce_8: 0.1367  loss_mask_8: 0.2522  loss_dice_8: 0.1463  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:21] d2.utils.events INFO:  eta: 1:06:53  iter: 18419  total_loss: 5.23  loss_ce: 0.1285  loss_mask: 0.2425  loss_dice: 0.1636  loss_ce_0: 0.06379  loss_mask_0: 0.2435  loss_dice_0: 0.1646  loss_ce_1: 0.1287  loss_mask_1: 0.2442  loss_dice_1: 0.1577  loss_ce_2: 0.1286  loss_mask_2: 0.2538  loss_dice_2: 0.1539  loss_ce_3: 0.1288  loss_mask_3: 0.2455  loss_dice_3: 0.1614  loss_ce_4: 0.1288  loss_mask_4: 0.2482  loss_dice_4: 0.1565  loss_ce_5: 0.1287  loss_mask_5: 0.2403  loss_dice_5: 0.1544  loss_ce_6: 0.1288  loss_mask_6: 0.2435  loss_dice_6: 0.1614  loss_ce_7: 0.1287  loss_mask_7: 0.2562  loss_dice_7: 0.1611  loss_ce_8: 0.1285  loss_mask_8: 0.2538  loss_dice_8: 0.1526  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:24] d2.utils.events INFO:  eta: 1:06:52  iter: 18439  total_loss: 5.122  loss_ce: 0.118  loss_mask: 0.1713  loss_dice: 0.2329  loss_ce_0: 0.06032  loss_mask_0: 0.162  loss_dice_0: 0.2208  loss_ce_1: 0.1155  loss_mask_1: 0.1671  loss_dice_1: 0.2375  loss_ce_2: 0.1174  loss_mask_2: 0.1622  loss_dice_2: 0.2201  loss_ce_3: 0.1168  loss_mask_3: 0.1616  loss_dice_3: 0.2256  loss_ce_4: 0.1158  loss_mask_4: 0.1614  loss_dice_4: 0.2198  loss_ce_5: 0.1169  loss_mask_5: 0.1594  loss_dice_5: 0.2207  loss_ce_6: 0.1163  loss_mask_6: 0.1552  loss_dice_6: 0.2252  loss_ce_7: 0.116  loss_mask_7: 0.1613  loss_dice_7: 0.2305  loss_ce_8: 0.1178  loss_mask_8: 0.1727  loss_dice_8: 0.2424  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:26] d2.utils.events INFO:  eta: 1:06:51  iter: 18459  total_loss: 4.812  loss_ce: 0.1566  loss_mask: 0.221  loss_dice: 0.1367  loss_ce_0: 0.07248  loss_mask_0: 0.2287  loss_dice_0: 0.1334  loss_ce_1: 0.159  loss_mask_1: 0.2342  loss_dice_1: 0.1311  loss_ce_2: 0.1561  loss_mask_2: 0.2198  loss_dice_2: 0.129  loss_ce_3: 0.1571  loss_mask_3: 0.2263  loss_dice_3: 0.121  loss_ce_4: 0.1584  loss_mask_4: 0.2183  loss_dice_4: 0.1313  loss_ce_5: 0.157  loss_mask_5: 0.2306  loss_dice_5: 0.1231  loss_ce_6: 0.1587  loss_mask_6: 0.2286  loss_dice_6: 0.1247  loss_ce_7: 0.1591  loss_mask_7: 0.2305  loss_dice_7: 0.1277  loss_ce_8: 0.1571  loss_mask_8: 0.2248  loss_dice_8: 0.1268  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:29] d2.utils.events INFO:  eta: 1:06:50  iter: 18479  total_loss: 4.728  loss_ce: 0.128  loss_mask: 0.1735  loss_dice: 0.1462  loss_ce_0: 0.0636  loss_mask_0: 0.1807  loss_dice_0: 0.1482  loss_ce_1: 0.1276  loss_mask_1: 0.1801  loss_dice_1: 0.1441  loss_ce_2: 0.1277  loss_mask_2: 0.1848  loss_dice_2: 0.1455  loss_ce_3: 0.1279  loss_mask_3: 0.1821  loss_dice_3: 0.1417  loss_ce_4: 0.1277  loss_mask_4: 0.1765  loss_dice_4: 0.1443  loss_ce_5: 0.1278  loss_mask_5: 0.1861  loss_dice_5: 0.1436  loss_ce_6: 0.1279  loss_mask_6: 0.1779  loss_dice_6: 0.1512  loss_ce_7: 0.1275  loss_mask_7: 0.179  loss_dice_7: 0.1467  loss_ce_8: 0.1279  loss_mask_8: 0.1904  loss_dice_8: 0.1477  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:31] d2.utils.events INFO:  eta: 1:06:47  iter: 18499  total_loss: 5.155  loss_ce: 0.1109  loss_mask: 0.1813  loss_dice: 0.179  loss_ce_0: 0.05724  loss_mask_0: 0.1829  loss_dice_0: 0.173  loss_ce_1: 0.111  loss_mask_1: 0.1852  loss_dice_1: 0.177  loss_ce_2: 0.112  loss_mask_2: 0.1856  loss_dice_2: 0.1654  loss_ce_3: 0.1114  loss_mask_3: 0.185  loss_dice_3: 0.1794  loss_ce_4: 0.1116  loss_mask_4: 0.187  loss_dice_4: 0.1827  loss_ce_5: 0.112  loss_mask_5: 0.1843  loss_dice_5: 0.176  loss_ce_6: 0.1118  loss_mask_6: 0.1842  loss_dice_6: 0.1763  loss_ce_7: 0.1122  loss_mask_7: 0.1872  loss_dice_7: 0.1777  loss_ce_8: 0.1115  loss_mask_8: 0.1902  loss_dice_8: 0.1753  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:34] d2.utils.events INFO:  eta: 1:06:45  iter: 18519  total_loss: 5.63  loss_ce: 0.1291  loss_mask: 0.2102  loss_dice: 0.1784  loss_ce_0: 0.06389  loss_mask_0: 0.2085  loss_dice_0: 0.1916  loss_ce_1: 0.1293  loss_mask_1: 0.2057  loss_dice_1: 0.1897  loss_ce_2: 0.1292  loss_mask_2: 0.226  loss_dice_2: 0.1877  loss_ce_3: 0.1293  loss_mask_3: 0.2103  loss_dice_3: 0.1827  loss_ce_4: 0.1294  loss_mask_4: 0.2137  loss_dice_4: 0.1907  loss_ce_5: 0.1293  loss_mask_5: 0.2117  loss_dice_5: 0.1907  loss_ce_6: 0.1293  loss_mask_6: 0.2146  loss_dice_6: 0.1875  loss_ce_7: 0.1292  loss_mask_7: 0.2108  loss_dice_7: 0.1926  loss_ce_8: 0.1291  loss_mask_8: 0.2195  loss_dice_8: 0.1936  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:36] d2.utils.events INFO:  eta: 1:06:41  iter: 18539  total_loss: 5.334  loss_ce: 0.1068  loss_mask: 0.1608  loss_dice: 0.2012  loss_ce_0: 0.05557  loss_mask_0: 0.1581  loss_dice_0: 0.2108  loss_ce_1: 0.1077  loss_mask_1: 0.1679  loss_dice_1: 0.2103  loss_ce_2: 0.1075  loss_mask_2: 0.1511  loss_dice_2: 0.2153  loss_ce_3: 0.1073  loss_mask_3: 0.1599  loss_dice_3: 0.2122  loss_ce_4: 0.1076  loss_mask_4: 0.1502  loss_dice_4: 0.2138  loss_ce_5: 0.1074  loss_mask_5: 0.1547  loss_dice_5: 0.2123  loss_ce_6: 0.107  loss_mask_6: 0.1576  loss_dice_6: 0.214  loss_ce_7: 0.1073  loss_mask_7: 0.1618  loss_dice_7: 0.2017  loss_ce_8: 0.1068  loss_mask_8: 0.1595  loss_dice_8: 0.2062  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:39] d2.utils.events INFO:  eta: 1:06:39  iter: 18559  total_loss: 4.966  loss_ce: 0.142  loss_mask: 0.2033  loss_dice: 0.1697  loss_ce_0: 0.06904  loss_mask_0: 0.1986  loss_dice_0: 0.1681  loss_ce_1: 0.1396  loss_mask_1: 0.1907  loss_dice_1: 0.1682  loss_ce_2: 0.1408  loss_mask_2: 0.1913  loss_dice_2: 0.1737  loss_ce_3: 0.1408  loss_mask_3: 0.1865  loss_dice_3: 0.1693  loss_ce_4: 0.14  loss_mask_4: 0.1902  loss_dice_4: 0.1728  loss_ce_5: 0.1406  loss_mask_5: 0.1833  loss_dice_5: 0.182  loss_ce_6: 0.1405  loss_mask_6: 0.1882  loss_dice_6: 0.1747  loss_ce_7: 0.14  loss_mask_7: 0.1911  loss_dice_7: 0.1664  loss_ce_8: 0.1415  loss_mask_8: 0.1886  loss_dice_8: 0.1829  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:41] d2.utils.events INFO:  eta: 1:06:35  iter: 18579  total_loss: 4.826  loss_ce: 0.1175  loss_mask: 0.2092  loss_dice: 0.1367  loss_ce_0: 0.05927  loss_mask_0: 0.2026  loss_dice_0: 0.1335  loss_ce_1: 0.119  loss_mask_1: 0.2194  loss_dice_1: 0.1351  loss_ce_2: 0.118  loss_mask_2: 0.2036  loss_dice_2: 0.1286  loss_ce_3: 0.1182  loss_mask_3: 0.2081  loss_dice_3: 0.1398  loss_ce_4: 0.1185  loss_mask_4: 0.2059  loss_dice_4: 0.1353  loss_ce_5: 0.118  loss_mask_5: 0.2124  loss_dice_5: 0.1378  loss_ce_6: 0.1184  loss_mask_6: 0.2088  loss_dice_6: 0.1367  loss_ce_7: 0.1187  loss_mask_7: 0.2167  loss_dice_7: 0.133  loss_ce_8: 0.1179  loss_mask_8: 0.2053  loss_dice_8: 0.1377  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:44] d2.utils.events INFO:  eta: 1:06:32  iter: 18599  total_loss: 4.675  loss_ce: 0.1316  loss_mask: 0.1643  loss_dice: 0.1616  loss_ce_0: 0.06518  loss_mask_0: 0.1666  loss_dice_0: 0.1614  loss_ce_1: 0.1291  loss_mask_1: 0.1627  loss_dice_1: 0.166  loss_ce_2: 0.1305  loss_mask_2: 0.1652  loss_dice_2: 0.1688  loss_ce_3: 0.1301  loss_mask_3: 0.1619  loss_dice_3: 0.1599  loss_ce_4: 0.1295  loss_mask_4: 0.166  loss_dice_4: 0.1603  loss_ce_5: 0.1303  loss_mask_5: 0.1657  loss_dice_5: 0.1676  loss_ce_6: 0.1303  loss_mask_6: 0.1683  loss_dice_6: 0.1657  loss_ce_7: 0.1299  loss_mask_7: 0.1699  loss_dice_7: 0.1613  loss_ce_8: 0.1312  loss_mask_8: 0.1657  loss_dice_8: 0.1615  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:46] d2.utils.events INFO:  eta: 1:06:29  iter: 18619  total_loss: 5.076  loss_ce: 0.1266  loss_mask: 0.2244  loss_dice: 0.1546  loss_ce_0: 0.0632  loss_mask_0: 0.2225  loss_dice_0: 0.1488  loss_ce_1: 0.1264  loss_mask_1: 0.2238  loss_dice_1: 0.1522  loss_ce_2: 0.1262  loss_mask_2: 0.2213  loss_dice_2: 0.1516  loss_ce_3: 0.1254  loss_mask_3: 0.2232  loss_dice_3: 0.1499  loss_ce_4: 0.1256  loss_mask_4: 0.2192  loss_dice_4: 0.1505  loss_ce_5: 0.1257  loss_mask_5: 0.226  loss_dice_5: 0.1468  loss_ce_6: 0.1255  loss_mask_6: 0.2351  loss_dice_6: 0.1541  loss_ce_7: 0.1261  loss_mask_7: 0.2344  loss_dice_7: 0.1521  loss_ce_8: 0.1266  loss_mask_8: 0.2243  loss_dice_8: 0.1541  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:49] d2.utils.events INFO:  eta: 1:06:27  iter: 18639  total_loss: 5.059  loss_ce: 0.1317  loss_mask: 0.1939  loss_dice: 0.1807  loss_ce_0: 0.06429  loss_mask_0: 0.1963  loss_dice_0: 0.184  loss_ce_1: 0.132  loss_mask_1: 0.1992  loss_dice_1: 0.1783  loss_ce_2: 0.131  loss_mask_2: 0.192  loss_dice_2: 0.1849  loss_ce_3: 0.1315  loss_mask_3: 0.1865  loss_dice_3: 0.1805  loss_ce_4: 0.1315  loss_mask_4: 0.1892  loss_dice_4: 0.1787  loss_ce_5: 0.1314  loss_mask_5: 0.1905  loss_dice_5: 0.1707  loss_ce_6: 0.132  loss_mask_6: 0.1841  loss_dice_6: 0.1738  loss_ce_7: 0.1323  loss_mask_7: 0.187  loss_dice_7: 0.1813  loss_ce_8: 0.1319  loss_mask_8: 0.1914  loss_dice_8: 0.1812  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:52] d2.utils.events INFO:  eta: 1:06:25  iter: 18659  total_loss: 4.611  loss_ce: 0.1275  loss_mask: 0.2088  loss_dice: 0.1173  loss_ce_0: 0.06333  loss_mask_0: 0.2022  loss_dice_0: 0.1154  loss_ce_1: 0.1275  loss_mask_1: 0.2088  loss_dice_1: 0.1172  loss_ce_2: 0.1276  loss_mask_2: 0.208  loss_dice_2: 0.1164  loss_ce_3: 0.1276  loss_mask_3: 0.2032  loss_dice_3: 0.1133  loss_ce_4: 0.1276  loss_mask_4: 0.1971  loss_dice_4: 0.1181  loss_ce_5: 0.1275  loss_mask_5: 0.1934  loss_dice_5: 0.1185  loss_ce_6: 0.1274  loss_mask_6: 0.2053  loss_dice_6: 0.1164  loss_ce_7: 0.1275  loss_mask_7: 0.203  loss_dice_7: 0.1108  loss_ce_8: 0.1275  loss_mask_8: 0.2073  loss_dice_8: 0.1159  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:54] d2.utils.events INFO:  eta: 1:06:23  iter: 18679  total_loss: 5.021  loss_ce: 0.1333  loss_mask: 0.1977  loss_dice: 0.1453  loss_ce_0: 0.06512  loss_mask_0: 0.1994  loss_dice_0: 0.1393  loss_ce_1: 0.1321  loss_mask_1: 0.1962  loss_dice_1: 0.1352  loss_ce_2: 0.132  loss_mask_2: 0.1946  loss_dice_2: 0.1359  loss_ce_3: 0.1326  loss_mask_3: 0.2032  loss_dice_3: 0.1392  loss_ce_4: 0.1323  loss_mask_4: 0.1971  loss_dice_4: 0.1414  loss_ce_5: 0.1325  loss_mask_5: 0.1985  loss_dice_5: 0.1382  loss_ce_6: 0.133  loss_mask_6: 0.2072  loss_dice_6: 0.1423  loss_ce_7: 0.1325  loss_mask_7: 0.1953  loss_dice_7: 0.1402  loss_ce_8: 0.1331  loss_mask_8: 0.21  loss_dice_8: 0.1451  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:57] d2.utils.events INFO:  eta: 1:06:19  iter: 18699  total_loss: 5.314  loss_ce: 0.1277  loss_mask: 0.1852  loss_dice: 0.1572  loss_ce_0: 0.06338  loss_mask_0: 0.1954  loss_dice_0: 0.1593  loss_ce_1: 0.1297  loss_mask_1: 0.2023  loss_dice_1: 0.1673  loss_ce_2: 0.1288  loss_mask_2: 0.1877  loss_dice_2: 0.1655  loss_ce_3: 0.1289  loss_mask_3: 0.1928  loss_dice_3: 0.1675  loss_ce_4: 0.1293  loss_mask_4: 0.1866  loss_dice_4: 0.1588  loss_ce_5: 0.1285  loss_mask_5: 0.1906  loss_dice_5: 0.1637  loss_ce_6: 0.1279  loss_mask_6: 0.1962  loss_dice_6: 0.1577  loss_ce_7: 0.1284  loss_mask_7: 0.1869  loss_dice_7: 0.1649  loss_ce_8: 0.1278  loss_mask_8: 0.1959  loss_dice_8: 0.1628  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:24:59] d2.utils.events INFO:  eta: 1:06:17  iter: 18719  total_loss: 5.393  loss_ce: 0.1278  loss_mask: 0.2244  loss_dice: 0.1705  loss_ce_0: 0.06361  loss_mask_0: 0.2152  loss_dice_0: 0.1708  loss_ce_1: 0.1288  loss_mask_1: 0.2227  loss_dice_1: 0.1665  loss_ce_2: 0.1288  loss_mask_2: 0.2174  loss_dice_2: 0.1689  loss_ce_3: 0.1287  loss_mask_3: 0.2178  loss_dice_3: 0.173  loss_ce_4: 0.1288  loss_mask_4: 0.2098  loss_dice_4: 0.163  loss_ce_5: 0.1287  loss_mask_5: 0.225  loss_dice_5: 0.1755  loss_ce_6: 0.1284  loss_mask_6: 0.2166  loss_dice_6: 0.1661  loss_ce_7: 0.1286  loss_mask_7: 0.2234  loss_dice_7: 0.176  loss_ce_8: 0.1279  loss_mask_8: 0.2172  loss_dice_8: 0.1713  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:02] d2.utils.events INFO:  eta: 1:06:11  iter: 18739  total_loss: 4.792  loss_ce: 0.1265  loss_mask: 0.1811  loss_dice: 0.1356  loss_ce_0: 0.06313  loss_mask_0: 0.1884  loss_dice_0: 0.13  loss_ce_1: 0.1268  loss_mask_1: 0.1869  loss_dice_1: 0.1324  loss_ce_2: 0.1271  loss_mask_2: 0.1878  loss_dice_2: 0.1341  loss_ce_3: 0.1271  loss_mask_3: 0.1961  loss_dice_3: 0.139  loss_ce_4: 0.1271  loss_mask_4: 0.1884  loss_dice_4: 0.1338  loss_ce_5: 0.1271  loss_mask_5: 0.1864  loss_dice_5: 0.1317  loss_ce_6: 0.127  loss_mask_6: 0.1903  loss_dice_6: 0.1339  loss_ce_7: 0.1268  loss_mask_7: 0.1925  loss_dice_7: 0.1309  loss_ce_8: 0.1266  loss_mask_8: 0.1812  loss_dice_8: 0.1359  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:04] d2.utils.events INFO:  eta: 1:06:10  iter: 18759  total_loss: 5.254  loss_ce: 0.1213  loss_mask: 0.2351  loss_dice: 0.1574  loss_ce_0: 0.06087  loss_mask_0: 0.2356  loss_dice_0: 0.1587  loss_ce_1: 0.1204  loss_mask_1: 0.2416  loss_dice_1: 0.1549  loss_ce_2: 0.1209  loss_mask_2: 0.2412  loss_dice_2: 0.161  loss_ce_3: 0.1207  loss_mask_3: 0.2353  loss_dice_3: 0.1603  loss_ce_4: 0.1202  loss_mask_4: 0.2382  loss_dice_4: 0.1593  loss_ce_5: 0.1207  loss_mask_5: 0.241  loss_dice_5: 0.1619  loss_ce_6: 0.1208  loss_mask_6: 0.2346  loss_dice_6: 0.1512  loss_ce_7: 0.1209  loss_mask_7: 0.2312  loss_dice_7: 0.1638  loss_ce_8: 0.1213  loss_mask_8: 0.2354  loss_dice_8: 0.1578  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:07] d2.utils.events INFO:  eta: 1:06:06  iter: 18779  total_loss: 4.982  loss_ce: 0.1118  loss_mask: 0.2219  loss_dice: 0.1486  loss_ce_0: 0.0575  loss_mask_0: 0.2243  loss_dice_0: 0.1493  loss_ce_1: 0.1101  loss_mask_1: 0.2313  loss_dice_1: 0.1436  loss_ce_2: 0.1112  loss_mask_2: 0.2243  loss_dice_2: 0.1464  loss_ce_3: 0.1108  loss_mask_3: 0.2234  loss_dice_3: 0.1507  loss_ce_4: 0.1102  loss_mask_4: 0.2242  loss_dice_4: 0.1436  loss_ce_5: 0.1106  loss_mask_5: 0.2283  loss_dice_5: 0.1483  loss_ce_6: 0.1103  loss_mask_6: 0.2318  loss_dice_6: 0.1456  loss_ce_7: 0.1104  loss_mask_7: 0.2292  loss_dice_7: 0.148  loss_ce_8: 0.1114  loss_mask_8: 0.228  loss_dice_8: 0.1484  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:09] d2.utils.events INFO:  eta: 1:06:05  iter: 18799  total_loss: 5.424  loss_ce: 0.1019  loss_mask: 0.2107  loss_dice: 0.1704  loss_ce_0: 0.05423  loss_mask_0: 0.2116  loss_dice_0: 0.1643  loss_ce_1: 0.1009  loss_mask_1: 0.2172  loss_dice_1: 0.17  loss_ce_2: 0.1019  loss_mask_2: 0.2213  loss_dice_2: 0.1631  loss_ce_3: 0.1017  loss_mask_3: 0.2007  loss_dice_3: 0.1746  loss_ce_4: 0.1011  loss_mask_4: 0.2148  loss_dice_4: 0.1724  loss_ce_5: 0.1014  loss_mask_5: 0.1997  loss_dice_5: 0.178  loss_ce_6: 0.1007  loss_mask_6: 0.2154  loss_dice_6: 0.167  loss_ce_7: 0.1007  loss_mask_7: 0.2067  loss_dice_7: 0.1665  loss_ce_8: 0.1015  loss_mask_8: 0.2118  loss_dice_8: 0.1665  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:12] d2.utils.events INFO:  eta: 1:06:03  iter: 18819  total_loss: 4.942  loss_ce: 0.09905  loss_mask: 0.2079  loss_dice: 0.1661  loss_ce_0: 0.05288  loss_mask_0: 0.2076  loss_dice_0: 0.1631  loss_ce_1: 0.09868  loss_mask_1: 0.2077  loss_dice_1: 0.1697  loss_ce_2: 0.09935  loss_mask_2: 0.2105  loss_dice_2: 0.1677  loss_ce_3: 0.09898  loss_mask_3: 0.2017  loss_dice_3: 0.1718  loss_ce_4: 0.09898  loss_mask_4: 0.208  loss_dice_4: 0.1696  loss_ce_5: 0.09928  loss_mask_5: 0.2092  loss_dice_5: 0.1587  loss_ce_6: 0.09898  loss_mask_6: 0.2094  loss_dice_6: 0.1626  loss_ce_7: 0.09906  loss_mask_7: 0.2135  loss_dice_7: 0.1692  loss_ce_8: 0.09918  loss_mask_8: 0.2042  loss_dice_8: 0.1657  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:14] d2.utils.events INFO:  eta: 1:06:00  iter: 18839  total_loss: 5.238  loss_ce: 0.1544  loss_mask: 0.2082  loss_dice: 0.1856  loss_ce_0: 0.07328  loss_mask_0: 0.2175  loss_dice_0: 0.1779  loss_ce_1: 0.1532  loss_mask_1: 0.2161  loss_dice_1: 0.1889  loss_ce_2: 0.1537  loss_mask_2: 0.2068  loss_dice_2: 0.1886  loss_ce_3: 0.1543  loss_mask_3: 0.2083  loss_dice_3: 0.175  loss_ce_4: 0.1537  loss_mask_4: 0.2103  loss_dice_4: 0.1856  loss_ce_5: 0.1535  loss_mask_5: 0.2092  loss_dice_5: 0.179  loss_ce_6: 0.1537  loss_mask_6: 0.2134  loss_dice_6: 0.1806  loss_ce_7: 0.153  loss_mask_7: 0.2107  loss_dice_7: 0.1835  loss_ce_8: 0.1539  loss_mask_8: 0.2085  loss_dice_8: 0.1809  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:17] d2.utils.events INFO:  eta: 1:05:57  iter: 18859  total_loss: 5.18  loss_ce: 0.1398  loss_mask: 0.1606  loss_dice: 0.1786  loss_ce_0: 0.06841  loss_mask_0: 0.1752  loss_dice_0: 0.1825  loss_ce_1: 0.1375  loss_mask_1: 0.1704  loss_dice_1: 0.1766  loss_ce_2: 0.1389  loss_mask_2: 0.1652  loss_dice_2: 0.175  loss_ce_3: 0.1392  loss_mask_3: 0.1657  loss_dice_3: 0.1762  loss_ce_4: 0.1385  loss_mask_4: 0.1612  loss_dice_4: 0.174  loss_ce_5: 0.1384  loss_mask_5: 0.164  loss_dice_5: 0.1714  loss_ce_6: 0.138  loss_mask_6: 0.165  loss_dice_6: 0.1776  loss_ce_7: 0.1376  loss_mask_7: 0.1617  loss_dice_7: 0.172  loss_ce_8: 0.139  loss_mask_8: 0.1669  loss_dice_8: 0.175  time: 0.1261  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:25:19] d2.utils.events INFO:  eta: 1:05:53  iter: 18879  total_loss: 5.794  loss_ce: 0.1324  loss_mask: 0.1915  loss_dice: 0.2189  loss_ce_0: 0.06552  loss_mask_0: 0.193  loss_dice_0: 0.2216  loss_ce_1: 0.1298  loss_mask_1: 0.203  loss_dice_1: 0.2185  loss_ce_2: 0.1316  loss_mask_2: 0.201  loss_dice_2: 0.2273  loss_ce_3: 0.1313  loss_mask_3: 0.1966  loss_dice_3: 0.2256  loss_ce_4: 0.1304  loss_mask_4: 0.1976  loss_dice_4: 0.215  loss_ce_5: 0.131  loss_mask_5: 0.2014  loss_dice_5: 0.2178  loss_ce_6: 0.1307  loss_mask_6: 0.2029  loss_dice_6: 0.2139  loss_ce_7: 0.1305  loss_mask_7: 0.1926  loss_dice_7: 0.2199  loss_ce_8: 0.1316  loss_mask_8: 0.1934  loss_dice_8: 0.2086  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:22] d2.utils.events INFO:  eta: 1:05:51  iter: 18899  total_loss: 4.955  loss_ce: 0.1271  loss_mask: 0.2191  loss_dice: 0.1274  loss_ce_0: 0.06314  loss_mask_0: 0.2109  loss_dice_0: 0.1277  loss_ce_1: 0.1274  loss_mask_1: 0.2077  loss_dice_1: 0.1254  loss_ce_2: 0.1272  loss_mask_2: 0.2095  loss_dice_2: 0.1239  loss_ce_3: 0.1271  loss_mask_3: 0.2187  loss_dice_3: 0.1304  loss_ce_4: 0.1274  loss_mask_4: 0.2077  loss_dice_4: 0.1259  loss_ce_5: 0.1272  loss_mask_5: 0.2064  loss_dice_5: 0.1316  loss_ce_6: 0.1275  loss_mask_6: 0.206  loss_dice_6: 0.1282  loss_ce_7: 0.1272  loss_mask_7: 0.2123  loss_dice_7: 0.1304  loss_ce_8: 0.1272  loss_mask_8: 0.2051  loss_dice_8: 0.1286  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:24] d2.utils.events INFO:  eta: 1:05:47  iter: 18919  total_loss: 4.281  loss_ce: 0.1215  loss_mask: 0.1917  loss_dice: 0.1311  loss_ce_0: 0.06135  loss_mask_0: 0.1893  loss_dice_0: 0.1295  loss_ce_1: 0.1192  loss_mask_1: 0.1952  loss_dice_1: 0.1283  loss_ce_2: 0.121  loss_mask_2: 0.1866  loss_dice_2: 0.1326  loss_ce_3: 0.1202  loss_mask_3: 0.1922  loss_dice_3: 0.1313  loss_ce_4: 0.1194  loss_mask_4: 0.1938  loss_dice_4: 0.1285  loss_ce_5: 0.1206  loss_mask_5: 0.1995  loss_dice_5: 0.1296  loss_ce_6: 0.1201  loss_mask_6: 0.1932  loss_dice_6: 0.1296  loss_ce_7: 0.1202  loss_mask_7: 0.1948  loss_dice_7: 0.1259  loss_ce_8: 0.1209  loss_mask_8: 0.1895  loss_dice_8: 0.131  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:27] d2.utils.events INFO:  eta: 1:05:43  iter: 18939  total_loss: 4.722  loss_ce: 0.1058  loss_mask: 0.2035  loss_dice: 0.1493  loss_ce_0: 0.05564  loss_mask_0: 0.207  loss_dice_0: 0.1549  loss_ce_1: 0.101  loss_mask_1: 0.2018  loss_dice_1: 0.1586  loss_ce_2: 0.104  loss_mask_2: 0.2031  loss_dice_2: 0.1504  loss_ce_3: 0.1023  loss_mask_3: 0.2074  loss_dice_3: 0.1582  loss_ce_4: 0.1011  loss_mask_4: 0.2104  loss_dice_4: 0.1575  loss_ce_5: 0.1039  loss_mask_5: 0.2049  loss_dice_5: 0.1597  loss_ce_6: 0.1034  loss_mask_6: 0.2172  loss_dice_6: 0.1593  loss_ce_7: 0.1036  loss_mask_7: 0.2019  loss_dice_7: 0.1548  loss_ce_8: 0.105  loss_mask_8: 0.2156  loss_dice_8: 0.1604  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:29] d2.utils.events INFO:  eta: 1:05:37  iter: 18959  total_loss: 5.369  loss_ce: 0.1309  loss_mask: 0.1701  loss_dice: 0.235  loss_ce_0: 0.06399  loss_mask_0: 0.168  loss_dice_0: 0.2367  loss_ce_1: 0.1321  loss_mask_1: 0.1722  loss_dice_1: 0.2358  loss_ce_2: 0.1307  loss_mask_2: 0.1677  loss_dice_2: 0.2423  loss_ce_3: 0.1311  loss_mask_3: 0.1619  loss_dice_3: 0.2328  loss_ce_4: 0.1313  loss_mask_4: 0.1617  loss_dice_4: 0.2397  loss_ce_5: 0.1308  loss_mask_5: 0.1546  loss_dice_5: 0.2282  loss_ce_6: 0.1312  loss_mask_6: 0.1588  loss_dice_6: 0.2208  loss_ce_7: 0.1311  loss_mask_7: 0.1718  loss_dice_7: 0.2442  loss_ce_8: 0.131  loss_mask_8: 0.1684  loss_dice_8: 0.2433  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:32] d2.utils.events INFO:  eta: 1:05:32  iter: 18979  total_loss: 5.112  loss_ce: 0.1638  loss_mask: 0.2183  loss_dice: 0.1743  loss_ce_0: 0.07598  loss_mask_0: 0.22  loss_dice_0: 0.1854  loss_ce_1: 0.1627  loss_mask_1: 0.2277  loss_dice_1: 0.1775  loss_ce_2: 0.1617  loss_mask_2: 0.23  loss_dice_2: 0.1767  loss_ce_3: 0.1624  loss_mask_3: 0.2251  loss_dice_3: 0.18  loss_ce_4: 0.1624  loss_mask_4: 0.22  loss_dice_4: 0.1793  loss_ce_5: 0.1631  loss_mask_5: 0.2335  loss_dice_5: 0.1806  loss_ce_6: 0.1645  loss_mask_6: 0.2298  loss_dice_6: 0.1791  loss_ce_7: 0.164  loss_mask_7: 0.2297  loss_dice_7: 0.1756  loss_ce_8: 0.1636  loss_mask_8: 0.226  loss_dice_8: 0.1785  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:35] d2.utils.events INFO:  eta: 1:05:28  iter: 18999  total_loss: 5.534  loss_ce: 0.1465  loss_mask: 0.1986  loss_dice: 0.1464  loss_ce_0: 0.07078  loss_mask_0: 0.2054  loss_dice_0: 0.1453  loss_ce_1: 0.1414  loss_mask_1: 0.2088  loss_dice_1: 0.1559  loss_ce_2: 0.144  loss_mask_2: 0.2012  loss_dice_2: 0.1479  loss_ce_3: 0.143  loss_mask_3: 0.1935  loss_dice_3: 0.1491  loss_ce_4: 0.142  loss_mask_4: 0.1998  loss_dice_4: 0.1509  loss_ce_5: 0.145  loss_mask_5: 0.1904  loss_dice_5: 0.1515  loss_ce_6: 0.1456  loss_mask_6: 0.1967  loss_dice_6: 0.1531  loss_ce_7: 0.145  loss_mask_7: 0.2  loss_dice_7: 0.1502  loss_ce_8: 0.1451  loss_mask_8: 0.2104  loss_dice_8: 0.1522  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:37] d2.utils.events INFO:  eta: 1:05:26  iter: 19019  total_loss: 4.94  loss_ce: 0.1324  loss_mask: 0.2217  loss_dice: 0.1367  loss_ce_0: 0.06561  loss_mask_0: 0.2111  loss_dice_0: 0.1345  loss_ce_1: 0.1295  loss_mask_1: 0.2217  loss_dice_1: 0.1319  loss_ce_2: 0.1318  loss_mask_2: 0.2202  loss_dice_2: 0.1346  loss_ce_3: 0.1304  loss_mask_3: 0.2214  loss_dice_3: 0.1322  loss_ce_4: 0.129  loss_mask_4: 0.224  loss_dice_4: 0.134  loss_ce_5: 0.1313  loss_mask_5: 0.2164  loss_dice_5: 0.1315  loss_ce_6: 0.1303  loss_mask_6: 0.2227  loss_dice_6: 0.1314  loss_ce_7: 0.1301  loss_mask_7: 0.2106  loss_dice_7: 0.1337  loss_ce_8: 0.1308  loss_mask_8: 0.2175  loss_dice_8: 0.1312  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:40] d2.utils.events INFO:  eta: 1:05:24  iter: 19039  total_loss: 5.296  loss_ce: 0.1292  loss_mask: 0.1649  loss_dice: 0.1848  loss_ce_0: 0.06453  loss_mask_0: 0.1704  loss_dice_0: 0.1854  loss_ce_1: 0.1275  loss_mask_1: 0.1736  loss_dice_1: 0.1862  loss_ce_2: 0.1299  loss_mask_2: 0.1603  loss_dice_2: 0.1841  loss_ce_3: 0.1289  loss_mask_3: 0.1632  loss_dice_3: 0.1841  loss_ce_4: 0.1277  loss_mask_4: 0.1615  loss_dice_4: 0.1881  loss_ce_5: 0.1292  loss_mask_5: 0.1659  loss_dice_5: 0.1771  loss_ce_6: 0.1279  loss_mask_6: 0.1705  loss_dice_6: 0.1903  loss_ce_7: 0.1277  loss_mask_7: 0.1672  loss_dice_7: 0.1808  loss_ce_8: 0.1283  loss_mask_8: 0.1649  loss_dice_8: 0.1884  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:42] d2.utils.events INFO:  eta: 1:05:21  iter: 19059  total_loss: 5.077  loss_ce: 0.1214  loss_mask: 0.1906  loss_dice: 0.1819  loss_ce_0: 0.06098  loss_mask_0: 0.1967  loss_dice_0: 0.1805  loss_ce_1: 0.1169  loss_mask_1: 0.1961  loss_dice_1: 0.1832  loss_ce_2: 0.1194  loss_mask_2: 0.2037  loss_dice_2: 0.1871  loss_ce_3: 0.1183  loss_mask_3: 0.2074  loss_dice_3: 0.1856  loss_ce_4: 0.1164  loss_mask_4: 0.1953  loss_dice_4: 0.186  loss_ce_5: 0.1188  loss_mask_5: 0.1843  loss_dice_5: 0.1828  loss_ce_6: 0.1179  loss_mask_6: 0.1967  loss_dice_6: 0.1796  loss_ce_7: 0.1181  loss_mask_7: 0.1971  loss_dice_7: 0.1935  loss_ce_8: 0.1197  loss_mask_8: 0.1939  loss_dice_8: 0.1821  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:45] d2.utils.events INFO:  eta: 1:05:19  iter: 19079  total_loss: 5.108  loss_ce: 0.1289  loss_mask: 0.2157  loss_dice: 0.1723  loss_ce_0: 0.0637  loss_mask_0: 0.2096  loss_dice_0: 0.1653  loss_ce_1: 0.1302  loss_mask_1: 0.206  loss_dice_1: 0.163  loss_ce_2: 0.1293  loss_mask_2: 0.2167  loss_dice_2: 0.1626  loss_ce_3: 0.1296  loss_mask_3: 0.2019  loss_dice_3: 0.1608  loss_ce_4: 0.1301  loss_mask_4: 0.2134  loss_dice_4: 0.167  loss_ce_5: 0.1295  loss_mask_5: 0.2097  loss_dice_5: 0.1663  loss_ce_6: 0.1297  loss_mask_6: 0.2023  loss_dice_6: 0.17  loss_ce_7: 0.1299  loss_mask_7: 0.2037  loss_dice_7: 0.1652  loss_ce_8: 0.1293  loss_mask_8: 0.2092  loss_dice_8: 0.163  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:47] d2.utils.events INFO:  eta: 1:05:16  iter: 19099  total_loss: 5.156  loss_ce: 0.1057  loss_mask: 0.2478  loss_dice: 0.1496  loss_ce_0: 0.05561  loss_mask_0: 0.2429  loss_dice_0: 0.1467  loss_ce_1: 0.1058  loss_mask_1: 0.2453  loss_dice_1: 0.1513  loss_ce_2: 0.1071  loss_mask_2: 0.2442  loss_dice_2: 0.1456  loss_ce_3: 0.1064  loss_mask_3: 0.2442  loss_dice_3: 0.1443  loss_ce_4: 0.1066  loss_mask_4: 0.2415  loss_dice_4: 0.1496  loss_ce_5: 0.1069  loss_mask_5: 0.237  loss_dice_5: 0.1501  loss_ce_6: 0.1058  loss_mask_6: 0.2389  loss_dice_6: 0.1462  loss_ce_7: 0.1064  loss_mask_7: 0.2391  loss_dice_7: 0.1496  loss_ce_8: 0.1057  loss_mask_8: 0.2472  loss_dice_8: 0.146  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:50] d2.utils.events INFO:  eta: 1:05:12  iter: 19119  total_loss: 5.632  loss_ce: 0.1293  loss_mask: 0.2052  loss_dice: 0.1593  loss_ce_0: 0.06412  loss_mask_0: 0.2171  loss_dice_0: 0.1598  loss_ce_1: 0.1294  loss_mask_1: 0.2237  loss_dice_1: 0.1627  loss_ce_2: 0.1293  loss_mask_2: 0.2171  loss_dice_2: 0.1562  loss_ce_3: 0.1293  loss_mask_3: 0.2105  loss_dice_3: 0.1564  loss_ce_4: 0.1295  loss_mask_4: 0.2173  loss_dice_4: 0.1548  loss_ce_5: 0.1294  loss_mask_5: 0.2129  loss_dice_5: 0.1625  loss_ce_6: 0.1294  loss_mask_6: 0.2132  loss_dice_6: 0.1595  loss_ce_7: 0.1294  loss_mask_7: 0.2151  loss_dice_7: 0.1546  loss_ce_8: 0.1292  loss_mask_8: 0.2111  loss_dice_8: 0.1588  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:25:52] d2.utils.events INFO:  eta: 1:05:07  iter: 19139  total_loss: 5.163  loss_ce: 0.1134  loss_mask: 0.2082  loss_dice: 0.1523  loss_ce_0: 0.05817  loss_mask_0: 0.206  loss_dice_0: 0.1566  loss_ce_1: 0.1153  loss_mask_1: 0.2142  loss_dice_1: 0.1562  loss_ce_2: 0.1149  loss_mask_2: 0.213  loss_dice_2: 0.1598  loss_ce_3: 0.1147  loss_mask_3: 0.2059  loss_dice_3: 0.1555  loss_ce_4: 0.1158  loss_mask_4: 0.2095  loss_dice_4: 0.1552  loss_ce_5: 0.1149  loss_mask_5: 0.2023  loss_dice_5: 0.158  loss_ce_6: 0.1153  loss_mask_6: 0.2021  loss_dice_6: 0.1558  loss_ce_7: 0.1158  loss_mask_7: 0.2118  loss_dice_7: 0.1609  loss_ce_8: 0.1147  loss_mask_8: 0.2125  loss_dice_8: 0.1543  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:55] d2.utils.events INFO:  eta: 1:05:01  iter: 19159  total_loss: 5.57  loss_ce: 0.1386  loss_mask: 0.2134  loss_dice: 0.23  loss_ce_0: 0.06761  loss_mask_0: 0.2118  loss_dice_0: 0.2197  loss_ce_1: 0.1367  loss_mask_1: 0.2145  loss_dice_1: 0.2221  loss_ce_2: 0.1373  loss_mask_2: 0.2103  loss_dice_2: 0.2265  loss_ce_3: 0.1371  loss_mask_3: 0.2216  loss_dice_3: 0.2204  loss_ce_4: 0.1361  loss_mask_4: 0.2114  loss_dice_4: 0.2178  loss_ce_5: 0.1374  loss_mask_5: 0.216  loss_dice_5: 0.2259  loss_ce_6: 0.1367  loss_mask_6: 0.2163  loss_dice_6: 0.2124  loss_ce_7: 0.1367  loss_mask_7: 0.2089  loss_dice_7: 0.2311  loss_ce_8: 0.1374  loss_mask_8: 0.217  loss_dice_8: 0.2141  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:25:57] d2.utils.events INFO:  eta: 1:04:58  iter: 19179  total_loss: 5.244  loss_ce: 0.1278  loss_mask: 0.1958  loss_dice: 0.2026  loss_ce_0: 0.06324  loss_mask_0: 0.1915  loss_dice_0: 0.1984  loss_ce_1: 0.1267  loss_mask_1: 0.1911  loss_dice_1: 0.2118  loss_ce_2: 0.1269  loss_mask_2: 0.1971  loss_dice_2: 0.2065  loss_ce_3: 0.1269  loss_mask_3: 0.2071  loss_dice_3: 0.2112  loss_ce_4: 0.1262  loss_mask_4: 0.1938  loss_dice_4: 0.2171  loss_ce_5: 0.1269  loss_mask_5: 0.1956  loss_dice_5: 0.2026  loss_ce_6: 0.1268  loss_mask_6: 0.1948  loss_dice_6: 0.2158  loss_ce_7: 0.1269  loss_mask_7: 0.1983  loss_dice_7: 0.2052  loss_ce_8: 0.1269  loss_mask_8: 0.1914  loss_dice_8: 0.2042  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:26:00] d2.utils.events INFO:  eta: 1:04:56  iter: 19199  total_loss: 5.463  loss_ce: 0.1277  loss_mask: 0.2214  loss_dice: 0.1713  loss_ce_0: 0.06355  loss_mask_0: 0.2357  loss_dice_0: 0.1841  loss_ce_1: 0.1269  loss_mask_1: 0.2302  loss_dice_1: 0.176  loss_ce_2: 0.1278  loss_mask_2: 0.2346  loss_dice_2: 0.1773  loss_ce_3: 0.128  loss_mask_3: 0.2322  loss_dice_3: 0.1787  loss_ce_4: 0.1283  loss_mask_4: 0.2123  loss_dice_4: 0.1742  loss_ce_5: 0.1279  loss_mask_5: 0.2202  loss_dice_5: 0.1756  loss_ce_6: 0.1279  loss_mask_6: 0.2233  loss_dice_6: 0.1791  loss_ce_7: 0.1273  loss_mask_7: 0.2227  loss_dice_7: 0.1746  loss_ce_8: 0.1278  loss_mask_8: 0.2334  loss_dice_8: 0.1789  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:26:02] d2.utils.events INFO:  eta: 1:04:52  iter: 19219  total_loss: 5.661  loss_ce: 0.1387  loss_mask: 0.2031  loss_dice: 0.2326  loss_ce_0: 0.06721  loss_mask_0: 0.1909  loss_dice_0: 0.2243  loss_ce_1: 0.1391  loss_mask_1: 0.1988  loss_dice_1: 0.2242  loss_ce_2: 0.1385  loss_mask_2: 0.201  loss_dice_2: 0.229  loss_ce_3: 0.1389  loss_mask_3: 0.2002  loss_dice_3: 0.2398  loss_ce_4: 0.1392  loss_mask_4: 0.1987  loss_dice_4: 0.2301  loss_ce_5: 0.1385  loss_mask_5: 0.1936  loss_dice_5: 0.2289  loss_ce_6: 0.1397  loss_mask_6: 0.2139  loss_dice_6: 0.2361  loss_ce_7: 0.1393  loss_mask_7: 0.2015  loss_dice_7: 0.2343  loss_ce_8: 0.1397  loss_mask_8: 0.1903  loss_dice_8: 0.2356  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:05] d2.utils.events INFO:  eta: 1:04:49  iter: 19239  total_loss: 4.904  loss_ce: 0.1166  loss_mask: 0.183  loss_dice: 0.1661  loss_ce_0: 0.05985  loss_mask_0: 0.1822  loss_dice_0: 0.1686  loss_ce_1: 0.1165  loss_mask_1: 0.1851  loss_dice_1: 0.1765  loss_ce_2: 0.1172  loss_mask_2: 0.188  loss_dice_2: 0.1746  loss_ce_3: 0.117  loss_mask_3: 0.1798  loss_dice_3: 0.1742  loss_ce_4: 0.1169  loss_mask_4: 0.1854  loss_dice_4: 0.1695  loss_ce_5: 0.1174  loss_mask_5: 0.1723  loss_dice_5: 0.1726  loss_ce_6: 0.1165  loss_mask_6: 0.1774  loss_dice_6: 0.1725  loss_ce_7: 0.1168  loss_mask_7: 0.1845  loss_dice_7: 0.1734  loss_ce_8: 0.1162  loss_mask_8: 0.185  loss_dice_8: 0.1741  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:07] d2.utils.events INFO:  eta: 1:04:48  iter: 19259  total_loss: 5.176  loss_ce: 0.128  loss_mask: 0.1516  loss_dice: 0.1849  loss_ce_0: 0.06356  loss_mask_0: 0.1417  loss_dice_0: 0.1908  loss_ce_1: 0.1274  loss_mask_1: 0.1449  loss_dice_1: 0.1919  loss_ce_2: 0.1282  loss_mask_2: 0.1423  loss_dice_2: 0.1915  loss_ce_3: 0.1283  loss_mask_3: 0.1433  loss_dice_3: 0.1825  loss_ce_4: 0.1278  loss_mask_4: 0.1529  loss_dice_4: 0.1858  loss_ce_5: 0.1282  loss_mask_5: 0.1573  loss_dice_5: 0.1862  loss_ce_6: 0.1282  loss_mask_6: 0.1474  loss_dice_6: 0.1916  loss_ce_7: 0.1274  loss_mask_7: 0.154  loss_dice_7: 0.1924  loss_ce_8: 0.128  loss_mask_8: 0.144  loss_dice_8: 0.1844  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:10] d2.utils.events INFO:  eta: 1:04:43  iter: 19279  total_loss: 5.241  loss_ce: 0.1477  loss_mask: 0.2416  loss_dice: 0.1634  loss_ce_0: 0.07053  loss_mask_0: 0.238  loss_dice_0: 0.163  loss_ce_1: 0.1479  loss_mask_1: 0.2387  loss_dice_1: 0.1675  loss_ce_2: 0.1472  loss_mask_2: 0.2417  loss_dice_2: 0.1662  loss_ce_3: 0.1476  loss_mask_3: 0.2362  loss_dice_3: 0.166  loss_ce_4: 0.148  loss_mask_4: 0.2354  loss_dice_4: 0.1605  loss_ce_5: 0.1473  loss_mask_5: 0.2395  loss_dice_5: 0.1661  loss_ce_6: 0.1482  loss_mask_6: 0.2416  loss_dice_6: 0.1672  loss_ce_7: 0.148  loss_mask_7: 0.241  loss_dice_7: 0.1622  loss_ce_8: 0.1481  loss_mask_8: 0.2402  loss_dice_8: 0.1645  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:12] d2.utils.events INFO:  eta: 1:04:40  iter: 19299  total_loss: 5.497  loss_ce: 0.1408  loss_mask: 0.2171  loss_dice: 0.1952  loss_ce_0: 0.06805  loss_mask_0: 0.2183  loss_dice_0: 0.1906  loss_ce_1: 0.1386  loss_mask_1: 0.2062  loss_dice_1: 0.1838  loss_ce_2: 0.1399  loss_mask_2: 0.2215  loss_dice_2: 0.1985  loss_ce_3: 0.1395  loss_mask_3: 0.2224  loss_dice_3: 0.1934  loss_ce_4: 0.1382  loss_mask_4: 0.2153  loss_dice_4: 0.1949  loss_ce_5: 0.1398  loss_mask_5: 0.2159  loss_dice_5: 0.1987  loss_ce_6: 0.1401  loss_mask_6: 0.2114  loss_dice_6: 0.2048  loss_ce_7: 0.1398  loss_mask_7: 0.2207  loss_dice_7: 0.1859  loss_ce_8: 0.1407  loss_mask_8: 0.2119  loss_dice_8: 0.1968  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:15] d2.utils.events INFO:  eta: 1:04:36  iter: 19319  total_loss: 5.118  loss_ce: 0.1303  loss_mask: 0.2016  loss_dice: 0.1606  loss_ce_0: 0.06402  loss_mask_0: 0.2083  loss_dice_0: 0.1719  loss_ce_1: 0.1287  loss_mask_1: 0.1979  loss_dice_1: 0.1627  loss_ce_2: 0.1288  loss_mask_2: 0.2095  loss_dice_2: 0.1662  loss_ce_3: 0.1284  loss_mask_3: 0.2039  loss_dice_3: 0.176  loss_ce_4: 0.1283  loss_mask_4: 0.2052  loss_dice_4: 0.1682  loss_ce_5: 0.1285  loss_mask_5: 0.2034  loss_dice_5: 0.1699  loss_ce_6: 0.1285  loss_mask_6: 0.2069  loss_dice_6: 0.1759  loss_ce_7: 0.129  loss_mask_7: 0.2078  loss_dice_7: 0.1695  loss_ce_8: 0.1295  loss_mask_8: 0.1988  loss_dice_8: 0.1773  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:18] d2.utils.events INFO:  eta: 1:04:34  iter: 19339  total_loss: 4.939  loss_ce: 0.1263  loss_mask: 0.1913  loss_dice: 0.1554  loss_ce_0: 0.06267  loss_mask_0: 0.1956  loss_dice_0: 0.158  loss_ce_1: 0.1246  loss_mask_1: 0.1957  loss_dice_1: 0.165  loss_ce_2: 0.1254  loss_mask_2: 0.1926  loss_dice_2: 0.1575  loss_ce_3: 0.1249  loss_mask_3: 0.1873  loss_dice_3: 0.1564  loss_ce_4: 0.1234  loss_mask_4: 0.1846  loss_dice_4: 0.1578  loss_ce_5: 0.125  loss_mask_5: 0.1918  loss_dice_5: 0.1631  loss_ce_6: 0.1241  loss_mask_6: 0.1874  loss_dice_6: 0.1563  loss_ce_7: 0.1245  loss_mask_7: 0.194  loss_dice_7: 0.1572  loss_ce_8: 0.1254  loss_mask_8: 0.1882  loss_dice_8: 0.1589  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:20] d2.utils.events INFO:  eta: 1:04:31  iter: 19359  total_loss: 4.989  loss_ce: 0.1179  loss_mask: 0.2388  loss_dice: 0.1499  loss_ce_0: 0.0595  loss_mask_0: 0.2205  loss_dice_0: 0.1448  loss_ce_1: 0.1152  loss_mask_1: 0.2401  loss_dice_1: 0.1517  loss_ce_2: 0.1166  loss_mask_2: 0.2378  loss_dice_2: 0.1537  loss_ce_3: 0.1163  loss_mask_3: 0.2332  loss_dice_3: 0.1539  loss_ce_4: 0.1145  loss_mask_4: 0.2331  loss_dice_4: 0.155  loss_ce_5: 0.1161  loss_mask_5: 0.2401  loss_dice_5: 0.1542  loss_ce_6: 0.1151  loss_mask_6: 0.2272  loss_dice_6: 0.1464  loss_ce_7: 0.1152  loss_mask_7: 0.2276  loss_dice_7: 0.1513  loss_ce_8: 0.1168  loss_mask_8: 0.2259  loss_dice_8: 0.1485  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:26:23] d2.utils.events INFO:  eta: 1:04:25  iter: 19379  total_loss: 4.693  loss_ce: 0.1411  loss_mask: 0.1412  loss_dice: 0.1716  loss_ce_0: 0.06836  loss_mask_0: 0.1466  loss_dice_0: 0.1836  loss_ce_1: 0.1433  loss_mask_1: 0.1552  loss_dice_1: 0.1824  loss_ce_2: 0.142  loss_mask_2: 0.1514  loss_dice_2: 0.1839  loss_ce_3: 0.1422  loss_mask_3: 0.1424  loss_dice_3: 0.1767  loss_ce_4: 0.1438  loss_mask_4: 0.1557  loss_dice_4: 0.176  loss_ce_5: 0.1422  loss_mask_5: 0.155  loss_dice_5: 0.1796  loss_ce_6: 0.1438  loss_mask_6: 0.1531  loss_dice_6: 0.1787  loss_ce_7: 0.1435  loss_mask_7: 0.1447  loss_dice_7: 0.1855  loss_ce_8: 0.1422  loss_mask_8: 0.1386  loss_dice_8: 0.1779  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:25] d2.utils.events INFO:  eta: 1:04:20  iter: 19399  total_loss: 5.212  loss_ce: 0.1153  loss_mask: 0.1617  loss_dice: 0.2539  loss_ce_0: 0.05905  loss_mask_0: 0.1536  loss_dice_0: 0.2489  loss_ce_1: 0.1159  loss_mask_1: 0.1611  loss_dice_1: 0.259  loss_ce_2: 0.1161  loss_mask_2: 0.1471  loss_dice_2: 0.252  loss_ce_3: 0.1162  loss_mask_3: 0.159  loss_dice_3: 0.2395  loss_ce_4: 0.1163  loss_mask_4: 0.1535  loss_dice_4: 0.2698  loss_ce_5: 0.1164  loss_mask_5: 0.1554  loss_dice_5: 0.247  loss_ce_6: 0.1156  loss_mask_6: 0.1556  loss_dice_6: 0.2591  loss_ce_7: 0.1161  loss_mask_7: 0.1603  loss_dice_7: 0.2669  loss_ce_8: 0.1154  loss_mask_8: 0.1604  loss_dice_8: 0.2429  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:28] d2.utils.events INFO:  eta: 1:04:16  iter: 19419  total_loss: 5.271  loss_ce: 0.1117  loss_mask: 0.1358  loss_dice: 0.194  loss_ce_0: 0.05745  loss_mask_0: 0.1361  loss_dice_0: 0.1882  loss_ce_1: 0.1105  loss_mask_1: 0.1361  loss_dice_1: 0.1909  loss_ce_2: 0.1113  loss_mask_2: 0.1415  loss_dice_2: 0.1825  loss_ce_3: 0.1112  loss_mask_3: 0.1473  loss_dice_3: 0.1953  loss_ce_4: 0.1106  loss_mask_4: 0.1376  loss_dice_4: 0.1895  loss_ce_5: 0.1113  loss_mask_5: 0.1373  loss_dice_5: 0.1913  loss_ce_6: 0.1108  loss_mask_6: 0.1329  loss_dice_6: 0.1846  loss_ce_7: 0.1108  loss_mask_7: 0.1392  loss_dice_7: 0.1875  loss_ce_8: 0.1114  loss_mask_8: 0.1403  loss_dice_8: 0.1795  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:30] d2.utils.events INFO:  eta: 1:04:13  iter: 19439  total_loss: 5.724  loss_ce: 0.102  loss_mask: 0.1935  loss_dice: 0.2134  loss_ce_0: 0.05386  loss_mask_0: 0.1908  loss_dice_0: 0.226  loss_ce_1: 0.09996  loss_mask_1: 0.1831  loss_dice_1: 0.2304  loss_ce_2: 0.1014  loss_mask_2: 0.1971  loss_dice_2: 0.2377  loss_ce_3: 0.1012  loss_mask_3: 0.187  loss_dice_3: 0.2135  loss_ce_4: 0.1003  loss_mask_4: 0.186  loss_dice_4: 0.2203  loss_ce_5: 0.1012  loss_mask_5: 0.1881  loss_dice_5: 0.2217  loss_ce_6: 0.1003  loss_mask_6: 0.1923  loss_dice_6: 0.2305  loss_ce_7: 0.1003  loss_mask_7: 0.1859  loss_dice_7: 0.2257  loss_ce_8: 0.1015  loss_mask_8: 0.1916  loss_dice_8: 0.2215  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:33] d2.utils.events INFO:  eta: 1:04:10  iter: 19459  total_loss: 4.926  loss_ce: 0.1005  loss_mask: 0.1693  loss_dice: 0.1674  loss_ce_0: 0.05328  loss_mask_0: 0.1711  loss_dice_0: 0.1608  loss_ce_1: 0.09898  loss_mask_1: 0.1631  loss_dice_1: 0.1692  loss_ce_2: 0.1001  loss_mask_2: 0.1629  loss_dice_2: 0.175  loss_ce_3: 0.09991  loss_mask_3: 0.1676  loss_dice_3: 0.1706  loss_ce_4: 0.0985  loss_mask_4: 0.1614  loss_dice_4: 0.1669  loss_ce_5: 0.09988  loss_mask_5: 0.1667  loss_dice_5: 0.1681  loss_ce_6: 0.09905  loss_mask_6: 0.1609  loss_dice_6: 0.1662  loss_ce_7: 0.09839  loss_mask_7: 0.1638  loss_dice_7: 0.1595  loss_ce_8: 0.09995  loss_mask_8: 0.1667  loss_dice_8: 0.1694  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:35] d2.utils.events INFO:  eta: 1:04:05  iter: 19479  total_loss: 4.832  loss_ce: 0.1305  loss_mask: 0.1887  loss_dice: 0.1569  loss_ce_0: 0.06397  loss_mask_0: 0.1936  loss_dice_0: 0.1559  loss_ce_1: 0.131  loss_mask_1: 0.1948  loss_dice_1: 0.16  loss_ce_2: 0.1305  loss_mask_2: 0.2  loss_dice_2: 0.1532  loss_ce_3: 0.1305  loss_mask_3: 0.2029  loss_dice_3: 0.1541  loss_ce_4: 0.1309  loss_mask_4: 0.2035  loss_dice_4: 0.1596  loss_ce_5: 0.1307  loss_mask_5: 0.1919  loss_dice_5: 0.1565  loss_ce_6: 0.131  loss_mask_6: 0.1955  loss_dice_6: 0.1612  loss_ce_7: 0.1312  loss_mask_7: 0.1986  loss_dice_7: 0.1583  loss_ce_8: 0.1307  loss_mask_8: 0.1873  loss_dice_8: 0.1513  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:38] d2.utils.events INFO:  eta: 1:04:02  iter: 19499  total_loss: 5.114  loss_ce: 0.09849  loss_mask: 0.1364  loss_dice: 0.1533  loss_ce_0: 0.05215  loss_mask_0: 0.1313  loss_dice_0: 0.1688  loss_ce_1: 0.09931  loss_mask_1: 0.1341  loss_dice_1: 0.1646  loss_ce_2: 0.09905  loss_mask_2: 0.1511  loss_dice_2: 0.1598  loss_ce_3: 0.09864  loss_mask_3: 0.141  loss_dice_3: 0.1695  loss_ce_4: 0.09907  loss_mask_4: 0.1371  loss_dice_4: 0.1623  loss_ce_5: 0.09922  loss_mask_5: 0.1415  loss_dice_5: 0.1619  loss_ce_6: 0.09894  loss_mask_6: 0.1351  loss_dice_6: 0.1455  loss_ce_7: 0.09935  loss_mask_7: 0.1456  loss_dice_7: 0.1578  loss_ce_8: 0.09883  loss_mask_8: 0.1501  loss_dice_8: 0.1556  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:40] d2.utils.events INFO:  eta: 1:04:03  iter: 19519  total_loss: 5.202  loss_ce: 0.152  loss_mask: 0.2141  loss_dice: 0.1494  loss_ce_0: 0.07307  loss_mask_0: 0.2129  loss_dice_0: 0.1518  loss_ce_1: 0.1491  loss_mask_1: 0.2149  loss_dice_1: 0.1587  loss_ce_2: 0.1506  loss_mask_2: 0.2115  loss_dice_2: 0.1564  loss_ce_3: 0.1512  loss_mask_3: 0.2202  loss_dice_3: 0.1539  loss_ce_4: 0.1497  loss_mask_4: 0.2122  loss_dice_4: 0.1526  loss_ce_5: 0.1501  loss_mask_5: 0.2111  loss_dice_5: 0.1553  loss_ce_6: 0.15  loss_mask_6: 0.2088  loss_dice_6: 0.1503  loss_ce_7: 0.1491  loss_mask_7: 0.2161  loss_dice_7: 0.154  loss_ce_8: 0.151  loss_mask_8: 0.2124  loss_dice_8: 0.1488  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:26:43] d2.utils.events INFO:  eta: 1:04:01  iter: 19539  total_loss: 5.168  loss_ce: 0.1098  loss_mask: 0.1676  loss_dice: 0.1681  loss_ce_0: 0.05606  loss_mask_0: 0.1669  loss_dice_0: 0.1662  loss_ce_1: 0.1118  loss_mask_1: 0.1613  loss_dice_1: 0.1691  loss_ce_2: 0.1107  loss_mask_2: 0.1631  loss_dice_2: 0.1675  loss_ce_3: 0.1104  loss_mask_3: 0.1648  loss_dice_3: 0.1696  loss_ce_4: 0.1116  loss_mask_4: 0.159  loss_dice_4: 0.1638  loss_ce_5: 0.1109  loss_mask_5: 0.1565  loss_dice_5: 0.1671  loss_ce_6: 0.1112  loss_mask_6: 0.1598  loss_dice_6: 0.1588  loss_ce_7: 0.1117  loss_mask_7: 0.1595  loss_dice_7: 0.162  loss_ce_8: 0.1105  loss_mask_8: 0.1654  loss_dice_8: 0.1635  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:26:45] d2.utils.events INFO:  eta: 1:03:59  iter: 19559  total_loss: 4.974  loss_ce: 0.1453  loss_mask: 0.1901  loss_dice: 0.1503  loss_ce_0: 0.07056  loss_mask_0: 0.1828  loss_dice_0: 0.1511  loss_ce_1: 0.1438  loss_mask_1: 0.1849  loss_dice_1: 0.1487  loss_ce_2: 0.145  loss_mask_2: 0.1836  loss_dice_2: 0.1505  loss_ce_3: 0.1452  loss_mask_3: 0.1885  loss_dice_3: 0.1459  loss_ce_4: 0.1445  loss_mask_4: 0.184  loss_dice_4: 0.1456  loss_ce_5: 0.1451  loss_mask_5: 0.1902  loss_dice_5: 0.1472  loss_ce_6: 0.1446  loss_mask_6: 0.1914  loss_dice_6: 0.1503  loss_ce_7: 0.1445  loss_mask_7: 0.1797  loss_dice_7: 0.1459  loss_ce_8: 0.1449  loss_mask_8: 0.1855  loss_dice_8: 0.1446  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:48] d2.utils.events INFO:  eta: 1:03:57  iter: 19579  total_loss: 4.82  loss_ce: 0.1131  loss_mask: 0.2067  loss_dice: 0.1451  loss_ce_0: 0.05741  loss_mask_0: 0.1987  loss_dice_0: 0.1435  loss_ce_1: 0.1149  loss_mask_1: 0.2086  loss_dice_1: 0.1386  loss_ce_2: 0.1138  loss_mask_2: 0.2051  loss_dice_2: 0.1437  loss_ce_3: 0.1138  loss_mask_3: 0.2094  loss_dice_3: 0.1424  loss_ce_4: 0.1146  loss_mask_4: 0.2061  loss_dice_4: 0.1411  loss_ce_5: 0.1139  loss_mask_5: 0.2006  loss_dice_5: 0.1429  loss_ce_6: 0.1142  loss_mask_6: 0.2053  loss_dice_6: 0.1411  loss_ce_7: 0.1144  loss_mask_7: 0.2094  loss_dice_7: 0.143  loss_ce_8: 0.1136  loss_mask_8: 0.2072  loss_dice_8: 0.1429  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:50] d2.utils.events INFO:  eta: 1:03:55  iter: 19599  total_loss: 4.651  loss_ce: 0.1156  loss_mask: 0.1655  loss_dice: 0.1439  loss_ce_0: 0.05844  loss_mask_0: 0.1736  loss_dice_0: 0.1441  loss_ce_1: 0.1168  loss_mask_1: 0.1734  loss_dice_1: 0.1451  loss_ce_2: 0.1159  loss_mask_2: 0.1721  loss_dice_2: 0.1459  loss_ce_3: 0.1163  loss_mask_3: 0.1694  loss_dice_3: 0.1442  loss_ce_4: 0.1168  loss_mask_4: 0.1765  loss_dice_4: 0.1414  loss_ce_5: 0.1162  loss_mask_5: 0.1694  loss_dice_5: 0.1418  loss_ce_6: 0.1165  loss_mask_6: 0.1787  loss_dice_6: 0.1471  loss_ce_7: 0.1167  loss_mask_7: 0.1701  loss_dice_7: 0.1401  loss_ce_8: 0.1158  loss_mask_8: 0.1773  loss_dice_8: 0.1465  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:53] d2.utils.events INFO:  eta: 1:03:51  iter: 19619  total_loss: 5.005  loss_ce: 0.1132  loss_mask: 0.2038  loss_dice: 0.1425  loss_ce_0: 0.05746  loss_mask_0: 0.1946  loss_dice_0: 0.1336  loss_ce_1: 0.1126  loss_mask_1: 0.1913  loss_dice_1: 0.1402  loss_ce_2: 0.1128  loss_mask_2: 0.204  loss_dice_2: 0.1453  loss_ce_3: 0.1129  loss_mask_3: 0.2017  loss_dice_3: 0.1456  loss_ce_4: 0.1129  loss_mask_4: 0.1932  loss_dice_4: 0.1424  loss_ce_5: 0.1128  loss_mask_5: 0.1976  loss_dice_5: 0.1498  loss_ce_6: 0.113  loss_mask_6: 0.2008  loss_dice_6: 0.1452  loss_ce_7: 0.1128  loss_mask_7: 0.2029  loss_dice_7: 0.148  loss_ce_8: 0.1131  loss_mask_8: 0.198  loss_dice_8: 0.1379  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:26:55] d2.utils.events INFO:  eta: 1:03:48  iter: 19639  total_loss: 5.186  loss_ce: 0.1083  loss_mask: 0.2183  loss_dice: 0.1341  loss_ce_0: 0.05582  loss_mask_0: 0.2301  loss_dice_0: 0.1398  loss_ce_1: 0.1074  loss_mask_1: 0.2356  loss_dice_1: 0.1354  loss_ce_2: 0.1078  loss_mask_2: 0.2311  loss_dice_2: 0.1363  loss_ce_3: 0.1078  loss_mask_3: 0.2326  loss_dice_3: 0.1407  loss_ce_4: 0.1074  loss_mask_4: 0.2342  loss_dice_4: 0.1446  loss_ce_5: 0.1076  loss_mask_5: 0.2322  loss_dice_5: 0.1396  loss_ce_6: 0.1075  loss_mask_6: 0.2324  loss_dice_6: 0.1341  loss_ce_7: 0.1074  loss_mask_7: 0.2332  loss_dice_7: 0.1413  loss_ce_8: 0.1079  loss_mask_8: 0.2364  loss_dice_8: 0.143  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:26:58] d2.utils.events INFO:  eta: 1:03:44  iter: 19659  total_loss: 4.997  loss_ce: 0.1292  loss_mask: 0.1762  loss_dice: 0.1783  loss_ce_0: 0.06374  loss_mask_0: 0.182  loss_dice_0: 0.1668  loss_ce_1: 0.1291  loss_mask_1: 0.1857  loss_dice_1: 0.1771  loss_ce_2: 0.1293  loss_mask_2: 0.1775  loss_dice_2: 0.1602  loss_ce_3: 0.1293  loss_mask_3: 0.1864  loss_dice_3: 0.1772  loss_ce_4: 0.1292  loss_mask_4: 0.1841  loss_dice_4: 0.1711  loss_ce_5: 0.1293  loss_mask_5: 0.1812  loss_dice_5: 0.1803  loss_ce_6: 0.1294  loss_mask_6: 0.1839  loss_dice_6: 0.1715  loss_ce_7: 0.1292  loss_mask_7: 0.1846  loss_dice_7: 0.1703  loss_ce_8: 0.1293  loss_mask_8: 0.1734  loss_dice_8: 0.1705  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:00] d2.utils.events INFO:  eta: 1:03:41  iter: 19679  total_loss: 5.255  loss_ce: 0.1067  loss_mask: 0.2474  loss_dice: 0.1513  loss_ce_0: 0.05542  loss_mask_0: 0.2485  loss_dice_0: 0.1513  loss_ce_1: 0.1067  loss_mask_1: 0.2484  loss_dice_1: 0.1479  loss_ce_2: 0.1069  loss_mask_2: 0.2447  loss_dice_2: 0.1514  loss_ce_3: 0.1069  loss_mask_3: 0.2395  loss_dice_3: 0.1505  loss_ce_4: 0.1069  loss_mask_4: 0.2569  loss_dice_4: 0.1521  loss_ce_5: 0.1068  loss_mask_5: 0.2432  loss_dice_5: 0.1498  loss_ce_6: 0.1066  loss_mask_6: 0.2448  loss_dice_6: 0.1477  loss_ce_7: 0.1067  loss_mask_7: 0.2485  loss_dice_7: 0.15  loss_ce_8: 0.1067  loss_mask_8: 0.2408  loss_dice_8: 0.1463  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:27:03] d2.utils.events INFO:  eta: 1:03:40  iter: 19699  total_loss: 4.832  loss_ce: 0.1281  loss_mask: 0.2184  loss_dice: 0.1367  loss_ce_0: 0.06342  loss_mask_0: 0.2164  loss_dice_0: 0.1403  loss_ce_1: 0.1287  loss_mask_1: 0.2201  loss_dice_1: 0.1354  loss_ce_2: 0.1283  loss_mask_2: 0.2201  loss_dice_2: 0.1343  loss_ce_3: 0.1283  loss_mask_3: 0.2097  loss_dice_3: 0.1361  loss_ce_4: 0.1282  loss_mask_4: 0.2146  loss_dice_4: 0.1331  loss_ce_5: 0.1282  loss_mask_5: 0.2173  loss_dice_5: 0.1359  loss_ce_6: 0.1282  loss_mask_6: 0.2129  loss_dice_6: 0.1338  loss_ce_7: 0.1283  loss_mask_7: 0.2159  loss_dice_7: 0.1395  loss_ce_8: 0.1282  loss_mask_8: 0.2125  loss_dice_8: 0.1331  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:06] d2.utils.events INFO:  eta: 1:03:38  iter: 19719  total_loss: 5.078  loss_ce: 0.1452  loss_mask: 0.2281  loss_dice: 0.1426  loss_ce_0: 0.07006  loss_mask_0: 0.2281  loss_dice_0: 0.1435  loss_ce_1: 0.1447  loss_mask_1: 0.2264  loss_dice_1: 0.1383  loss_ce_2: 0.1453  loss_mask_2: 0.2275  loss_dice_2: 0.1417  loss_ce_3: 0.1453  loss_mask_3: 0.227  loss_dice_3: 0.1375  loss_ce_4: 0.1449  loss_mask_4: 0.2269  loss_dice_4: 0.1384  loss_ce_5: 0.1451  loss_mask_5: 0.2302  loss_dice_5: 0.1389  loss_ce_6: 0.1449  loss_mask_6: 0.2241  loss_dice_6: 0.1343  loss_ce_7: 0.1447  loss_mask_7: 0.2276  loss_dice_7: 0.1448  loss_ce_8: 0.1449  loss_mask_8: 0.2246  loss_dice_8: 0.1356  time: 0.1261  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:27:08] d2.utils.events INFO:  eta: 1:03:38  iter: 19739  total_loss: 5.258  loss_ce: 0.1118  loss_mask: 0.1967  loss_dice: 0.1504  loss_ce_0: 0.05708  loss_mask_0: 0.2139  loss_dice_0: 0.1456  loss_ce_1: 0.1127  loss_mask_1: 0.2065  loss_dice_1: 0.1508  loss_ce_2: 0.1119  loss_mask_2: 0.195  loss_dice_2: 0.1486  loss_ce_3: 0.1119  loss_mask_3: 0.2057  loss_dice_3: 0.1528  loss_ce_4: 0.1122  loss_mask_4: 0.1959  loss_dice_4: 0.1582  loss_ce_5: 0.112  loss_mask_5: 0.1972  loss_dice_5: 0.1451  loss_ce_6: 0.1121  loss_mask_6: 0.2016  loss_dice_6: 0.1495  loss_ce_7: 0.1122  loss_mask_7: 0.2002  loss_dice_7: 0.1483  loss_ce_8: 0.112  loss_mask_8: 0.2021  loss_dice_8: 0.1481  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:11] d2.utils.events INFO:  eta: 1:03:34  iter: 19759  total_loss: 5.024  loss_ce: 0.1088  loss_mask: 0.2037  loss_dice: 0.1654  loss_ce_0: 0.05601  loss_mask_0: 0.1933  loss_dice_0: 0.1723  loss_ce_1: 0.1088  loss_mask_1: 0.1952  loss_dice_1: 0.1688  loss_ce_2: 0.1087  loss_mask_2: 0.1962  loss_dice_2: 0.1717  loss_ce_3: 0.1086  loss_mask_3: 0.1916  loss_dice_3: 0.1705  loss_ce_4: 0.1085  loss_mask_4: 0.201  loss_dice_4: 0.1704  loss_ce_5: 0.1085  loss_mask_5: 0.2037  loss_dice_5: 0.1684  loss_ce_6: 0.1085  loss_mask_6: 0.1958  loss_dice_6: 0.1649  loss_ce_7: 0.1085  loss_mask_7: 0.1961  loss_dice_7: 0.1686  loss_ce_8: 0.1088  loss_mask_8: 0.201  loss_dice_8: 0.167  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:13] d2.utils.events INFO:  eta: 1:03:32  iter: 19779  total_loss: 5.094  loss_ce: 0.1074  loss_mask: 0.2072  loss_dice: 0.1633  loss_ce_0: 0.05563  loss_mask_0: 0.204  loss_dice_0: 0.157  loss_ce_1: 0.1072  loss_mask_1: 0.2039  loss_dice_1: 0.1532  loss_ce_2: 0.1074  loss_mask_2: 0.2034  loss_dice_2: 0.162  loss_ce_3: 0.1074  loss_mask_3: 0.2066  loss_dice_3: 0.1603  loss_ce_4: 0.1073  loss_mask_4: 0.2155  loss_dice_4: 0.1585  loss_ce_5: 0.1072  loss_mask_5: 0.2125  loss_dice_5: 0.158  loss_ce_6: 0.1071  loss_mask_6: 0.1937  loss_dice_6: 0.1537  loss_ce_7: 0.1071  loss_mask_7: 0.2019  loss_dice_7: 0.158  loss_ce_8: 0.1073  loss_mask_8: 0.2107  loss_dice_8: 0.1615  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:16] d2.utils.events INFO:  eta: 1:03:30  iter: 19799  total_loss: 5.355  loss_ce: 0.1049  loss_mask: 0.2282  loss_dice: 0.1719  loss_ce_0: 0.05476  loss_mask_0: 0.2236  loss_dice_0: 0.1662  loss_ce_1: 0.1045  loss_mask_1: 0.2258  loss_dice_1: 0.1778  loss_ce_2: 0.1049  loss_mask_2: 0.2181  loss_dice_2: 0.1808  loss_ce_3: 0.1048  loss_mask_3: 0.2201  loss_dice_3: 0.1845  loss_ce_4: 0.1045  loss_mask_4: 0.2352  loss_dice_4: 0.1824  loss_ce_5: 0.1046  loss_mask_5: 0.226  loss_dice_5: 0.1782  loss_ce_6: 0.1043  loss_mask_6: 0.2181  loss_dice_6: 0.1763  loss_ce_7: 0.1042  loss_mask_7: 0.2268  loss_dice_7: 0.1755  loss_ce_8: 0.1047  loss_mask_8: 0.221  loss_dice_8: 0.1767  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:18] d2.utils.events INFO:  eta: 1:03:29  iter: 19819  total_loss: 4.915  loss_ce: 0.1289  loss_mask: 0.1923  loss_dice: 0.1362  loss_ce_0: 0.06361  loss_mask_0: 0.1936  loss_dice_0: 0.1379  loss_ce_1: 0.1289  loss_mask_1: 0.1883  loss_dice_1: 0.1343  loss_ce_2: 0.129  loss_mask_2: 0.1968  loss_dice_2: 0.1423  loss_ce_3: 0.1289  loss_mask_3: 0.192  loss_dice_3: 0.1378  loss_ce_4: 0.1289  loss_mask_4: 0.1933  loss_dice_4: 0.1358  loss_ce_5: 0.1289  loss_mask_5: 0.1952  loss_dice_5: 0.1401  loss_ce_6: 0.129  loss_mask_6: 0.1906  loss_dice_6: 0.1351  loss_ce_7: 0.129  loss_mask_7: 0.188  loss_dice_7: 0.1314  loss_ce_8: 0.129  loss_mask_8: 0.1812  loss_dice_8: 0.1314  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:21] d2.utils.events INFO:  eta: 1:03:25  iter: 19839  total_loss: 5.117  loss_ce: 0.1085  loss_mask: 0.2034  loss_dice: 0.1745  loss_ce_0: 0.05587  loss_mask_0: 0.1955  loss_dice_0: 0.1772  loss_ce_1: 0.1093  loss_mask_1: 0.2086  loss_dice_1: 0.1848  loss_ce_2: 0.1088  loss_mask_2: 0.2099  loss_dice_2: 0.1793  loss_ce_3: 0.1088  loss_mask_3: 0.2084  loss_dice_3: 0.178  loss_ce_4: 0.1091  loss_mask_4: 0.1953  loss_dice_4: 0.1802  loss_ce_5: 0.1089  loss_mask_5: 0.2038  loss_dice_5: 0.1793  loss_ce_6: 0.1089  loss_mask_6: 0.2079  loss_dice_6: 0.1781  loss_ce_7: 0.1091  loss_mask_7: 0.2016  loss_dice_7: 0.1786  loss_ce_8: 0.1088  loss_mask_8: 0.2026  loss_dice_8: 0.1787  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:23] d2.utils.events INFO:  eta: 1:03:23  iter: 19859  total_loss: 4.807  loss_ce: 0.1489  loss_mask: 0.1686  loss_dice: 0.1768  loss_ce_0: 0.07114  loss_mask_0: 0.1564  loss_dice_0: 0.1731  loss_ce_1: 0.1481  loss_mask_1: 0.1565  loss_dice_1: 0.1672  loss_ce_2: 0.1487  loss_mask_2: 0.1527  loss_dice_2: 0.1771  loss_ce_3: 0.1488  loss_mask_3: 0.1569  loss_dice_3: 0.1738  loss_ce_4: 0.1486  loss_mask_4: 0.1571  loss_dice_4: 0.1714  loss_ce_5: 0.1488  loss_mask_5: 0.1596  loss_dice_5: 0.1773  loss_ce_6: 0.1486  loss_mask_6: 0.1653  loss_dice_6: 0.1681  loss_ce_7: 0.1486  loss_mask_7: 0.1606  loss_dice_7: 0.1748  loss_ce_8: 0.1488  loss_mask_8: 0.1607  loss_dice_8: 0.1752  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:26] d2.utils.events INFO:  eta: 1:03:21  iter: 19879  total_loss: 5.139  loss_ce: 0.1416  loss_mask: 0.1948  loss_dice: 0.1892  loss_ce_0: 0.06847  loss_mask_0: 0.184  loss_dice_0: 0.1814  loss_ce_1: 0.1408  loss_mask_1: 0.1894  loss_dice_1: 0.1875  loss_ce_2: 0.1413  loss_mask_2: 0.1965  loss_dice_2: 0.1712  loss_ce_3: 0.1411  loss_mask_3: 0.19  loss_dice_3: 0.1907  loss_ce_4: 0.141  loss_mask_4: 0.1927  loss_dice_4: 0.1839  loss_ce_5: 0.1412  loss_mask_5: 0.1956  loss_dice_5: 0.1886  loss_ce_6: 0.1409  loss_mask_6: 0.1951  loss_dice_6: 0.1893  loss_ce_7: 0.1411  loss_mask_7: 0.2009  loss_dice_7: 0.1821  loss_ce_8: 0.1413  loss_mask_8: 0.1945  loss_dice_8: 0.1821  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:27:29] d2.utils.events INFO:  eta: 1:03:17  iter: 19899  total_loss: 4.948  loss_ce: 0.1317  loss_mask: 0.1505  loss_dice: 0.1504  loss_ce_0: 0.06499  loss_mask_0: 0.1531  loss_dice_0: 0.1501  loss_ce_1: 0.1296  loss_mask_1: 0.155  loss_dice_1: 0.1524  loss_ce_2: 0.1311  loss_mask_2: 0.1583  loss_dice_2: 0.1466  loss_ce_3: 0.1305  loss_mask_3: 0.153  loss_dice_3: 0.15  loss_ce_4: 0.13  loss_mask_4: 0.146  loss_dice_4: 0.1465  loss_ce_5: 0.1311  loss_mask_5: 0.1602  loss_dice_5: 0.1482  loss_ce_6: 0.1305  loss_mask_6: 0.1493  loss_dice_6: 0.1519  loss_ce_7: 0.1303  loss_mask_7: 0.163  loss_dice_7: 0.1511  loss_ce_8: 0.1311  loss_mask_8: 0.1569  loss_dice_8: 0.1511  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:27:31] d2.utils.events INFO:  eta: 1:03:16  iter: 19919  total_loss: 5.244  loss_ce: 0.1278  loss_mask: 0.1872  loss_dice: 0.1689  loss_ce_0: 0.06302  loss_mask_0: 0.1854  loss_dice_0: 0.1642  loss_ce_1: 0.1279  loss_mask_1: 0.1887  loss_dice_1: 0.1679  loss_ce_2: 0.128  loss_mask_2: 0.189  loss_dice_2: 0.1707  loss_ce_3: 0.1284  loss_mask_3: 0.1815  loss_dice_3: 0.1673  loss_ce_4: 0.1286  loss_mask_4: 0.1875  loss_dice_4: 0.1685  loss_ce_5: 0.128  loss_mask_5: 0.1853  loss_dice_5: 0.1684  loss_ce_6: 0.1285  loss_mask_6: 0.1973  loss_dice_6: 0.1631  loss_ce_7: 0.1285  loss_mask_7: 0.1869  loss_dice_7: 0.165  loss_ce_8: 0.128  loss_mask_8: 0.1842  loss_dice_8: 0.1654  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:27:34] d2.utils.events INFO:  eta: 1:03:15  iter: 19939  total_loss: 5.458  loss_ce: 0.1289  loss_mask: 0.2252  loss_dice: 0.1573  loss_ce_0: 0.06336  loss_mask_0: 0.2282  loss_dice_0: 0.1657  loss_ce_1: 0.1288  loss_mask_1: 0.2247  loss_dice_1: 0.1552  loss_ce_2: 0.1292  loss_mask_2: 0.2319  loss_dice_2: 0.161  loss_ce_3: 0.1292  loss_mask_3: 0.2381  loss_dice_3: 0.1637  loss_ce_4: 0.1292  loss_mask_4: 0.2321  loss_dice_4: 0.1705  loss_ce_5: 0.1291  loss_mask_5: 0.2272  loss_dice_5: 0.1612  loss_ce_6: 0.129  loss_mask_6: 0.229  loss_dice_6: 0.1645  loss_ce_7: 0.1288  loss_mask_7: 0.2334  loss_dice_7: 0.1605  loss_ce_8: 0.1288  loss_mask_8: 0.2304  loss_dice_8: 0.1573  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:36] d2.utils.events INFO:  eta: 1:03:11  iter: 19959  total_loss: 5.425  loss_ce: 0.1237  loss_mask: 0.1976  loss_dice: 0.2062  loss_ce_0: 0.06132  loss_mask_0: 0.189  loss_dice_0: 0.2065  loss_ce_1: 0.1229  loss_mask_1: 0.192  loss_dice_1: 0.2142  loss_ce_2: 0.1228  loss_mask_2: 0.2023  loss_dice_2: 0.2125  loss_ce_3: 0.1228  loss_mask_3: 0.2  loss_dice_3: 0.2116  loss_ce_4: 0.1231  loss_mask_4: 0.1919  loss_dice_4: 0.1948  loss_ce_5: 0.1231  loss_mask_5: 0.2031  loss_dice_5: 0.2092  loss_ce_6: 0.1236  loss_mask_6: 0.1944  loss_dice_6: 0.2047  loss_ce_7: 0.1234  loss_mask_7: 0.202  loss_dice_7: 0.2019  loss_ce_8: 0.1238  loss_mask_8: 0.1964  loss_dice_8: 0.2115  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:39] d2.utils.events INFO:  eta: 1:03:11  iter: 19979  total_loss: 5.53  loss_ce: 0.1212  loss_mask: 0.1951  loss_dice: 0.1791  loss_ce_0: 0.06073  loss_mask_0: 0.1943  loss_dice_0: 0.1707  loss_ce_1: 0.1212  loss_mask_1: 0.1928  loss_dice_1: 0.1765  loss_ce_2: 0.1213  loss_mask_2: 0.1921  loss_dice_2: 0.1719  loss_ce_3: 0.1213  loss_mask_3: 0.1909  loss_dice_3: 0.1668  loss_ce_4: 0.1215  loss_mask_4: 0.1908  loss_dice_4: 0.1692  loss_ce_5: 0.1214  loss_mask_5: 0.1906  loss_dice_5: 0.1722  loss_ce_6: 0.1212  loss_mask_6: 0.1921  loss_dice_6: 0.1736  loss_ce_7: 0.1212  loss_mask_7: 0.1955  loss_dice_7: 0.1717  loss_ce_8: 0.1213  loss_mask_8: 0.1858  loss_dice_8: 0.1753  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:27:41] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0019999.pth
[04/13 15:27:41] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 15:27:41] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 15:27:41] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 15:27:41] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 15:27:41] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 15:27:45] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0528 s/iter. Eval: 0.2352 s/iter. Total: 0.2885 s/iter. ETA=0:04:03
[04/13 15:27:50] d2.evaluation.evaluator INFO: Inference done 29/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2349 s/iter. Total: 0.2885 s/iter. ETA=0:03:58
[04/13 15:27:55] d2.evaluation.evaluator INFO: Inference done 47/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2349 s/iter. Total: 0.2887 s/iter. ETA=0:03:53
[04/13 15:28:00] d2.evaluation.evaluator INFO: Inference done 65/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2357 s/iter. Total: 0.2895 s/iter. ETA=0:03:48
[04/13 15:28:05] d2.evaluation.evaluator INFO: Inference done 82/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2369 s/iter. Total: 0.2908 s/iter. ETA=0:03:45
[04/13 15:28:11] d2.evaluation.evaluator INFO: Inference done 100/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2370 s/iter. Total: 0.2909 s/iter. ETA=0:03:39
[04/13 15:28:16] d2.evaluation.evaluator INFO: Inference done 118/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2369 s/iter. Total: 0.2908 s/iter. ETA=0:03:34
[04/13 15:28:21] d2.evaluation.evaluator INFO: Inference done 136/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2369 s/iter. Total: 0.2907 s/iter. ETA=0:03:29
[04/13 15:28:26] d2.evaluation.evaluator INFO: Inference done 154/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2369 s/iter. Total: 0.2907 s/iter. ETA=0:03:24
[04/13 15:28:32] d2.evaluation.evaluator INFO: Inference done 172/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2372 s/iter. Total: 0.2910 s/iter. ETA=0:03:19
[04/13 15:28:37] d2.evaluation.evaluator INFO: Inference done 190/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2372 s/iter. Total: 0.2910 s/iter. ETA=0:03:13
[04/13 15:28:42] d2.evaluation.evaluator INFO: Inference done 208/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2371 s/iter. Total: 0.2910 s/iter. ETA=0:03:08
[04/13 15:28:47] d2.evaluation.evaluator INFO: Inference done 226/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2372 s/iter. Total: 0.2910 s/iter. ETA=0:03:03
[04/13 15:28:53] d2.evaluation.evaluator INFO: Inference done 244/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2372 s/iter. Total: 0.2910 s/iter. ETA=0:02:58
[04/13 15:28:58] d2.evaluation.evaluator INFO: Inference done 262/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2372 s/iter. Total: 0.2909 s/iter. ETA=0:02:52
[04/13 15:29:03] d2.evaluation.evaluator INFO: Inference done 280/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2373 s/iter. Total: 0.2911 s/iter. ETA=0:02:47
[04/13 15:29:08] d2.evaluation.evaluator INFO: Inference done 297/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2378 s/iter. Total: 0.2916 s/iter. ETA=0:02:43
[04/13 15:29:13] d2.evaluation.evaluator INFO: Inference done 315/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2377 s/iter. Total: 0.2915 s/iter. ETA=0:02:37
[04/13 15:29:19] d2.evaluation.evaluator INFO: Inference done 333/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2377 s/iter. Total: 0.2915 s/iter. ETA=0:02:32
[04/13 15:29:24] d2.evaluation.evaluator INFO: Inference done 351/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2376 s/iter. Total: 0.2914 s/iter. ETA=0:02:27
[04/13 15:29:29] d2.evaluation.evaluator INFO: Inference done 369/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2374 s/iter. Total: 0.2913 s/iter. ETA=0:02:21
[04/13 15:29:34] d2.evaluation.evaluator INFO: Inference done 387/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2374 s/iter. Total: 0.2912 s/iter. ETA=0:02:16
[04/13 15:29:40] d2.evaluation.evaluator INFO: Inference done 405/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2374 s/iter. Total: 0.2913 s/iter. ETA=0:02:11
[04/13 15:29:45] d2.evaluation.evaluator INFO: Inference done 423/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2373 s/iter. Total: 0.2912 s/iter. ETA=0:02:06
[04/13 15:29:50] d2.evaluation.evaluator INFO: Inference done 441/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2372 s/iter. Total: 0.2911 s/iter. ETA=0:02:00
[04/13 15:29:55] d2.evaluation.evaluator INFO: Inference done 459/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2371 s/iter. Total: 0.2910 s/iter. ETA=0:01:55
[04/13 15:30:00] d2.evaluation.evaluator INFO: Inference done 477/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2372 s/iter. Total: 0.2910 s/iter. ETA=0:01:50
[04/13 15:30:06] d2.evaluation.evaluator INFO: Inference done 495/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2372 s/iter. Total: 0.2911 s/iter. ETA=0:01:45
[04/13 15:30:11] d2.evaluation.evaluator INFO: Inference done 513/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2373 s/iter. Total: 0.2911 s/iter. ETA=0:01:39
[04/13 15:30:16] d2.evaluation.evaluator INFO: Inference done 530/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2375 s/iter. Total: 0.2914 s/iter. ETA=0:01:34
[04/13 15:30:21] d2.evaluation.evaluator INFO: Inference done 548/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2376 s/iter. Total: 0.2914 s/iter. ETA=0:01:29
[04/13 15:30:27] d2.evaluation.evaluator INFO: Inference done 566/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2376 s/iter. Total: 0.2915 s/iter. ETA=0:01:24
[04/13 15:30:32] d2.evaluation.evaluator INFO: Inference done 584/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2376 s/iter. Total: 0.2915 s/iter. ETA=0:01:19
[04/13 15:30:37] d2.evaluation.evaluator INFO: Inference done 602/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2377 s/iter. Total: 0.2915 s/iter. ETA=0:01:14
[04/13 15:30:42] d2.evaluation.evaluator INFO: Inference done 620/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2377 s/iter. Total: 0.2915 s/iter. ETA=0:01:08
[04/13 15:30:48] d2.evaluation.evaluator INFO: Inference done 638/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2377 s/iter. Total: 0.2916 s/iter. ETA=0:01:03
[04/13 15:30:53] d2.evaluation.evaluator INFO: Inference done 656/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2378 s/iter. Total: 0.2916 s/iter. ETA=0:00:58
[04/13 15:30:58] d2.evaluation.evaluator INFO: Inference done 674/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2378 s/iter. Total: 0.2916 s/iter. ETA=0:00:53
[04/13 15:31:03] d2.evaluation.evaluator INFO: Inference done 692/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2378 s/iter. Total: 0.2917 s/iter. ETA=0:00:47
[04/13 15:31:08] d2.evaluation.evaluator INFO: Inference done 709/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2378 s/iter. Total: 0.2917 s/iter. ETA=0:00:42
[04/13 15:31:14] d2.evaluation.evaluator INFO: Inference done 727/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2379 s/iter. Total: 0.2918 s/iter. ETA=0:00:37
[04/13 15:31:19] d2.evaluation.evaluator INFO: Inference done 745/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2379 s/iter. Total: 0.2918 s/iter. ETA=0:00:32
[04/13 15:31:24] d2.evaluation.evaluator INFO: Inference done 762/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2381 s/iter. Total: 0.2920 s/iter. ETA=0:00:27
[04/13 15:31:29] d2.evaluation.evaluator INFO: Inference done 780/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2381 s/iter. Total: 0.2920 s/iter. ETA=0:00:22
[04/13 15:31:35] d2.evaluation.evaluator INFO: Inference done 798/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2382 s/iter. Total: 0.2920 s/iter. ETA=0:00:16
[04/13 15:31:40] d2.evaluation.evaluator INFO: Inference done 816/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2382 s/iter. Total: 0.2920 s/iter. ETA=0:00:11
[04/13 15:31:45] d2.evaluation.evaluator INFO: Inference done 834/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2382 s/iter. Total: 0.2921 s/iter. ETA=0:00:06
[04/13 15:31:50] d2.evaluation.evaluator INFO: Inference done 852/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2382 s/iter. Total: 0.2921 s/iter. ETA=0:00:01
[04/13 15:31:52] d2.evaluation.evaluator INFO: Total inference time: 0:04:08.655054 (0.292192 s / iter per device, on 1 devices)
[04/13 15:31:52] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:45 (0.052964 s / iter per device, on 1 devices)
[04/13 15:31:53] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 15:31:53] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 15:31:54] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 15:31:54] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 15:31:55] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.27 seconds.
[04/13 15:31:55] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 15:31:55] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 15:31:55] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 15:31:55] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 15:31:55] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 15:31:59] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 15:32:00] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 1.84 seconds.
[04/13 15:32:00] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 15:32:00] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 15:32:00] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 61.296 | 61.296 | 61.296 |  nan  |  nan  | 61.296 |
[04/13 15:32:00] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 15:32:00] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 70.138 | defect     | 52.453 |
[04/13 15:32:00] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 15:32:00] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 15:32:00] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 15:32:00] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 15:32:00] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 15:32:00] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 15:32:00] d2.evaluation.testing INFO: copypaste: 61.2956,61.2956,61.2956,nan,nan,61.2956
[04/13 15:32:01] d2.utils.events INFO:  eta: 1:03:07  iter: 19999  total_loss: 5.342  loss_ce: 0.1147  loss_mask: 0.2318  loss_dice: 0.1492  loss_ce_0: 0.05843  loss_mask_0: 0.2251  loss_dice_0: 0.1499  loss_ce_1: 0.1139  loss_mask_1: 0.2232  loss_dice_1: 0.1504  loss_ce_2: 0.1143  loss_mask_2: 0.2392  loss_dice_2: 0.1473  loss_ce_3: 0.1141  loss_mask_3: 0.2315  loss_dice_3: 0.1543  loss_ce_4: 0.1141  loss_mask_4: 0.2268  loss_dice_4: 0.1504  loss_ce_5: 0.1144  loss_mask_5: 0.2338  loss_dice_5: 0.15  loss_ce_6: 0.1141  loss_mask_6: 0.2324  loss_dice_6: 0.1527  loss_ce_7: 0.1141  loss_mask_7: 0.2372  loss_dice_7: 0.15  loss_ce_8: 0.1147  loss_mask_8: 0.2317  loss_dice_8: 0.1489  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:03] d2.utils.events INFO:  eta: 1:03:02  iter: 20019  total_loss: 5.553  loss_ce: 0.1077  loss_mask: 0.1621  loss_dice: 0.1899  loss_ce_0: 0.05598  loss_mask_0: 0.1691  loss_dice_0: 0.1956  loss_ce_1: 0.1069  loss_mask_1: 0.1554  loss_dice_1: 0.181  loss_ce_2: 0.1077  loss_mask_2: 0.1737  loss_dice_2: 0.1905  loss_ce_3: 0.1075  loss_mask_3: 0.1642  loss_dice_3: 0.1807  loss_ce_4: 0.1074  loss_mask_4: 0.1671  loss_dice_4: 0.1909  loss_ce_5: 0.1076  loss_mask_5: 0.1735  loss_dice_5: 0.2045  loss_ce_6: 0.1071  loss_mask_6: 0.166  loss_dice_6: 0.1928  loss_ce_7: 0.1072  loss_mask_7: 0.1678  loss_dice_7: 0.1955  loss_ce_8: 0.1077  loss_mask_8: 0.1723  loss_dice_8: 0.1852  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:06] d2.utils.events INFO:  eta: 1:03:00  iter: 20039  total_loss: 4.729  loss_ce: 0.1553  loss_mask: 0.194  loss_dice: 0.1261  loss_ce_0: 0.07277  loss_mask_0: 0.201  loss_dice_0: 0.124  loss_ce_1: 0.1558  loss_mask_1: 0.1958  loss_dice_1: 0.1258  loss_ce_2: 0.155  loss_mask_2: 0.1938  loss_dice_2: 0.1194  loss_ce_3: 0.1554  loss_mask_3: 0.1951  loss_dice_3: 0.1214  loss_ce_4: 0.1556  loss_mask_4: 0.1906  loss_dice_4: 0.124  loss_ce_5: 0.1554  loss_mask_5: 0.1855  loss_dice_5: 0.1221  loss_ce_6: 0.1561  loss_mask_6: 0.1896  loss_dice_6: 0.1245  loss_ce_7: 0.1557  loss_mask_7: 0.1917  loss_dice_7: 0.1245  loss_ce_8: 0.1552  loss_mask_8: 0.2005  loss_dice_8: 0.1213  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:08] d2.utils.events INFO:  eta: 1:02:59  iter: 20059  total_loss: 5.065  loss_ce: 0.1053  loss_mask: 0.1963  loss_dice: 0.1693  loss_ce_0: 0.05496  loss_mask_0: 0.1978  loss_dice_0: 0.1687  loss_ce_1: 0.1056  loss_mask_1: 0.1868  loss_dice_1: 0.1694  loss_ce_2: 0.1056  loss_mask_2: 0.1936  loss_dice_2: 0.1691  loss_ce_3: 0.1054  loss_mask_3: 0.1952  loss_dice_3: 0.1724  loss_ce_4: 0.1056  loss_mask_4: 0.1802  loss_dice_4: 0.1694  loss_ce_5: 0.1056  loss_mask_5: 0.1908  loss_dice_5: 0.1732  loss_ce_6: 0.1054  loss_mask_6: 0.1987  loss_dice_6: 0.1678  loss_ce_7: 0.1055  loss_mask_7: 0.1944  loss_dice_7: 0.1697  loss_ce_8: 0.1055  loss_mask_8: 0.1888  loss_dice_8: 0.1751  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:32:11] d2.utils.events INFO:  eta: 1:02:53  iter: 20079  total_loss: 5.206  loss_ce: 0.1036  loss_mask: 0.1796  loss_dice: 0.1772  loss_ce_0: 0.05413  loss_mask_0: 0.1718  loss_dice_0: 0.1771  loss_ce_1: 0.1038  loss_mask_1: 0.1742  loss_dice_1: 0.1785  loss_ce_2: 0.1038  loss_mask_2: 0.1854  loss_dice_2: 0.1875  loss_ce_3: 0.1035  loss_mask_3: 0.1743  loss_dice_3: 0.1846  loss_ce_4: 0.1035  loss_mask_4: 0.1727  loss_dice_4: 0.1725  loss_ce_5: 0.1038  loss_mask_5: 0.1658  loss_dice_5: 0.1787  loss_ce_6: 0.1036  loss_mask_6: 0.1805  loss_dice_6: 0.1833  loss_ce_7: 0.1037  loss_mask_7: 0.1843  loss_dice_7: 0.1816  loss_ce_8: 0.1037  loss_mask_8: 0.1727  loss_dice_8: 0.1772  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:13] d2.utils.events INFO:  eta: 1:02:50  iter: 20099  total_loss: 5.569  loss_ce: 0.1068  loss_mask: 0.1956  loss_dice: 0.2117  loss_ce_0: 0.05516  loss_mask_0: 0.1907  loss_dice_0: 0.2133  loss_ce_1: 0.1073  loss_mask_1: 0.2002  loss_dice_1: 0.2236  loss_ce_2: 0.107  loss_mask_2: 0.1919  loss_dice_2: 0.2134  loss_ce_3: 0.1068  loss_mask_3: 0.2034  loss_dice_3: 0.217  loss_ce_4: 0.1071  loss_mask_4: 0.1974  loss_dice_4: 0.2105  loss_ce_5: 0.1071  loss_mask_5: 0.1985  loss_dice_5: 0.2175  loss_ce_6: 0.1071  loss_mask_6: 0.2027  loss_dice_6: 0.22  loss_ce_7: 0.1073  loss_mask_7: 0.2017  loss_dice_7: 0.2131  loss_ce_8: 0.1069  loss_mask_8: 0.1949  loss_dice_8: 0.2142  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:16] d2.utils.events INFO:  eta: 1:02:48  iter: 20119  total_loss: 5.167  loss_ce: 0.1087  loss_mask: 0.2177  loss_dice: 0.1613  loss_ce_0: 0.05593  loss_mask_0: 0.2029  loss_dice_0: 0.1651  loss_ce_1: 0.1094  loss_mask_1: 0.1941  loss_dice_1: 0.1643  loss_ce_2: 0.1092  loss_mask_2: 0.2086  loss_dice_2: 0.166  loss_ce_3: 0.109  loss_mask_3: 0.2071  loss_dice_3: 0.173  loss_ce_4: 0.1092  loss_mask_4: 0.1927  loss_dice_4: 0.163  loss_ce_5: 0.1092  loss_mask_5: 0.2124  loss_dice_5: 0.1601  loss_ce_6: 0.1091  loss_mask_6: 0.2018  loss_dice_6: 0.1673  loss_ce_7: 0.1094  loss_mask_7: 0.2066  loss_dice_7: 0.1618  loss_ce_8: 0.109  loss_mask_8: 0.2163  loss_dice_8: 0.1624  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:18] d2.utils.events INFO:  eta: 1:02:48  iter: 20139  total_loss: 5.201  loss_ce: 0.1487  loss_mask: 0.1811  loss_dice: 0.1877  loss_ce_0: 0.07099  loss_mask_0: 0.1711  loss_dice_0: 0.1823  loss_ce_1: 0.1478  loss_mask_1: 0.1741  loss_dice_1: 0.1854  loss_ce_2: 0.1484  loss_mask_2: 0.1764  loss_dice_2: 0.1884  loss_ce_3: 0.1485  loss_mask_3: 0.182  loss_dice_3: 0.1789  loss_ce_4: 0.1486  loss_mask_4: 0.1757  loss_dice_4: 0.1789  loss_ce_5: 0.1487  loss_mask_5: 0.1765  loss_dice_5: 0.179  loss_ce_6: 0.1486  loss_mask_6: 0.1824  loss_dice_6: 0.1784  loss_ce_7: 0.1483  loss_mask_7: 0.184  loss_dice_7: 0.1845  loss_ce_8: 0.1485  loss_mask_8: 0.1811  loss_dice_8: 0.1847  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:32:21] d2.utils.events INFO:  eta: 1:02:49  iter: 20159  total_loss: 4.749  loss_ce: 0.1283  loss_mask: 0.2177  loss_dice: 0.1355  loss_ce_0: 0.06332  loss_mask_0: 0.2215  loss_dice_0: 0.1371  loss_ce_1: 0.1283  loss_mask_1: 0.2113  loss_dice_1: 0.1379  loss_ce_2: 0.1283  loss_mask_2: 0.2185  loss_dice_2: 0.1415  loss_ce_3: 0.1284  loss_mask_3: 0.2166  loss_dice_3: 0.135  loss_ce_4: 0.1284  loss_mask_4: 0.211  loss_dice_4: 0.1395  loss_ce_5: 0.1283  loss_mask_5: 0.2178  loss_dice_5: 0.1298  loss_ce_6: 0.1284  loss_mask_6: 0.2149  loss_dice_6: 0.1388  loss_ce_7: 0.1284  loss_mask_7: 0.2132  loss_dice_7: 0.1369  loss_ce_8: 0.1283  loss_mask_8: 0.2171  loss_dice_8: 0.1396  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:32:23] d2.utils.events INFO:  eta: 1:02:44  iter: 20179  total_loss: 4.963  loss_ce: 0.1109  loss_mask: 0.2227  loss_dice: 0.1552  loss_ce_0: 0.05672  loss_mask_0: 0.2403  loss_dice_0: 0.163  loss_ce_1: 0.1112  loss_mask_1: 0.2335  loss_dice_1: 0.162  loss_ce_2: 0.1108  loss_mask_2: 0.2363  loss_dice_2: 0.1699  loss_ce_3: 0.1109  loss_mask_3: 0.2299  loss_dice_3: 0.1572  loss_ce_4: 0.111  loss_mask_4: 0.2331  loss_dice_4: 0.1598  loss_ce_5: 0.1108  loss_mask_5: 0.2383  loss_dice_5: 0.1546  loss_ce_6: 0.111  loss_mask_6: 0.2182  loss_dice_6: 0.162  loss_ce_7: 0.1111  loss_mask_7: 0.2321  loss_dice_7: 0.155  loss_ce_8: 0.111  loss_mask_8: 0.2226  loss_dice_8: 0.1593  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:32:26] d2.utils.events INFO:  eta: 1:02:39  iter: 20199  total_loss: 5.245  loss_ce: 0.1081  loss_mask: 0.2081  loss_dice: 0.1718  loss_ce_0: 0.0558  loss_mask_0: 0.2061  loss_dice_0: 0.1857  loss_ce_1: 0.108  loss_mask_1: 0.2059  loss_dice_1: 0.1682  loss_ce_2: 0.108  loss_mask_2: 0.2103  loss_dice_2: 0.1762  loss_ce_3: 0.108  loss_mask_3: 0.2094  loss_dice_3: 0.1768  loss_ce_4: 0.108  loss_mask_4: 0.21  loss_dice_4: 0.1769  loss_ce_5: 0.108  loss_mask_5: 0.207  loss_dice_5: 0.1647  loss_ce_6: 0.1079  loss_mask_6: 0.2048  loss_dice_6: 0.1711  loss_ce_7: 0.1078  loss_mask_7: 0.2089  loss_dice_7: 0.1729  loss_ce_8: 0.1082  loss_mask_8: 0.2085  loss_dice_8: 0.1797  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:32:28] d2.utils.events INFO:  eta: 1:02:36  iter: 20219  total_loss: 4.817  loss_ce: 0.1457  loss_mask: 0.1646  loss_dice: 0.1641  loss_ce_0: 0.06977  loss_mask_0: 0.1693  loss_dice_0: 0.1652  loss_ce_1: 0.1449  loss_mask_1: 0.167  loss_dice_1: 0.1595  loss_ce_2: 0.1457  loss_mask_2: 0.1704  loss_dice_2: 0.1585  loss_ce_3: 0.1455  loss_mask_3: 0.1652  loss_dice_3: 0.1599  loss_ce_4: 0.1454  loss_mask_4: 0.1733  loss_dice_4: 0.1555  loss_ce_5: 0.1456  loss_mask_5: 0.1723  loss_dice_5: 0.1642  loss_ce_6: 0.1456  loss_mask_6: 0.1654  loss_dice_6: 0.1599  loss_ce_7: 0.1457  loss_mask_7: 0.1639  loss_dice_7: 0.1535  loss_ce_8: 0.1456  loss_mask_8: 0.1627  loss_dice_8: 0.1504  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:32:31] d2.utils.events INFO:  eta: 1:02:32  iter: 20239  total_loss: 5.005  loss_ce: 0.1194  loss_mask: 0.1683  loss_dice: 0.1455  loss_ce_0: 0.05972  loss_mask_0: 0.1703  loss_dice_0: 0.1416  loss_ce_1: 0.12  loss_mask_1: 0.1679  loss_dice_1: 0.1403  loss_ce_2: 0.1196  loss_mask_2: 0.1654  loss_dice_2: 0.1449  loss_ce_3: 0.12  loss_mask_3: 0.1681  loss_dice_3: 0.1491  loss_ce_4: 0.1203  loss_mask_4: 0.1751  loss_dice_4: 0.1462  loss_ce_5: 0.1199  loss_mask_5: 0.1727  loss_dice_5: 0.1438  loss_ce_6: 0.1202  loss_mask_6: 0.1767  loss_dice_6: 0.1451  loss_ce_7: 0.1202  loss_mask_7: 0.1861  loss_dice_7: 0.1462  loss_ce_8: 0.1195  loss_mask_8: 0.1761  loss_dice_8: 0.1453  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:32:33] d2.utils.events INFO:  eta: 1:02:30  iter: 20259  total_loss: 5.547  loss_ce: 0.1123  loss_mask: 0.1886  loss_dice: 0.2315  loss_ce_0: 0.05698  loss_mask_0: 0.1807  loss_dice_0: 0.222  loss_ce_1: 0.112  loss_mask_1: 0.1801  loss_dice_1: 0.2219  loss_ce_2: 0.1116  loss_mask_2: 0.194  loss_dice_2: 0.2266  loss_ce_3: 0.1114  loss_mask_3: 0.1875  loss_dice_3: 0.2194  loss_ce_4: 0.1114  loss_mask_4: 0.1989  loss_dice_4: 0.2186  loss_ce_5: 0.1118  loss_mask_5: 0.1781  loss_dice_5: 0.2155  loss_ce_6: 0.112  loss_mask_6: 0.1786  loss_dice_6: 0.2218  loss_ce_7: 0.1122  loss_mask_7: 0.1859  loss_dice_7: 0.2191  loss_ce_8: 0.1125  loss_mask_8: 0.1881  loss_dice_8: 0.226  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:36] d2.utils.events INFO:  eta: 1:02:29  iter: 20279  total_loss: 4.892  loss_ce: 0.1064  loss_mask: 0.1749  loss_dice: 0.171  loss_ce_0: 0.0551  loss_mask_0: 0.171  loss_dice_0: 0.176  loss_ce_1: 0.1059  loss_mask_1: 0.1653  loss_dice_1: 0.167  loss_ce_2: 0.1064  loss_mask_2: 0.1691  loss_dice_2: 0.1657  loss_ce_3: 0.106  loss_mask_3: 0.1736  loss_dice_3: 0.1673  loss_ce_4: 0.1059  loss_mask_4: 0.1715  loss_dice_4: 0.1775  loss_ce_5: 0.1061  loss_mask_5: 0.1771  loss_dice_5: 0.1707  loss_ce_6: 0.1059  loss_mask_6: 0.1791  loss_dice_6: 0.1671  loss_ce_7: 0.1058  loss_mask_7: 0.1758  loss_dice_7: 0.1727  loss_ce_8: 0.1064  loss_mask_8: 0.1809  loss_dice_8: 0.1726  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:32:38] d2.utils.events INFO:  eta: 1:02:27  iter: 20299  total_loss: 5.026  loss_ce: 0.1617  loss_mask: 0.1996  loss_dice: 0.1614  loss_ce_0: 0.07531  loss_mask_0: 0.2027  loss_dice_0: 0.1643  loss_ce_1: 0.1629  loss_mask_1: 0.2066  loss_dice_1: 0.16  loss_ce_2: 0.162  loss_mask_2: 0.1999  loss_dice_2: 0.1677  loss_ce_3: 0.1624  loss_mask_3: 0.2042  loss_dice_3: 0.1659  loss_ce_4: 0.1629  loss_mask_4: 0.2014  loss_dice_4: 0.1631  loss_ce_5: 0.1625  loss_mask_5: 0.2038  loss_dice_5: 0.1614  loss_ce_6: 0.163  loss_mask_6: 0.2005  loss_dice_6: 0.1637  loss_ce_7: 0.163  loss_mask_7: 0.1988  loss_dice_7: 0.1621  loss_ce_8: 0.1619  loss_mask_8: 0.2046  loss_dice_8: 0.1652  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:41] d2.utils.events INFO:  eta: 1:02:24  iter: 20319  total_loss: 5.061  loss_ce: 0.1025  loss_mask: 0.2241  loss_dice: 0.1428  loss_ce_0: 0.05363  loss_mask_0: 0.2182  loss_dice_0: 0.1528  loss_ce_1: 0.1022  loss_mask_1: 0.2199  loss_dice_1: 0.1487  loss_ce_2: 0.1026  loss_mask_2: 0.2346  loss_dice_2: 0.1566  loss_ce_3: 0.1024  loss_mask_3: 0.2199  loss_dice_3: 0.1467  loss_ce_4: 0.1026  loss_mask_4: 0.2278  loss_dice_4: 0.1524  loss_ce_5: 0.1029  loss_mask_5: 0.2236  loss_dice_5: 0.1504  loss_ce_6: 0.1027  loss_mask_6: 0.2237  loss_dice_6: 0.1472  loss_ce_7: 0.1026  loss_mask_7: 0.2151  loss_dice_7: 0.1449  loss_ce_8: 0.1027  loss_mask_8: 0.2247  loss_dice_8: 0.1462  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:44] d2.utils.events INFO:  eta: 1:02:21  iter: 20339  total_loss: 5.631  loss_ce: 0.09252  loss_mask: 0.1859  loss_dice: 0.1874  loss_ce_0: 0.0503  loss_mask_0: 0.1879  loss_dice_0: 0.1908  loss_ce_1: 0.09192  loss_mask_1: 0.1875  loss_dice_1: 0.183  loss_ce_2: 0.09267  loss_mask_2: 0.1851  loss_dice_2: 0.1794  loss_ce_3: 0.09235  loss_mask_3: 0.1937  loss_dice_3: 0.1913  loss_ce_4: 0.09206  loss_mask_4: 0.1918  loss_dice_4: 0.1826  loss_ce_5: 0.09214  loss_mask_5: 0.1936  loss_dice_5: 0.1881  loss_ce_6: 0.09164  loss_mask_6: 0.1831  loss_dice_6: 0.188  loss_ce_7: 0.09157  loss_mask_7: 0.1827  loss_dice_7: 0.191  loss_ce_8: 0.09237  loss_mask_8: 0.193  loss_dice_8: 0.1842  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:46] d2.utils.events INFO:  eta: 1:02:17  iter: 20359  total_loss: 5.337  loss_ce: 0.1688  loss_mask: 0.2029  loss_dice: 0.1671  loss_ce_0: 0.07888  loss_mask_0: 0.1992  loss_dice_0: 0.1676  loss_ce_1: 0.1702  loss_mask_1: 0.2096  loss_dice_1: 0.1688  loss_ce_2: 0.1697  loss_mask_2: 0.1993  loss_dice_2: 0.1646  loss_ce_3: 0.1705  loss_mask_3: 0.205  loss_dice_3: 0.1688  loss_ce_4: 0.1705  loss_mask_4: 0.1999  loss_dice_4: 0.1723  loss_ce_5: 0.1696  loss_mask_5: 0.2022  loss_dice_5: 0.1723  loss_ce_6: 0.1696  loss_mask_6: 0.1999  loss_dice_6: 0.167  loss_ce_7: 0.1691  loss_mask_7: 0.1983  loss_dice_7: 0.1661  loss_ce_8: 0.1686  loss_mask_8: 0.2079  loss_dice_8: 0.1713  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:49] d2.utils.events INFO:  eta: 1:02:16  iter: 20379  total_loss: 5.5  loss_ce: 0.1014  loss_mask: 0.1859  loss_dice: 0.2334  loss_ce_0: 0.05227  loss_mask_0: 0.1816  loss_dice_0: 0.2331  loss_ce_1: 0.1023  loss_mask_1: 0.1797  loss_dice_1: 0.2271  loss_ce_2: 0.1012  loss_mask_2: 0.1813  loss_dice_2: 0.2389  loss_ce_3: 0.1007  loss_mask_3: 0.1895  loss_dice_3: 0.2369  loss_ce_4: 0.1008  loss_mask_4: 0.1836  loss_dice_4: 0.2392  loss_ce_5: 0.1015  loss_mask_5: 0.1808  loss_dice_5: 0.2281  loss_ce_6: 0.1018  loss_mask_6: 0.1875  loss_dice_6: 0.2375  loss_ce_7: 0.1022  loss_mask_7: 0.1882  loss_dice_7: 0.236  loss_ce_8: 0.1018  loss_mask_8: 0.1874  loss_dice_8: 0.2372  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:51] d2.utils.events INFO:  eta: 1:02:17  iter: 20399  total_loss: 5.401  loss_ce: 0.09622  loss_mask: 0.1873  loss_dice: 0.2159  loss_ce_0: 0.05064  loss_mask_0: 0.1823  loss_dice_0: 0.2117  loss_ce_1: 0.09668  loss_mask_1: 0.2034  loss_dice_1: 0.2046  loss_ce_2: 0.0962  loss_mask_2: 0.2004  loss_dice_2: 0.2055  loss_ce_3: 0.09573  loss_mask_3: 0.2044  loss_dice_3: 0.2044  loss_ce_4: 0.09569  loss_mask_4: 0.1988  loss_dice_4: 0.2063  loss_ce_5: 0.09595  loss_mask_5: 0.1916  loss_dice_5: 0.2069  loss_ce_6: 0.09562  loss_mask_6: 0.1888  loss_dice_6: 0.2029  loss_ce_7: 0.09591  loss_mask_7: 0.1988  loss_dice_7: 0.1963  loss_ce_8: 0.09597  loss_mask_8: 0.1998  loss_dice_8: 0.2013  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:54] d2.utils.events INFO:  eta: 1:02:16  iter: 20419  total_loss: 4.835  loss_ce: 0.087  loss_mask: 0.143  loss_dice: 0.2095  loss_ce_0: 0.04774  loss_mask_0: 0.1524  loss_dice_0: 0.2126  loss_ce_1: 0.08625  loss_mask_1: 0.1476  loss_dice_1: 0.2066  loss_ce_2: 0.08666  loss_mask_2: 0.1595  loss_dice_2: 0.2177  loss_ce_3: 0.08644  loss_mask_3: 0.1536  loss_dice_3: 0.2057  loss_ce_4: 0.08632  loss_mask_4: 0.1517  loss_dice_4: 0.2107  loss_ce_5: 0.08636  loss_mask_5: 0.1497  loss_dice_5: 0.2132  loss_ce_6: 0.08591  loss_mask_6: 0.1482  loss_dice_6: 0.2098  loss_ce_7: 0.08608  loss_mask_7: 0.1519  loss_dice_7: 0.2163  loss_ce_8: 0.08646  loss_mask_8: 0.1527  loss_dice_8: 0.2017  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:56] d2.utils.events INFO:  eta: 1:02:13  iter: 20439  total_loss: 5.059  loss_ce: 0.07704  loss_mask: 0.2124  loss_dice: 0.1642  loss_ce_0: 0.04504  loss_mask_0: 0.2041  loss_dice_0: 0.1626  loss_ce_1: 0.07637  loss_mask_1: 0.2009  loss_dice_1: 0.1616  loss_ce_2: 0.07717  loss_mask_2: 0.2057  loss_dice_2: 0.1656  loss_ce_3: 0.07701  loss_mask_3: 0.2014  loss_dice_3: 0.1616  loss_ce_4: 0.07687  loss_mask_4: 0.2025  loss_dice_4: 0.1645  loss_ce_5: 0.07674  loss_mask_5: 0.1974  loss_dice_5: 0.1644  loss_ce_6: 0.07573  loss_mask_6: 0.2037  loss_dice_6: 0.16  loss_ce_7: 0.0757  loss_mask_7: 0.1961  loss_dice_7: 0.1576  loss_ce_8: 0.07638  loss_mask_8: 0.2051  loss_dice_8: 0.1544  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:32:59] d2.utils.events INFO:  eta: 1:02:13  iter: 20459  total_loss: 5.24  loss_ce: 0.1846  loss_mask: 0.2209  loss_dice: 0.155  loss_ce_0: 0.08561  loss_mask_0: 0.2214  loss_dice_0: 0.1521  loss_ce_1: 0.1856  loss_mask_1: 0.2244  loss_dice_1: 0.1543  loss_ce_2: 0.1858  loss_mask_2: 0.2227  loss_dice_2: 0.1509  loss_ce_3: 0.1866  loss_mask_3: 0.2219  loss_dice_3: 0.154  loss_ce_4: 0.1862  loss_mask_4: 0.2325  loss_dice_4: 0.1571  loss_ce_5: 0.1852  loss_mask_5: 0.2231  loss_dice_5: 0.1511  loss_ce_6: 0.1844  loss_mask_6: 0.2246  loss_dice_6: 0.1546  loss_ce_7: 0.1839  loss_mask_7: 0.2206  loss_dice_7: 0.1465  loss_ce_8: 0.184  loss_mask_8: 0.2166  loss_dice_8: 0.1519  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:01] d2.utils.events INFO:  eta: 1:02:13  iter: 20479  total_loss: 4.967  loss_ce: 0.1302  loss_mask: 0.1956  loss_dice: 0.143  loss_ce_0: 0.06441  loss_mask_0: 0.1921  loss_dice_0: 0.1361  loss_ce_1: 0.1299  loss_mask_1: 0.1928  loss_dice_1: 0.1393  loss_ce_2: 0.13  loss_mask_2: 0.1988  loss_dice_2: 0.1424  loss_ce_3: 0.1301  loss_mask_3: 0.1952  loss_dice_3: 0.1428  loss_ce_4: 0.13  loss_mask_4: 0.1947  loss_dice_4: 0.1411  loss_ce_5: 0.1301  loss_mask_5: 0.2063  loss_dice_5: 0.1334  loss_ce_6: 0.1301  loss_mask_6: 0.1971  loss_dice_6: 0.141  loss_ce_7: 0.1301  loss_mask_7: 0.2  loss_dice_7: 0.144  loss_ce_8: 0.1301  loss_mask_8: 0.209  loss_dice_8: 0.1355  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:04] d2.utils.events INFO:  eta: 1:02:13  iter: 20499  total_loss: 4.7  loss_ce: 0.1295  loss_mask: 0.1747  loss_dice: 0.1355  loss_ce_0: 0.06405  loss_mask_0: 0.1814  loss_dice_0: 0.1369  loss_ce_1: 0.1291  loss_mask_1: 0.1743  loss_dice_1: 0.1395  loss_ce_2: 0.1294  loss_mask_2: 0.1847  loss_dice_2: 0.1402  loss_ce_3: 0.1293  loss_mask_3: 0.1683  loss_dice_3: 0.1379  loss_ce_4: 0.1293  loss_mask_4: 0.1816  loss_dice_4: 0.1436  loss_ce_5: 0.1293  loss_mask_5: 0.1889  loss_dice_5: 0.142  loss_ce_6: 0.1293  loss_mask_6: 0.1908  loss_dice_6: 0.138  loss_ce_7: 0.1293  loss_mask_7: 0.1771  loss_dice_7: 0.1351  loss_ce_8: 0.1294  loss_mask_8: 0.184  loss_dice_8: 0.1369  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:06] d2.utils.events INFO:  eta: 1:02:10  iter: 20519  total_loss: 5.19  loss_ce: 0.1422  loss_mask: 0.2313  loss_dice: 0.1299  loss_ce_0: 0.0696  loss_mask_0: 0.2322  loss_dice_0: 0.1305  loss_ce_1: 0.1397  loss_mask_1: 0.2329  loss_dice_1: 0.1315  loss_ce_2: 0.1415  loss_mask_2: 0.2293  loss_dice_2: 0.1306  loss_ce_3: 0.1397  loss_mask_3: 0.2331  loss_dice_3: 0.1341  loss_ce_4: 0.1387  loss_mask_4: 0.2335  loss_dice_4: 0.1317  loss_ce_5: 0.1396  loss_mask_5: 0.2428  loss_dice_5: 0.1312  loss_ce_6: 0.1382  loss_mask_6: 0.2288  loss_dice_6: 0.1268  loss_ce_7: 0.1383  loss_mask_7: 0.2289  loss_dice_7: 0.1293  loss_ce_8: 0.1405  loss_mask_8: 0.2297  loss_dice_8: 0.1308  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:09] d2.utils.events INFO:  eta: 1:02:05  iter: 20539  total_loss: 4.942  loss_ce: 0.1278  loss_mask: 0.1897  loss_dice: 0.1688  loss_ce_0: 0.06329  loss_mask_0: 0.1952  loss_dice_0: 0.176  loss_ce_1: 0.128  loss_mask_1: 0.1902  loss_dice_1: 0.1763  loss_ce_2: 0.128  loss_mask_2: 0.2  loss_dice_2: 0.1722  loss_ce_3: 0.1293  loss_mask_3: 0.1958  loss_dice_3: 0.181  loss_ce_4: 0.1293  loss_mask_4: 0.1938  loss_dice_4: 0.1759  loss_ce_5: 0.1288  loss_mask_5: 0.1974  loss_dice_5: 0.1632  loss_ce_6: 0.1292  loss_mask_6: 0.1844  loss_dice_6: 0.1761  loss_ce_7: 0.1292  loss_mask_7: 0.198  loss_dice_7: 0.1826  loss_ce_8: 0.1284  loss_mask_8: 0.1893  loss_dice_8: 0.1768  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:12] d2.utils.events INFO:  eta: 1:02:04  iter: 20559  total_loss: 4.953  loss_ce: 0.1272  loss_mask: 0.206  loss_dice: 0.1391  loss_ce_0: 0.06302  loss_mask_0: 0.2079  loss_dice_0: 0.1427  loss_ce_1: 0.1274  loss_mask_1: 0.1979  loss_dice_1: 0.1498  loss_ce_2: 0.1272  loss_mask_2: 0.2039  loss_dice_2: 0.1388  loss_ce_3: 0.1271  loss_mask_3: 0.1975  loss_dice_3: 0.1434  loss_ce_4: 0.1271  loss_mask_4: 0.2015  loss_dice_4: 0.1456  loss_ce_5: 0.1271  loss_mask_5: 0.2034  loss_dice_5: 0.1436  loss_ce_6: 0.1271  loss_mask_6: 0.2074  loss_dice_6: 0.145  loss_ce_7: 0.1272  loss_mask_7: 0.201  loss_dice_7: 0.1404  loss_ce_8: 0.1273  loss_mask_8: 0.2063  loss_dice_8: 0.1363  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:14] d2.utils.events INFO:  eta: 1:02:03  iter: 20579  total_loss: 5.38  loss_ce: 0.1231  loss_mask: 0.2107  loss_dice: 0.1768  loss_ce_0: 0.06058  loss_mask_0: 0.2179  loss_dice_0: 0.1772  loss_ce_1: 0.1227  loss_mask_1: 0.2247  loss_dice_1: 0.1773  loss_ce_2: 0.1218  loss_mask_2: 0.2187  loss_dice_2: 0.1773  loss_ce_3: 0.1213  loss_mask_3: 0.2137  loss_dice_3: 0.177  loss_ce_4: 0.1205  loss_mask_4: 0.2256  loss_dice_4: 0.1681  loss_ce_5: 0.1212  loss_mask_5: 0.2136  loss_dice_5: 0.1773  loss_ce_6: 0.1218  loss_mask_6: 0.2208  loss_dice_6: 0.1723  loss_ce_7: 0.1215  loss_mask_7: 0.228  loss_dice_7: 0.1793  loss_ce_8: 0.1227  loss_mask_8: 0.2249  loss_dice_8: 0.1721  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:17] d2.utils.events INFO:  eta: 1:01:57  iter: 20599  total_loss: 5.462  loss_ce: 0.1151  loss_mask: 0.1722  loss_dice: 0.2146  loss_ce_0: 0.05807  loss_mask_0: 0.1661  loss_dice_0: 0.2211  loss_ce_1: 0.114  loss_mask_1: 0.1683  loss_dice_1: 0.2185  loss_ce_2: 0.1144  loss_mask_2: 0.1716  loss_dice_2: 0.2172  loss_ce_3: 0.1138  loss_mask_3: 0.1785  loss_dice_3: 0.2265  loss_ce_4: 0.1131  loss_mask_4: 0.1737  loss_dice_4: 0.2143  loss_ce_5: 0.114  loss_mask_5: 0.1624  loss_dice_5: 0.2273  loss_ce_6: 0.1134  loss_mask_6: 0.1668  loss_dice_6: 0.2197  loss_ce_7: 0.1129  loss_mask_7: 0.178  loss_dice_7: 0.2163  loss_ce_8: 0.1141  loss_mask_8: 0.174  loss_dice_8: 0.2193  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:19] d2.utils.events INFO:  eta: 1:01:54  iter: 20619  total_loss: 5.695  loss_ce: 0.1086  loss_mask: 0.2551  loss_dice: 0.1893  loss_ce_0: 0.05573  loss_mask_0: 0.2515  loss_dice_0: 0.1819  loss_ce_1: 0.1067  loss_mask_1: 0.2401  loss_dice_1: 0.1824  loss_ce_2: 0.1077  loss_mask_2: 0.2497  loss_dice_2: 0.1801  loss_ce_3: 0.1072  loss_mask_3: 0.2546  loss_dice_3: 0.1828  loss_ce_4: 0.1062  loss_mask_4: 0.2502  loss_dice_4: 0.1792  loss_ce_5: 0.1071  loss_mask_5: 0.2583  loss_dice_5: 0.1867  loss_ce_6: 0.1064  loss_mask_6: 0.2441  loss_dice_6: 0.184  loss_ce_7: 0.106  loss_mask_7: 0.2513  loss_dice_7: 0.184  loss_ce_8: 0.1075  loss_mask_8: 0.2626  loss_dice_8: 0.1791  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:22] d2.utils.events INFO:  eta: 1:01:48  iter: 20639  total_loss: 4.865  loss_ce: 0.1004  loss_mask: 0.1747  loss_dice: 0.1356  loss_ce_0: 0.05247  loss_mask_0: 0.1779  loss_dice_0: 0.1341  loss_ce_1: 0.09831  loss_mask_1: 0.1855  loss_dice_1: 0.1371  loss_ce_2: 0.09921  loss_mask_2: 0.1722  loss_dice_2: 0.1383  loss_ce_3: 0.09805  loss_mask_3: 0.1782  loss_dice_3: 0.1361  loss_ce_4: 0.09641  loss_mask_4: 0.1753  loss_dice_4: 0.1285  loss_ce_5: 0.09755  loss_mask_5: 0.184  loss_dice_5: 0.1256  loss_ce_6: 0.0965  loss_mask_6: 0.1823  loss_dice_6: 0.1286  loss_ce_7: 0.0964  loss_mask_7: 0.1812  loss_dice_7: 0.1282  loss_ce_8: 0.09891  loss_mask_8: 0.1713  loss_dice_8: 0.1317  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:24] d2.utils.events INFO:  eta: 1:01:48  iter: 20659  total_loss: 5.093  loss_ce: 0.1295  loss_mask: 0.2018  loss_dice: 0.1242  loss_ce_0: 0.06392  loss_mask_0: 0.2011  loss_dice_0: 0.1252  loss_ce_1: 0.1291  loss_mask_1: 0.2074  loss_dice_1: 0.1251  loss_ce_2: 0.1294  loss_mask_2: 0.2036  loss_dice_2: 0.1267  loss_ce_3: 0.1293  loss_mask_3: 0.1993  loss_dice_3: 0.1222  loss_ce_4: 0.1291  loss_mask_4: 0.2133  loss_dice_4: 0.1229  loss_ce_5: 0.1292  loss_mask_5: 0.2038  loss_dice_5: 0.1211  loss_ce_6: 0.1292  loss_mask_6: 0.2014  loss_dice_6: 0.1236  loss_ce_7: 0.1291  loss_mask_7: 0.1984  loss_dice_7: 0.1238  loss_ce_8: 0.1294  loss_mask_8: 0.1994  loss_dice_8: 0.125  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:27] d2.utils.events INFO:  eta: 1:01:46  iter: 20679  total_loss: 5.077  loss_ce: 0.1291  loss_mask: 0.1865  loss_dice: 0.1568  loss_ce_0: 0.06387  loss_mask_0: 0.1881  loss_dice_0: 0.156  loss_ce_1: 0.1292  loss_mask_1: 0.209  loss_dice_1: 0.1611  loss_ce_2: 0.1292  loss_mask_2: 0.1981  loss_dice_2: 0.163  loss_ce_3: 0.1292  loss_mask_3: 0.2023  loss_dice_3: 0.1552  loss_ce_4: 0.1293  loss_mask_4: 0.2059  loss_dice_4: 0.1672  loss_ce_5: 0.1292  loss_mask_5: 0.1985  loss_dice_5: 0.1637  loss_ce_6: 0.1291  loss_mask_6: 0.1987  loss_dice_6: 0.161  loss_ce_7: 0.1293  loss_mask_7: 0.2029  loss_dice_7: 0.1456  loss_ce_8: 0.1292  loss_mask_8: 0.1937  loss_dice_8: 0.1526  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:29] d2.utils.events INFO:  eta: 1:01:41  iter: 20699  total_loss: 5.415  loss_ce: 0.1448  loss_mask: 0.1894  loss_dice: 0.1567  loss_ce_0: 0.06892  loss_mask_0: 0.1922  loss_dice_0: 0.1552  loss_ce_1: 0.1417  loss_mask_1: 0.1786  loss_dice_1: 0.1615  loss_ce_2: 0.1428  loss_mask_2: 0.1818  loss_dice_2: 0.1493  loss_ce_3: 0.1413  loss_mask_3: 0.1859  loss_dice_3: 0.1554  loss_ce_4: 0.1394  loss_mask_4: 0.1847  loss_dice_4: 0.1543  loss_ce_5: 0.141  loss_mask_5: 0.1884  loss_dice_5: 0.1566  loss_ce_6: 0.1401  loss_mask_6: 0.1951  loss_dice_6: 0.1622  loss_ce_7: 0.1396  loss_mask_7: 0.1846  loss_dice_7: 0.1493  loss_ce_8: 0.1428  loss_mask_8: 0.1941  loss_dice_8: 0.149  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:32] d2.utils.events INFO:  eta: 1:01:38  iter: 20719  total_loss: 5.222  loss_ce: 0.1194  loss_mask: 0.2571  loss_dice: 0.1579  loss_ce_0: 0.06066  loss_mask_0: 0.2567  loss_dice_0: 0.1519  loss_ce_1: 0.1217  loss_mask_1: 0.2551  loss_dice_1: 0.1557  loss_ce_2: 0.1203  loss_mask_2: 0.2484  loss_dice_2: 0.1518  loss_ce_3: 0.1212  loss_mask_3: 0.2646  loss_dice_3: 0.1516  loss_ce_4: 0.1221  loss_mask_4: 0.2591  loss_dice_4: 0.1524  loss_ce_5: 0.121  loss_mask_5: 0.2609  loss_dice_5: 0.1542  loss_ce_6: 0.1223  loss_mask_6: 0.2634  loss_dice_6: 0.1494  loss_ce_7: 0.1225  loss_mask_7: 0.2571  loss_dice_7: 0.1507  loss_ce_8: 0.1208  loss_mask_8: 0.2585  loss_dice_8: 0.1478  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:34] d2.utils.events INFO:  eta: 1:01:35  iter: 20739  total_loss: 5.169  loss_ce: 0.1187  loss_mask: 0.2088  loss_dice: 0.1752  loss_ce_0: 0.05995  loss_mask_0: 0.21  loss_dice_0: 0.1661  loss_ce_1: 0.1202  loss_mask_1: 0.1976  loss_dice_1: 0.1635  loss_ce_2: 0.1189  loss_mask_2: 0.2051  loss_dice_2: 0.1683  loss_ce_3: 0.1193  loss_mask_3: 0.2115  loss_dice_3: 0.1738  loss_ce_4: 0.1193  loss_mask_4: 0.1972  loss_dice_4: 0.1695  loss_ce_5: 0.1189  loss_mask_5: 0.2104  loss_dice_5: 0.1611  loss_ce_6: 0.1201  loss_mask_6: 0.21  loss_dice_6: 0.1704  loss_ce_7: 0.1203  loss_mask_7: 0.202  loss_dice_7: 0.1633  loss_ce_8: 0.1197  loss_mask_8: 0.2156  loss_dice_8: 0.1679  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:37] d2.utils.events INFO:  eta: 1:01:34  iter: 20759  total_loss: 5.255  loss_ce: 0.1158  loss_mask: 0.2253  loss_dice: 0.1613  loss_ce_0: 0.05883  loss_mask_0: 0.2209  loss_dice_0: 0.1564  loss_ce_1: 0.1165  loss_mask_1: 0.221  loss_dice_1: 0.158  loss_ce_2: 0.1161  loss_mask_2: 0.2174  loss_dice_2: 0.1626  loss_ce_3: 0.1164  loss_mask_3: 0.2171  loss_dice_3: 0.1581  loss_ce_4: 0.1162  loss_mask_4: 0.2225  loss_dice_4: 0.1607  loss_ce_5: 0.1163  loss_mask_5: 0.2245  loss_dice_5: 0.154  loss_ce_6: 0.116  loss_mask_6: 0.2149  loss_dice_6: 0.154  loss_ce_7: 0.1157  loss_mask_7: 0.2137  loss_dice_7: 0.151  loss_ce_8: 0.1157  loss_mask_8: 0.2212  loss_dice_8: 0.1588  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:39] d2.utils.events INFO:  eta: 1:01:30  iter: 20779  total_loss: 5.09  loss_ce: 0.1277  loss_mask: 0.1655  loss_dice: 0.1914  loss_ce_0: 0.06332  loss_mask_0: 0.1638  loss_dice_0: 0.1884  loss_ce_1: 0.1272  loss_mask_1: 0.165  loss_dice_1: 0.1806  loss_ce_2: 0.1276  loss_mask_2: 0.1657  loss_dice_2: 0.1803  loss_ce_3: 0.1276  loss_mask_3: 0.1668  loss_dice_3: 0.1866  loss_ce_4: 0.1277  loss_mask_4: 0.1642  loss_dice_4: 0.192  loss_ce_5: 0.1276  loss_mask_5: 0.1719  loss_dice_5: 0.1763  loss_ce_6: 0.1276  loss_mask_6: 0.1542  loss_dice_6: 0.1844  loss_ce_7: 0.1275  loss_mask_7: 0.1693  loss_dice_7: 0.1878  loss_ce_8: 0.1276  loss_mask_8: 0.1743  loss_dice_8: 0.1715  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:42] d2.utils.events INFO:  eta: 1:01:23  iter: 20799  total_loss: 5.558  loss_ce: 0.1275  loss_mask: 0.233  loss_dice: 0.1933  loss_ce_0: 0.06323  loss_mask_0: 0.2286  loss_dice_0: 0.1937  loss_ce_1: 0.1279  loss_mask_1: 0.2317  loss_dice_1: 0.1898  loss_ce_2: 0.1275  loss_mask_2: 0.2307  loss_dice_2: 0.2056  loss_ce_3: 0.1275  loss_mask_3: 0.2373  loss_dice_3: 0.1997  loss_ce_4: 0.1275  loss_mask_4: 0.23  loss_dice_4: 0.1949  loss_ce_5: 0.1275  loss_mask_5: 0.2301  loss_dice_5: 0.1952  loss_ce_6: 0.1275  loss_mask_6: 0.2359  loss_dice_6: 0.2061  loss_ce_7: 0.1275  loss_mask_7: 0.2289  loss_dice_7: 0.1982  loss_ce_8: 0.1275  loss_mask_8: 0.2281  loss_dice_8: 0.2042  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:44] d2.utils.events INFO:  eta: 1:01:18  iter: 20819  total_loss: 5.214  loss_ce: 0.1177  loss_mask: 0.1867  loss_dice: 0.1594  loss_ce_0: 0.05972  loss_mask_0: 0.1788  loss_dice_0: 0.1631  loss_ce_1: 0.1182  loss_mask_1: 0.1962  loss_dice_1: 0.1669  loss_ce_2: 0.1178  loss_mask_2: 0.1922  loss_dice_2: 0.1722  loss_ce_3: 0.1182  loss_mask_3: 0.183  loss_dice_3: 0.1656  loss_ce_4: 0.1182  loss_mask_4: 0.1932  loss_dice_4: 0.1678  loss_ce_5: 0.118  loss_mask_5: 0.1854  loss_dice_5: 0.1673  loss_ce_6: 0.1181  loss_mask_6: 0.1847  loss_dice_6: 0.1626  loss_ce_7: 0.118  loss_mask_7: 0.187  loss_dice_7: 0.1639  loss_ce_8: 0.1179  loss_mask_8: 0.1787  loss_dice_8: 0.165  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:33:47] d2.utils.events INFO:  eta: 1:01:17  iter: 20839  total_loss: 4.804  loss_ce: 0.1368  loss_mask: 0.1547  loss_dice: 0.1534  loss_ce_0: 0.06619  loss_mask_0: 0.1599  loss_dice_0: 0.1495  loss_ce_1: 0.1367  loss_mask_1: 0.1505  loss_dice_1: 0.154  loss_ce_2: 0.1357  loss_mask_2: 0.1528  loss_dice_2: 0.1519  loss_ce_3: 0.135  loss_mask_3: 0.159  loss_dice_3: 0.1568  loss_ce_4: 0.1348  loss_mask_4: 0.1535  loss_dice_4: 0.1502  loss_ce_5: 0.135  loss_mask_5: 0.1527  loss_dice_5: 0.1483  loss_ce_6: 0.1351  loss_mask_6: 0.1534  loss_dice_6: 0.1515  loss_ce_7: 0.1354  loss_mask_7: 0.1562  loss_dice_7: 0.1511  loss_ce_8: 0.1362  loss_mask_8: 0.1583  loss_dice_8: 0.154  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:49] d2.utils.events INFO:  eta: 1:01:14  iter: 20859  total_loss: 4.786  loss_ce: 0.1275  loss_mask: 0.1926  loss_dice: 0.1284  loss_ce_0: 0.06313  loss_mask_0: 0.1811  loss_dice_0: 0.1195  loss_ce_1: 0.1275  loss_mask_1: 0.1982  loss_dice_1: 0.1323  loss_ce_2: 0.1274  loss_mask_2: 0.1953  loss_dice_2: 0.1305  loss_ce_3: 0.1273  loss_mask_3: 0.1975  loss_dice_3: 0.1339  loss_ce_4: 0.1275  loss_mask_4: 0.1951  loss_dice_4: 0.1292  loss_ce_5: 0.1274  loss_mask_5: 0.186  loss_dice_5: 0.1297  loss_ce_6: 0.1274  loss_mask_6: 0.1897  loss_dice_6: 0.1319  loss_ce_7: 0.1275  loss_mask_7: 0.1838  loss_dice_7: 0.127  loss_ce_8: 0.1274  loss_mask_8: 0.1905  loss_dice_8: 0.1305  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:52] d2.utils.events INFO:  eta: 1:01:10  iter: 20879  total_loss: 4.522  loss_ce: 0.1222  loss_mask: 0.2045  loss_dice: 0.138  loss_ce_0: 0.06146  loss_mask_0: 0.2074  loss_dice_0: 0.1405  loss_ce_1: 0.1222  loss_mask_1: 0.2141  loss_dice_1: 0.1332  loss_ce_2: 0.1223  loss_mask_2: 0.2032  loss_dice_2: 0.1391  loss_ce_3: 0.1225  loss_mask_3: 0.2058  loss_dice_3: 0.1337  loss_ce_4: 0.1227  loss_mask_4: 0.1993  loss_dice_4: 0.1286  loss_ce_5: 0.1225  loss_mask_5: 0.2035  loss_dice_5: 0.1348  loss_ce_6: 0.123  loss_mask_6: 0.2118  loss_dice_6: 0.1324  loss_ce_7: 0.1229  loss_mask_7: 0.2121  loss_dice_7: 0.1369  loss_ce_8: 0.1227  loss_mask_8: 0.2077  loss_dice_8: 0.1347  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:54] d2.utils.events INFO:  eta: 1:01:08  iter: 20899  total_loss: 4.651  loss_ce: 0.1352  loss_mask: 0.1998  loss_dice: 0.1207  loss_ce_0: 0.06604  loss_mask_0: 0.1953  loss_dice_0: 0.1231  loss_ce_1: 0.1353  loss_mask_1: 0.1967  loss_dice_1: 0.1213  loss_ce_2: 0.1352  loss_mask_2: 0.1992  loss_dice_2: 0.1204  loss_ce_3: 0.1352  loss_mask_3: 0.2026  loss_dice_3: 0.1215  loss_ce_4: 0.1353  loss_mask_4: 0.2013  loss_dice_4: 0.1255  loss_ce_5: 0.1354  loss_mask_5: 0.1976  loss_dice_5: 0.1192  loss_ce_6: 0.1352  loss_mask_6: 0.1987  loss_dice_6: 0.1222  loss_ce_7: 0.1354  loss_mask_7: 0.1946  loss_dice_7: 0.1264  loss_ce_8: 0.1352  loss_mask_8: 0.1966  loss_dice_8: 0.116  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:57] d2.utils.events INFO:  eta: 1:01:06  iter: 20919  total_loss: 4.803  loss_ce: 0.1276  loss_mask: 0.1938  loss_dice: 0.1553  loss_ce_0: 0.06319  loss_mask_0: 0.199  loss_dice_0: 0.1519  loss_ce_1: 0.1271  loss_mask_1: 0.1976  loss_dice_1: 0.1551  loss_ce_2: 0.1276  loss_mask_2: 0.1958  loss_dice_2: 0.1552  loss_ce_3: 0.1276  loss_mask_3: 0.2039  loss_dice_3: 0.1596  loss_ce_4: 0.1276  loss_mask_4: 0.1993  loss_dice_4: 0.1453  loss_ce_5: 0.1276  loss_mask_5: 0.2037  loss_dice_5: 0.1491  loss_ce_6: 0.1276  loss_mask_6: 0.1945  loss_dice_6: 0.1536  loss_ce_7: 0.1275  loss_mask_7: 0.1965  loss_dice_7: 0.1539  loss_ce_8: 0.1276  loss_mask_8: 0.1951  loss_dice_8: 0.1529  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:33:59] d2.utils.events INFO:  eta: 1:01:06  iter: 20939  total_loss: 4.438  loss_ce: 0.1257  loss_mask: 0.1694  loss_dice: 0.1326  loss_ce_0: 0.06283  loss_mask_0: 0.1842  loss_dice_0: 0.1415  loss_ce_1: 0.1263  loss_mask_1: 0.1792  loss_dice_1: 0.144  loss_ce_2: 0.1259  loss_mask_2: 0.1726  loss_dice_2: 0.1317  loss_ce_3: 0.1265  loss_mask_3: 0.1764  loss_dice_3: 0.1411  loss_ce_4: 0.1266  loss_mask_4: 0.1658  loss_dice_4: 0.1374  loss_ce_5: 0.1263  loss_mask_5: 0.1759  loss_dice_5: 0.1391  loss_ce_6: 0.1263  loss_mask_6: 0.181  loss_dice_6: 0.1405  loss_ce_7: 0.1263  loss_mask_7: 0.167  loss_dice_7: 0.1362  loss_ce_8: 0.1259  loss_mask_8: 0.1711  loss_dice_8: 0.1375  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:02] d2.utils.events INFO:  eta: 1:01:04  iter: 20959  total_loss: 4.926  loss_ce: 0.1241  loss_mask: 0.2107  loss_dice: 0.1492  loss_ce_0: 0.06201  loss_mask_0: 0.2078  loss_dice_0: 0.145  loss_ce_1: 0.1249  loss_mask_1: 0.1994  loss_dice_1: 0.148  loss_ce_2: 0.1239  loss_mask_2: 0.2034  loss_dice_2: 0.151  loss_ce_3: 0.1241  loss_mask_3: 0.2052  loss_dice_3: 0.1585  loss_ce_4: 0.124  loss_mask_4: 0.207  loss_dice_4: 0.1514  loss_ce_5: 0.1239  loss_mask_5: 0.2076  loss_dice_5: 0.147  loss_ce_6: 0.1242  loss_mask_6: 0.202  loss_dice_6: 0.15  loss_ce_7: 0.1243  loss_mask_7: 0.2099  loss_dice_7: 0.1476  loss_ce_8: 0.1244  loss_mask_8: 0.2001  loss_dice_8: 0.1556  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:04] d2.utils.events INFO:  eta: 1:01:01  iter: 20979  total_loss: 4.914  loss_ce: 0.1219  loss_mask: 0.1668  loss_dice: 0.163  loss_ce_0: 0.06118  loss_mask_0: 0.1791  loss_dice_0: 0.1667  loss_ce_1: 0.1226  loss_mask_1: 0.169  loss_dice_1: 0.1723  loss_ce_2: 0.122  loss_mask_2: 0.177  loss_dice_2: 0.1613  loss_ce_3: 0.122  loss_mask_3: 0.1739  loss_dice_3: 0.1743  loss_ce_4: 0.1218  loss_mask_4: 0.1778  loss_dice_4: 0.1625  loss_ce_5: 0.1218  loss_mask_5: 0.1823  loss_dice_5: 0.1654  loss_ce_6: 0.1218  loss_mask_6: 0.1711  loss_dice_6: 0.1674  loss_ce_7: 0.1217  loss_mask_7: 0.1879  loss_dice_7: 0.1642  loss_ce_8: 0.1219  loss_mask_8: 0.1771  loss_dice_8: 0.1677  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:07] d2.utils.events INFO:  eta: 1:01:00  iter: 20999  total_loss: 5.026  loss_ce: 0.1276  loss_mask: 0.1866  loss_dice: 0.1562  loss_ce_0: 0.06317  loss_mask_0: 0.1798  loss_dice_0: 0.1649  loss_ce_1: 0.1273  loss_mask_1: 0.1828  loss_dice_1: 0.1561  loss_ce_2: 0.1276  loss_mask_2: 0.1794  loss_dice_2: 0.1607  loss_ce_3: 0.1276  loss_mask_3: 0.1725  loss_dice_3: 0.162  loss_ce_4: 0.1277  loss_mask_4: 0.1873  loss_dice_4: 0.1607  loss_ce_5: 0.1277  loss_mask_5: 0.1779  loss_dice_5: 0.1548  loss_ce_6: 0.1276  loss_mask_6: 0.1834  loss_dice_6: 0.1585  loss_ce_7: 0.1276  loss_mask_7: 0.173  loss_dice_7: 0.1594  loss_ce_8: 0.1276  loss_mask_8: 0.1868  loss_dice_8: 0.1645  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:10] d2.utils.events INFO:  eta: 1:00:58  iter: 21019  total_loss: 4.823  loss_ce: 0.1338  loss_mask: 0.1463  loss_dice: 0.1925  loss_ce_0: 0.06516  loss_mask_0: 0.1553  loss_dice_0: 0.1717  loss_ce_1: 0.1333  loss_mask_1: 0.148  loss_dice_1: 0.1924  loss_ce_2: 0.1333  loss_mask_2: 0.1545  loss_dice_2: 0.1888  loss_ce_3: 0.1331  loss_mask_3: 0.1533  loss_dice_3: 0.1862  loss_ce_4: 0.133  loss_mask_4: 0.1539  loss_dice_4: 0.1854  loss_ce_5: 0.1333  loss_mask_5: 0.1526  loss_dice_5: 0.1906  loss_ce_6: 0.1334  loss_mask_6: 0.1569  loss_dice_6: 0.1895  loss_ce_7: 0.1335  loss_mask_7: 0.1529  loss_dice_7: 0.1848  loss_ce_8: 0.1337  loss_mask_8: 0.147  loss_dice_8: 0.1848  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:12] d2.utils.events INFO:  eta: 1:00:54  iter: 21039  total_loss: 4.466  loss_ce: 0.1266  loss_mask: 0.1749  loss_dice: 0.1763  loss_ce_0: 0.0627  loss_mask_0: 0.1654  loss_dice_0: 0.1688  loss_ce_1: 0.1265  loss_mask_1: 0.1621  loss_dice_1: 0.1719  loss_ce_2: 0.1265  loss_mask_2: 0.1695  loss_dice_2: 0.1789  loss_ce_3: 0.1265  loss_mask_3: 0.1727  loss_dice_3: 0.1801  loss_ce_4: 0.1265  loss_mask_4: 0.1617  loss_dice_4: 0.1707  loss_ce_5: 0.1264  loss_mask_5: 0.1667  loss_dice_5: 0.1729  loss_ce_6: 0.1264  loss_mask_6: 0.1667  loss_dice_6: 0.1688  loss_ce_7: 0.1263  loss_mask_7: 0.1671  loss_dice_7: 0.1699  loss_ce_8: 0.1267  loss_mask_8: 0.1673  loss_dice_8: 0.1713  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:15] d2.utils.events INFO:  eta: 1:00:53  iter: 21059  total_loss: 5.007  loss_ce: 0.136  loss_mask: 0.1789  loss_dice: 0.2181  loss_ce_0: 0.06813  loss_mask_0: 0.1793  loss_dice_0: 0.1917  loss_ce_1: 0.1366  loss_mask_1: 0.1757  loss_dice_1: 0.2091  loss_ce_2: 0.1365  loss_mask_2: 0.1705  loss_dice_2: 0.2108  loss_ce_3: 0.1369  loss_mask_3: 0.1796  loss_dice_3: 0.2039  loss_ce_4: 0.137  loss_mask_4: 0.1772  loss_dice_4: 0.2038  loss_ce_5: 0.1374  loss_mask_5: 0.1748  loss_dice_5: 0.2007  loss_ce_6: 0.1373  loss_mask_6: 0.1702  loss_dice_6: 0.2087  loss_ce_7: 0.1371  loss_mask_7: 0.1696  loss_dice_7: 0.2098  loss_ce_8: 0.1366  loss_mask_8: 0.1696  loss_dice_8: 0.2092  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:17] d2.utils.events INFO:  eta: 1:00:51  iter: 21079  total_loss: 5.122  loss_ce: 0.1326  loss_mask: 0.2107  loss_dice: 0.194  loss_ce_0: 0.06483  loss_mask_0: 0.2131  loss_dice_0: 0.1892  loss_ce_1: 0.1282  loss_mask_1: 0.2001  loss_dice_1: 0.1929  loss_ce_2: 0.1287  loss_mask_2: 0.2208  loss_dice_2: 0.1951  loss_ce_3: 0.1286  loss_mask_3: 0.207  loss_dice_3: 0.1879  loss_ce_4: 0.1297  loss_mask_4: 0.2019  loss_dice_4: 0.197  loss_ce_5: 0.1295  loss_mask_5: 0.2093  loss_dice_5: 0.1979  loss_ce_6: 0.1293  loss_mask_6: 0.2136  loss_dice_6: 0.1933  loss_ce_7: 0.1284  loss_mask_7: 0.2025  loss_dice_7: 0.1985  loss_ce_8: 0.1297  loss_mask_8: 0.2083  loss_dice_8: 0.1961  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:20] d2.utils.events INFO:  eta: 1:00:48  iter: 21099  total_loss: 4.679  loss_ce: 0.1285  loss_mask: 0.2031  loss_dice: 0.1331  loss_ce_0: 0.06327  loss_mask_0: 0.1942  loss_dice_0: 0.1288  loss_ce_1: 0.1276  loss_mask_1: 0.2017  loss_dice_1: 0.1242  loss_ce_2: 0.1284  loss_mask_2: 0.2063  loss_dice_2: 0.1272  loss_ce_3: 0.1284  loss_mask_3: 0.2056  loss_dice_3: 0.1337  loss_ce_4: 0.1293  loss_mask_4: 0.1963  loss_dice_4: 0.1228  loss_ce_5: 0.129  loss_mask_5: 0.1965  loss_dice_5: 0.1354  loss_ce_6: 0.129  loss_mask_6: 0.2104  loss_dice_6: 0.1299  loss_ce_7: 0.1288  loss_mask_7: 0.1951  loss_dice_7: 0.1347  loss_ce_8: 0.1288  loss_mask_8: 0.2035  loss_dice_8: 0.1285  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:22] d2.utils.events INFO:  eta: 1:00:46  iter: 21119  total_loss: 4.621  loss_ce: 0.1248  loss_mask: 0.2  loss_dice: 0.1126  loss_ce_0: 0.06131  loss_mask_0: 0.1971  loss_dice_0: 0.1156  loss_ce_1: 0.1237  loss_mask_1: 0.2036  loss_dice_1: 0.1114  loss_ce_2: 0.1221  loss_mask_2: 0.2039  loss_dice_2: 0.1112  loss_ce_3: 0.1223  loss_mask_3: 0.1958  loss_dice_3: 0.1134  loss_ce_4: 0.1208  loss_mask_4: 0.2051  loss_dice_4: 0.114  loss_ce_5: 0.1211  loss_mask_5: 0.2051  loss_dice_5: 0.1149  loss_ce_6: 0.1209  loss_mask_6: 0.2011  loss_dice_6: 0.1113  loss_ce_7: 0.1219  loss_mask_7: 0.2046  loss_dice_7: 0.1123  loss_ce_8: 0.1222  loss_mask_8: 0.2051  loss_dice_8: 0.1109  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:25] d2.utils.events INFO:  eta: 1:00:44  iter: 21139  total_loss: 5.275  loss_ce: 0.1285  loss_mask: 0.2269  loss_dice: 0.1526  loss_ce_0: 0.06328  loss_mask_0: 0.225  loss_dice_0: 0.1574  loss_ce_1: 0.1287  loss_mask_1: 0.2261  loss_dice_1: 0.1547  loss_ce_2: 0.1292  loss_mask_2: 0.2193  loss_dice_2: 0.1509  loss_ce_3: 0.1293  loss_mask_3: 0.2227  loss_dice_3: 0.1584  loss_ce_4: 0.1291  loss_mask_4: 0.2334  loss_dice_4: 0.1544  loss_ce_5: 0.1292  loss_mask_5: 0.2328  loss_dice_5: 0.1534  loss_ce_6: 0.1294  loss_mask_6: 0.226  loss_dice_6: 0.157  loss_ce_7: 0.1294  loss_mask_7: 0.23  loss_dice_7: 0.1529  loss_ce_8: 0.1291  loss_mask_8: 0.2245  loss_dice_8: 0.1596  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:27] d2.utils.events INFO:  eta: 1:00:40  iter: 21159  total_loss: 5.551  loss_ce: 0.1162  loss_mask: 0.217  loss_dice: 0.1792  loss_ce_0: 0.05858  loss_mask_0: 0.2159  loss_dice_0: 0.1717  loss_ce_1: 0.1175  loss_mask_1: 0.2145  loss_dice_1: 0.1727  loss_ce_2: 0.1162  loss_mask_2: 0.2168  loss_dice_2: 0.1738  loss_ce_3: 0.117  loss_mask_3: 0.2068  loss_dice_3: 0.1791  loss_ce_4: 0.1174  loss_mask_4: 0.2109  loss_dice_4: 0.1694  loss_ce_5: 0.1168  loss_mask_5: 0.2117  loss_dice_5: 0.1736  loss_ce_6: 0.1165  loss_mask_6: 0.2192  loss_dice_6: 0.1679  loss_ce_7: 0.1168  loss_mask_7: 0.2233  loss_dice_7: 0.1847  loss_ce_8: 0.1154  loss_mask_8: 0.2163  loss_dice_8: 0.1731  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:30] d2.utils.events INFO:  eta: 1:00:39  iter: 21179  total_loss: 5.684  loss_ce: 0.128  loss_mask: 0.2023  loss_dice: 0.1666  loss_ce_0: 0.06312  loss_mask_0: 0.2007  loss_dice_0: 0.1645  loss_ce_1: 0.1282  loss_mask_1: 0.2088  loss_dice_1: 0.1649  loss_ce_2: 0.1281  loss_mask_2: 0.2201  loss_dice_2: 0.1683  loss_ce_3: 0.128  loss_mask_3: 0.1982  loss_dice_3: 0.1659  loss_ce_4: 0.1281  loss_mask_4: 0.2026  loss_dice_4: 0.1572  loss_ce_5: 0.128  loss_mask_5: 0.2032  loss_dice_5: 0.1592  loss_ce_6: 0.1281  loss_mask_6: 0.2182  loss_dice_6: 0.1654  loss_ce_7: 0.1281  loss_mask_7: 0.2216  loss_dice_7: 0.1634  loss_ce_8: 0.1281  loss_mask_8: 0.2119  loss_dice_8: 0.1687  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:32] d2.utils.events INFO:  eta: 1:00:38  iter: 21199  total_loss: 5.849  loss_ce: 0.1142  loss_mask: 0.2288  loss_dice: 0.2111  loss_ce_0: 0.05748  loss_mask_0: 0.2267  loss_dice_0: 0.2037  loss_ce_1: 0.1141  loss_mask_1: 0.2337  loss_dice_1: 0.2082  loss_ce_2: 0.1128  loss_mask_2: 0.2212  loss_dice_2: 0.2016  loss_ce_3: 0.1129  loss_mask_3: 0.2381  loss_dice_3: 0.2108  loss_ce_4: 0.1123  loss_mask_4: 0.2306  loss_dice_4: 0.2057  loss_ce_5: 0.1124  loss_mask_5: 0.2318  loss_dice_5: 0.2072  loss_ce_6: 0.1126  loss_mask_6: 0.2296  loss_dice_6: 0.2036  loss_ce_7: 0.1133  loss_mask_7: 0.2278  loss_dice_7: 0.2118  loss_ce_8: 0.1135  loss_mask_8: 0.2288  loss_dice_8: 0.2093  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:35] d2.utils.events INFO:  eta: 1:00:37  iter: 21219  total_loss: 5.4  loss_ce: 0.1042  loss_mask: 0.2017  loss_dice: 0.1907  loss_ce_0: 0.05419  loss_mask_0: 0.1895  loss_dice_0: 0.1932  loss_ce_1: 0.1005  loss_mask_1: 0.2034  loss_dice_1: 0.1993  loss_ce_2: 0.1008  loss_mask_2: 0.196  loss_dice_2: 0.1911  loss_ce_3: 0.09968  loss_mask_3: 0.1927  loss_dice_3: 0.1947  loss_ce_4: 0.09762  loss_mask_4: 0.1941  loss_dice_4: 0.1931  loss_ce_5: 0.09864  loss_mask_5: 0.1943  loss_dice_5: 0.1895  loss_ce_6: 0.09827  loss_mask_6: 0.1975  loss_dice_6: 0.1952  loss_ce_7: 0.09945  loss_mask_7: 0.1865  loss_dice_7: 0.1886  loss_ce_8: 0.1015  loss_mask_8: 0.1912  loss_dice_8: 0.1862  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:37] d2.utils.events INFO:  eta: 1:00:37  iter: 21239  total_loss: 5.38  loss_ce: 0.09728  loss_mask: 0.2039  loss_dice: 0.1922  loss_ce_0: 0.05215  loss_mask_0: 0.2117  loss_dice_0: 0.1869  loss_ce_1: 0.09559  loss_mask_1: 0.2123  loss_dice_1: 0.1922  loss_ce_2: 0.09561  loss_mask_2: 0.1969  loss_dice_2: 0.1919  loss_ce_3: 0.09519  loss_mask_3: 0.2109  loss_dice_3: 0.1891  loss_ce_4: 0.09414  loss_mask_4: 0.2038  loss_dice_4: 0.1868  loss_ce_5: 0.0945  loss_mask_5: 0.1996  loss_dice_5: 0.1914  loss_ce_6: 0.09345  loss_mask_6: 0.1976  loss_dice_6: 0.1827  loss_ce_7: 0.09414  loss_mask_7: 0.2047  loss_dice_7: 0.1818  loss_ce_8: 0.09473  loss_mask_8: 0.1944  loss_dice_8: 0.1848  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:40] d2.utils.events INFO:  eta: 1:00:35  iter: 21259  total_loss: 4.861  loss_ce: 0.1313  loss_mask: 0.2103  loss_dice: 0.1446  loss_ce_0: 0.06404  loss_mask_0: 0.2045  loss_dice_0: 0.1449  loss_ce_1: 0.1309  loss_mask_1: 0.2049  loss_dice_1: 0.1423  loss_ce_2: 0.1314  loss_mask_2: 0.2087  loss_dice_2: 0.1454  loss_ce_3: 0.1314  loss_mask_3: 0.2077  loss_dice_3: 0.1423  loss_ce_4: 0.1314  loss_mask_4: 0.2001  loss_dice_4: 0.1411  loss_ce_5: 0.1314  loss_mask_5: 0.2074  loss_dice_5: 0.1442  loss_ce_6: 0.1315  loss_mask_6: 0.2104  loss_dice_6: 0.1456  loss_ce_7: 0.1314  loss_mask_7: 0.2059  loss_dice_7: 0.1441  loss_ce_8: 0.1314  loss_mask_8: 0.202  loss_dice_8: 0.1451  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:43] d2.utils.events INFO:  eta: 1:00:31  iter: 21279  total_loss: 4.839  loss_ce: 0.1025  loss_mask: 0.1903  loss_dice: 0.1506  loss_ce_0: 0.05284  loss_mask_0: 0.1981  loss_dice_0: 0.1524  loss_ce_1: 0.1057  loss_mask_1: 0.1927  loss_dice_1: 0.1573  loss_ce_2: 0.1038  loss_mask_2: 0.1963  loss_dice_2: 0.1606  loss_ce_3: 0.1046  loss_mask_3: 0.1883  loss_dice_3: 0.153  loss_ce_4: 0.106  loss_mask_4: 0.1867  loss_dice_4: 0.1526  loss_ce_5: 0.105  loss_mask_5: 0.1908  loss_dice_5: 0.1503  loss_ce_6: 0.1054  loss_mask_6: 0.1949  loss_dice_6: 0.1548  loss_ce_7: 0.1054  loss_mask_7: 0.1981  loss_dice_7: 0.1505  loss_ce_8: 0.1038  loss_mask_8: 0.2016  loss_dice_8: 0.1514  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:45] d2.utils.events INFO:  eta: 1:00:29  iter: 21299  total_loss: 4.678  loss_ce: 0.151  loss_mask: 0.1764  loss_dice: 0.1549  loss_ce_0: 0.07313  loss_mask_0: 0.18  loss_dice_0: 0.1591  loss_ce_1: 0.1478  loss_mask_1: 0.175  loss_dice_1: 0.155  loss_ce_2: 0.1496  loss_mask_2: 0.1843  loss_dice_2: 0.1518  loss_ce_3: 0.1487  loss_mask_3: 0.1828  loss_dice_3: 0.156  loss_ce_4: 0.1474  loss_mask_4: 0.1777  loss_dice_4: 0.1525  loss_ce_5: 0.1485  loss_mask_5: 0.1829  loss_dice_5: 0.1521  loss_ce_6: 0.1477  loss_mask_6: 0.1762  loss_dice_6: 0.1558  loss_ce_7: 0.1478  loss_mask_7: 0.1829  loss_dice_7: 0.1548  loss_ce_8: 0.1492  loss_mask_8: 0.1803  loss_dice_8: 0.1519  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:34:48] d2.utils.events INFO:  eta: 1:00:26  iter: 21319  total_loss: 5.306  loss_ce: 0.1368  loss_mask: 0.1481  loss_dice: 0.2122  loss_ce_0: 0.06776  loss_mask_0: 0.1547  loss_dice_0: 0.2099  loss_ce_1: 0.1332  loss_mask_1: 0.1454  loss_dice_1: 0.2067  loss_ce_2: 0.1343  loss_mask_2: 0.1606  loss_dice_2: 0.2167  loss_ce_3: 0.1325  loss_mask_3: 0.1546  loss_dice_3: 0.2102  loss_ce_4: 0.1309  loss_mask_4: 0.1591  loss_dice_4: 0.2211  loss_ce_5: 0.1324  loss_mask_5: 0.1551  loss_dice_5: 0.2202  loss_ce_6: 0.1314  loss_mask_6: 0.1478  loss_dice_6: 0.2153  loss_ce_7: 0.1323  loss_mask_7: 0.1587  loss_dice_7: 0.2127  loss_ce_8: 0.1344  loss_mask_8: 0.1537  loss_dice_8: 0.2095  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:50] d2.utils.events INFO:  eta: 1:00:23  iter: 21339  total_loss: 5.486  loss_ce: 0.1237  loss_mask: 0.2138  loss_dice: 0.1611  loss_ce_0: 0.0603  loss_mask_0: 0.2197  loss_dice_0: 0.1654  loss_ce_1: 0.1254  loss_mask_1: 0.209  loss_dice_1: 0.1657  loss_ce_2: 0.1249  loss_mask_2: 0.2286  loss_dice_2: 0.1603  loss_ce_3: 0.1261  loss_mask_3: 0.2139  loss_dice_3: 0.1571  loss_ce_4: 0.1272  loss_mask_4: 0.2279  loss_dice_4: 0.1637  loss_ce_5: 0.1261  loss_mask_5: 0.2157  loss_dice_5: 0.1612  loss_ce_6: 0.1274  loss_mask_6: 0.2221  loss_dice_6: 0.1603  loss_ce_7: 0.1263  loss_mask_7: 0.2279  loss_dice_7: 0.1623  loss_ce_8: 0.1256  loss_mask_8: 0.2124  loss_dice_8: 0.1597  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:53] d2.utils.events INFO:  eta: 1:00:21  iter: 21359  total_loss: 5.011  loss_ce: 0.1273  loss_mask: 0.1839  loss_dice: 0.1661  loss_ce_0: 0.06297  loss_mask_0: 0.1917  loss_dice_0: 0.1683  loss_ce_1: 0.1277  loss_mask_1: 0.1877  loss_dice_1: 0.1743  loss_ce_2: 0.1273  loss_mask_2: 0.193  loss_dice_2: 0.1675  loss_ce_3: 0.1273  loss_mask_3: 0.1775  loss_dice_3: 0.1653  loss_ce_4: 0.1273  loss_mask_4: 0.1944  loss_dice_4: 0.177  loss_ce_5: 0.1274  loss_mask_5: 0.1867  loss_dice_5: 0.166  loss_ce_6: 0.1274  loss_mask_6: 0.1815  loss_dice_6: 0.1669  loss_ce_7: 0.1273  loss_mask_7: 0.1939  loss_dice_7: 0.1712  loss_ce_8: 0.1273  loss_mask_8: 0.1851  loss_dice_8: 0.1683  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:55] d2.utils.events INFO:  eta: 1:00:17  iter: 21379  total_loss: 4.739  loss_ce: 0.127  loss_mask: 0.1722  loss_dice: 0.183  loss_ce_0: 0.06325  loss_mask_0: 0.1834  loss_dice_0: 0.1842  loss_ce_1: 0.1276  loss_mask_1: 0.181  loss_dice_1: 0.18  loss_ce_2: 0.1273  loss_mask_2: 0.1742  loss_dice_2: 0.186  loss_ce_3: 0.1273  loss_mask_3: 0.1802  loss_dice_3: 0.189  loss_ce_4: 0.1274  loss_mask_4: 0.1781  loss_dice_4: 0.1897  loss_ce_5: 0.1272  loss_mask_5: 0.1715  loss_dice_5: 0.1952  loss_ce_6: 0.1275  loss_mask_6: 0.1876  loss_dice_6: 0.1789  loss_ce_7: 0.1274  loss_mask_7: 0.1767  loss_dice_7: 0.1842  loss_ce_8: 0.1273  loss_mask_8: 0.1784  loss_dice_8: 0.1881  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:34:58] d2.utils.events INFO:  eta: 1:00:14  iter: 21399  total_loss: 5.392  loss_ce: 0.1219  loss_mask: 0.2012  loss_dice: 0.1399  loss_ce_0: 0.06144  loss_mask_0: 0.1992  loss_dice_0: 0.1397  loss_ce_1: 0.1198  loss_mask_1: 0.2046  loss_dice_1: 0.1402  loss_ce_2: 0.1205  loss_mask_2: 0.2001  loss_dice_2: 0.142  loss_ce_3: 0.1193  loss_mask_3: 0.2011  loss_dice_3: 0.1404  loss_ce_4: 0.1184  loss_mask_4: 0.2009  loss_dice_4: 0.1449  loss_ce_5: 0.1196  loss_mask_5: 0.2073  loss_dice_5: 0.1361  loss_ce_6: 0.1191  loss_mask_6: 0.1993  loss_dice_6: 0.141  loss_ce_7: 0.12  loss_mask_7: 0.1997  loss_dice_7: 0.1393  loss_ce_8: 0.1209  loss_mask_8: 0.1988  loss_dice_8: 0.1407  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:00] d2.utils.events INFO:  eta: 1:00:11  iter: 21419  total_loss: 5.713  loss_ce: 0.1515  loss_mask: 0.2019  loss_dice: 0.2174  loss_ce_0: 0.07074  loss_mask_0: 0.2283  loss_dice_0: 0.2094  loss_ce_1: 0.1516  loss_mask_1: 0.21  loss_dice_1: 0.218  loss_ce_2: 0.1516  loss_mask_2: 0.2153  loss_dice_2: 0.2295  loss_ce_3: 0.1524  loss_mask_3: 0.2168  loss_dice_3: 0.2253  loss_ce_4: 0.1528  loss_mask_4: 0.205  loss_dice_4: 0.2214  loss_ce_5: 0.1522  loss_mask_5: 0.2197  loss_dice_5: 0.2278  loss_ce_6: 0.1542  loss_mask_6: 0.2102  loss_dice_6: 0.2297  loss_ce_7: 0.1533  loss_mask_7: 0.2152  loss_dice_7: 0.2244  loss_ce_8: 0.1525  loss_mask_8: 0.2136  loss_dice_8: 0.2202  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:03] d2.utils.events INFO:  eta: 1:00:07  iter: 21439  total_loss: 5.515  loss_ce: 0.128  loss_mask: 0.2148  loss_dice: 0.2219  loss_ce_0: 0.06378  loss_mask_0: 0.2155  loss_dice_0: 0.2319  loss_ce_1: 0.1272  loss_mask_1: 0.2151  loss_dice_1: 0.2192  loss_ce_2: 0.1278  loss_mask_2: 0.2126  loss_dice_2: 0.2274  loss_ce_3: 0.1279  loss_mask_3: 0.2198  loss_dice_3: 0.2309  loss_ce_4: 0.1278  loss_mask_4: 0.2222  loss_dice_4: 0.2267  loss_ce_5: 0.128  loss_mask_5: 0.213  loss_dice_5: 0.2265  loss_ce_6: 0.128  loss_mask_6: 0.2119  loss_dice_6: 0.2203  loss_ce_7: 0.128  loss_mask_7: 0.2127  loss_dice_7: 0.2228  loss_ce_8: 0.128  loss_mask_8: 0.2145  loss_dice_8: 0.2272  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:05] d2.utils.events INFO:  eta: 1:00:04  iter: 21459  total_loss: 5.092  loss_ce: 0.128  loss_mask: 0.2138  loss_dice: 0.188  loss_ce_0: 0.06378  loss_mask_0: 0.205  loss_dice_0: 0.1793  loss_ce_1: 0.1239  loss_mask_1: 0.2129  loss_dice_1: 0.182  loss_ce_2: 0.1279  loss_mask_2: 0.2029  loss_dice_2: 0.1901  loss_ce_3: 0.1279  loss_mask_3: 0.205  loss_dice_3: 0.1813  loss_ce_4: 0.1272  loss_mask_4: 0.2148  loss_dice_4: 0.1838  loss_ce_5: 0.1279  loss_mask_5: 0.2101  loss_dice_5: 0.1889  loss_ce_6: 0.1281  loss_mask_6: 0.2198  loss_dice_6: 0.1875  loss_ce_7: 0.127  loss_mask_7: 0.2058  loss_dice_7: 0.1881  loss_ce_8: 0.1279  loss_mask_8: 0.2089  loss_dice_8: 0.1898  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:35:08] d2.utils.events INFO:  eta: 1:00:01  iter: 21479  total_loss: 5.036  loss_ce: 0.1281  loss_mask: 0.1817  loss_dice: 0.1876  loss_ce_0: 0.0632  loss_mask_0: 0.1784  loss_dice_0: 0.1867  loss_ce_1: 0.1371  loss_mask_1: 0.1796  loss_dice_1: 0.1927  loss_ce_2: 0.1284  loss_mask_2: 0.1771  loss_dice_2: 0.1899  loss_ce_3: 0.1282  loss_mask_3: 0.1795  loss_dice_3: 0.1856  loss_ce_4: 0.1301  loss_mask_4: 0.1881  loss_dice_4: 0.1861  loss_ce_5: 0.1282  loss_mask_5: 0.1798  loss_dice_5: 0.1889  loss_ce_6: 0.1283  loss_mask_6: 0.191  loss_dice_6: 0.1871  loss_ce_7: 0.1294  loss_mask_7: 0.1847  loss_dice_7: 0.1891  loss_ce_8: 0.1282  loss_mask_8: 0.1812  loss_dice_8: 0.1902  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:10] d2.utils.events INFO:  eta: 0:59:59  iter: 21499  total_loss: 4.685  loss_ce: 0.1184  loss_mask: 0.1813  loss_dice: 0.1433  loss_ce_0: 0.0598  loss_mask_0: 0.1825  loss_dice_0: 0.1463  loss_ce_1: 0.1221  loss_mask_1: 0.1893  loss_dice_1: 0.1419  loss_ce_2: 0.1198  loss_mask_2: 0.1876  loss_dice_2: 0.1467  loss_ce_3: 0.1204  loss_mask_3: 0.1826  loss_dice_3: 0.1461  loss_ce_4: 0.1215  loss_mask_4: 0.1914  loss_dice_4: 0.1439  loss_ce_5: 0.1211  loss_mask_5: 0.1975  loss_dice_5: 0.1438  loss_ce_6: 0.1205  loss_mask_6: 0.1796  loss_dice_6: 0.1367  loss_ce_7: 0.1204  loss_mask_7: 0.1837  loss_dice_7: 0.1428  loss_ce_8: 0.1187  loss_mask_8: 0.1772  loss_dice_8: 0.148  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:13] d2.utils.events INFO:  eta: 0:59:55  iter: 21519  total_loss: 5.407  loss_ce: 0.1444  loss_mask: 0.2268  loss_dice: 0.1554  loss_ce_0: 0.06992  loss_mask_0: 0.2272  loss_dice_0: 0.1522  loss_ce_1: 0.1468  loss_mask_1: 0.2269  loss_dice_1: 0.156  loss_ce_2: 0.1472  loss_mask_2: 0.2231  loss_dice_2: 0.1557  loss_ce_3: 0.1482  loss_mask_3: 0.2209  loss_dice_3: 0.1524  loss_ce_4: 0.1493  loss_mask_4: 0.2237  loss_dice_4: 0.1556  loss_ce_5: 0.1487  loss_mask_5: 0.2244  loss_dice_5: 0.1505  loss_ce_6: 0.1481  loss_mask_6: 0.2278  loss_dice_6: 0.162  loss_ce_7: 0.1468  loss_mask_7: 0.2276  loss_dice_7: 0.1505  loss_ce_8: 0.1457  loss_mask_8: 0.2216  loss_dice_8: 0.1515  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:35:16] d2.utils.events INFO:  eta: 0:59:53  iter: 21539  total_loss: 4.957  loss_ce: 0.1286  loss_mask: 0.2065  loss_dice: 0.1512  loss_ce_0: 0.06318  loss_mask_0: 0.2069  loss_dice_0: 0.1605  loss_ce_1: 0.1279  loss_mask_1: 0.205  loss_dice_1: 0.1576  loss_ce_2: 0.1302  loss_mask_2: 0.213  loss_dice_2: 0.161  loss_ce_3: 0.1287  loss_mask_3: 0.215  loss_dice_3: 0.1627  loss_ce_4: 0.1287  loss_mask_4: 0.2076  loss_dice_4: 0.1568  loss_ce_5: 0.1296  loss_mask_5: 0.2093  loss_dice_5: 0.1578  loss_ce_6: 0.1288  loss_mask_6: 0.2139  loss_dice_6: 0.1614  loss_ce_7: 0.1288  loss_mask_7: 0.2095  loss_dice_7: 0.1625  loss_ce_8: 0.1294  loss_mask_8: 0.2069  loss_dice_8: 0.1602  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:18] d2.utils.events INFO:  eta: 0:59:50  iter: 21559  total_loss: 4.878  loss_ce: 0.1355  loss_mask: 0.213  loss_dice: 0.168  loss_ce_0: 0.06486  loss_mask_0: 0.2024  loss_dice_0: 0.1714  loss_ce_1: 0.1343  loss_mask_1: 0.2068  loss_dice_1: 0.1669  loss_ce_2: 0.1329  loss_mask_2: 0.2013  loss_dice_2: 0.1643  loss_ce_3: 0.1319  loss_mask_3: 0.2125  loss_dice_3: 0.1741  loss_ce_4: 0.13  loss_mask_4: 0.2094  loss_dice_4: 0.1694  loss_ce_5: 0.1308  loss_mask_5: 0.2053  loss_dice_5: 0.172  loss_ce_6: 0.1315  loss_mask_6: 0.2103  loss_dice_6: 0.1638  loss_ce_7: 0.1323  loss_mask_7: 0.2083  loss_dice_7: 0.1654  loss_ce_8: 0.1347  loss_mask_8: 0.2055  loss_dice_8: 0.178  time: 0.1261  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:35:21] d2.utils.events INFO:  eta: 0:59:47  iter: 21579  total_loss: 5.165  loss_ce: 0.1315  loss_mask: 0.2118  loss_dice: 0.171  loss_ce_0: 0.06388  loss_mask_0: 0.2034  loss_dice_0: 0.1705  loss_ce_1: 0.1305  loss_mask_1: 0.2082  loss_dice_1: 0.1692  loss_ce_2: 0.1305  loss_mask_2: 0.2191  loss_dice_2: 0.1744  loss_ce_3: 0.1303  loss_mask_3: 0.2233  loss_dice_3: 0.1774  loss_ce_4: 0.1286  loss_mask_4: 0.2153  loss_dice_4: 0.1707  loss_ce_5: 0.1294  loss_mask_5: 0.2171  loss_dice_5: 0.1733  loss_ce_6: 0.1292  loss_mask_6: 0.2068  loss_dice_6: 0.1734  loss_ce_7: 0.129  loss_mask_7: 0.2138  loss_dice_7: 0.1657  loss_ce_8: 0.1305  loss_mask_8: 0.2169  loss_dice_8: 0.1754  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:23] d2.utils.events INFO:  eta: 0:59:45  iter: 21599  total_loss: 5.054  loss_ce: 0.1276  loss_mask: 0.1985  loss_dice: 0.1619  loss_ce_0: 0.06295  loss_mask_0: 0.1936  loss_dice_0: 0.1662  loss_ce_1: 0.1259  loss_mask_1: 0.194  loss_dice_1: 0.1647  loss_ce_2: 0.1276  loss_mask_2: 0.1875  loss_dice_2: 0.1635  loss_ce_3: 0.1277  loss_mask_3: 0.1985  loss_dice_3: 0.1704  loss_ce_4: 0.1278  loss_mask_4: 0.1954  loss_dice_4: 0.1598  loss_ce_5: 0.1277  loss_mask_5: 0.1883  loss_dice_5: 0.1663  loss_ce_6: 0.1277  loss_mask_6: 0.1966  loss_dice_6: 0.1632  loss_ce_7: 0.1276  loss_mask_7: 0.1986  loss_dice_7: 0.1716  loss_ce_8: 0.1275  loss_mask_8: 0.194  loss_dice_8: 0.1664  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:26] d2.utils.events INFO:  eta: 0:59:44  iter: 21619  total_loss: 4.863  loss_ce: 0.1281  loss_mask: 0.2153  loss_dice: 0.1359  loss_ce_0: 0.06331  loss_mask_0: 0.2234  loss_dice_0: 0.1384  loss_ce_1: 0.128  loss_mask_1: 0.219  loss_dice_1: 0.141  loss_ce_2: 0.1284  loss_mask_2: 0.2175  loss_dice_2: 0.1367  loss_ce_3: 0.1287  loss_mask_3: 0.2158  loss_dice_3: 0.1397  loss_ce_4: 0.1289  loss_mask_4: 0.2133  loss_dice_4: 0.1392  loss_ce_5: 0.1288  loss_mask_5: 0.2153  loss_dice_5: 0.1396  loss_ce_6: 0.1286  loss_mask_6: 0.2057  loss_dice_6: 0.1388  loss_ce_7: 0.1285  loss_mask_7: 0.2131  loss_dice_7: 0.1391  loss_ce_8: 0.1285  loss_mask_8: 0.2174  loss_dice_8: 0.1397  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:28] d2.utils.events INFO:  eta: 0:59:44  iter: 21639  total_loss: 4.944  loss_ce: 0.1255  loss_mask: 0.2361  loss_dice: 0.1377  loss_ce_0: 0.06135  loss_mask_0: 0.2223  loss_dice_0: 0.1359  loss_ce_1: 0.1249  loss_mask_1: 0.2342  loss_dice_1: 0.1376  loss_ce_2: 0.1237  loss_mask_2: 0.2306  loss_dice_2: 0.1353  loss_ce_3: 0.1233  loss_mask_3: 0.2318  loss_dice_3: 0.1348  loss_ce_4: 0.1223  loss_mask_4: 0.2276  loss_dice_4: 0.1347  loss_ce_5: 0.1225  loss_mask_5: 0.2369  loss_dice_5: 0.1398  loss_ce_6: 0.1227  loss_mask_6: 0.2263  loss_dice_6: 0.1353  loss_ce_7: 0.1236  loss_mask_7: 0.2313  loss_dice_7: 0.1415  loss_ce_8: 0.1245  loss_mask_8: 0.2364  loss_dice_8: 0.1366  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:31] d2.utils.events INFO:  eta: 0:59:42  iter: 21659  total_loss: 4.703  loss_ce: 0.1208  loss_mask: 0.209  loss_dice: 0.1496  loss_ce_0: 0.0599  loss_mask_0: 0.205  loss_dice_0: 0.1457  loss_ce_1: 0.1192  loss_mask_1: 0.2052  loss_dice_1: 0.1457  loss_ce_2: 0.1189  loss_mask_2: 0.2009  loss_dice_2: 0.1481  loss_ce_3: 0.1189  loss_mask_3: 0.1992  loss_dice_3: 0.1432  loss_ce_4: 0.118  loss_mask_4: 0.2032  loss_dice_4: 0.1454  loss_ce_5: 0.118  loss_mask_5: 0.1976  loss_dice_5: 0.1383  loss_ce_6: 0.1179  loss_mask_6: 0.1972  loss_dice_6: 0.1477  loss_ce_7: 0.1187  loss_mask_7: 0.1982  loss_dice_7: 0.1496  loss_ce_8: 0.1193  loss_mask_8: 0.2044  loss_dice_8: 0.151  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:33] d2.utils.events INFO:  eta: 0:59:39  iter: 21679  total_loss: 4.946  loss_ce: 0.1286  loss_mask: 0.173  loss_dice: 0.1552  loss_ce_0: 0.06331  loss_mask_0: 0.1756  loss_dice_0: 0.1477  loss_ce_1: 0.1301  loss_mask_1: 0.1675  loss_dice_1: 0.1523  loss_ce_2: 0.1295  loss_mask_2: 0.1701  loss_dice_2: 0.1496  loss_ce_3: 0.1293  loss_mask_3: 0.1791  loss_dice_3: 0.1481  loss_ce_4: 0.1297  loss_mask_4: 0.1664  loss_dice_4: 0.1496  loss_ce_5: 0.1296  loss_mask_5: 0.1737  loss_dice_5: 0.1527  loss_ce_6: 0.1295  loss_mask_6: 0.1752  loss_dice_6: 0.1537  loss_ce_7: 0.1297  loss_mask_7: 0.1717  loss_dice_7: 0.1483  loss_ce_8: 0.1292  loss_mask_8: 0.1839  loss_dice_8: 0.1508  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:36] d2.utils.events INFO:  eta: 0:59:34  iter: 21699  total_loss: 5.236  loss_ce: 0.1211  loss_mask: 0.1774  loss_dice: 0.1612  loss_ce_0: 0.06303  loss_mask_0: 0.1819  loss_dice_0: 0.1649  loss_ce_1: 0.1175  loss_mask_1: 0.1847  loss_dice_1: 0.1583  loss_ce_2: 0.1161  loss_mask_2: 0.1772  loss_dice_2: 0.159  loss_ce_3: 0.1199  loss_mask_3: 0.1764  loss_dice_3: 0.1539  loss_ce_4: 0.1206  loss_mask_4: 0.1783  loss_dice_4: 0.1627  loss_ce_5: 0.1178  loss_mask_5: 0.1844  loss_dice_5: 0.1633  loss_ce_6: 0.1208  loss_mask_6: 0.1775  loss_dice_6: 0.1634  loss_ce_7: 0.1218  loss_mask_7: 0.1843  loss_dice_7: 0.1602  loss_ce_8: 0.1184  loss_mask_8: 0.18  loss_dice_8: 0.1535  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:38] d2.utils.events INFO:  eta: 0:59:32  iter: 21719  total_loss: 5.111  loss_ce: 0.1447  loss_mask: 0.1971  loss_dice: 0.1658  loss_ce_0: 0.06705  loss_mask_0: 0.1959  loss_dice_0: 0.1685  loss_ce_1: 0.1405  loss_mask_1: 0.2004  loss_dice_1: 0.1713  loss_ce_2: 0.1401  loss_mask_2: 0.1994  loss_dice_2: 0.1672  loss_ce_3: 0.1416  loss_mask_3: 0.1933  loss_dice_3: 0.1641  loss_ce_4: 0.1402  loss_mask_4: 0.1877  loss_dice_4: 0.1577  loss_ce_5: 0.1402  loss_mask_5: 0.1948  loss_dice_5: 0.1701  loss_ce_6: 0.1414  loss_mask_6: 0.2008  loss_dice_6: 0.1699  loss_ce_7: 0.1422  loss_mask_7: 0.2015  loss_dice_7: 0.1744  loss_ce_8: 0.144  loss_mask_8: 0.2015  loss_dice_8: 0.1683  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:41] d2.utils.events INFO:  eta: 0:59:28  iter: 21739  total_loss: 4.677  loss_ce: 0.1159  loss_mask: 0.2084  loss_dice: 0.138  loss_ce_0: 0.06067  loss_mask_0: 0.2078  loss_dice_0: 0.1332  loss_ce_1: 0.1295  loss_mask_1: 0.2101  loss_dice_1: 0.1395  loss_ce_2: 0.1223  loss_mask_2: 0.2202  loss_dice_2: 0.141  loss_ce_3: 0.121  loss_mask_3: 0.2024  loss_dice_3: 0.1432  loss_ce_4: 0.1216  loss_mask_4: 0.2105  loss_dice_4: 0.1443  loss_ce_5: 0.1211  loss_mask_5: 0.2142  loss_dice_5: 0.1384  loss_ce_6: 0.1197  loss_mask_6: 0.2118  loss_dice_6: 0.1438  loss_ce_7: 0.1187  loss_mask_7: 0.2095  loss_dice_7: 0.1415  loss_ce_8: 0.1183  loss_mask_8: 0.2174  loss_dice_8: 0.1416  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:43] d2.utils.events INFO:  eta: 0:59:25  iter: 21759  total_loss: 4.892  loss_ce: 0.1168  loss_mask: 0.2172  loss_dice: 0.1599  loss_ce_0: 0.05879  loss_mask_0: 0.2178  loss_dice_0: 0.1432  loss_ce_1: 0.1182  loss_mask_1: 0.218  loss_dice_1: 0.1495  loss_ce_2: 0.1164  loss_mask_2: 0.2158  loss_dice_2: 0.1565  loss_ce_3: 0.1227  loss_mask_3: 0.2109  loss_dice_3: 0.1553  loss_ce_4: 0.1142  loss_mask_4: 0.205  loss_dice_4: 0.1483  loss_ce_5: 0.1146  loss_mask_5: 0.2122  loss_dice_5: 0.1515  loss_ce_6: 0.1285  loss_mask_6: 0.2148  loss_dice_6: 0.1538  loss_ce_7: 0.1176  loss_mask_7: 0.2117  loss_dice_7: 0.1555  loss_ce_8: 0.1283  loss_mask_8: 0.2141  loss_dice_8: 0.1578  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:35:46] d2.utils.events INFO:  eta: 0:59:23  iter: 21779  total_loss: 5.01  loss_ce: 0.114  loss_mask: 0.1644  loss_dice: 0.2028  loss_ce_0: 0.05755  loss_mask_0: 0.1522  loss_dice_0: 0.1889  loss_ce_1: 0.1148  loss_mask_1: 0.1581  loss_dice_1: 0.1968  loss_ce_2: 0.1129  loss_mask_2: 0.1629  loss_dice_2: 0.196  loss_ce_3: 0.112  loss_mask_3: 0.156  loss_dice_3: 0.1878  loss_ce_4: 0.1125  loss_mask_4: 0.1641  loss_dice_4: 0.2142  loss_ce_5: 0.1121  loss_mask_5: 0.1614  loss_dice_5: 0.2061  loss_ce_6: 0.1145  loss_mask_6: 0.1674  loss_dice_6: 0.2014  loss_ce_7: 0.1153  loss_mask_7: 0.156  loss_dice_7: 0.2009  loss_ce_8: 0.1379  loss_mask_8: 0.1655  loss_dice_8: 0.2083  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:48] d2.utils.events INFO:  eta: 0:59:20  iter: 21799  total_loss: 5.31  loss_ce: 0.1512  loss_mask: 0.1858  loss_dice: 0.1966  loss_ce_0: 0.07053  loss_mask_0: 0.1815  loss_dice_0: 0.1724  loss_ce_1: 0.1436  loss_mask_1: 0.1854  loss_dice_1: 0.1871  loss_ce_2: 0.1494  loss_mask_2: 0.1867  loss_dice_2: 0.191  loss_ce_3: 0.1487  loss_mask_3: 0.1865  loss_dice_3: 0.1912  loss_ce_4: 0.1473  loss_mask_4: 0.1785  loss_dice_4: 0.1849  loss_ce_5: 0.1517  loss_mask_5: 0.1921  loss_dice_5: 0.1892  loss_ce_6: 0.1512  loss_mask_6: 0.1848  loss_dice_6: 0.1777  loss_ce_7: 0.1496  loss_mask_7: 0.1798  loss_dice_7: 0.188  loss_ce_8: 0.1498  loss_mask_8: 0.1963  loss_dice_8: 0.187  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:51] d2.utils.events INFO:  eta: 0:59:18  iter: 21819  total_loss: 5.235  loss_ce: 0.1164  loss_mask: 0.1921  loss_dice: 0.1433  loss_ce_0: 0.05807  loss_mask_0: 0.1884  loss_dice_0: 0.1487  loss_ce_1: 0.1184  loss_mask_1: 0.1941  loss_dice_1: 0.1391  loss_ce_2: 0.116  loss_mask_2: 0.2033  loss_dice_2: 0.1436  loss_ce_3: 0.1175  loss_mask_3: 0.1891  loss_dice_3: 0.145  loss_ce_4: 0.128  loss_mask_4: 0.1948  loss_dice_4: 0.1428  loss_ce_5: 0.1175  loss_mask_5: 0.1939  loss_dice_5: 0.1423  loss_ce_6: 0.117  loss_mask_6: 0.1893  loss_dice_6: 0.1503  loss_ce_7: 0.1186  loss_mask_7: 0.204  loss_dice_7: 0.1394  loss_ce_8: 0.1178  loss_mask_8: 0.2007  loss_dice_8: 0.145  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:35:53] d2.utils.events INFO:  eta: 0:59:15  iter: 21839  total_loss: 5.472  loss_ce: 0.1114  loss_mask: 0.1699  loss_dice: 0.1759  loss_ce_0: 0.05599  loss_mask_0: 0.1747  loss_dice_0: 0.1736  loss_ce_1: 0.1054  loss_mask_1: 0.1674  loss_dice_1: 0.1744  loss_ce_2: 0.1068  loss_mask_2: 0.1778  loss_dice_2: 0.1779  loss_ce_3: 0.1081  loss_mask_3: 0.1785  loss_dice_3: 0.1773  loss_ce_4: 0.1042  loss_mask_4: 0.1749  loss_dice_4: 0.177  loss_ce_5: 0.1072  loss_mask_5: 0.1689  loss_dice_5: 0.175  loss_ce_6: 0.1079  loss_mask_6: 0.1737  loss_dice_6: 0.1747  loss_ce_7: 0.1088  loss_mask_7: 0.1822  loss_dice_7: 0.1806  loss_ce_8: 0.1104  loss_mask_8: 0.1819  loss_dice_8: 0.1829  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:35:56] d2.utils.events INFO:  eta: 0:59:13  iter: 21859  total_loss: 5.199  loss_ce: 0.1021  loss_mask: 0.2072  loss_dice: 0.1327  loss_ce_0: 0.05283  loss_mask_0: 0.2159  loss_dice_0: 0.1337  loss_ce_1: 0.09668  loss_mask_1: 0.21  loss_dice_1: 0.1261  loss_ce_2: 0.09618  loss_mask_2: 0.2132  loss_dice_2: 0.1278  loss_ce_3: 0.0965  loss_mask_3: 0.2161  loss_dice_3: 0.1318  loss_ce_4: 0.08557  loss_mask_4: 0.2072  loss_dice_4: 0.1356  loss_ce_5: 0.09524  loss_mask_5: 0.2076  loss_dice_5: 0.1293  loss_ce_6: 0.09428  loss_mask_6: 0.2122  loss_dice_6: 0.1332  loss_ce_7: 0.09499  loss_mask_7: 0.2117  loss_dice_7: 0.1326  loss_ce_8: 0.09988  loss_mask_8: 0.2071  loss_dice_8: 0.1318  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:35:58] d2.utils.events INFO:  eta: 0:59:11  iter: 21879  total_loss: 5.14  loss_ce: 0.1331  loss_mask: 0.1872  loss_dice: 0.1447  loss_ce_0: 0.06427  loss_mask_0: 0.1787  loss_dice_0: 0.1408  loss_ce_1: 0.1323  loss_mask_1: 0.1963  loss_dice_1: 0.1474  loss_ce_2: 0.1334  loss_mask_2: 0.1836  loss_dice_2: 0.1495  loss_ce_3: 0.1327  loss_mask_3: 0.1791  loss_dice_3: 0.1397  loss_ce_4: 0.1338  loss_mask_4: 0.192  loss_dice_4: 0.1383  loss_ce_5: 0.1328  loss_mask_5: 0.1918  loss_dice_5: 0.1417  loss_ce_6: 0.1338  loss_mask_6: 0.1911  loss_dice_6: 0.15  loss_ce_7: 0.1338  loss_mask_7: 0.1828  loss_dice_7: 0.1466  loss_ce_8: 0.1337  loss_mask_8: 0.1783  loss_dice_8: 0.155  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:01] d2.utils.events INFO:  eta: 0:59:07  iter: 21899  total_loss: 5.545  loss_ce: 0.1714  loss_mask: 0.2257  loss_dice: 0.1506  loss_ce_0: 0.07835  loss_mask_0: 0.2017  loss_dice_0: 0.1493  loss_ce_1: 0.1685  loss_mask_1: 0.2155  loss_dice_1: 0.1448  loss_ce_2: 0.1724  loss_mask_2: 0.2122  loss_dice_2: 0.1491  loss_ce_3: 0.1691  loss_mask_3: 0.2233  loss_dice_3: 0.1457  loss_ce_4: 0.1657  loss_mask_4: 0.2271  loss_dice_4: 0.1559  loss_ce_5: 0.1678  loss_mask_5: 0.2115  loss_dice_5: 0.1537  loss_ce_6: 0.1704  loss_mask_6: 0.2131  loss_dice_6: 0.1496  loss_ce_7: 0.17  loss_mask_7: 0.2265  loss_dice_7: 0.149  loss_ce_8: 0.172  loss_mask_8: 0.2143  loss_dice_8: 0.1556  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:36:03] d2.utils.events INFO:  eta: 0:59:05  iter: 21919  total_loss: 5.935  loss_ce: 0.095  loss_mask: 0.2225  loss_dice: 0.2479  loss_ce_0: 0.05046  loss_mask_0: 0.2069  loss_dice_0: 0.2613  loss_ce_1: 0.09596  loss_mask_1: 0.2159  loss_dice_1: 0.2553  loss_ce_2: 0.09383  loss_mask_2: 0.2056  loss_dice_2: 0.257  loss_ce_3: 0.0956  loss_mask_3: 0.2183  loss_dice_3: 0.2685  loss_ce_4: 0.09772  loss_mask_4: 0.2196  loss_dice_4: 0.2525  loss_ce_5: 0.09604  loss_mask_5: 0.2183  loss_dice_5: 0.2551  loss_ce_6: 0.09561  loss_mask_6: 0.2265  loss_dice_6: 0.2625  loss_ce_7: 0.09586  loss_mask_7: 0.2243  loss_dice_7: 0.2579  loss_ce_8: 0.09493  loss_mask_8: 0.2151  loss_dice_8: 0.2697  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:36:06] d2.utils.events INFO:  eta: 0:59:02  iter: 21939  total_loss: 5.082  loss_ce: 0.1614  loss_mask: 0.2295  loss_dice: 0.1174  loss_ce_0: 0.07622  loss_mask_0: 0.2239  loss_dice_0: 0.1275  loss_ce_1: 0.1586  loss_mask_1: 0.2344  loss_dice_1: 0.1233  loss_ce_2: 0.161  loss_mask_2: 0.2303  loss_dice_2: 0.121  loss_ce_3: 0.1589  loss_mask_3: 0.2361  loss_dice_3: 0.1203  loss_ce_4: 0.1554  loss_mask_4: 0.2346  loss_dice_4: 0.1218  loss_ce_5: 0.159  loss_mask_5: 0.2337  loss_dice_5: 0.125  loss_ce_6: 0.158  loss_mask_6: 0.2342  loss_dice_6: 0.1228  loss_ce_7: 0.1576  loss_mask_7: 0.228  loss_dice_7: 0.1196  loss_ce_8: 0.1601  loss_mask_8: 0.2342  loss_dice_8: 0.1255  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:36:08] d2.utils.events INFO:  eta: 0:59:00  iter: 21959  total_loss: 6.004  loss_ce: 0.149  loss_mask: 0.1721  loss_dice: 0.1686  loss_ce_0: 0.07223  loss_mask_0: 0.174  loss_dice_0: 0.1669  loss_ce_1: 0.1451  loss_mask_1: 0.181  loss_dice_1: 0.1591  loss_ce_2: 0.1452  loss_mask_2: 0.177  loss_dice_2: 0.1594  loss_ce_3: 0.1441  loss_mask_3: 0.1783  loss_dice_3: 0.1685  loss_ce_4: 0.1378  loss_mask_4: 0.1701  loss_dice_4: 0.1686  loss_ce_5: 0.1438  loss_mask_5: 0.1695  loss_dice_5: 0.1683  loss_ce_6: 0.1426  loss_mask_6: 0.1736  loss_dice_6: 0.1684  loss_ce_7: 0.1422  loss_mask_7: 0.1776  loss_dice_7: 0.1658  loss_ce_8: 0.1453  loss_mask_8: 0.1634  loss_dice_8: 0.1697  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:36:11] d2.utils.events INFO:  eta: 0:58:57  iter: 21979  total_loss: 4.981  loss_ce: 0.1137  loss_mask: 0.1874  loss_dice: 0.1236  loss_ce_0: 0.05653  loss_mask_0: 0.1856  loss_dice_0: 0.1316  loss_ce_1: 0.1161  loss_mask_1: 0.2015  loss_dice_1: 0.1317  loss_ce_2: 0.1164  loss_mask_2: 0.1839  loss_dice_2: 0.1277  loss_ce_3: 0.1169  loss_mask_3: 0.184  loss_dice_3: 0.1232  loss_ce_4: 0.1228  loss_mask_4: 0.1898  loss_dice_4: 0.1307  loss_ce_5: 0.1171  loss_mask_5: 0.1907  loss_dice_5: 0.1262  loss_ce_6: 0.1189  loss_mask_6: 0.1835  loss_dice_6: 0.1257  loss_ce_7: 0.1186  loss_mask_7: 0.1805  loss_dice_7: 0.1208  loss_ce_8: 0.117  loss_mask_8: 0.192  loss_dice_8: 0.1276  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:14] d2.utils.events INFO:  eta: 0:58:56  iter: 21999  total_loss: 5.959  loss_ce: 0.1401  loss_mask: 0.1749  loss_dice: 0.1904  loss_ce_0: 0.06932  loss_mask_0: 0.1733  loss_dice_0: 0.2049  loss_ce_1: 0.1396  loss_mask_1: 0.1764  loss_dice_1: 0.2051  loss_ce_2: 0.1391  loss_mask_2: 0.1813  loss_dice_2: 0.2013  loss_ce_3: 0.1391  loss_mask_3: 0.1769  loss_dice_3: 0.1965  loss_ce_4: 0.1364  loss_mask_4: 0.1719  loss_dice_4: 0.2022  loss_ce_5: 0.1399  loss_mask_5: 0.167  loss_dice_5: 0.2054  loss_ce_6: 0.1377  loss_mask_6: 0.1706  loss_dice_6: 0.2028  loss_ce_7: 0.1377  loss_mask_7: 0.1762  loss_dice_7: 0.2089  loss_ce_8: 0.1379  loss_mask_8: 0.1812  loss_dice_8: 0.2066  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:16] d2.utils.events INFO:  eta: 0:58:53  iter: 22019  total_loss: 5.958  loss_ce: 0.1214  loss_mask: 0.2018  loss_dice: 0.2022  loss_ce_0: 0.05954  loss_mask_0: 0.197  loss_dice_0: 0.1978  loss_ce_1: 0.1234  loss_mask_1: 0.1939  loss_dice_1: 0.2032  loss_ce_2: 0.1238  loss_mask_2: 0.1936  loss_dice_2: 0.2093  loss_ce_3: 0.1239  loss_mask_3: 0.2039  loss_dice_3: 0.2019  loss_ce_4: 0.1277  loss_mask_4: 0.206  loss_dice_4: 0.1933  loss_ce_5: 0.1234  loss_mask_5: 0.2124  loss_dice_5: 0.2033  loss_ce_6: 0.125  loss_mask_6: 0.1992  loss_dice_6: 0.1922  loss_ce_7: 0.1248  loss_mask_7: 0.1902  loss_dice_7: 0.2003  loss_ce_8: 0.1241  loss_mask_8: 0.2005  loss_dice_8: 0.1977  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:36:19] d2.utils.events INFO:  eta: 0:58:49  iter: 22039  total_loss: 5.631  loss_ce: 0.1225  loss_mask: 0.208  loss_dice: 0.1946  loss_ce_0: 0.05984  loss_mask_0: 0.2054  loss_dice_0: 0.1937  loss_ce_1: 0.1245  loss_mask_1: 0.2087  loss_dice_1: 0.1924  loss_ce_2: 0.1244  loss_mask_2: 0.2127  loss_dice_2: 0.1926  loss_ce_3: 0.1249  loss_mask_3: 0.221  loss_dice_3: 0.1905  loss_ce_4: 0.1284  loss_mask_4: 0.2099  loss_dice_4: 0.1899  loss_ce_5: 0.1239  loss_mask_5: 0.2167  loss_dice_5: 0.1957  loss_ce_6: 0.1257  loss_mask_6: 0.204  loss_dice_6: 0.1967  loss_ce_7: 0.1256  loss_mask_7: 0.2066  loss_dice_7: 0.1883  loss_ce_8: 0.1248  loss_mask_8: 0.2008  loss_dice_8: 0.1893  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:36:21] d2.utils.events INFO:  eta: 0:58:46  iter: 22059  total_loss: 6.633  loss_ce: 0.1267  loss_mask: 0.1523  loss_dice: 0.3178  loss_ce_0: 0.06274  loss_mask_0: 0.1444  loss_dice_0: 0.3246  loss_ce_1: 0.1267  loss_mask_1: 0.1505  loss_dice_1: 0.3344  loss_ce_2: 0.1275  loss_mask_2: 0.1542  loss_dice_2: 0.3295  loss_ce_3: 0.1273  loss_mask_3: 0.1616  loss_dice_3: 0.341  loss_ce_4: 0.129  loss_mask_4: 0.1499  loss_dice_4: 0.3285  loss_ce_5: 0.1275  loss_mask_5: 0.1532  loss_dice_5: 0.3215  loss_ce_6: 0.1278  loss_mask_6: 0.1502  loss_dice_6: 0.3306  loss_ce_7: 0.1271  loss_mask_7: 0.1549  loss_dice_7: 0.3431  loss_ce_8: 0.1271  loss_mask_8: 0.1607  loss_dice_8: 0.3308  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:36:24] d2.utils.events INFO:  eta: 0:58:44  iter: 22079  total_loss: 5.758  loss_ce: 0.1259  loss_mask: 0.2088  loss_dice: 0.1519  loss_ce_0: 0.06312  loss_mask_0: 0.2164  loss_dice_0: 0.1551  loss_ce_1: 0.1245  loss_mask_1: 0.2105  loss_dice_1: 0.1598  loss_ce_2: 0.124  loss_mask_2: 0.2035  loss_dice_2: 0.1598  loss_ce_3: 0.1238  loss_mask_3: 0.2119  loss_dice_3: 0.1576  loss_ce_4: 0.1193  loss_mask_4: 0.2104  loss_dice_4: 0.1536  loss_ce_5: 0.1243  loss_mask_5: 0.215  loss_dice_5: 0.1557  loss_ce_6: 0.1229  loss_mask_6: 0.2084  loss_dice_6: 0.1559  loss_ce_7: 0.1229  loss_mask_7: 0.208  loss_dice_7: 0.1594  loss_ce_8: 0.1232  loss_mask_8: 0.2161  loss_dice_8: 0.1536  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:26] d2.utils.events INFO:  eta: 0:58:45  iter: 22099  total_loss: 4.916  loss_ce: 0.1359  loss_mask: 0.214  loss_dice: 0.116  loss_ce_0: 0.06477  loss_mask_0: 0.212  loss_dice_0: 0.1147  loss_ce_1: 0.1355  loss_mask_1: 0.2192  loss_dice_1: 0.1193  loss_ce_2: 0.1364  loss_mask_2: 0.2132  loss_dice_2: 0.1144  loss_ce_3: 0.1357  loss_mask_3: 0.2135  loss_dice_3: 0.1155  loss_ce_4: 0.1364  loss_mask_4: 0.2096  loss_dice_4: 0.1162  loss_ce_5: 0.1352  loss_mask_5: 0.2026  loss_dice_5: 0.118  loss_ce_6: 0.1374  loss_mask_6: 0.2133  loss_dice_6: 0.1179  loss_ce_7: 0.1363  loss_mask_7: 0.2124  loss_dice_7: 0.1135  loss_ce_8: 0.137  loss_mask_8: 0.2091  loss_dice_8: 0.1132  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:29] d2.utils.events INFO:  eta: 0:58:40  iter: 22119  total_loss: 5.047  loss_ce: 0.1282  loss_mask: 0.2297  loss_dice: 0.1334  loss_ce_0: 0.06268  loss_mask_0: 0.2275  loss_dice_0: 0.1353  loss_ce_1: 0.1274  loss_mask_1: 0.2288  loss_dice_1: 0.1412  loss_ce_2: 0.1282  loss_mask_2: 0.2305  loss_dice_2: 0.1341  loss_ce_3: 0.1275  loss_mask_3: 0.2223  loss_dice_3: 0.1374  loss_ce_4: 0.1269  loss_mask_4: 0.2302  loss_dice_4: 0.1409  loss_ce_5: 0.1271  loss_mask_5: 0.2262  loss_dice_5: 0.1388  loss_ce_6: 0.1272  loss_mask_6: 0.2334  loss_dice_6: 0.1377  loss_ce_7: 0.1271  loss_mask_7: 0.2232  loss_dice_7: 0.1391  loss_ce_8: 0.1283  loss_mask_8: 0.2282  loss_dice_8: 0.1396  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:36:31] d2.utils.events INFO:  eta: 0:58:37  iter: 22139  total_loss: 5.014  loss_ce: 0.1213  loss_mask: 0.2117  loss_dice: 0.1394  loss_ce_0: 0.0597  loss_mask_0: 0.2038  loss_dice_0: 0.1386  loss_ce_1: 0.1192  loss_mask_1: 0.2092  loss_dice_1: 0.1393  loss_ce_2: 0.1187  loss_mask_2: 0.1999  loss_dice_2: 0.1397  loss_ce_3: 0.1168  loss_mask_3: 0.2127  loss_dice_3: 0.1394  loss_ce_4: 0.1119  loss_mask_4: 0.211  loss_dice_4: 0.1447  loss_ce_5: 0.1168  loss_mask_5: 0.2019  loss_dice_5: 0.1467  loss_ce_6: 0.1166  loss_mask_6: 0.2147  loss_dice_6: 0.1476  loss_ce_7: 0.1159  loss_mask_7: 0.2092  loss_dice_7: 0.1438  loss_ce_8: 0.1188  loss_mask_8: 0.2068  loss_dice_8: 0.1413  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:34] d2.utils.events INFO:  eta: 0:58:37  iter: 22159  total_loss: 5.469  loss_ce: 0.1292  loss_mask: 0.1946  loss_dice: 0.1625  loss_ce_0: 0.06331  loss_mask_0: 0.195  loss_dice_0: 0.1614  loss_ce_1: 0.1299  loss_mask_1: 0.1901  loss_dice_1: 0.1624  loss_ce_2: 0.1294  loss_mask_2: 0.1864  loss_dice_2: 0.159  loss_ce_3: 0.1297  loss_mask_3: 0.1981  loss_dice_3: 0.1589  loss_ce_4: 0.1306  loss_mask_4: 0.1881  loss_dice_4: 0.1647  loss_ce_5: 0.1293  loss_mask_5: 0.193  loss_dice_5: 0.1551  loss_ce_6: 0.1296  loss_mask_6: 0.1876  loss_dice_6: 0.1659  loss_ce_7: 0.13  loss_mask_7: 0.1964  loss_dice_7: 0.1603  loss_ce_8: 0.1302  loss_mask_8: 0.1917  loss_dice_8: 0.153  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:36] d2.utils.events INFO:  eta: 0:58:35  iter: 22179  total_loss: 5.504  loss_ce: 0.1105  loss_mask: 0.2204  loss_dice: 0.1668  loss_ce_0: 0.0569  loss_mask_0: 0.223  loss_dice_0: 0.1718  loss_ce_1: 0.1101  loss_mask_1: 0.2184  loss_dice_1: 0.168  loss_ce_2: 0.1094  loss_mask_2: 0.2252  loss_dice_2: 0.1727  loss_ce_3: 0.1099  loss_mask_3: 0.218  loss_dice_3: 0.1638  loss_ce_4: 0.1081  loss_mask_4: 0.2279  loss_dice_4: 0.1694  loss_ce_5: 0.1105  loss_mask_5: 0.2169  loss_dice_5: 0.1597  loss_ce_6: 0.1088  loss_mask_6: 0.212  loss_dice_6: 0.1692  loss_ce_7: 0.1088  loss_mask_7: 0.2267  loss_dice_7: 0.1621  loss_ce_8: 0.1088  loss_mask_8: 0.2297  loss_dice_8: 0.1592  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:39] d2.utils.events INFO:  eta: 0:58:33  iter: 22199  total_loss: 5.602  loss_ce: 0.1096  loss_mask: 0.2123  loss_dice: 0.1746  loss_ce_0: 0.05672  loss_mask_0: 0.2057  loss_dice_0: 0.1737  loss_ce_1: 0.1089  loss_mask_1: 0.1987  loss_dice_1: 0.1802  loss_ce_2: 0.109  loss_mask_2: 0.2156  loss_dice_2: 0.1745  loss_ce_3: 0.1097  loss_mask_3: 0.2127  loss_dice_3: 0.1735  loss_ce_4: 0.1082  loss_mask_4: 0.2096  loss_dice_4: 0.1747  loss_ce_5: 0.1103  loss_mask_5: 0.2087  loss_dice_5: 0.1714  loss_ce_6: 0.1089  loss_mask_6: 0.2056  loss_dice_6: 0.177  loss_ce_7: 0.1086  loss_mask_7: 0.2159  loss_dice_7: 0.1745  loss_ce_8: 0.1082  loss_mask_8: 0.2131  loss_dice_8: 0.1734  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:42] d2.utils.events INFO:  eta: 0:58:32  iter: 22219  total_loss: 5.813  loss_ce: 0.1074  loss_mask: 0.1931  loss_dice: 0.1848  loss_ce_0: 0.05576  loss_mask_0: 0.199  loss_dice_0: 0.1761  loss_ce_1: 0.1065  loss_mask_1: 0.2036  loss_dice_1: 0.1671  loss_ce_2: 0.1064  loss_mask_2: 0.2001  loss_dice_2: 0.1807  loss_ce_3: 0.1069  loss_mask_3: 0.2024  loss_dice_3: 0.1808  loss_ce_4: 0.1053  loss_mask_4: 0.2029  loss_dice_4: 0.1817  loss_ce_5: 0.1071  loss_mask_5: 0.2118  loss_dice_5: 0.1837  loss_ce_6: 0.1061  loss_mask_6: 0.196  loss_dice_6: 0.1813  loss_ce_7: 0.1061  loss_mask_7: 0.202  loss_dice_7: 0.1756  loss_ce_8: 0.1063  loss_mask_8: 0.1962  loss_dice_8: 0.1798  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:44] d2.utils.events INFO:  eta: 0:58:27  iter: 22239  total_loss: 6.378  loss_ce: 0.1036  loss_mask: 0.2138  loss_dice: 0.2824  loss_ce_0: 0.0545  loss_mask_0: 0.2164  loss_dice_0: 0.2833  loss_ce_1: 0.1025  loss_mask_1: 0.2263  loss_dice_1: 0.2836  loss_ce_2: 0.1024  loss_mask_2: 0.2267  loss_dice_2: 0.2713  loss_ce_3: 0.1026  loss_mask_3: 0.2142  loss_dice_3: 0.2708  loss_ce_4: 0.1013  loss_mask_4: 0.207  loss_dice_4: 0.2766  loss_ce_5: 0.1027  loss_mask_5: 0.2135  loss_dice_5: 0.2807  loss_ce_6: 0.1021  loss_mask_6: 0.2185  loss_dice_6: 0.2868  loss_ce_7: 0.1019  loss_mask_7: 0.2276  loss_dice_7: 0.263  loss_ce_8: 0.1025  loss_mask_8: 0.2212  loss_dice_8: 0.2767  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:47] d2.utils.events INFO:  eta: 0:58:25  iter: 22259  total_loss: 5.676  loss_ce: 0.1302  loss_mask: 0.1985  loss_dice: 0.1733  loss_ce_0: 0.06372  loss_mask_0: 0.1955  loss_dice_0: 0.177  loss_ce_1: 0.1305  loss_mask_1: 0.2026  loss_dice_1: 0.1847  loss_ce_2: 0.1305  loss_mask_2: 0.1965  loss_dice_2: 0.1767  loss_ce_3: 0.1308  loss_mask_3: 0.2104  loss_dice_3: 0.1805  loss_ce_4: 0.1317  loss_mask_4: 0.1963  loss_dice_4: 0.1739  loss_ce_5: 0.1309  loss_mask_5: 0.2022  loss_dice_5: 0.1736  loss_ce_6: 0.1313  loss_mask_6: 0.1957  loss_dice_6: 0.1766  loss_ce_7: 0.1312  loss_mask_7: 0.1976  loss_dice_7: 0.1793  loss_ce_8: 0.1304  loss_mask_8: 0.1992  loss_dice_8: 0.1754  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:49] d2.utils.events INFO:  eta: 0:58:22  iter: 22279  total_loss: 5.954  loss_ce: 0.09816  loss_mask: 0.2267  loss_dice: 0.2031  loss_ce_0: 0.05231  loss_mask_0: 0.2348  loss_dice_0: 0.1969  loss_ce_1: 0.09838  loss_mask_1: 0.2318  loss_dice_1: 0.1971  loss_ce_2: 0.09807  loss_mask_2: 0.2151  loss_dice_2: 0.2046  loss_ce_3: 0.09877  loss_mask_3: 0.2334  loss_dice_3: 0.1971  loss_ce_4: 0.09871  loss_mask_4: 0.2185  loss_dice_4: 0.1953  loss_ce_5: 0.09883  loss_mask_5: 0.2248  loss_dice_5: 0.1984  loss_ce_6: 0.09798  loss_mask_6: 0.2258  loss_dice_6: 0.2021  loss_ce_7: 0.09832  loss_mask_7: 0.2321  loss_dice_7: 0.2053  loss_ce_8: 0.09814  loss_mask_8: 0.2288  loss_dice_8: 0.2023  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:52] d2.utils.events INFO:  eta: 0:58:22  iter: 22299  total_loss: 4.932  loss_ce: 0.1304  loss_mask: 0.1946  loss_dice: 0.1687  loss_ce_0: 0.06382  loss_mask_0: 0.1881  loss_dice_0: 0.1607  loss_ce_1: 0.1299  loss_mask_1: 0.1966  loss_dice_1: 0.1614  loss_ce_2: 0.1304  loss_mask_2: 0.1926  loss_dice_2: 0.1605  loss_ce_3: 0.1303  loss_mask_3: 0.1977  loss_dice_3: 0.1603  loss_ce_4: 0.1303  loss_mask_4: 0.1932  loss_dice_4: 0.1616  loss_ce_5: 0.1303  loss_mask_5: 0.199  loss_dice_5: 0.1607  loss_ce_6: 0.1303  loss_mask_6: 0.1962  loss_dice_6: 0.1637  loss_ce_7: 0.1303  loss_mask_7: 0.1876  loss_dice_7: 0.1615  loss_ce_8: 0.1307  loss_mask_8: 0.191  loss_dice_8: 0.1628  time: 0.1261  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:36:54] d2.utils.events INFO:  eta: 0:58:21  iter: 22319  total_loss: 5.405  loss_ce: 0.1552  loss_mask: 0.2311  loss_dice: 0.1549  loss_ce_0: 0.07325  loss_mask_0: 0.2201  loss_dice_0: 0.1483  loss_ce_1: 0.1533  loss_mask_1: 0.2168  loss_dice_1: 0.1432  loss_ce_2: 0.1531  loss_mask_2: 0.22  loss_dice_2: 0.1537  loss_ce_3: 0.1522  loss_mask_3: 0.2138  loss_dice_3: 0.1524  loss_ce_4: 0.1504  loss_mask_4: 0.2119  loss_dice_4: 0.1458  loss_ce_5: 0.1519  loss_mask_5: 0.2231  loss_dice_5: 0.1511  loss_ce_6: 0.1513  loss_mask_6: 0.2202  loss_dice_6: 0.1464  loss_ce_7: 0.1511  loss_mask_7: 0.2137  loss_dice_7: 0.1454  loss_ce_8: 0.1544  loss_mask_8: 0.2194  loss_dice_8: 0.1488  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:57] d2.utils.events INFO:  eta: 0:58:18  iter: 22339  total_loss: 5.864  loss_ce: 0.1499  loss_mask: 0.2112  loss_dice: 0.233  loss_ce_0: 0.07197  loss_mask_0: 0.2045  loss_dice_0: 0.2578  loss_ce_1: 0.1475  loss_mask_1: 0.2196  loss_dice_1: 0.2568  loss_ce_2: 0.148  loss_mask_2: 0.2041  loss_dice_2: 0.2502  loss_ce_3: 0.1475  loss_mask_3: 0.2148  loss_dice_3: 0.2597  loss_ce_4: 0.1449  loss_mask_4: 0.2078  loss_dice_4: 0.2595  loss_ce_5: 0.1477  loss_mask_5: 0.2077  loss_dice_5: 0.2502  loss_ce_6: 0.1469  loss_mask_6: 0.2133  loss_dice_6: 0.2597  loss_ce_7: 0.1461  loss_mask_7: 0.2096  loss_dice_7: 0.2547  loss_ce_8: 0.148  loss_mask_8: 0.2049  loss_dice_8: 0.2369  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:36:59] d2.utils.events INFO:  eta: 0:58:15  iter: 22359  total_loss: 5.1  loss_ce: 0.1101  loss_mask: 0.2214  loss_dice: 0.1488  loss_ce_0: 0.05582  loss_mask_0: 0.2104  loss_dice_0: 0.1468  loss_ce_1: 0.1117  loss_mask_1: 0.2167  loss_dice_1: 0.1512  loss_ce_2: 0.1117  loss_mask_2: 0.2077  loss_dice_2: 0.1479  loss_ce_3: 0.1123  loss_mask_3: 0.2205  loss_dice_3: 0.1449  loss_ce_4: 0.1149  loss_mask_4: 0.2025  loss_dice_4: 0.1513  loss_ce_5: 0.1119  loss_mask_5: 0.2114  loss_dice_5: 0.1452  loss_ce_6: 0.1129  loss_mask_6: 0.2145  loss_dice_6: 0.1488  loss_ce_7: 0.1136  loss_mask_7: 0.2108  loss_dice_7: 0.1494  loss_ce_8: 0.1121  loss_mask_8: 0.2157  loss_dice_8: 0.1493  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:37:02] d2.utils.events INFO:  eta: 0:58:13  iter: 22379  total_loss: 4.934  loss_ce: 0.1085  loss_mask: 0.2292  loss_dice: 0.1233  loss_ce_0: 0.05531  loss_mask_0: 0.2382  loss_dice_0: 0.1266  loss_ce_1: 0.1091  loss_mask_1: 0.2382  loss_dice_1: 0.1244  loss_ce_2: 0.1092  loss_mask_2: 0.2299  loss_dice_2: 0.1248  loss_ce_3: 0.1088  loss_mask_3: 0.2283  loss_dice_3: 0.1249  loss_ce_4: 0.1098  loss_mask_4: 0.2305  loss_dice_4: 0.1248  loss_ce_5: 0.1087  loss_mask_5: 0.2301  loss_dice_5: 0.1244  loss_ce_6: 0.1095  loss_mask_6: 0.2395  loss_dice_6: 0.1272  loss_ce_7: 0.1097  loss_mask_7: 0.2288  loss_dice_7: 0.1242  loss_ce_8: 0.1093  loss_mask_8: 0.229  loss_dice_8: 0.1253  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:37:04] d2.utils.events INFO:  eta: 0:58:12  iter: 22399  total_loss: 5.388  loss_ce: 0.1523  loss_mask: 0.1978  loss_dice: 0.1811  loss_ce_0: 0.07257  loss_mask_0: 0.2097  loss_dice_0: 0.1846  loss_ce_1: 0.1525  loss_mask_1: 0.2061  loss_dice_1: 0.1858  loss_ce_2: 0.1528  loss_mask_2: 0.2  loss_dice_2: 0.1869  loss_ce_3: 0.1537  loss_mask_3: 0.1977  loss_dice_3: 0.1835  loss_ce_4: 0.1547  loss_mask_4: 0.2037  loss_dice_4: 0.1807  loss_ce_5: 0.1539  loss_mask_5: 0.2021  loss_dice_5: 0.1806  loss_ce_6: 0.154  loss_mask_6: 0.2013  loss_dice_6: 0.1858  loss_ce_7: 0.1543  loss_mask_7: 0.2081  loss_dice_7: 0.1836  loss_ce_8: 0.1525  loss_mask_8: 0.204  loss_dice_8: 0.1855  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:37:07] d2.utils.events INFO:  eta: 0:58:10  iter: 22419  total_loss: 6.672  loss_ce: 0.1518  loss_mask: 0.2025  loss_dice: 0.2957  loss_ce_0: 0.07204  loss_mask_0: 0.2002  loss_dice_0: 0.2915  loss_ce_1: 0.1511  loss_mask_1: 0.185  loss_dice_1: 0.2992  loss_ce_2: 0.1513  loss_mask_2: 0.1907  loss_dice_2: 0.3012  loss_ce_3: 0.1511  loss_mask_3: 0.1804  loss_dice_3: 0.2775  loss_ce_4: 0.1508  loss_mask_4: 0.1881  loss_dice_4: 0.289  loss_ce_5: 0.1512  loss_mask_5: 0.19  loss_dice_5: 0.2876  loss_ce_6: 0.1516  loss_mask_6: 0.1817  loss_dice_6: 0.2913  loss_ce_7: 0.1513  loss_mask_7: 0.1913  loss_dice_7: 0.2986  loss_ce_8: 0.1511  loss_mask_8: 0.1923  loss_dice_8: 0.2831  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:37:10] d2.utils.events INFO:  eta: 0:58:12  iter: 22439  total_loss: 6.384  loss_ce: 0.128  loss_mask: 0.2209  loss_dice: 0.191  loss_ce_0: 0.06312  loss_mask_0: 0.2281  loss_dice_0: 0.193  loss_ce_1: 0.1278  loss_mask_1: 0.2251  loss_dice_1: 0.2008  loss_ce_2: 0.1279  loss_mask_2: 0.2268  loss_dice_2: 0.1957  loss_ce_3: 0.128  loss_mask_3: 0.2288  loss_dice_3: 0.1889  loss_ce_4: 0.1276  loss_mask_4: 0.2207  loss_dice_4: 0.1894  loss_ce_5: 0.1279  loss_mask_5: 0.2334  loss_dice_5: 0.1962  loss_ce_6: 0.128  loss_mask_6: 0.2294  loss_dice_6: 0.1958  loss_ce_7: 0.1277  loss_mask_7: 0.2278  loss_dice_7: 0.1903  loss_ce_8: 0.1279  loss_mask_8: 0.2197  loss_dice_8: 0.1986  time: 0.1261  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:37:12] d2.utils.events INFO:  eta: 0:58:06  iter: 22459  total_loss: 5.691  loss_ce: 0.1105  loss_mask: 0.2333  loss_dice: 0.1754  loss_ce_0: 0.05623  loss_mask_0: 0.2102  loss_dice_0: 0.1716  loss_ce_1: 0.1112  loss_mask_1: 0.2259  loss_dice_1: 0.1725  loss_ce_2: 0.111  loss_mask_2: 0.2376  loss_dice_2: 0.1758  loss_ce_3: 0.1115  loss_mask_3: 0.2315  loss_dice_3: 0.166  loss_ce_4: 0.1127  loss_mask_4: 0.2245  loss_dice_4: 0.1718  loss_ce_5: 0.1114  loss_mask_5: 0.2196  loss_dice_5: 0.1732  loss_ce_6: 0.1116  loss_mask_6: 0.216  loss_dice_6: 0.1686  loss_ce_7: 0.1119  loss_mask_7: 0.2316  loss_dice_7: 0.1756  loss_ce_8: 0.1116  loss_mask_8: 0.2364  loss_dice_8: 0.1748  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:37:15] d2.utils.events INFO:  eta: 0:58:05  iter: 22479  total_loss: 5.809  loss_ce: 0.1092  loss_mask: 0.171  loss_dice: 0.2163  loss_ce_0: 0.0557  loss_mask_0: 0.1729  loss_dice_0: 0.2102  loss_ce_1: 0.1091  loss_mask_1: 0.1672  loss_dice_1: 0.1932  loss_ce_2: 0.1092  loss_mask_2: 0.1716  loss_dice_2: 0.2071  loss_ce_3: 0.1091  loss_mask_3: 0.1653  loss_dice_3: 0.2087  loss_ce_4: 0.1091  loss_mask_4: 0.1724  loss_dice_4: 0.2107  loss_ce_5: 0.109  loss_mask_5: 0.1783  loss_dice_5: 0.2036  loss_ce_6: 0.1092  loss_mask_6: 0.168  loss_dice_6: 0.2017  loss_ce_7: 0.109  loss_mask_7: 0.171  loss_dice_7: 0.2065  loss_ce_8: 0.1099  loss_mask_8: 0.1715  loss_dice_8: 0.2016  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:17] d2.utils.events INFO:  eta: 0:57:59  iter: 22499  total_loss: 5.92  loss_ce: 0.1043  loss_mask: 0.1866  loss_dice: 0.2713  loss_ce_0: 0.05426  loss_mask_0: 0.1811  loss_dice_0: 0.2846  loss_ce_1: 0.1047  loss_mask_1: 0.1798  loss_dice_1: 0.284  loss_ce_2: 0.1043  loss_mask_2: 0.1829  loss_dice_2: 0.2693  loss_ce_3: 0.1041  loss_mask_3: 0.1842  loss_dice_3: 0.2699  loss_ce_4: 0.1037  loss_mask_4: 0.1786  loss_dice_4: 0.2703  loss_ce_5: 0.1042  loss_mask_5: 0.1975  loss_dice_5: 0.2713  loss_ce_6: 0.1038  loss_mask_6: 0.1868  loss_dice_6: 0.2755  loss_ce_7: 0.1037  loss_mask_7: 0.1881  loss_dice_7: 0.2801  loss_ce_8: 0.104  loss_mask_8: 0.1844  loss_dice_8: 0.2813  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:20] d2.utils.events INFO:  eta: 0:57:57  iter: 22519  total_loss: 4.922  loss_ce: 0.1031  loss_mask: 0.1843  loss_dice: 0.1327  loss_ce_0: 0.05399  loss_mask_0: 0.1729  loss_dice_0: 0.1336  loss_ce_1: 0.1024  loss_mask_1: 0.1637  loss_dice_1: 0.1351  loss_ce_2: 0.103  loss_mask_2: 0.1819  loss_dice_2: 0.1328  loss_ce_3: 0.1029  loss_mask_3: 0.1753  loss_dice_3: 0.1303  loss_ce_4: 0.1022  loss_mask_4: 0.1617  loss_dice_4: 0.1364  loss_ce_5: 0.1031  loss_mask_5: 0.1706  loss_dice_5: 0.1323  loss_ce_6: 0.1027  loss_mask_6: 0.1725  loss_dice_6: 0.135  loss_ce_7: 0.1024  loss_mask_7: 0.1674  loss_dice_7: 0.135  loss_ce_8: 0.1022  loss_mask_8: 0.1747  loss_dice_8: 0.1344  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:22] d2.utils.events INFO:  eta: 0:57:53  iter: 22539  total_loss: 4.851  loss_ce: 0.1006  loss_mask: 0.2087  loss_dice: 0.1299  loss_ce_0: 0.05303  loss_mask_0: 0.2052  loss_dice_0: 0.1297  loss_ce_1: 0.1007  loss_mask_1: 0.206  loss_dice_1: 0.1227  loss_ce_2: 0.1004  loss_mask_2: 0.2117  loss_dice_2: 0.1303  loss_ce_3: 0.1006  loss_mask_3: 0.2085  loss_dice_3: 0.129  loss_ce_4: 0.09967  loss_mask_4: 0.2069  loss_dice_4: 0.1304  loss_ce_5: 0.1007  loss_mask_5: 0.2144  loss_dice_5: 0.1332  loss_ce_6: 0.1001  loss_mask_6: 0.2074  loss_dice_6: 0.1276  loss_ce_7: 0.09992  loss_mask_7: 0.2057  loss_dice_7: 0.1298  loss_ce_8: 0.1005  loss_mask_8: 0.2017  loss_dice_8: 0.1276  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:25] d2.utils.events INFO:  eta: 0:57:50  iter: 22559  total_loss: 4.825  loss_ce: 0.1013  loss_mask: 0.2004  loss_dice: 0.1437  loss_ce_0: 0.05333  loss_mask_0: 0.2129  loss_dice_0: 0.1372  loss_ce_1: 0.1018  loss_mask_1: 0.2025  loss_dice_1: 0.1442  loss_ce_2: 0.1017  loss_mask_2: 0.2116  loss_dice_2: 0.1397  loss_ce_3: 0.1021  loss_mask_3: 0.2106  loss_dice_3: 0.1406  loss_ce_4: 0.1021  loss_mask_4: 0.2053  loss_dice_4: 0.1424  loss_ce_5: 0.1026  loss_mask_5: 0.2  loss_dice_5: 0.1411  loss_ce_6: 0.1022  loss_mask_6: 0.2013  loss_dice_6: 0.1436  loss_ce_7: 0.102  loss_mask_7: 0.2082  loss_dice_7: 0.1386  loss_ce_8: 0.1011  loss_mask_8: 0.2024  loss_dice_8: 0.1501  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:27] d2.utils.events INFO:  eta: 0:57:48  iter: 22579  total_loss: 5.229  loss_ce: 0.1662  loss_mask: 0.196  loss_dice: 0.1461  loss_ce_0: 0.07656  loss_mask_0: 0.1995  loss_dice_0: 0.1445  loss_ce_1: 0.1667  loss_mask_1: 0.2025  loss_dice_1: 0.1512  loss_ce_2: 0.1674  loss_mask_2: 0.1925  loss_dice_2: 0.1512  loss_ce_3: 0.1675  loss_mask_3: 0.1915  loss_dice_3: 0.1497  loss_ce_4: 0.1696  loss_mask_4: 0.2027  loss_dice_4: 0.1534  loss_ce_5: 0.1673  loss_mask_5: 0.2034  loss_dice_5: 0.1455  loss_ce_6: 0.168  loss_mask_6: 0.1974  loss_dice_6: 0.1441  loss_ce_7: 0.1684  loss_mask_7: 0.1963  loss_dice_7: 0.1516  loss_ce_8: 0.167  loss_mask_8: 0.2004  loss_dice_8: 0.1496  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:30] d2.utils.events INFO:  eta: 0:57:42  iter: 22599  total_loss: 5.124  loss_ce: 0.1053  loss_mask: 0.2207  loss_dice: 0.1657  loss_ce_0: 0.05427  loss_mask_0: 0.2175  loss_dice_0: 0.1631  loss_ce_1: 0.1083  loss_mask_1: 0.2096  loss_dice_1: 0.1667  loss_ce_2: 0.1071  loss_mask_2: 0.2065  loss_dice_2: 0.1633  loss_ce_3: 0.1084  loss_mask_3: 0.2091  loss_dice_3: 0.1606  loss_ce_4: 0.1103  loss_mask_4: 0.2141  loss_dice_4: 0.1658  loss_ce_5: 0.1084  loss_mask_5: 0.2135  loss_dice_5: 0.1625  loss_ce_6: 0.1086  loss_mask_6: 0.2109  loss_dice_6: 0.1662  loss_ce_7: 0.1095  loss_mask_7: 0.2167  loss_dice_7: 0.1627  loss_ce_8: 0.1071  loss_mask_8: 0.2262  loss_dice_8: 0.1621  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:37:32] d2.utils.events INFO:  eta: 0:57:38  iter: 22619  total_loss: 4.966  loss_ce: 0.15  loss_mask: 0.188  loss_dice: 0.1481  loss_ce_0: 0.07199  loss_mask_0: 0.1833  loss_dice_0: 0.1505  loss_ce_1: 0.1478  loss_mask_1: 0.1962  loss_dice_1: 0.1509  loss_ce_2: 0.1486  loss_mask_2: 0.1899  loss_dice_2: 0.1466  loss_ce_3: 0.1477  loss_mask_3: 0.1894  loss_dice_3: 0.145  loss_ce_4: 0.1466  loss_mask_4: 0.1969  loss_dice_4: 0.1432  loss_ce_5: 0.1479  loss_mask_5: 0.1978  loss_dice_5: 0.1483  loss_ce_6: 0.1472  loss_mask_6: 0.189  loss_dice_6: 0.1547  loss_ce_7: 0.1467  loss_mask_7: 0.1932  loss_dice_7: 0.1535  loss_ce_8: 0.1485  loss_mask_8: 0.195  loss_dice_8: 0.1499  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:35] d2.utils.events INFO:  eta: 0:57:34  iter: 22639  total_loss: 5.184  loss_ce: 0.1083  loss_mask: 0.2392  loss_dice: 0.1425  loss_ce_0: 0.0552  loss_mask_0: 0.2385  loss_dice_0: 0.1416  loss_ce_1: 0.1088  loss_mask_1: 0.2406  loss_dice_1: 0.1429  loss_ce_2: 0.1094  loss_mask_2: 0.2395  loss_dice_2: 0.1449  loss_ce_3: 0.1098  loss_mask_3: 0.244  loss_dice_3: 0.1442  loss_ce_4: 0.1104  loss_mask_4: 0.2381  loss_dice_4: 0.1359  loss_ce_5: 0.1095  loss_mask_5: 0.2368  loss_dice_5: 0.1435  loss_ce_6: 0.1102  loss_mask_6: 0.2374  loss_dice_6: 0.143  loss_ce_7: 0.1105  loss_mask_7: 0.2412  loss_dice_7: 0.1402  loss_ce_8: 0.1093  loss_mask_8: 0.2444  loss_dice_8: 0.1382  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:37] d2.utils.events INFO:  eta: 0:57:31  iter: 22659  total_loss: 4.997  loss_ce: 0.1294  loss_mask: 0.1956  loss_dice: 0.1187  loss_ce_0: 0.06358  loss_mask_0: 0.207  loss_dice_0: 0.1163  loss_ce_1: 0.1298  loss_mask_1: 0.2022  loss_dice_1: 0.1201  loss_ce_2: 0.1296  loss_mask_2: 0.2013  loss_dice_2: 0.1183  loss_ce_3: 0.1298  loss_mask_3: 0.2052  loss_dice_3: 0.1183  loss_ce_4: 0.13  loss_mask_4: 0.1916  loss_dice_4: 0.1165  loss_ce_5: 0.1298  loss_mask_5: 0.198  loss_dice_5: 0.1187  loss_ce_6: 0.1296  loss_mask_6: 0.2056  loss_dice_6: 0.1183  loss_ce_7: 0.1297  loss_mask_7: 0.1955  loss_dice_7: 0.1175  loss_ce_8: 0.1294  loss_mask_8: 0.2103  loss_dice_8: 0.1177  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:37:40] d2.utils.events INFO:  eta: 0:57:28  iter: 22679  total_loss: 4.69  loss_ce: 0.09401  loss_mask: 0.189  loss_dice: 0.1174  loss_ce_0: 0.0502  loss_mask_0: 0.1991  loss_dice_0: 0.12  loss_ce_1: 0.0937  loss_mask_1: 0.19  loss_dice_1: 0.1178  loss_ce_2: 0.09343  loss_mask_2: 0.198  loss_dice_2: 0.1145  loss_ce_3: 0.09333  loss_mask_3: 0.1965  loss_dice_3: 0.1172  loss_ce_4: 0.09182  loss_mask_4: 0.181  loss_dice_4: 0.118  loss_ce_5: 0.09332  loss_mask_5: 0.1937  loss_dice_5: 0.1171  loss_ce_6: 0.09261  loss_mask_6: 0.1913  loss_dice_6: 0.1149  loss_ce_7: 0.09223  loss_mask_7: 0.1987  loss_dice_7: 0.1164  loss_ce_8: 0.09335  loss_mask_8: 0.2004  loss_dice_8: 0.1164  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:37:42] d2.utils.events INFO:  eta: 0:57:29  iter: 22699  total_loss: 5.556  loss_ce: 0.09003  loss_mask: 0.1966  loss_dice: 0.2152  loss_ce_0: 0.04909  loss_mask_0: 0.1935  loss_dice_0: 0.2092  loss_ce_1: 0.08917  loss_mask_1: 0.1972  loss_dice_1: 0.1985  loss_ce_2: 0.08934  loss_mask_2: 0.1989  loss_dice_2: 0.2024  loss_ce_3: 0.08922  loss_mask_3: 0.2007  loss_dice_3: 0.2022  loss_ce_4: 0.08826  loss_mask_4: 0.2018  loss_dice_4: 0.1966  loss_ce_5: 0.0895  loss_mask_5: 0.1969  loss_dice_5: 0.1991  loss_ce_6: 0.08861  loss_mask_6: 0.2043  loss_dice_6: 0.2112  loss_ce_7: 0.08832  loss_mask_7: 0.2049  loss_dice_7: 0.2035  loss_ce_8: 0.08907  loss_mask_8: 0.201  loss_dice_8: 0.1972  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:45] d2.utils.events INFO:  eta: 0:57:28  iter: 22719  total_loss: 5.727  loss_ce: 0.1325  loss_mask: 0.2287  loss_dice: 0.1796  loss_ce_0: 0.06456  loss_mask_0: 0.2303  loss_dice_0: 0.1812  loss_ce_1: 0.1325  loss_mask_1: 0.2284  loss_dice_1: 0.1782  loss_ce_2: 0.1326  loss_mask_2: 0.2289  loss_dice_2: 0.1781  loss_ce_3: 0.1324  loss_mask_3: 0.232  loss_dice_3: 0.1688  loss_ce_4: 0.1326  loss_mask_4: 0.2273  loss_dice_4: 0.1794  loss_ce_5: 0.1324  loss_mask_5: 0.2406  loss_dice_5: 0.1829  loss_ce_6: 0.1326  loss_mask_6: 0.2292  loss_dice_6: 0.1756  loss_ce_7: 0.1325  loss_mask_7: 0.2324  loss_dice_7: 0.175  loss_ce_8: 0.1325  loss_mask_8: 0.2262  loss_dice_8: 0.1755  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:47] d2.utils.events INFO:  eta: 0:57:26  iter: 22739  total_loss: 6.285  loss_ce: 0.08829  loss_mask: 0.248  loss_dice: 0.2188  loss_ce_0: 0.04829  loss_mask_0: 0.2445  loss_dice_0: 0.2124  loss_ce_1: 0.08813  loss_mask_1: 0.2322  loss_dice_1: 0.2172  loss_ce_2: 0.0881  loss_mask_2: 0.2541  loss_dice_2: 0.2077  loss_ce_3: 0.08844  loss_mask_3: 0.2441  loss_dice_3: 0.2146  loss_ce_4: 0.08849  loss_mask_4: 0.2608  loss_dice_4: 0.2112  loss_ce_5: 0.0886  loss_mask_5: 0.246  loss_dice_5: 0.2103  loss_ce_6: 0.08818  loss_mask_6: 0.2437  loss_dice_6: 0.2091  loss_ce_7: 0.08819  loss_mask_7: 0.2452  loss_dice_7: 0.2133  loss_ce_8: 0.08815  loss_mask_8: 0.2475  loss_dice_8: 0.2133  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:50] d2.utils.events INFO:  eta: 0:57:23  iter: 22759  total_loss: 5.192  loss_ce: 0.08717  loss_mask: 0.1839  loss_dice: 0.1323  loss_ce_0: 0.04793  loss_mask_0: 0.1779  loss_dice_0: 0.1401  loss_ce_1: 0.08699  loss_mask_1: 0.1866  loss_dice_1: 0.1394  loss_ce_2: 0.08701  loss_mask_2: 0.1809  loss_dice_2: 0.1349  loss_ce_3: 0.08705  loss_mask_3: 0.1821  loss_dice_3: 0.1446  loss_ce_4: 0.08676  loss_mask_4: 0.1846  loss_dice_4: 0.1382  loss_ce_5: 0.08708  loss_mask_5: 0.1766  loss_dice_5: 0.1384  loss_ce_6: 0.08674  loss_mask_6: 0.1843  loss_dice_6: 0.1393  loss_ce_7: 0.08687  loss_mask_7: 0.182  loss_dice_7: 0.1366  loss_ce_8: 0.08708  loss_mask_8: 0.1802  loss_dice_8: 0.141  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:52] d2.utils.events INFO:  eta: 0:57:20  iter: 22779  total_loss: 5.488  loss_ce: 0.1328  loss_mask: 0.2278  loss_dice: 0.1555  loss_ce_0: 0.06489  loss_mask_0: 0.2185  loss_dice_0: 0.1584  loss_ce_1: 0.1314  loss_mask_1: 0.2135  loss_dice_1: 0.1561  loss_ce_2: 0.1324  loss_mask_2: 0.2172  loss_dice_2: 0.1515  loss_ce_3: 0.1323  loss_mask_3: 0.2087  loss_dice_3: 0.1522  loss_ce_4: 0.1321  loss_mask_4: 0.2171  loss_dice_4: 0.1514  loss_ce_5: 0.1324  loss_mask_5: 0.2158  loss_dice_5: 0.1569  loss_ce_6: 0.1324  loss_mask_6: 0.2147  loss_dice_6: 0.1578  loss_ce_7: 0.1322  loss_mask_7: 0.2173  loss_dice_7: 0.1574  loss_ce_8: 0.1324  loss_mask_8: 0.2202  loss_dice_8: 0.1512  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:55] d2.utils.events INFO:  eta: 0:57:20  iter: 22799  total_loss: 5.188  loss_ce: 0.163  loss_mask: 0.1965  loss_dice: 0.1626  loss_ce_0: 0.07777  loss_mask_0: 0.1921  loss_dice_0: 0.1593  loss_ce_1: 0.1592  loss_mask_1: 0.1859  loss_dice_1: 0.1582  loss_ce_2: 0.1598  loss_mask_2: 0.1916  loss_dice_2: 0.1622  loss_ce_3: 0.1584  loss_mask_3: 0.1902  loss_dice_3: 0.1651  loss_ce_4: 0.1547  loss_mask_4: 0.1906  loss_dice_4: 0.1593  loss_ce_5: 0.158  loss_mask_5: 0.2015  loss_dice_5: 0.1613  loss_ce_6: 0.1572  loss_mask_6: 0.1999  loss_dice_6: 0.1644  loss_ce_7: 0.1557  loss_mask_7: 0.1935  loss_dice_7: 0.1606  loss_ce_8: 0.1606  loss_mask_8: 0.2039  loss_dice_8: 0.1667  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:37:57] d2.utils.events INFO:  eta: 0:57:19  iter: 22819  total_loss: 4.744  loss_ce: 0.1511  loss_mask: 0.202  loss_dice: 0.1375  loss_ce_0: 0.07341  loss_mask_0: 0.2041  loss_dice_0: 0.1384  loss_ce_1: 0.1476  loss_mask_1: 0.1938  loss_dice_1: 0.1304  loss_ce_2: 0.1486  loss_mask_2: 0.197  loss_dice_2: 0.1358  loss_ce_3: 0.1477  loss_mask_3: 0.1962  loss_dice_3: 0.1353  loss_ce_4: 0.1444  loss_mask_4: 0.2014  loss_dice_4: 0.134  loss_ce_5: 0.148  loss_mask_5: 0.1966  loss_dice_5: 0.1356  loss_ce_6: 0.1466  loss_mask_6: 0.1976  loss_dice_6: 0.1381  loss_ce_7: 0.1452  loss_mask_7: 0.1862  loss_dice_7: 0.1301  loss_ce_8: 0.1485  loss_mask_8: 0.1942  loss_dice_8: 0.1405  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:00] d2.utils.events INFO:  eta: 0:57:18  iter: 22839  total_loss: 4.994  loss_ce: 0.1361  loss_mask: 0.1821  loss_dice: 0.1562  loss_ce_0: 0.06712  loss_mask_0: 0.1919  loss_dice_0: 0.1528  loss_ce_1: 0.1315  loss_mask_1: 0.1851  loss_dice_1: 0.1547  loss_ce_2: 0.1326  loss_mask_2: 0.1787  loss_dice_2: 0.1508  loss_ce_3: 0.1314  loss_mask_3: 0.1957  loss_dice_3: 0.1542  loss_ce_4: 0.1273  loss_mask_4: 0.1866  loss_dice_4: 0.1512  loss_ce_5: 0.1321  loss_mask_5: 0.1912  loss_dice_5: 0.1506  loss_ce_6: 0.1302  loss_mask_6: 0.1962  loss_dice_6: 0.1534  loss_ce_7: 0.1289  loss_mask_7: 0.192  loss_dice_7: 0.1511  loss_ce_8: 0.1322  loss_mask_8: 0.187  loss_dice_8: 0.1549  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:38:03] d2.utils.events INFO:  eta: 0:57:16  iter: 22859  total_loss: 5.606  loss_ce: 0.1293  loss_mask: 0.1936  loss_dice: 0.1587  loss_ce_0: 0.06291  loss_mask_0: 0.1848  loss_dice_0: 0.1563  loss_ce_1: 0.1315  loss_mask_1: 0.1796  loss_dice_1: 0.1584  loss_ce_2: 0.1304  loss_mask_2: 0.1817  loss_dice_2: 0.1627  loss_ce_3: 0.1306  loss_mask_3: 0.1822  loss_dice_3: 0.1625  loss_ce_4: 0.1351  loss_mask_4: 0.1852  loss_dice_4: 0.1617  loss_ce_5: 0.1304  loss_mask_5: 0.1877  loss_dice_5: 0.1667  loss_ce_6: 0.1307  loss_mask_6: 0.1904  loss_dice_6: 0.1635  loss_ce_7: 0.1324  loss_mask_7: 0.1754  loss_dice_7: 0.1599  loss_ce_8: 0.1307  loss_mask_8: 0.1925  loss_dice_8: 0.1631  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:05] d2.utils.events INFO:  eta: 0:57:16  iter: 22879  total_loss: 5.242  loss_ce: 0.1276  loss_mask: 0.1935  loss_dice: 0.1521  loss_ce_0: 0.0631  loss_mask_0: 0.1878  loss_dice_0: 0.1553  loss_ce_1: 0.1272  loss_mask_1: 0.205  loss_dice_1: 0.1505  loss_ce_2: 0.1272  loss_mask_2: 0.2014  loss_dice_2: 0.1617  loss_ce_3: 0.1273  loss_mask_3: 0.1913  loss_dice_3: 0.157  loss_ce_4: 0.128  loss_mask_4: 0.1931  loss_dice_4: 0.1538  loss_ce_5: 0.1273  loss_mask_5: 0.1916  loss_dice_5: 0.1561  loss_ce_6: 0.1272  loss_mask_6: 0.201  loss_dice_6: 0.1552  loss_ce_7: 0.1273  loss_mask_7: 0.1847  loss_dice_7: 0.1483  loss_ce_8: 0.1271  loss_mask_8: 0.1893  loss_dice_8: 0.1528  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:38:08] d2.utils.events INFO:  eta: 0:57:12  iter: 22899  total_loss: 4.261  loss_ce: 0.1242  loss_mask: 0.1737  loss_dice: 0.1202  loss_ce_0: 0.06295  loss_mask_0: 0.1668  loss_dice_0: 0.1232  loss_ce_1: 0.1235  loss_mask_1: 0.1659  loss_dice_1: 0.1219  loss_ce_2: 0.1239  loss_mask_2: 0.1628  loss_dice_2: 0.1221  loss_ce_3: 0.1244  loss_mask_3: 0.168  loss_dice_3: 0.1236  loss_ce_4: 0.1239  loss_mask_4: 0.1646  loss_dice_4: 0.1213  loss_ce_5: 0.1248  loss_mask_5: 0.1698  loss_dice_5: 0.1179  loss_ce_6: 0.1234  loss_mask_6: 0.1707  loss_dice_6: 0.1222  loss_ce_7: 0.1237  loss_mask_7: 0.1711  loss_dice_7: 0.128  loss_ce_8: 0.1231  loss_mask_8: 0.1747  loss_dice_8: 0.1283  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:38:10] d2.utils.events INFO:  eta: 0:57:10  iter: 22919  total_loss: 5.202  loss_ce: 0.13  loss_mask: 0.2456  loss_dice: 0.1529  loss_ce_0: 0.06384  loss_mask_0: 0.2384  loss_dice_0: 0.1513  loss_ce_1: 0.1302  loss_mask_1: 0.2396  loss_dice_1: 0.1531  loss_ce_2: 0.1303  loss_mask_2: 0.2408  loss_dice_2: 0.1481  loss_ce_3: 0.1307  loss_mask_3: 0.236  loss_dice_3: 0.1489  loss_ce_4: 0.1318  loss_mask_4: 0.2442  loss_dice_4: 0.1522  loss_ce_5: 0.1303  loss_mask_5: 0.234  loss_dice_5: 0.1494  loss_ce_6: 0.1305  loss_mask_6: 0.2446  loss_dice_6: 0.154  loss_ce_7: 0.131  loss_mask_7: 0.2472  loss_dice_7: 0.1496  loss_ce_8: 0.1307  loss_mask_8: 0.2334  loss_dice_8: 0.1501  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:38:13] d2.utils.events INFO:  eta: 0:57:07  iter: 22939  total_loss: 5  loss_ce: 0.1279  loss_mask: 0.1927  loss_dice: 0.134  loss_ce_0: 0.06304  loss_mask_0: 0.199  loss_dice_0: 0.1399  loss_ce_1: 0.128  loss_mask_1: 0.192  loss_dice_1: 0.1357  loss_ce_2: 0.1274  loss_mask_2: 0.2058  loss_dice_2: 0.1415  loss_ce_3: 0.1277  loss_mask_3: 0.1932  loss_dice_3: 0.1369  loss_ce_4: 0.1276  loss_mask_4: 0.2036  loss_dice_4: 0.136  loss_ce_5: 0.1275  loss_mask_5: 0.2052  loss_dice_5: 0.1361  loss_ce_6: 0.1276  loss_mask_6: 0.1986  loss_dice_6: 0.1327  loss_ce_7: 0.1276  loss_mask_7: 0.1979  loss_dice_7: 0.1378  loss_ce_8: 0.1276  loss_mask_8: 0.203  loss_dice_8: 0.1359  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:15] d2.utils.events INFO:  eta: 0:57:04  iter: 22959  total_loss: 4.765  loss_ce: 0.1267  loss_mask: 0.2068  loss_dice: 0.1348  loss_ce_0: 0.06279  loss_mask_0: 0.2146  loss_dice_0: 0.129  loss_ce_1: 0.1246  loss_mask_1: 0.2108  loss_dice_1: 0.1341  loss_ce_2: 0.1262  loss_mask_2: 0.2037  loss_dice_2: 0.1334  loss_ce_3: 0.1259  loss_mask_3: 0.2011  loss_dice_3: 0.1374  loss_ce_4: 0.1257  loss_mask_4: 0.2148  loss_dice_4: 0.1381  loss_ce_5: 0.1259  loss_mask_5: 0.2186  loss_dice_5: 0.1376  loss_ce_6: 0.1262  loss_mask_6: 0.2086  loss_dice_6: 0.1303  loss_ce_7: 0.1263  loss_mask_7: 0.2065  loss_dice_7: 0.1323  loss_ce_8: 0.1262  loss_mask_8: 0.2026  loss_dice_8: 0.1358  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:18] d2.utils.events INFO:  eta: 0:57:02  iter: 22979  total_loss: 4.863  loss_ce: 0.1142  loss_mask: 0.1915  loss_dice: 0.1394  loss_ce_0: 0.05787  loss_mask_0: 0.1937  loss_dice_0: 0.1388  loss_ce_1: 0.112  loss_mask_1: 0.1883  loss_dice_1: 0.1418  loss_ce_2: 0.1127  loss_mask_2: 0.1891  loss_dice_2: 0.1373  loss_ce_3: 0.1103  loss_mask_3: 0.1962  loss_dice_3: 0.1505  loss_ce_4: 0.1067  loss_mask_4: 0.1777  loss_dice_4: 0.138  loss_ce_5: 0.1117  loss_mask_5: 0.1802  loss_dice_5: 0.14  loss_ce_6: 0.1104  loss_mask_6: 0.1939  loss_dice_6: 0.1397  loss_ce_7: 0.1101  loss_mask_7: 0.1901  loss_dice_7: 0.1444  loss_ce_8: 0.1125  loss_mask_8: 0.188  loss_dice_8: 0.1424  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:20] d2.utils.events INFO:  eta: 0:57:00  iter: 22999  total_loss: 7.083  loss_ce: 0.1515  loss_mask: 0.1738  loss_dice: 0.255  loss_ce_0: 0.07147  loss_mask_0: 0.1659  loss_dice_0: 0.2574  loss_ce_1: 0.1541  loss_mask_1: 0.1734  loss_dice_1: 0.2489  loss_ce_2: 0.1524  loss_mask_2: 0.1671  loss_dice_2: 0.2526  loss_ce_3: 0.1548  loss_mask_3: 0.1707  loss_dice_3: 0.2578  loss_ce_4: 0.1599  loss_mask_4: 0.1683  loss_dice_4: 0.266  loss_ce_5: 0.1529  loss_mask_5: 0.1693  loss_dice_5: 0.2557  loss_ce_6: 0.156  loss_mask_6: 0.1658  loss_dice_6: 0.2519  loss_ce_7: 0.1567  loss_mask_7: 0.1617  loss_dice_7: 0.2556  loss_ce_8: 0.1531  loss_mask_8: 0.1733  loss_dice_8: 0.262  time: 0.1261  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:38:23] d2.utils.events INFO:  eta: 0:56:56  iter: 23019  total_loss: 5.189  loss_ce: 0.1292  loss_mask: 0.1904  loss_dice: 0.1636  loss_ce_0: 0.06349  loss_mask_0: 0.1994  loss_dice_0: 0.1676  loss_ce_1: 0.129  loss_mask_1: 0.1929  loss_dice_1: 0.1657  loss_ce_2: 0.1292  loss_mask_2: 0.1958  loss_dice_2: 0.1648  loss_ce_3: 0.1291  loss_mask_3: 0.1995  loss_dice_3: 0.1737  loss_ce_4: 0.1272  loss_mask_4: 0.1938  loss_dice_4: 0.1628  loss_ce_5: 0.1291  loss_mask_5: 0.1944  loss_dice_5: 0.1627  loss_ce_6: 0.1291  loss_mask_6: 0.1891  loss_dice_6: 0.1675  loss_ce_7: 0.1288  loss_mask_7: 0.1937  loss_dice_7: 0.1662  loss_ce_8: 0.1291  loss_mask_8: 0.1908  loss_dice_8: 0.1632  time: 0.1261  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:25] d2.utils.events INFO:  eta: 0:56:52  iter: 23039  total_loss: 5.764  loss_ce: 0.148  loss_mask: 0.2018  loss_dice: 0.1998  loss_ce_0: 0.07045  loss_mask_0: 0.2036  loss_dice_0: 0.1956  loss_ce_1: 0.148  loss_mask_1: 0.1969  loss_dice_1: 0.202  loss_ce_2: 0.148  loss_mask_2: 0.1985  loss_dice_2: 0.1982  loss_ce_3: 0.1473  loss_mask_3: 0.1936  loss_dice_3: 0.2  loss_ce_4: 0.1451  loss_mask_4: 0.2003  loss_dice_4: 0.196  loss_ce_5: 0.1475  loss_mask_5: 0.2009  loss_dice_5: 0.2053  loss_ce_6: 0.1481  loss_mask_6: 0.1988  loss_dice_6: 0.1958  loss_ce_7: 0.1469  loss_mask_7: 0.2021  loss_dice_7: 0.1942  loss_ce_8: 0.1471  loss_mask_8: 0.1941  loss_dice_8: 0.1966  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:38:28] d2.utils.events INFO:  eta: 0:56:49  iter: 23059  total_loss: 5.383  loss_ce: 0.1366  loss_mask: 0.1786  loss_dice: 0.1965  loss_ce_0: 0.06657  loss_mask_0: 0.1819  loss_dice_0: 0.1934  loss_ce_1: 0.1351  loss_mask_1: 0.1875  loss_dice_1: 0.2046  loss_ce_2: 0.1358  loss_mask_2: 0.1843  loss_dice_2: 0.1944  loss_ce_3: 0.1344  loss_mask_3: 0.1703  loss_dice_3: 0.1876  loss_ce_4: 0.1307  loss_mask_4: 0.1835  loss_dice_4: 0.2029  loss_ce_5: 0.1353  loss_mask_5: 0.1819  loss_dice_5: 0.2037  loss_ce_6: 0.1352  loss_mask_6: 0.1915  loss_dice_6: 0.1968  loss_ce_7: 0.1337  loss_mask_7: 0.1859  loss_dice_7: 0.1984  loss_ce_8: 0.1344  loss_mask_8: 0.1841  loss_dice_8: 0.2006  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:30] d2.utils.events INFO:  eta: 0:56:46  iter: 23079  total_loss: 5.309  loss_ce: 0.1274  loss_mask: 0.177  loss_dice: 0.1794  loss_ce_0: 0.06306  loss_mask_0: 0.1805  loss_dice_0: 0.194  loss_ce_1: 0.1276  loss_mask_1: 0.179  loss_dice_1: 0.1854  loss_ce_2: 0.1276  loss_mask_2: 0.1865  loss_dice_2: 0.1883  loss_ce_3: 0.1277  loss_mask_3: 0.1791  loss_dice_3: 0.1867  loss_ce_4: 0.1265  loss_mask_4: 0.1733  loss_dice_4: 0.1912  loss_ce_5: 0.1276  loss_mask_5: 0.1756  loss_dice_5: 0.1903  loss_ce_6: 0.1277  loss_mask_6: 0.1786  loss_dice_6: 0.1802  loss_ce_7: 0.1277  loss_mask_7: 0.1706  loss_dice_7: 0.189  loss_ce_8: 0.1276  loss_mask_8: 0.1773  loss_dice_8: 0.1904  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:33] d2.utils.events INFO:  eta: 0:56:43  iter: 23099  total_loss: 5.207  loss_ce: 0.1275  loss_mask: 0.2076  loss_dice: 0.1511  loss_ce_0: 0.06298  loss_mask_0: 0.2132  loss_dice_0: 0.1482  loss_ce_1: 0.1278  loss_mask_1: 0.2154  loss_dice_1: 0.1541  loss_ce_2: 0.1275  loss_mask_2: 0.2127  loss_dice_2: 0.1492  loss_ce_3: 0.1277  loss_mask_3: 0.2134  loss_dice_3: 0.1417  loss_ce_4: 0.1278  loss_mask_4: 0.2253  loss_dice_4: 0.1474  loss_ce_5: 0.1277  loss_mask_5: 0.2123  loss_dice_5: 0.1419  loss_ce_6: 0.1274  loss_mask_6: 0.2153  loss_dice_6: 0.1461  loss_ce_7: 0.1276  loss_mask_7: 0.2148  loss_dice_7: 0.1418  loss_ce_8: 0.1277  loss_mask_8: 0.2222  loss_dice_8: 0.148  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:36] d2.utils.events INFO:  eta: 0:56:41  iter: 23119  total_loss: 5.974  loss_ce: 0.1231  loss_mask: 0.2007  loss_dice: 0.2258  loss_ce_0: 0.06181  loss_mask_0: 0.2065  loss_dice_0: 0.2229  loss_ce_1: 0.1228  loss_mask_1: 0.2094  loss_dice_1: 0.2361  loss_ce_2: 0.123  loss_mask_2: 0.1964  loss_dice_2: 0.2479  loss_ce_3: 0.123  loss_mask_3: 0.201  loss_dice_3: 0.239  loss_ce_4: 0.1216  loss_mask_4: 0.1982  loss_dice_4: 0.2283  loss_ce_5: 0.1233  loss_mask_5: 0.2001  loss_dice_5: 0.2366  loss_ce_6: 0.1221  loss_mask_6: 0.1964  loss_dice_6: 0.2376  loss_ce_7: 0.1219  loss_mask_7: 0.2053  loss_dice_7: 0.2355  loss_ce_8: 0.1221  loss_mask_8: 0.2127  loss_dice_8: 0.2394  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:38] d2.utils.events INFO:  eta: 0:56:39  iter: 23139  total_loss: 5.071  loss_ce: 0.1148  loss_mask: 0.2443  loss_dice: 0.1561  loss_ce_0: 0.0586  loss_mask_0: 0.2347  loss_dice_0: 0.1542  loss_ce_1: 0.1131  loss_mask_1: 0.2391  loss_dice_1: 0.1531  loss_ce_2: 0.1138  loss_mask_2: 0.2414  loss_dice_2: 0.1544  loss_ce_3: 0.1132  loss_mask_3: 0.24  loss_dice_3: 0.1575  loss_ce_4: 0.1107  loss_mask_4: 0.2419  loss_dice_4: 0.1592  loss_ce_5: 0.1137  loss_mask_5: 0.2436  loss_dice_5: 0.1583  loss_ce_6: 0.1128  loss_mask_6: 0.2336  loss_dice_6: 0.1556  loss_ce_7: 0.112  loss_mask_7: 0.241  loss_dice_7: 0.1561  loss_ce_8: 0.1132  loss_mask_8: 0.2374  loss_dice_8: 0.1503  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:41] d2.utils.events INFO:  eta: 0:56:35  iter: 23159  total_loss: 4.838  loss_ce: 0.128  loss_mask: 0.2077  loss_dice: 0.1543  loss_ce_0: 0.0631  loss_mask_0: 0.211  loss_dice_0: 0.1503  loss_ce_1: 0.1273  loss_mask_1: 0.2167  loss_dice_1: 0.1525  loss_ce_2: 0.1278  loss_mask_2: 0.206  loss_dice_2: 0.149  loss_ce_3: 0.1278  loss_mask_3: 0.2126  loss_dice_3: 0.1516  loss_ce_4: 0.1276  loss_mask_4: 0.2092  loss_dice_4: 0.1516  loss_ce_5: 0.1278  loss_mask_5: 0.2122  loss_dice_5: 0.1533  loss_ce_6: 0.128  loss_mask_6: 0.2129  loss_dice_6: 0.1436  loss_ce_7: 0.1277  loss_mask_7: 0.2079  loss_dice_7: 0.1557  loss_ce_8: 0.1279  loss_mask_8: 0.2121  loss_dice_8: 0.1534  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:43] d2.utils.events INFO:  eta: 0:56:30  iter: 23179  total_loss: 5.067  loss_ce: 0.1103  loss_mask: 0.2057  loss_dice: 0.1645  loss_ce_0: 0.05708  loss_mask_0: 0.215  loss_dice_0: 0.1686  loss_ce_1: 0.1097  loss_mask_1: 0.1983  loss_dice_1: 0.1623  loss_ce_2: 0.1104  loss_mask_2: 0.2142  loss_dice_2: 0.1674  loss_ce_3: 0.1109  loss_mask_3: 0.2055  loss_dice_3: 0.1692  loss_ce_4: 0.1107  loss_mask_4: 0.2084  loss_dice_4: 0.1673  loss_ce_5: 0.1113  loss_mask_5: 0.2035  loss_dice_5: 0.1613  loss_ce_6: 0.1104  loss_mask_6: 0.2046  loss_dice_6: 0.1589  loss_ce_7: 0.1107  loss_mask_7: 0.1952  loss_dice_7: 0.1614  loss_ce_8: 0.1101  loss_mask_8: 0.2148  loss_dice_8: 0.1654  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:38:46] d2.utils.events INFO:  eta: 0:56:28  iter: 23199  total_loss: 5.084  loss_ce: 0.1019  loss_mask: 0.1996  loss_dice: 0.1371  loss_ce_0: 0.0538  loss_mask_0: 0.1974  loss_dice_0: 0.1382  loss_ce_1: 0.1005  loss_mask_1: 0.1945  loss_dice_1: 0.1373  loss_ce_2: 0.1008  loss_mask_2: 0.2001  loss_dice_2: 0.1368  loss_ce_3: 0.1002  loss_mask_3: 0.1953  loss_dice_3: 0.1404  loss_ce_4: 0.09842  loss_mask_4: 0.1857  loss_dice_4: 0.1358  loss_ce_5: 0.1005  loss_mask_5: 0.1773  loss_dice_5: 0.1366  loss_ce_6: 0.09991  loss_mask_6: 0.1881  loss_dice_6: 0.1412  loss_ce_7: 0.09918  loss_mask_7: 0.1972  loss_dice_7: 0.1347  loss_ce_8: 0.1004  loss_mask_8: 0.1905  loss_dice_8: 0.1372  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:48] d2.utils.events INFO:  eta: 0:56:23  iter: 23219  total_loss: 5.327  loss_ce: 0.09744  loss_mask: 0.2045  loss_dice: 0.161  loss_ce_0: 0.05221  loss_mask_0: 0.209  loss_dice_0: 0.1564  loss_ce_1: 0.09712  loss_mask_1: 0.2078  loss_dice_1: 0.1625  loss_ce_2: 0.09695  loss_mask_2: 0.2007  loss_dice_2: 0.1606  loss_ce_3: 0.0968  loss_mask_3: 0.2027  loss_dice_3: 0.1561  loss_ce_4: 0.09611  loss_mask_4: 0.2001  loss_dice_4: 0.1538  loss_ce_5: 0.0971  loss_mask_5: 0.2055  loss_dice_5: 0.1616  loss_ce_6: 0.09652  loss_mask_6: 0.2092  loss_dice_6: 0.1613  loss_ce_7: 0.09629  loss_mask_7: 0.1987  loss_dice_7: 0.1605  loss_ce_8: 0.09657  loss_mask_8: 0.2067  loss_dice_8: 0.1629  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:51] d2.utils.events INFO:  eta: 0:56:20  iter: 23239  total_loss: 4.835  loss_ce: 0.1521  loss_mask: 0.237  loss_dice: 0.1323  loss_ce_0: 0.07195  loss_mask_0: 0.2216  loss_dice_0: 0.131  loss_ce_1: 0.1507  loss_mask_1: 0.2284  loss_dice_1: 0.1359  loss_ce_2: 0.1511  loss_mask_2: 0.2258  loss_dice_2: 0.1239  loss_ce_3: 0.15  loss_mask_3: 0.2393  loss_dice_3: 0.136  loss_ce_4: 0.1482  loss_mask_4: 0.2303  loss_dice_4: 0.1383  loss_ce_5: 0.1494  loss_mask_5: 0.2375  loss_dice_5: 0.1316  loss_ce_6: 0.1495  loss_mask_6: 0.2392  loss_dice_6: 0.129  loss_ce_7: 0.1484  loss_mask_7: 0.2371  loss_dice_7: 0.1299  loss_ce_8: 0.1509  loss_mask_8: 0.2299  loss_dice_8: 0.1349  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:53] d2.utils.events INFO:  eta: 0:56:13  iter: 23259  total_loss: 5.226  loss_ce: 0.1122  loss_mask: 0.1877  loss_dice: 0.1577  loss_ce_0: 0.05735  loss_mask_0: 0.1753  loss_dice_0: 0.1628  loss_ce_1: 0.114  loss_mask_1: 0.1776  loss_dice_1: 0.1547  loss_ce_2: 0.1136  loss_mask_2: 0.1751  loss_dice_2: 0.1551  loss_ce_3: 0.1149  loss_mask_3: 0.1788  loss_dice_3: 0.1589  loss_ce_4: 0.117  loss_mask_4: 0.1709  loss_dice_4: 0.1568  loss_ce_5: 0.115  loss_mask_5: 0.1879  loss_dice_5: 0.1549  loss_ce_6: 0.1154  loss_mask_6: 0.1838  loss_dice_6: 0.1606  loss_ce_7: 0.1165  loss_mask_7: 0.1779  loss_dice_7: 0.1561  loss_ce_8: 0.1139  loss_mask_8: 0.1818  loss_dice_8: 0.1614  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:56] d2.utils.events INFO:  eta: 0:56:14  iter: 23279  total_loss: 5.341  loss_ce: 0.1105  loss_mask: 0.2324  loss_dice: 0.1579  loss_ce_0: 0.05611  loss_mask_0: 0.2386  loss_dice_0: 0.1574  loss_ce_1: 0.1109  loss_mask_1: 0.2319  loss_dice_1: 0.1627  loss_ce_2: 0.1106  loss_mask_2: 0.2253  loss_dice_2: 0.1569  loss_ce_3: 0.1104  loss_mask_3: 0.2275  loss_dice_3: 0.162  loss_ce_4: 0.1109  loss_mask_4: 0.225  loss_dice_4: 0.1537  loss_ce_5: 0.1102  loss_mask_5: 0.2269  loss_dice_5: 0.1592  loss_ce_6: 0.1107  loss_mask_6: 0.229  loss_dice_6: 0.1559  loss_ce_7: 0.1113  loss_mask_7: 0.235  loss_dice_7: 0.1614  loss_ce_8: 0.1112  loss_mask_8: 0.2291  loss_dice_8: 0.1593  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:38:58] d2.utils.events INFO:  eta: 0:56:11  iter: 23299  total_loss: 4.94  loss_ce: 0.1286  loss_mask: 0.1915  loss_dice: 0.1534  loss_ce_0: 0.06339  loss_mask_0: 0.2033  loss_dice_0: 0.154  loss_ce_1: 0.1303  loss_mask_1: 0.1894  loss_dice_1: 0.1551  loss_ce_2: 0.1289  loss_mask_2: 0.1926  loss_dice_2: 0.1485  loss_ce_3: 0.1289  loss_mask_3: 0.1867  loss_dice_3: 0.1534  loss_ce_4: 0.1293  loss_mask_4: 0.1826  loss_dice_4: 0.1514  loss_ce_5: 0.1289  loss_mask_5: 0.1857  loss_dice_5: 0.1533  loss_ce_6: 0.1288  loss_mask_6: 0.185  loss_dice_6: 0.1512  loss_ce_7: 0.129  loss_mask_7: 0.1879  loss_dice_7: 0.1497  loss_ce_8: 0.1291  loss_mask_8: 0.1905  loss_dice_8: 0.1545  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:01] d2.utils.events INFO:  eta: 0:56:05  iter: 23319  total_loss: 4.989  loss_ce: 0.1027  loss_mask: 0.21  loss_dice: 0.149  loss_ce_0: 0.05347  loss_mask_0: 0.2129  loss_dice_0: 0.146  loss_ce_1: 0.104  loss_mask_1: 0.2115  loss_dice_1: 0.1473  loss_ce_2: 0.1026  loss_mask_2: 0.2086  loss_dice_2: 0.1433  loss_ce_3: 0.1025  loss_mask_3: 0.2028  loss_dice_3: 0.1459  loss_ce_4: 0.1025  loss_mask_4: 0.2111  loss_dice_4: 0.1482  loss_ce_5: 0.1027  loss_mask_5: 0.212  loss_dice_5: 0.1512  loss_ce_6: 0.1024  loss_mask_6: 0.2121  loss_dice_6: 0.1464  loss_ce_7: 0.1021  loss_mask_7: 0.2018  loss_dice_7: 0.1444  loss_ce_8: 0.1025  loss_mask_8: 0.2026  loss_dice_8: 0.1479  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:03] d2.utils.events INFO:  eta: 0:56:03  iter: 23339  total_loss: 4.864  loss_ce: 0.1292  loss_mask: 0.207  loss_dice: 0.1413  loss_ce_0: 0.06361  loss_mask_0: 0.1916  loss_dice_0: 0.1305  loss_ce_1: 0.1291  loss_mask_1: 0.1993  loss_dice_1: 0.1366  loss_ce_2: 0.1292  loss_mask_2: 0.1872  loss_dice_2: 0.1355  loss_ce_3: 0.1293  loss_mask_3: 0.2009  loss_dice_3: 0.1392  loss_ce_4: 0.1292  loss_mask_4: 0.1956  loss_dice_4: 0.1372  loss_ce_5: 0.1292  loss_mask_5: 0.1968  loss_dice_5: 0.1383  loss_ce_6: 0.1292  loss_mask_6: 0.2015  loss_dice_6: 0.1351  loss_ce_7: 0.1293  loss_mask_7: 0.2093  loss_dice_7: 0.1398  loss_ce_8: 0.1293  loss_mask_8: 0.1977  loss_dice_8: 0.1429  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:06] d2.utils.events INFO:  eta: 0:56:00  iter: 23359  total_loss: 5.692  loss_ce: 0.1279  loss_mask: 0.1994  loss_dice: 0.1682  loss_ce_0: 0.06317  loss_mask_0: 0.1995  loss_dice_0: 0.1748  loss_ce_1: 0.1281  loss_mask_1: 0.1942  loss_dice_1: 0.1748  loss_ce_2: 0.1279  loss_mask_2: 0.2038  loss_dice_2: 0.1703  loss_ce_3: 0.1277  loss_mask_3: 0.2068  loss_dice_3: 0.1769  loss_ce_4: 0.1275  loss_mask_4: 0.2066  loss_dice_4: 0.1689  loss_ce_5: 0.1277  loss_mask_5: 0.2001  loss_dice_5: 0.1697  loss_ce_6: 0.1277  loss_mask_6: 0.1943  loss_dice_6: 0.1715  loss_ce_7: 0.1276  loss_mask_7: 0.1946  loss_dice_7: 0.1686  loss_ce_8: 0.1279  loss_mask_8: 0.1976  loss_dice_8: 0.174  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:39:08] d2.utils.events INFO:  eta: 0:55:58  iter: 23379  total_loss: 5.143  loss_ce: 0.1124  loss_mask: 0.2041  loss_dice: 0.1582  loss_ce_0: 0.05728  loss_mask_0: 0.2067  loss_dice_0: 0.1655  loss_ce_1: 0.1134  loss_mask_1: 0.212  loss_dice_1: 0.1604  loss_ce_2: 0.113  loss_mask_2: 0.2125  loss_dice_2: 0.1651  loss_ce_3: 0.1137  loss_mask_3: 0.2062  loss_dice_3: 0.1639  loss_ce_4: 0.1147  loss_mask_4: 0.2052  loss_dice_4: 0.164  loss_ce_5: 0.1135  loss_mask_5: 0.2005  loss_dice_5: 0.1638  loss_ce_6: 0.1138  loss_mask_6: 0.2012  loss_dice_6: 0.1643  loss_ce_7: 0.1144  loss_mask_7: 0.2091  loss_dice_7: 0.1683  loss_ce_8: 0.1133  loss_mask_8: 0.1986  loss_dice_8: 0.1599  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:11] d2.utils.events INFO:  eta: 0:55:51  iter: 23399  total_loss: 5.247  loss_ce: 0.1278  loss_mask: 0.1802  loss_dice: 0.2039  loss_ce_0: 0.06314  loss_mask_0: 0.1899  loss_dice_0: 0.2046  loss_ce_1: 0.1272  loss_mask_1: 0.1698  loss_dice_1: 0.1953  loss_ce_2: 0.1279  loss_mask_2: 0.1831  loss_dice_2: 0.205  loss_ce_3: 0.1279  loss_mask_3: 0.1702  loss_dice_3: 0.201  loss_ce_4: 0.1279  loss_mask_4: 0.1754  loss_dice_4: 0.1961  loss_ce_5: 0.128  loss_mask_5: 0.1821  loss_dice_5: 0.2015  loss_ce_6: 0.1279  loss_mask_6: 0.1755  loss_dice_6: 0.201  loss_ce_7: 0.1279  loss_mask_7: 0.191  loss_dice_7: 0.1995  loss_ce_8: 0.1278  loss_mask_8: 0.1787  loss_dice_8: 0.1993  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:13] d2.utils.events INFO:  eta: 0:55:46  iter: 23419  total_loss: 4.983  loss_ce: 0.1416  loss_mask: 0.189  loss_dice: 0.1456  loss_ce_0: 0.06831  loss_mask_0: 0.1847  loss_dice_0: 0.1513  loss_ce_1: 0.1406  loss_mask_1: 0.1878  loss_dice_1: 0.1501  loss_ce_2: 0.1411  loss_mask_2: 0.1824  loss_dice_2: 0.1475  loss_ce_3: 0.1408  loss_mask_3: 0.1882  loss_dice_3: 0.1499  loss_ce_4: 0.1403  loss_mask_4: 0.1763  loss_dice_4: 0.1519  loss_ce_5: 0.1411  loss_mask_5: 0.1831  loss_dice_5: 0.1471  loss_ce_6: 0.141  loss_mask_6: 0.1862  loss_dice_6: 0.1519  loss_ce_7: 0.1409  loss_mask_7: 0.1771  loss_dice_7: 0.1455  loss_ce_8: 0.1412  loss_mask_8: 0.1828  loss_dice_8: 0.1511  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:16] d2.utils.events INFO:  eta: 0:55:43  iter: 23439  total_loss: 5.005  loss_ce: 0.1341  loss_mask: 0.23  loss_dice: 0.1586  loss_ce_0: 0.06537  loss_mask_0: 0.2276  loss_dice_0: 0.1487  loss_ce_1: 0.1321  loss_mask_1: 0.2343  loss_dice_1: 0.148  loss_ce_2: 0.133  loss_mask_2: 0.2215  loss_dice_2: 0.1486  loss_ce_3: 0.1323  loss_mask_3: 0.2238  loss_dice_3: 0.1533  loss_ce_4: 0.1303  loss_mask_4: 0.2258  loss_dice_4: 0.1454  loss_ce_5: 0.1324  loss_mask_5: 0.2124  loss_dice_5: 0.1532  loss_ce_6: 0.1322  loss_mask_6: 0.2231  loss_dice_6: 0.1502  loss_ce_7: 0.1317  loss_mask_7: 0.2174  loss_dice_7: 0.15  loss_ce_8: 0.1331  loss_mask_8: 0.2145  loss_dice_8: 0.1487  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:19] d2.utils.events INFO:  eta: 0:55:42  iter: 23459  total_loss: 4.929  loss_ce: 0.1278  loss_mask: 0.184  loss_dice: 0.1443  loss_ce_0: 0.06292  loss_mask_0: 0.1799  loss_dice_0: 0.1412  loss_ce_1: 0.1273  loss_mask_1: 0.1762  loss_dice_1: 0.1464  loss_ce_2: 0.1275  loss_mask_2: 0.1836  loss_dice_2: 0.1464  loss_ce_3: 0.1288  loss_mask_3: 0.1742  loss_dice_3: 0.145  loss_ce_4: 0.1314  loss_mask_4: 0.1844  loss_dice_4: 0.1411  loss_ce_5: 0.1284  loss_mask_5: 0.1765  loss_dice_5: 0.1398  loss_ce_6: 0.1289  loss_mask_6: 0.1695  loss_dice_6: 0.1477  loss_ce_7: 0.1294  loss_mask_7: 0.1769  loss_dice_7: 0.148  loss_ce_8: 0.1274  loss_mask_8: 0.1874  loss_dice_8: 0.1461  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:39:21] d2.utils.events INFO:  eta: 0:55:40  iter: 23479  total_loss: 4.762  loss_ce: 0.1269  loss_mask: 0.1744  loss_dice: 0.1765  loss_ce_0: 0.06305  loss_mask_0: 0.1682  loss_dice_0: 0.1681  loss_ce_1: 0.1267  loss_mask_1: 0.1723  loss_dice_1: 0.177  loss_ce_2: 0.1273  loss_mask_2: 0.1855  loss_dice_2: 0.1677  loss_ce_3: 0.1276  loss_mask_3: 0.1768  loss_dice_3: 0.1722  loss_ce_4: 0.1274  loss_mask_4: 0.1768  loss_dice_4: 0.1689  loss_ce_5: 0.1275  loss_mask_5: 0.175  loss_dice_5: 0.1721  loss_ce_6: 0.1268  loss_mask_6: 0.1808  loss_dice_6: 0.1685  loss_ce_7: 0.1268  loss_mask_7: 0.1786  loss_dice_7: 0.1666  loss_ce_8: 0.1266  loss_mask_8: 0.1805  loss_dice_8: 0.1723  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:39:24] d2.utils.events INFO:  eta: 0:55:42  iter: 23499  total_loss: 4.937  loss_ce: 0.1272  loss_mask: 0.1719  loss_dice: 0.1436  loss_ce_0: 0.06285  loss_mask_0: 0.1809  loss_dice_0: 0.1495  loss_ce_1: 0.1271  loss_mask_1: 0.1759  loss_dice_1: 0.1462  loss_ce_2: 0.1273  loss_mask_2: 0.1674  loss_dice_2: 0.145  loss_ce_3: 0.1273  loss_mask_3: 0.1741  loss_dice_3: 0.1433  loss_ce_4: 0.1279  loss_mask_4: 0.1675  loss_dice_4: 0.14  loss_ce_5: 0.1273  loss_mask_5: 0.1682  loss_dice_5: 0.145  loss_ce_6: 0.1273  loss_mask_6: 0.1771  loss_dice_6: 0.1468  loss_ce_7: 0.1276  loss_mask_7: 0.1719  loss_dice_7: 0.1385  loss_ce_8: 0.1275  loss_mask_8: 0.1726  loss_dice_8: 0.1465  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:26] d2.utils.events INFO:  eta: 0:55:41  iter: 23519  total_loss: 4.87  loss_ce: 0.1329  loss_mask: 0.2049  loss_dice: 0.1477  loss_ce_0: 0.06469  loss_mask_0: 0.2163  loss_dice_0: 0.1545  loss_ce_1: 0.1323  loss_mask_1: 0.202  loss_dice_1: 0.1483  loss_ce_2: 0.1326  loss_mask_2: 0.2024  loss_dice_2: 0.15  loss_ce_3: 0.1324  loss_mask_3: 0.2007  loss_dice_3: 0.1442  loss_ce_4: 0.1334  loss_mask_4: 0.2128  loss_dice_4: 0.1528  loss_ce_5: 0.1325  loss_mask_5: 0.2078  loss_dice_5: 0.1511  loss_ce_6: 0.1331  loss_mask_6: 0.205  loss_dice_6: 0.149  loss_ce_7: 0.133  loss_mask_7: 0.1978  loss_dice_7: 0.1498  loss_ce_8: 0.1332  loss_mask_8: 0.2053  loss_dice_8: 0.147  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:29] d2.utils.events INFO:  eta: 0:55:40  iter: 23539  total_loss: 5.034  loss_ce: 0.1278  loss_mask: 0.1647  loss_dice: 0.1575  loss_ce_0: 0.06316  loss_mask_0: 0.1738  loss_dice_0: 0.1619  loss_ce_1: 0.128  loss_mask_1: 0.1762  loss_dice_1: 0.1579  loss_ce_2: 0.128  loss_mask_2: 0.1711  loss_dice_2: 0.1544  loss_ce_3: 0.1282  loss_mask_3: 0.1614  loss_dice_3: 0.1576  loss_ce_4: 0.1288  loss_mask_4: 0.1682  loss_dice_4: 0.1505  loss_ce_5: 0.1281  loss_mask_5: 0.166  loss_dice_5: 0.1541  loss_ce_6: 0.128  loss_mask_6: 0.1713  loss_dice_6: 0.1568  loss_ce_7: 0.1282  loss_mask_7: 0.1632  loss_dice_7: 0.1608  loss_ce_8: 0.1279  loss_mask_8: 0.1652  loss_dice_8: 0.1594  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:31] d2.utils.events INFO:  eta: 0:55:39  iter: 23559  total_loss: 4.652  loss_ce: 0.1262  loss_mask: 0.1939  loss_dice: 0.1584  loss_ce_0: 0.0626  loss_mask_0: 0.2001  loss_dice_0: 0.1589  loss_ce_1: 0.1256  loss_mask_1: 0.2048  loss_dice_1: 0.1568  loss_ce_2: 0.1265  loss_mask_2: 0.191  loss_dice_2: 0.1562  loss_ce_3: 0.1263  loss_mask_3: 0.2111  loss_dice_3: 0.1514  loss_ce_4: 0.1256  loss_mask_4: 0.2087  loss_dice_4: 0.161  loss_ce_5: 0.1264  loss_mask_5: 0.1942  loss_dice_5: 0.1563  loss_ce_6: 0.1263  loss_mask_6: 0.2013  loss_dice_6: 0.1532  loss_ce_7: 0.1261  loss_mask_7: 0.1986  loss_dice_7: 0.157  loss_ce_8: 0.1262  loss_mask_8: 0.1927  loss_dice_8: 0.157  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:34] d2.utils.events INFO:  eta: 0:55:39  iter: 23579  total_loss: 4.979  loss_ce: 0.1158  loss_mask: 0.2224  loss_dice: 0.1371  loss_ce_0: 0.0585  loss_mask_0: 0.2199  loss_dice_0: 0.1375  loss_ce_1: 0.114  loss_mask_1: 0.2229  loss_dice_1: 0.1354  loss_ce_2: 0.1146  loss_mask_2: 0.2267  loss_dice_2: 0.1381  loss_ce_3: 0.1138  loss_mask_3: 0.2267  loss_dice_3: 0.1391  loss_ce_4: 0.1123  loss_mask_4: 0.2204  loss_dice_4: 0.1417  loss_ce_5: 0.1143  loss_mask_5: 0.2282  loss_dice_5: 0.1408  loss_ce_6: 0.114  loss_mask_6: 0.2238  loss_dice_6: 0.1404  loss_ce_7: 0.1134  loss_mask_7: 0.2235  loss_dice_7: 0.1401  loss_ce_8: 0.1148  loss_mask_8: 0.2279  loss_dice_8: 0.1412  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:36] d2.utils.events INFO:  eta: 0:55:37  iter: 23599  total_loss: 5.118  loss_ce: 0.1071  loss_mask: 0.1562  loss_dice: 0.1809  loss_ce_0: 0.05531  loss_mask_0: 0.1726  loss_dice_0: 0.174  loss_ce_1: 0.1056  loss_mask_1: 0.1572  loss_dice_1: 0.1745  loss_ce_2: 0.1063  loss_mask_2: 0.1667  loss_dice_2: 0.1859  loss_ce_3: 0.1054  loss_mask_3: 0.161  loss_dice_3: 0.1772  loss_ce_4: 0.1033  loss_mask_4: 0.1671  loss_dice_4: 0.1727  loss_ce_5: 0.1057  loss_mask_5: 0.1663  loss_dice_5: 0.1798  loss_ce_6: 0.1047  loss_mask_6: 0.1682  loss_dice_6: 0.1818  loss_ce_7: 0.1041  loss_mask_7: 0.1681  loss_dice_7: 0.1845  loss_ce_8: 0.1061  loss_mask_8: 0.1723  loss_dice_8: 0.1771  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:39] d2.utils.events INFO:  eta: 0:55:34  iter: 23619  total_loss: 5.016  loss_ce: 0.1512  loss_mask: 0.2201  loss_dice: 0.1438  loss_ce_0: 0.07118  loss_mask_0: 0.2192  loss_dice_0: 0.1503  loss_ce_1: 0.1508  loss_mask_1: 0.2164  loss_dice_1: 0.1477  loss_ce_2: 0.1505  loss_mask_2: 0.2136  loss_dice_2: 0.1515  loss_ce_3: 0.1501  loss_mask_3: 0.2118  loss_dice_3: 0.148  loss_ce_4: 0.1504  loss_mask_4: 0.2224  loss_dice_4: 0.1527  loss_ce_5: 0.1497  loss_mask_5: 0.2148  loss_dice_5: 0.15  loss_ce_6: 0.1505  loss_mask_6: 0.2166  loss_dice_6: 0.1409  loss_ce_7: 0.1504  loss_mask_7: 0.2177  loss_dice_7: 0.1541  loss_ce_8: 0.151  loss_mask_8: 0.2058  loss_dice_8: 0.1488  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:41] d2.utils.events INFO:  eta: 0:55:33  iter: 23639  total_loss: 5.211  loss_ce: 0.1284  loss_mask: 0.2048  loss_dice: 0.1669  loss_ce_0: 0.0632  loss_mask_0: 0.2107  loss_dice_0: 0.1695  loss_ce_1: 0.1286  loss_mask_1: 0.2049  loss_dice_1: 0.1668  loss_ce_2: 0.1284  loss_mask_2: 0.2107  loss_dice_2: 0.163  loss_ce_3: 0.1283  loss_mask_3: 0.2192  loss_dice_3: 0.1641  loss_ce_4: 0.1284  loss_mask_4: 0.209  loss_dice_4: 0.1607  loss_ce_5: 0.1282  loss_mask_5: 0.2023  loss_dice_5: 0.165  loss_ce_6: 0.1281  loss_mask_6: 0.2055  loss_dice_6: 0.1666  loss_ce_7: 0.1283  loss_mask_7: 0.2009  loss_dice_7: 0.1718  loss_ce_8: 0.1285  loss_mask_8: 0.2071  loss_dice_8: 0.1723  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:44] d2.utils.events INFO:  eta: 0:55:29  iter: 23659  total_loss: 4.664  loss_ce: 0.1098  loss_mask: 0.2029  loss_dice: 0.1405  loss_ce_0: 0.05649  loss_mask_0: 0.1975  loss_dice_0: 0.1334  loss_ce_1: 0.1106  loss_mask_1: 0.2051  loss_dice_1: 0.1426  loss_ce_2: 0.1102  loss_mask_2: 0.1995  loss_dice_2: 0.1416  loss_ce_3: 0.1104  loss_mask_3: 0.1937  loss_dice_3: 0.1441  loss_ce_4: 0.1106  loss_mask_4: 0.199  loss_dice_4: 0.137  loss_ce_5: 0.1105  loss_mask_5: 0.1995  loss_dice_5: 0.1352  loss_ce_6: 0.1104  loss_mask_6: 0.1971  loss_dice_6: 0.143  loss_ce_7: 0.1109  loss_mask_7: 0.1988  loss_dice_7: 0.1466  loss_ce_8: 0.1104  loss_mask_8: 0.1921  loss_dice_8: 0.1436  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:46] d2.utils.events INFO:  eta: 0:55:29  iter: 23679  total_loss: 5.093  loss_ce: 0.1458  loss_mask: 0.1802  loss_dice: 0.1624  loss_ce_0: 0.06961  loss_mask_0: 0.1709  loss_dice_0: 0.1596  loss_ce_1: 0.1457  loss_mask_1: 0.1966  loss_dice_1: 0.1516  loss_ce_2: 0.1453  loss_mask_2: 0.1785  loss_dice_2: 0.1612  loss_ce_3: 0.145  loss_mask_3: 0.1844  loss_dice_3: 0.1643  loss_ce_4: 0.1447  loss_mask_4: 0.1939  loss_dice_4: 0.1548  loss_ce_5: 0.1449  loss_mask_5: 0.1942  loss_dice_5: 0.1616  loss_ce_6: 0.1451  loss_mask_6: 0.1876  loss_dice_6: 0.1567  loss_ce_7: 0.1448  loss_mask_7: 0.1849  loss_dice_7: 0.1555  loss_ce_8: 0.1456  loss_mask_8: 0.1858  loss_dice_8: 0.1584  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:39:49] d2.utils.events INFO:  eta: 0:55:24  iter: 23699  total_loss: 4.605  loss_ce: 0.1131  loss_mask: 0.1735  loss_dice: 0.1165  loss_ce_0: 0.05768  loss_mask_0: 0.1761  loss_dice_0: 0.1215  loss_ce_1: 0.1135  loss_mask_1: 0.1711  loss_dice_1: 0.1163  loss_ce_2: 0.1137  loss_mask_2: 0.1731  loss_dice_2: 0.1149  loss_ce_3: 0.1142  loss_mask_3: 0.1707  loss_dice_3: 0.1192  loss_ce_4: 0.1148  loss_mask_4: 0.1731  loss_dice_4: 0.1143  loss_ce_5: 0.1142  loss_mask_5: 0.1784  loss_dice_5: 0.1222  loss_ce_6: 0.1143  loss_mask_6: 0.1743  loss_dice_6: 0.1211  loss_ce_7: 0.1146  loss_mask_7: 0.1714  loss_dice_7: 0.1187  loss_ce_8: 0.1136  loss_mask_8: 0.1714  loss_dice_8: 0.118  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:51] d2.utils.events INFO:  eta: 0:55:19  iter: 23719  total_loss: 4.956  loss_ce: 0.128  loss_mask: 0.1913  loss_dice: 0.1638  loss_ce_0: 0.06315  loss_mask_0: 0.1845  loss_dice_0: 0.1654  loss_ce_1: 0.1279  loss_mask_1: 0.1924  loss_dice_1: 0.1549  loss_ce_2: 0.128  loss_mask_2: 0.1845  loss_dice_2: 0.1614  loss_ce_3: 0.128  loss_mask_3: 0.1895  loss_dice_3: 0.1582  loss_ce_4: 0.128  loss_mask_4: 0.1831  loss_dice_4: 0.1551  loss_ce_5: 0.128  loss_mask_5: 0.1797  loss_dice_5: 0.161  loss_ce_6: 0.128  loss_mask_6: 0.1985  loss_dice_6: 0.1554  loss_ce_7: 0.128  loss_mask_7: 0.1806  loss_dice_7: 0.1608  loss_ce_8: 0.128  loss_mask_8: 0.1918  loss_dice_8: 0.1612  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:54] d2.utils.events INFO:  eta: 0:55:17  iter: 23739  total_loss: 4.898  loss_ce: 0.1112  loss_mask: 0.1946  loss_dice: 0.1395  loss_ce_0: 0.0568  loss_mask_0: 0.1834  loss_dice_0: 0.1415  loss_ce_1: 0.1116  loss_mask_1: 0.1904  loss_dice_1: 0.1428  loss_ce_2: 0.1114  loss_mask_2: 0.2034  loss_dice_2: 0.1444  loss_ce_3: 0.1114  loss_mask_3: 0.1878  loss_dice_3: 0.1441  loss_ce_4: 0.1116  loss_mask_4: 0.1926  loss_dice_4: 0.1407  loss_ce_5: 0.1114  loss_mask_5: 0.1957  loss_dice_5: 0.1417  loss_ce_6: 0.1112  loss_mask_6: 0.1886  loss_dice_6: 0.1425  loss_ce_7: 0.1115  loss_mask_7: 0.1902  loss_dice_7: 0.1462  loss_ce_8: 0.1112  loss_mask_8: 0.1937  loss_dice_8: 0.1467  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:56] d2.utils.events INFO:  eta: 0:55:14  iter: 23759  total_loss: 5.175  loss_ce: 0.1278  loss_mask: 0.2227  loss_dice: 0.1476  loss_ce_0: 0.06308  loss_mask_0: 0.2211  loss_dice_0: 0.1482  loss_ce_1: 0.1274  loss_mask_1: 0.2268  loss_dice_1: 0.1486  loss_ce_2: 0.1278  loss_mask_2: 0.2284  loss_dice_2: 0.1522  loss_ce_3: 0.1276  loss_mask_3: 0.2304  loss_dice_3: 0.1526  loss_ce_4: 0.1276  loss_mask_4: 0.2227  loss_dice_4: 0.1539  loss_ce_5: 0.1278  loss_mask_5: 0.2267  loss_dice_5: 0.156  loss_ce_6: 0.1277  loss_mask_6: 0.2254  loss_dice_6: 0.1495  loss_ce_7: 0.1277  loss_mask_7: 0.224  loss_dice_7: 0.1527  loss_ce_8: 0.1278  loss_mask_8: 0.2243  loss_dice_8: 0.1475  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:39:59] d2.utils.events INFO:  eta: 0:55:11  iter: 23779  total_loss: 5.199  loss_ce: 0.1276  loss_mask: 0.1533  loss_dice: 0.206  loss_ce_0: 0.06297  loss_mask_0: 0.146  loss_dice_0: 0.1877  loss_ce_1: 0.1286  loss_mask_1: 0.1456  loss_dice_1: 0.1981  loss_ce_2: 0.1278  loss_mask_2: 0.1449  loss_dice_2: 0.2023  loss_ce_3: 0.1278  loss_mask_3: 0.1422  loss_dice_3: 0.1955  loss_ce_4: 0.1277  loss_mask_4: 0.1571  loss_dice_4: 0.1986  loss_ce_5: 0.1277  loss_mask_5: 0.149  loss_dice_5: 0.1998  loss_ce_6: 0.1276  loss_mask_6: 0.1419  loss_dice_6: 0.1907  loss_ce_7: 0.1277  loss_mask_7: 0.1526  loss_dice_7: 0.1964  loss_ce_8: 0.1277  loss_mask_8: 0.1509  loss_dice_8: 0.2108  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:02] d2.utils.events INFO:  eta: 0:55:09  iter: 23799  total_loss: 5.245  loss_ce: 0.1352  loss_mask: 0.1893  loss_dice: 0.1824  loss_ce_0: 0.06587  loss_mask_0: 0.1897  loss_dice_0: 0.1751  loss_ce_1: 0.1339  loss_mask_1: 0.2006  loss_dice_1: 0.1854  loss_ce_2: 0.135  loss_mask_2: 0.1979  loss_dice_2: 0.1841  loss_ce_3: 0.1347  loss_mask_3: 0.1906  loss_dice_3: 0.1754  loss_ce_4: 0.1335  loss_mask_4: 0.1972  loss_dice_4: 0.1757  loss_ce_5: 0.1348  loss_mask_5: 0.1966  loss_dice_5: 0.1776  loss_ce_6: 0.1344  loss_mask_6: 0.1934  loss_dice_6: 0.1747  loss_ce_7: 0.1341  loss_mask_7: 0.1853  loss_dice_7: 0.1751  loss_ce_8: 0.1347  loss_mask_8: 0.1921  loss_dice_8: 0.175  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:04] d2.utils.events INFO:  eta: 0:55:07  iter: 23819  total_loss: 4.909  loss_ce: 0.1289  loss_mask: 0.1942  loss_dice: 0.1422  loss_ce_0: 0.06357  loss_mask_0: 0.2076  loss_dice_0: 0.1429  loss_ce_1: 0.1289  loss_mask_1: 0.1983  loss_dice_1: 0.1416  loss_ce_2: 0.1292  loss_mask_2: 0.1895  loss_dice_2: 0.1408  loss_ce_3: 0.1294  loss_mask_3: 0.1918  loss_dice_3: 0.1428  loss_ce_4: 0.1288  loss_mask_4: 0.1952  loss_dice_4: 0.143  loss_ce_5: 0.1293  loss_mask_5: 0.2023  loss_dice_5: 0.1429  loss_ce_6: 0.1293  loss_mask_6: 0.2031  loss_dice_6: 0.1442  loss_ce_7: 0.1289  loss_mask_7: 0.1939  loss_dice_7: 0.1419  loss_ce_8: 0.1288  loss_mask_8: 0.2097  loss_dice_8: 0.1434  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:07] d2.utils.events INFO:  eta: 0:55:04  iter: 23839  total_loss: 4.615  loss_ce: 0.1243  loss_mask: 0.1756  loss_dice: 0.1539  loss_ce_0: 0.0619  loss_mask_0: 0.176  loss_dice_0: 0.1589  loss_ce_1: 0.1241  loss_mask_1: 0.1689  loss_dice_1: 0.1536  loss_ce_2: 0.1243  loss_mask_2: 0.1754  loss_dice_2: 0.1525  loss_ce_3: 0.1241  loss_mask_3: 0.176  loss_dice_3: 0.1471  loss_ce_4: 0.1233  loss_mask_4: 0.1754  loss_dice_4: 0.1558  loss_ce_5: 0.1243  loss_mask_5: 0.1702  loss_dice_5: 0.1547  loss_ce_6: 0.1236  loss_mask_6: 0.1755  loss_dice_6: 0.1609  loss_ce_7: 0.1232  loss_mask_7: 0.1725  loss_dice_7: 0.1535  loss_ce_8: 0.1242  loss_mask_8: 0.1781  loss_dice_8: 0.151  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:09] d2.utils.events INFO:  eta: 0:54:58  iter: 23859  total_loss: 4.821  loss_ce: 0.1376  loss_mask: 0.1917  loss_dice: 0.1543  loss_ce_0: 0.06666  loss_mask_0: 0.1957  loss_dice_0: 0.1476  loss_ce_1: 0.1383  loss_mask_1: 0.1985  loss_dice_1: 0.1504  loss_ce_2: 0.1375  loss_mask_2: 0.1988  loss_dice_2: 0.1562  loss_ce_3: 0.1377  loss_mask_3: 0.1923  loss_dice_3: 0.1535  loss_ce_4: 0.1398  loss_mask_4: 0.195  loss_dice_4: 0.1539  loss_ce_5: 0.1374  loss_mask_5: 0.203  loss_dice_5: 0.1528  loss_ce_6: 0.1384  loss_mask_6: 0.2062  loss_dice_6: 0.1493  loss_ce_7: 0.1385  loss_mask_7: 0.1938  loss_dice_7: 0.1481  loss_ce_8: 0.1376  loss_mask_8: 0.199  loss_dice_8: 0.151  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:12] d2.utils.events INFO:  eta: 0:54:54  iter: 23879  total_loss: 4.957  loss_ce: 0.1272  loss_mask: 0.1674  loss_dice: 0.1753  loss_ce_0: 0.06277  loss_mask_0: 0.1641  loss_dice_0: 0.171  loss_ce_1: 0.1286  loss_mask_1: 0.1662  loss_dice_1: 0.1699  loss_ce_2: 0.127  loss_mask_2: 0.1736  loss_dice_2: 0.1712  loss_ce_3: 0.127  loss_mask_3: 0.1743  loss_dice_3: 0.1706  loss_ce_4: 0.1291  loss_mask_4: 0.1672  loss_dice_4: 0.1682  loss_ce_5: 0.127  loss_mask_5: 0.1564  loss_dice_5: 0.1832  loss_ce_6: 0.127  loss_mask_6: 0.1639  loss_dice_6: 0.1719  loss_ce_7: 0.1277  loss_mask_7: 0.1675  loss_dice_7: 0.1754  loss_ce_8: 0.1273  loss_mask_8: 0.164  loss_dice_8: 0.1802  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:14] d2.utils.events INFO:  eta: 0:54:52  iter: 23899  total_loss: 5.014  loss_ce: 0.1213  loss_mask: 0.1746  loss_dice: 0.2213  loss_ce_0: 0.0607  loss_mask_0: 0.1747  loss_dice_0: 0.223  loss_ce_1: 0.1222  loss_mask_1: 0.1707  loss_dice_1: 0.2139  loss_ce_2: 0.1218  loss_mask_2: 0.1635  loss_dice_2: 0.2207  loss_ce_3: 0.1225  loss_mask_3: 0.1728  loss_dice_3: 0.225  loss_ce_4: 0.1225  loss_mask_4: 0.17  loss_dice_4: 0.2232  loss_ce_5: 0.1217  loss_mask_5: 0.1743  loss_dice_5: 0.2241  loss_ce_6: 0.1212  loss_mask_6: 0.1725  loss_dice_6: 0.223  loss_ce_7: 0.1216  loss_mask_7: 0.1704  loss_dice_7: 0.2244  loss_ce_8: 0.1214  loss_mask_8: 0.1799  loss_dice_8: 0.231  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:17] d2.utils.events INFO:  eta: 0:54:48  iter: 23919  total_loss: 4.931  loss_ce: 0.1406  loss_mask: 0.1969  loss_dice: 0.1489  loss_ce_0: 0.06806  loss_mask_0: 0.1971  loss_dice_0: 0.1475  loss_ce_1: 0.1415  loss_mask_1: 0.1993  loss_dice_1: 0.1444  loss_ce_2: 0.1418  loss_mask_2: 0.1975  loss_dice_2: 0.1395  loss_ce_3: 0.1423  loss_mask_3: 0.192  loss_dice_3: 0.1433  loss_ce_4: 0.1429  loss_mask_4: 0.1998  loss_dice_4: 0.1469  loss_ce_5: 0.1418  loss_mask_5: 0.2051  loss_dice_5: 0.1476  loss_ce_6: 0.1419  loss_mask_6: 0.1937  loss_dice_6: 0.1427  loss_ce_7: 0.1417  loss_mask_7: 0.2064  loss_dice_7: 0.1491  loss_ce_8: 0.1409  loss_mask_8: 0.1991  loss_dice_8: 0.1449  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:19] d2.utils.events INFO:  eta: 0:54:45  iter: 23939  total_loss: 5.272  loss_ce: 0.1404  loss_mask: 0.1725  loss_dice: 0.1595  loss_ce_0: 0.06788  loss_mask_0: 0.1808  loss_dice_0: 0.1595  loss_ce_1: 0.1378  loss_mask_1: 0.1738  loss_dice_1: 0.1678  loss_ce_2: 0.1398  loss_mask_2: 0.1717  loss_dice_2: 0.1561  loss_ce_3: 0.1397  loss_mask_3: 0.1686  loss_dice_3: 0.1737  loss_ce_4: 0.1394  loss_mask_4: 0.1804  loss_dice_4: 0.1694  loss_ce_5: 0.1403  loss_mask_5: 0.1799  loss_dice_5: 0.1685  loss_ce_6: 0.141  loss_mask_6: 0.1758  loss_dice_6: 0.1663  loss_ce_7: 0.1405  loss_mask_7: 0.1745  loss_dice_7: 0.1778  loss_ce_8: 0.1407  loss_mask_8: 0.1735  loss_dice_8: 0.1783  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:22] d2.utils.events INFO:  eta: 0:54:41  iter: 23959  total_loss: 4.524  loss_ce: 0.1274  loss_mask: 0.1779  loss_dice: 0.1516  loss_ce_0: 0.06305  loss_mask_0: 0.1775  loss_dice_0: 0.1485  loss_ce_1: 0.1279  loss_mask_1: 0.1748  loss_dice_1: 0.1513  loss_ce_2: 0.1277  loss_mask_2: 0.1769  loss_dice_2: 0.1498  loss_ce_3: 0.1281  loss_mask_3: 0.1774  loss_dice_3: 0.1499  loss_ce_4: 0.1286  loss_mask_4: 0.1767  loss_dice_4: 0.1496  loss_ce_5: 0.1279  loss_mask_5: 0.1748  loss_dice_5: 0.1489  loss_ce_6: 0.1276  loss_mask_6: 0.1803  loss_dice_6: 0.1515  loss_ce_7: 0.1278  loss_mask_7: 0.1753  loss_dice_7: 0.1529  loss_ce_8: 0.1276  loss_mask_8: 0.1792  loss_dice_8: 0.1485  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:24] d2.utils.events INFO:  eta: 0:54:37  iter: 23979  total_loss: 4.834  loss_ce: 0.1279  loss_mask: 0.1692  loss_dice: 0.1652  loss_ce_0: 0.06333  loss_mask_0: 0.1705  loss_dice_0: 0.161  loss_ce_1: 0.1285  loss_mask_1: 0.1696  loss_dice_1: 0.1636  loss_ce_2: 0.1285  loss_mask_2: 0.1707  loss_dice_2: 0.1631  loss_ce_3: 0.1286  loss_mask_3: 0.1694  loss_dice_3: 0.1614  loss_ce_4: 0.1278  loss_mask_4: 0.1707  loss_dice_4: 0.1617  loss_ce_5: 0.1286  loss_mask_5: 0.1751  loss_dice_5: 0.1623  loss_ce_6: 0.1285  loss_mask_6: 0.1786  loss_dice_6: 0.1594  loss_ce_7: 0.1282  loss_mask_7: 0.1706  loss_dice_7: 0.1629  loss_ce_8: 0.1281  loss_mask_8: 0.1688  loss_dice_8: 0.1559  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:27] d2.utils.events INFO:  eta: 0:54:34  iter: 23999  total_loss: 4.949  loss_ce: 0.1275  loss_mask: 0.1924  loss_dice: 0.1493  loss_ce_0: 0.06301  loss_mask_0: 0.1937  loss_dice_0: 0.1465  loss_ce_1: 0.1276  loss_mask_1: 0.1951  loss_dice_1: 0.1505  loss_ce_2: 0.1271  loss_mask_2: 0.2024  loss_dice_2: 0.1453  loss_ce_3: 0.1272  loss_mask_3: 0.1852  loss_dice_3: 0.1509  loss_ce_4: 0.1278  loss_mask_4: 0.1928  loss_dice_4: 0.1485  loss_ce_5: 0.1274  loss_mask_5: 0.1904  loss_dice_5: 0.1457  loss_ce_6: 0.1277  loss_mask_6: 0.1881  loss_dice_6: 0.1472  loss_ce_7: 0.1276  loss_mask_7: 0.1934  loss_dice_7: 0.1431  loss_ce_8: 0.1277  loss_mask_8: 0.1903  loss_dice_8: 0.1489  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:29] d2.utils.events INFO:  eta: 0:54:35  iter: 24019  total_loss: 5.75  loss_ce: 0.1275  loss_mask: 0.2115  loss_dice: 0.1837  loss_ce_0: 0.06313  loss_mask_0: 0.2202  loss_dice_0: 0.1817  loss_ce_1: 0.1273  loss_mask_1: 0.2112  loss_dice_1: 0.1779  loss_ce_2: 0.1277  loss_mask_2: 0.2208  loss_dice_2: 0.1808  loss_ce_3: 0.1277  loss_mask_3: 0.2232  loss_dice_3: 0.1797  loss_ce_4: 0.127  loss_mask_4: 0.2085  loss_dice_4: 0.1755  loss_ce_5: 0.1277  loss_mask_5: 0.2168  loss_dice_5: 0.1756  loss_ce_6: 0.1276  loss_mask_6: 0.2166  loss_dice_6: 0.1771  loss_ce_7: 0.1273  loss_mask_7: 0.2058  loss_dice_7: 0.1837  loss_ce_8: 0.1278  loss_mask_8: 0.22  loss_dice_8: 0.1794  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:40:32] d2.utils.events INFO:  eta: 0:54:34  iter: 24039  total_loss: 5.104  loss_ce: 0.1273  loss_mask: 0.2056  loss_dice: 0.1639  loss_ce_0: 0.06283  loss_mask_0: 0.1952  loss_dice_0: 0.1684  loss_ce_1: 0.1282  loss_mask_1: 0.1986  loss_dice_1: 0.1583  loss_ce_2: 0.1274  loss_mask_2: 0.2034  loss_dice_2: 0.1671  loss_ce_3: 0.1274  loss_mask_3: 0.1949  loss_dice_3: 0.1638  loss_ce_4: 0.1272  loss_mask_4: 0.2084  loss_dice_4: 0.1671  loss_ce_5: 0.1273  loss_mask_5: 0.2006  loss_dice_5: 0.1643  loss_ce_6: 0.1274  loss_mask_6: 0.2027  loss_dice_6: 0.1648  loss_ce_7: 0.1274  loss_mask_7: 0.2  loss_dice_7: 0.1617  loss_ce_8: 0.1275  loss_mask_8: 0.2036  loss_dice_8: 0.1648  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:40:34] d2.utils.events INFO:  eta: 0:54:33  iter: 24059  total_loss: 5.268  loss_ce: 0.1211  loss_mask: 0.2024  loss_dice: 0.1628  loss_ce_0: 0.06094  loss_mask_0: 0.1967  loss_dice_0: 0.1667  loss_ce_1: 0.1224  loss_mask_1: 0.1894  loss_dice_1: 0.1696  loss_ce_2: 0.1221  loss_mask_2: 0.2021  loss_dice_2: 0.1693  loss_ce_3: 0.1223  loss_mask_3: 0.2066  loss_dice_3: 0.1728  loss_ce_4: 0.1228  loss_mask_4: 0.1988  loss_dice_4: 0.1686  loss_ce_5: 0.1224  loss_mask_5: 0.1915  loss_dice_5: 0.1725  loss_ce_6: 0.1219  loss_mask_6: 0.202  loss_dice_6: 0.1696  loss_ce_7: 0.1222  loss_mask_7: 0.2009  loss_dice_7: 0.1732  loss_ce_8: 0.1215  loss_mask_8: 0.1858  loss_dice_8: 0.1687  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:37] d2.utils.events INFO:  eta: 0:54:28  iter: 24079  total_loss: 5.242  loss_ce: 0.1193  loss_mask: 0.2178  loss_dice: 0.1693  loss_ce_0: 0.06005  loss_mask_0: 0.2057  loss_dice_0: 0.1668  loss_ce_1: 0.1188  loss_mask_1: 0.216  loss_dice_1: 0.1696  loss_ce_2: 0.1197  loss_mask_2: 0.2125  loss_dice_2: 0.1679  loss_ce_3: 0.1197  loss_mask_3: 0.2204  loss_dice_3: 0.1704  loss_ce_4: 0.1196  loss_mask_4: 0.2057  loss_dice_4: 0.1709  loss_ce_5: 0.1198  loss_mask_5: 0.2124  loss_dice_5: 0.165  loss_ce_6: 0.1195  loss_mask_6: 0.213  loss_dice_6: 0.1686  loss_ce_7: 0.1195  loss_mask_7: 0.217  loss_dice_7: 0.1642  loss_ce_8: 0.1194  loss_mask_8: 0.2099  loss_dice_8: 0.165  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:39] d2.utils.events INFO:  eta: 0:54:25  iter: 24099  total_loss: 4.811  loss_ce: 0.1052  loss_mask: 0.2056  loss_dice: 0.1564  loss_ce_0: 0.05432  loss_mask_0: 0.1911  loss_dice_0: 0.1549  loss_ce_1: 0.1021  loss_mask_1: 0.1962  loss_dice_1: 0.1585  loss_ce_2: 0.1042  loss_mask_2: 0.197  loss_dice_2: 0.1567  loss_ce_3: 0.1035  loss_mask_3: 0.2027  loss_dice_3: 0.1537  loss_ce_4: 0.1022  loss_mask_4: 0.2075  loss_dice_4: 0.1554  loss_ce_5: 0.1036  loss_mask_5: 0.1998  loss_dice_5: 0.1576  loss_ce_6: 0.1034  loss_mask_6: 0.1958  loss_dice_6: 0.1513  loss_ce_7: 0.103  loss_mask_7: 0.1941  loss_dice_7: 0.1561  loss_ce_8: 0.1047  loss_mask_8: 0.2025  loss_dice_8: 0.156  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:42] d2.utils.events INFO:  eta: 0:54:24  iter: 24119  total_loss: 5.237  loss_ce: 0.0953  loss_mask: 0.2352  loss_dice: 0.1605  loss_ce_0: 0.05114  loss_mask_0: 0.2368  loss_dice_0: 0.1581  loss_ce_1: 0.09426  loss_mask_1: 0.2413  loss_dice_1: 0.1577  loss_ce_2: 0.09527  loss_mask_2: 0.2454  loss_dice_2: 0.1588  loss_ce_3: 0.09503  loss_mask_3: 0.2388  loss_dice_3: 0.1537  loss_ce_4: 0.09416  loss_mask_4: 0.239  loss_dice_4: 0.1565  loss_ce_5: 0.09504  loss_mask_5: 0.2387  loss_dice_5: 0.1547  loss_ce_6: 0.0942  loss_mask_6: 0.2468  loss_dice_6: 0.1583  loss_ce_7: 0.0937  loss_mask_7: 0.2392  loss_dice_7: 0.1602  loss_ce_8: 0.09483  loss_mask_8: 0.2438  loss_dice_8: 0.1568  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:45] d2.utils.events INFO:  eta: 0:54:23  iter: 24139  total_loss: 5.136  loss_ce: 0.09239  loss_mask: 0.1992  loss_dice: 0.169  loss_ce_0: 0.05004  loss_mask_0: 0.2012  loss_dice_0: 0.1642  loss_ce_1: 0.09272  loss_mask_1: 0.215  loss_dice_1: 0.1638  loss_ce_2: 0.09267  loss_mask_2: 0.2092  loss_dice_2: 0.1728  loss_ce_3: 0.09234  loss_mask_3: 0.2058  loss_dice_3: 0.1703  loss_ce_4: 0.09153  loss_mask_4: 0.2032  loss_dice_4: 0.1659  loss_ce_5: 0.09225  loss_mask_5: 0.1966  loss_dice_5: 0.1656  loss_ce_6: 0.09158  loss_mask_6: 0.2054  loss_dice_6: 0.1693  loss_ce_7: 0.09132  loss_mask_7: 0.2019  loss_dice_7: 0.1572  loss_ce_8: 0.09224  loss_mask_8: 0.2063  loss_dice_8: 0.1645  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:47] d2.utils.events INFO:  eta: 0:54:23  iter: 24159  total_loss: 4.983  loss_ce: 0.08899  loss_mask: 0.2021  loss_dice: 0.156  loss_ce_0: 0.04901  loss_mask_0: 0.1935  loss_dice_0: 0.1458  loss_ce_1: 0.08805  loss_mask_1: 0.195  loss_dice_1: 0.153  loss_ce_2: 0.08903  loss_mask_2: 0.202  loss_dice_2: 0.152  loss_ce_3: 0.08878  loss_mask_3: 0.1999  loss_dice_3: 0.1451  loss_ce_4: 0.0881  loss_mask_4: 0.1988  loss_dice_4: 0.1448  loss_ce_5: 0.08891  loss_mask_5: 0.2088  loss_dice_5: 0.1485  loss_ce_6: 0.08821  loss_mask_6: 0.1979  loss_dice_6: 0.15  loss_ce_7: 0.08786  loss_mask_7: 0.2049  loss_dice_7: 0.1426  loss_ce_8: 0.08858  loss_mask_8: 0.1955  loss_dice_8: 0.1483  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:50] d2.utils.events INFO:  eta: 0:54:21  iter: 24179  total_loss: 4.961  loss_ce: 0.1322  loss_mask: 0.21  loss_dice: 0.1438  loss_ce_0: 0.06461  loss_mask_0: 0.2167  loss_dice_0: 0.1474  loss_ce_1: 0.1327  loss_mask_1: 0.2138  loss_dice_1: 0.1437  loss_ce_2: 0.1321  loss_mask_2: 0.2096  loss_dice_2: 0.1455  loss_ce_3: 0.1322  loss_mask_3: 0.2129  loss_dice_3: 0.1441  loss_ce_4: 0.1321  loss_mask_4: 0.2049  loss_dice_4: 0.1372  loss_ce_5: 0.1321  loss_mask_5: 0.2038  loss_dice_5: 0.1402  loss_ce_6: 0.1322  loss_mask_6: 0.208  loss_dice_6: 0.1457  loss_ce_7: 0.132  loss_mask_7: 0.214  loss_dice_7: 0.1376  loss_ce_8: 0.1319  loss_mask_8: 0.2102  loss_dice_8: 0.1443  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:40:52] d2.utils.events INFO:  eta: 0:54:18  iter: 24199  total_loss: 4.949  loss_ce: 0.09253  loss_mask: 0.1707  loss_dice: 0.1729  loss_ce_0: 0.04922  loss_mask_0: 0.1712  loss_dice_0: 0.1778  loss_ce_1: 0.0934  loss_mask_1: 0.1746  loss_dice_1: 0.1742  loss_ce_2: 0.0929  loss_mask_2: 0.1586  loss_dice_2: 0.1728  loss_ce_3: 0.09287  loss_mask_3: 0.1696  loss_dice_3: 0.1786  loss_ce_4: 0.09346  loss_mask_4: 0.1782  loss_dice_4: 0.171  loss_ce_5: 0.09335  loss_mask_5: 0.1776  loss_dice_5: 0.1763  loss_ce_6: 0.09333  loss_mask_6: 0.169  loss_dice_6: 0.1724  loss_ce_7: 0.09416  loss_mask_7: 0.1663  loss_dice_7: 0.17  loss_ce_8: 0.09336  loss_mask_8: 0.1692  loss_dice_8: 0.1588  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:40:55] d2.utils.events INFO:  eta: 0:54:16  iter: 24219  total_loss: 5.125  loss_ce: 0.09109  loss_mask: 0.1835  loss_dice: 0.2053  loss_ce_0: 0.04881  loss_mask_0: 0.1788  loss_dice_0: 0.206  loss_ce_1: 0.09205  loss_mask_1: 0.1919  loss_dice_1: 0.1964  loss_ce_2: 0.09152  loss_mask_2: 0.1836  loss_dice_2: 0.204  loss_ce_3: 0.09125  loss_mask_3: 0.1848  loss_dice_3: 0.2055  loss_ce_4: 0.09153  loss_mask_4: 0.1839  loss_dice_4: 0.2124  loss_ce_5: 0.09152  loss_mask_5: 0.1836  loss_dice_5: 0.2087  loss_ce_6: 0.09131  loss_mask_6: 0.1807  loss_dice_6: 0.2064  loss_ce_7: 0.09181  loss_mask_7: 0.1756  loss_dice_7: 0.2009  loss_ce_8: 0.09173  loss_mask_8: 0.1786  loss_dice_8: 0.2018  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:40:57] d2.utils.events INFO:  eta: 0:54:13  iter: 24239  total_loss: 5.465  loss_ce: 0.08452  loss_mask: 0.2244  loss_dice: 0.1596  loss_ce_0: 0.04687  loss_mask_0: 0.2236  loss_dice_0: 0.1564  loss_ce_1: 0.08295  loss_mask_1: 0.2128  loss_dice_1: 0.1532  loss_ce_2: 0.08452  loss_mask_2: 0.2177  loss_dice_2: 0.1518  loss_ce_3: 0.08413  loss_mask_3: 0.2311  loss_dice_3: 0.1533  loss_ce_4: 0.08329  loss_mask_4: 0.2319  loss_dice_4: 0.1589  loss_ce_5: 0.0839  loss_mask_5: 0.221  loss_dice_5: 0.1551  loss_ce_6: 0.08347  loss_mask_6: 0.2237  loss_dice_6: 0.1536  loss_ce_7: 0.08295  loss_mask_7: 0.2238  loss_dice_7: 0.1555  loss_ce_8: 0.08415  loss_mask_8: 0.2139  loss_dice_8: 0.1556  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:00] d2.utils.events INFO:  eta: 0:54:12  iter: 24259  total_loss: 5.696  loss_ce: 0.08292  loss_mask: 0.1651  loss_dice: 0.2127  loss_ce_0: 0.04617  loss_mask_0: 0.1658  loss_dice_0: 0.2084  loss_ce_1: 0.08272  loss_mask_1: 0.1691  loss_dice_1: 0.2195  loss_ce_2: 0.08272  loss_mask_2: 0.1765  loss_dice_2: 0.217  loss_ce_3: 0.08243  loss_mask_3: 0.1605  loss_dice_3: 0.2217  loss_ce_4: 0.08223  loss_mask_4: 0.1681  loss_dice_4: 0.2235  loss_ce_5: 0.08259  loss_mask_5: 0.1649  loss_dice_5: 0.2105  loss_ce_6: 0.08233  loss_mask_6: 0.1591  loss_dice_6: 0.228  loss_ce_7: 0.08227  loss_mask_7: 0.1653  loss_dice_7: 0.2219  loss_ce_8: 0.08261  loss_mask_8: 0.1742  loss_dice_8: 0.2135  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:02] d2.utils.events INFO:  eta: 0:54:10  iter: 24279  total_loss: 5.132  loss_ce: 0.09223  loss_mask: 0.1948  loss_dice: 0.1617  loss_ce_0: 0.04856  loss_mask_0: 0.1993  loss_dice_0: 0.1636  loss_ce_1: 0.09379  loss_mask_1: 0.1935  loss_dice_1: 0.1702  loss_ce_2: 0.09247  loss_mask_2: 0.194  loss_dice_2: 0.1712  loss_ce_3: 0.09241  loss_mask_3: 0.1894  loss_dice_3: 0.1666  loss_ce_4: 0.09305  loss_mask_4: 0.1889  loss_dice_4: 0.1634  loss_ce_5: 0.09281  loss_mask_5: 0.1899  loss_dice_5: 0.166  loss_ce_6: 0.0931  loss_mask_6: 0.1879  loss_dice_6: 0.166  loss_ce_7: 0.09409  loss_mask_7: 0.1974  loss_dice_7: 0.1679  loss_ce_8: 0.09333  loss_mask_8: 0.1892  loss_dice_8: 0.166  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:05] d2.utils.events INFO:  eta: 0:54:07  iter: 24299  total_loss: 6.061  loss_ce: 0.09168  loss_mask: 0.2165  loss_dice: 0.2295  loss_ce_0: 0.04843  loss_mask_0: 0.223  loss_dice_0: 0.2229  loss_ce_1: 0.09294  loss_mask_1: 0.2203  loss_dice_1: 0.2168  loss_ce_2: 0.09164  loss_mask_2: 0.2156  loss_dice_2: 0.215  loss_ce_3: 0.09156  loss_mask_3: 0.2125  loss_dice_3: 0.222  loss_ce_4: 0.09218  loss_mask_4: 0.2162  loss_dice_4: 0.2297  loss_ce_5: 0.09194  loss_mask_5: 0.2148  loss_dice_5: 0.2133  loss_ce_6: 0.09209  loss_mask_6: 0.2173  loss_dice_6: 0.2099  loss_ce_7: 0.09291  loss_mask_7: 0.2184  loss_dice_7: 0.2243  loss_ce_8: 0.09247  loss_mask_8: 0.2214  loss_dice_8: 0.2085  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:07] d2.utils.events INFO:  eta: 0:54:06  iter: 24319  total_loss: 5.57  loss_ce: 0.08587  loss_mask: 0.183  loss_dice: 0.1972  loss_ce_0: 0.04667  loss_mask_0: 0.1909  loss_dice_0: 0.1927  loss_ce_1: 0.08646  loss_mask_1: 0.1978  loss_dice_1: 0.1947  loss_ce_2: 0.08562  loss_mask_2: 0.1958  loss_dice_2: 0.1883  loss_ce_3: 0.08549  loss_mask_3: 0.1968  loss_dice_3: 0.1983  loss_ce_4: 0.08532  loss_mask_4: 0.1941  loss_dice_4: 0.1946  loss_ce_5: 0.08532  loss_mask_5: 0.184  loss_dice_5: 0.1953  loss_ce_6: 0.08529  loss_mask_6: 0.1903  loss_dice_6: 0.1862  loss_ce_7: 0.08526  loss_mask_7: 0.1894  loss_dice_7: 0.1929  loss_ce_8: 0.0858  loss_mask_8: 0.1823  loss_dice_8: 0.1824  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:10] d2.utils.events INFO:  eta: 0:54:03  iter: 24339  total_loss: 4.813  loss_ce: 0.1763  loss_mask: 0.1811  loss_dice: 0.1414  loss_ce_0: 0.08189  loss_mask_0: 0.1828  loss_dice_0: 0.1416  loss_ce_1: 0.175  loss_mask_1: 0.1795  loss_dice_1: 0.1471  loss_ce_2: 0.1763  loss_mask_2: 0.1812  loss_dice_2: 0.142  loss_ce_3: 0.176  loss_mask_3: 0.1774  loss_dice_3: 0.1372  loss_ce_4: 0.1757  loss_mask_4: 0.1874  loss_dice_4: 0.1433  loss_ce_5: 0.1758  loss_mask_5: 0.1758  loss_dice_5: 0.1409  loss_ce_6: 0.1758  loss_mask_6: 0.1828  loss_dice_6: 0.1378  loss_ce_7: 0.1754  loss_mask_7: 0.1781  loss_dice_7: 0.1349  loss_ce_8: 0.176  loss_mask_8: 0.1791  loss_dice_8: 0.1403  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:13] d2.utils.events INFO:  eta: 0:54:02  iter: 24359  total_loss: 4.639  loss_ce: 0.1292  loss_mask: 0.1656  loss_dice: 0.147  loss_ce_0: 0.06376  loss_mask_0: 0.1763  loss_dice_0: 0.1468  loss_ce_1: 0.1295  loss_mask_1: 0.1741  loss_dice_1: 0.1505  loss_ce_2: 0.1292  loss_mask_2: 0.1781  loss_dice_2: 0.1515  loss_ce_3: 0.1291  loss_mask_3: 0.1752  loss_dice_3: 0.1473  loss_ce_4: 0.1291  loss_mask_4: 0.1766  loss_dice_4: 0.1571  loss_ce_5: 0.129  loss_mask_5: 0.1701  loss_dice_5: 0.1537  loss_ce_6: 0.1289  loss_mask_6: 0.1809  loss_dice_6: 0.1533  loss_ce_7: 0.1289  loss_mask_7: 0.1817  loss_dice_7: 0.1539  loss_ce_8: 0.1292  loss_mask_8: 0.1662  loss_dice_8: 0.1501  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:15] d2.utils.events INFO:  eta: 0:53:58  iter: 24379  total_loss: 4.856  loss_ce: 0.1457  loss_mask: 0.1903  loss_dice: 0.1458  loss_ce_0: 0.07048  loss_mask_0: 0.1921  loss_dice_0: 0.1465  loss_ce_1: 0.141  loss_mask_1: 0.1917  loss_dice_1: 0.1412  loss_ce_2: 0.1445  loss_mask_2: 0.1909  loss_dice_2: 0.144  loss_ce_3: 0.1433  loss_mask_3: 0.1872  loss_dice_3: 0.1421  loss_ce_4: 0.1414  loss_mask_4: 0.1921  loss_dice_4: 0.1416  loss_ce_5: 0.143  loss_mask_5: 0.1895  loss_dice_5: 0.1398  loss_ce_6: 0.1422  loss_mask_6: 0.1876  loss_dice_6: 0.1465  loss_ce_7: 0.1414  loss_mask_7: 0.1928  loss_dice_7: 0.1445  loss_ce_8: 0.1445  loss_mask_8: 0.189  loss_dice_8: 0.1418  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:18] d2.utils.events INFO:  eta: 0:54:00  iter: 24399  total_loss: 4.996  loss_ce: 0.1131  loss_mask: 0.2116  loss_dice: 0.137  loss_ce_0: 0.05714  loss_mask_0: 0.2169  loss_dice_0: 0.1361  loss_ce_1: 0.1165  loss_mask_1: 0.2189  loss_dice_1: 0.134  loss_ce_2: 0.1139  loss_mask_2: 0.2076  loss_dice_2: 0.1372  loss_ce_3: 0.1149  loss_mask_3: 0.2198  loss_dice_3: 0.1357  loss_ce_4: 0.1164  loss_mask_4: 0.2179  loss_dice_4: 0.1362  loss_ce_5: 0.1149  loss_mask_5: 0.2094  loss_dice_5: 0.1365  loss_ce_6: 0.1158  loss_mask_6: 0.2202  loss_dice_6: 0.1418  loss_ce_7: 0.1163  loss_mask_7: 0.2174  loss_dice_7: 0.1401  loss_ce_8: 0.1141  loss_mask_8: 0.2178  loss_dice_8: 0.137  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:20] d2.utils.events INFO:  eta: 0:53:59  iter: 24419  total_loss: 4.918  loss_ce: 0.1418  loss_mask: 0.2177  loss_dice: 0.1349  loss_ce_0: 0.06869  loss_mask_0: 0.2114  loss_dice_0: 0.1367  loss_ce_1: 0.1413  loss_mask_1: 0.2115  loss_dice_1: 0.1365  loss_ce_2: 0.1417  loss_mask_2: 0.2143  loss_dice_2: 0.1365  loss_ce_3: 0.1411  loss_mask_3: 0.2077  loss_dice_3: 0.1275  loss_ce_4: 0.1406  loss_mask_4: 0.2106  loss_dice_4: 0.1364  loss_ce_5: 0.1414  loss_mask_5: 0.2011  loss_dice_5: 0.1321  loss_ce_6: 0.141  loss_mask_6: 0.1998  loss_dice_6: 0.1377  loss_ce_7: 0.141  loss_mask_7: 0.2106  loss_dice_7: 0.1386  loss_ce_8: 0.1417  loss_mask_8: 0.2144  loss_dice_8: 0.1397  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:23] d2.utils.events INFO:  eta: 0:53:54  iter: 24439  total_loss: 5.431  loss_ce: 0.1273  loss_mask: 0.1942  loss_dice: 0.1729  loss_ce_0: 0.06292  loss_mask_0: 0.1987  loss_dice_0: 0.1653  loss_ce_1: 0.1278  loss_mask_1: 0.1921  loss_dice_1: 0.1711  loss_ce_2: 0.1274  loss_mask_2: 0.1845  loss_dice_2: 0.1671  loss_ce_3: 0.1279  loss_mask_3: 0.1982  loss_dice_3: 0.1646  loss_ce_4: 0.1287  loss_mask_4: 0.1959  loss_dice_4: 0.162  loss_ce_5: 0.1276  loss_mask_5: 0.1886  loss_dice_5: 0.1676  loss_ce_6: 0.1279  loss_mask_6: 0.1881  loss_dice_6: 0.1627  loss_ce_7: 0.128  loss_mask_7: 0.201  loss_dice_7: 0.1648  loss_ce_8: 0.1271  loss_mask_8: 0.1829  loss_dice_8: 0.1675  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:25] d2.utils.events INFO:  eta: 0:53:51  iter: 24459  total_loss: 5.388  loss_ce: 0.1281  loss_mask: 0.1964  loss_dice: 0.1794  loss_ce_0: 0.06316  loss_mask_0: 0.1991  loss_dice_0: 0.1852  loss_ce_1: 0.1292  loss_mask_1: 0.195  loss_dice_1: 0.1839  loss_ce_2: 0.1284  loss_mask_2: 0.2066  loss_dice_2: 0.1811  loss_ce_3: 0.1293  loss_mask_3: 0.1927  loss_dice_3: 0.1816  loss_ce_4: 0.1301  loss_mask_4: 0.1951  loss_dice_4: 0.1817  loss_ce_5: 0.1288  loss_mask_5: 0.2065  loss_dice_5: 0.1844  loss_ce_6: 0.1299  loss_mask_6: 0.2008  loss_dice_6: 0.1801  loss_ce_7: 0.1299  loss_mask_7: 0.1911  loss_dice_7: 0.1876  loss_ce_8: 0.1286  loss_mask_8: 0.198  loss_dice_8: 0.1872  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:28] d2.utils.events INFO:  eta: 0:53:48  iter: 24479  total_loss: 4.749  loss_ce: 0.1274  loss_mask: 0.2128  loss_dice: 0.1445  loss_ce_0: 0.06298  loss_mask_0: 0.2107  loss_dice_0: 0.1419  loss_ce_1: 0.1278  loss_mask_1: 0.2193  loss_dice_1: 0.1498  loss_ce_2: 0.1275  loss_mask_2: 0.2169  loss_dice_2: 0.1459  loss_ce_3: 0.1276  loss_mask_3: 0.215  loss_dice_3: 0.1486  loss_ce_4: 0.1276  loss_mask_4: 0.2044  loss_dice_4: 0.1406  loss_ce_5: 0.1275  loss_mask_5: 0.226  loss_dice_5: 0.1449  loss_ce_6: 0.1273  loss_mask_6: 0.21  loss_dice_6: 0.1447  loss_ce_7: 0.1275  loss_mask_7: 0.2155  loss_dice_7: 0.1426  loss_ce_8: 0.1274  loss_mask_8: 0.1981  loss_dice_8: 0.1417  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:30] d2.utils.events INFO:  eta: 0:53:44  iter: 24499  total_loss: 5.004  loss_ce: 0.1187  loss_mask: 0.2119  loss_dice: 0.1475  loss_ce_0: 0.05965  loss_mask_0: 0.2217  loss_dice_0: 0.1427  loss_ce_1: 0.1195  loss_mask_1: 0.2181  loss_dice_1: 0.1437  loss_ce_2: 0.119  loss_mask_2: 0.2108  loss_dice_2: 0.1452  loss_ce_3: 0.119  loss_mask_3: 0.2168  loss_dice_3: 0.1441  loss_ce_4: 0.1187  loss_mask_4: 0.2165  loss_dice_4: 0.1394  loss_ce_5: 0.1189  loss_mask_5: 0.2241  loss_dice_5: 0.1468  loss_ce_6: 0.1188  loss_mask_6: 0.2269  loss_dice_6: 0.1432  loss_ce_7: 0.1187  loss_mask_7: 0.2153  loss_dice_7: 0.1429  loss_ce_8: 0.1188  loss_mask_8: 0.2235  loss_dice_8: 0.1447  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:33] d2.utils.events INFO:  eta: 0:53:39  iter: 24519  total_loss: 5.075  loss_ce: 0.1277  loss_mask: 0.1849  loss_dice: 0.1716  loss_ce_0: 0.0631  loss_mask_0: 0.1829  loss_dice_0: 0.1708  loss_ce_1: 0.1269  loss_mask_1: 0.1858  loss_dice_1: 0.1822  loss_ce_2: 0.1277  loss_mask_2: 0.1918  loss_dice_2: 0.1769  loss_ce_3: 0.1277  loss_mask_3: 0.1851  loss_dice_3: 0.1758  loss_ce_4: 0.1279  loss_mask_4: 0.1918  loss_dice_4: 0.1782  loss_ce_5: 0.1277  loss_mask_5: 0.1883  loss_dice_5: 0.1731  loss_ce_6: 0.1277  loss_mask_6: 0.1904  loss_dice_6: 0.1802  loss_ce_7: 0.1278  loss_mask_7: 0.1905  loss_dice_7: 0.1724  loss_ce_8: 0.1277  loss_mask_8: 0.1938  loss_dice_8: 0.1768  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:35] d2.utils.events INFO:  eta: 0:53:35  iter: 24539  total_loss: 5.332  loss_ce: 0.1275  loss_mask: 0.2169  loss_dice: 0.1887  loss_ce_0: 0.06299  loss_mask_0: 0.2176  loss_dice_0: 0.1805  loss_ce_1: 0.1274  loss_mask_1: 0.2229  loss_dice_1: 0.1895  loss_ce_2: 0.1274  loss_mask_2: 0.2243  loss_dice_2: 0.179  loss_ce_3: 0.1275  loss_mask_3: 0.2173  loss_dice_3: 0.1866  loss_ce_4: 0.1275  loss_mask_4: 0.2137  loss_dice_4: 0.1853  loss_ce_5: 0.1275  loss_mask_5: 0.2277  loss_dice_5: 0.1854  loss_ce_6: 0.1274  loss_mask_6: 0.202  loss_dice_6: 0.1844  loss_ce_7: 0.1275  loss_mask_7: 0.2181  loss_dice_7: 0.1801  loss_ce_8: 0.1275  loss_mask_8: 0.2151  loss_dice_8: 0.1841  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:38] d2.utils.events INFO:  eta: 0:53:29  iter: 24559  total_loss: 5.176  loss_ce: 0.1275  loss_mask: 0.2066  loss_dice: 0.1563  loss_ce_0: 0.06298  loss_mask_0: 0.2239  loss_dice_0: 0.1553  loss_ce_1: 0.127  loss_mask_1: 0.2211  loss_dice_1: 0.163  loss_ce_2: 0.1274  loss_mask_2: 0.2044  loss_dice_2: 0.1531  loss_ce_3: 0.1273  loss_mask_3: 0.2146  loss_dice_3: 0.1562  loss_ce_4: 0.1274  loss_mask_4: 0.2239  loss_dice_4: 0.1555  loss_ce_5: 0.1274  loss_mask_5: 0.2053  loss_dice_5: 0.1556  loss_ce_6: 0.1274  loss_mask_6: 0.2128  loss_dice_6: 0.1547  loss_ce_7: 0.1274  loss_mask_7: 0.2161  loss_dice_7: 0.1571  loss_ce_8: 0.1275  loss_mask_8: 0.2212  loss_dice_8: 0.1598  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:41:40] d2.utils.events INFO:  eta: 0:53:26  iter: 24579  total_loss: 4.961  loss_ce: 0.1198  loss_mask: 0.1966  loss_dice: 0.1971  loss_ce_0: 0.06013  loss_mask_0: 0.2  loss_dice_0: 0.1995  loss_ce_1: 0.1197  loss_mask_1: 0.208  loss_dice_1: 0.1939  loss_ce_2: 0.1195  loss_mask_2: 0.1914  loss_dice_2: 0.1982  loss_ce_3: 0.1196  loss_mask_3: 0.2052  loss_dice_3: 0.1975  loss_ce_4: 0.1196  loss_mask_4: 0.1932  loss_dice_4: 0.1982  loss_ce_5: 0.1196  loss_mask_5: 0.1928  loss_dice_5: 0.193  loss_ce_6: 0.1198  loss_mask_6: 0.1997  loss_dice_6: 0.2018  loss_ce_7: 0.1197  loss_mask_7: 0.1957  loss_dice_7: 0.1955  loss_ce_8: 0.1199  loss_mask_8: 0.1953  loss_dice_8: 0.195  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:43] d2.utils.events INFO:  eta: 0:53:23  iter: 24599  total_loss: 5.193  loss_ce: 0.1091  loss_mask: 0.1695  loss_dice: 0.1947  loss_ce_0: 0.05572  loss_mask_0: 0.1727  loss_dice_0: 0.1992  loss_ce_1: 0.1069  loss_mask_1: 0.1764  loss_dice_1: 0.2021  loss_ce_2: 0.1079  loss_mask_2: 0.1747  loss_dice_2: 0.1897  loss_ce_3: 0.1074  loss_mask_3: 0.1677  loss_dice_3: 0.1952  loss_ce_4: 0.1065  loss_mask_4: 0.1637  loss_dice_4: 0.1999  loss_ce_5: 0.1075  loss_mask_5: 0.1735  loss_dice_5: 0.2014  loss_ce_6: 0.1077  loss_mask_6: 0.1636  loss_dice_6: 0.1923  loss_ce_7: 0.1074  loss_mask_7: 0.1767  loss_dice_7: 0.2027  loss_ce_8: 0.1089  loss_mask_8: 0.1725  loss_dice_8: 0.1979  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:41:45] d2.utils.events INFO:  eta: 0:53:21  iter: 24619  total_loss: 4.918  loss_ce: 0.1014  loss_mask: 0.1785  loss_dice: 0.1828  loss_ce_0: 0.0531  loss_mask_0: 0.1813  loss_dice_0: 0.1912  loss_ce_1: 0.1002  loss_mask_1: 0.1842  loss_dice_1: 0.1926  loss_ce_2: 0.101  loss_mask_2: 0.1836  loss_dice_2: 0.1815  loss_ce_3: 0.1004  loss_mask_3: 0.1923  loss_dice_3: 0.183  loss_ce_4: 0.09955  loss_mask_4: 0.1781  loss_dice_4: 0.1907  loss_ce_5: 0.1002  loss_mask_5: 0.1859  loss_dice_5: 0.1918  loss_ce_6: 0.09969  loss_mask_6: 0.1805  loss_dice_6: 0.1844  loss_ce_7: 0.09941  loss_mask_7: 0.1847  loss_dice_7: 0.1882  loss_ce_8: 0.1009  loss_mask_8: 0.1841  loss_dice_8: 0.1881  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:48] d2.utils.events INFO:  eta: 0:53:20  iter: 24639  total_loss: 4.796  loss_ce: 0.1677  loss_mask: 0.2092  loss_dice: 0.1341  loss_ce_0: 0.07762  loss_mask_0: 0.2105  loss_dice_0: 0.1366  loss_ce_1: 0.1686  loss_mask_1: 0.2085  loss_dice_1: 0.1328  loss_ce_2: 0.1678  loss_mask_2: 0.2031  loss_dice_2: 0.1383  loss_ce_3: 0.1683  loss_mask_3: 0.2169  loss_dice_3: 0.1384  loss_ce_4: 0.1691  loss_mask_4: 0.2166  loss_dice_4: 0.1403  loss_ce_5: 0.1684  loss_mask_5: 0.2089  loss_dice_5: 0.1319  loss_ce_6: 0.1695  loss_mask_6: 0.212  loss_dice_6: 0.1325  loss_ce_7: 0.1696  loss_mask_7: 0.2124  loss_dice_7: 0.1347  loss_ce_8: 0.1685  loss_mask_8: 0.203  loss_dice_8: 0.133  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:41:50] d2.utils.events INFO:  eta: 0:53:16  iter: 24659  total_loss: 5.136  loss_ce: 0.0995  loss_mask: 0.229  loss_dice: 0.1795  loss_ce_0: 0.05247  loss_mask_0: 0.2282  loss_dice_0: 0.1742  loss_ce_1: 0.1004  loss_mask_1: 0.2321  loss_dice_1: 0.1786  loss_ce_2: 0.1002  loss_mask_2: 0.2345  loss_dice_2: 0.1818  loss_ce_3: 0.1001  loss_mask_3: 0.2373  loss_dice_3: 0.1738  loss_ce_4: 0.1003  loss_mask_4: 0.2437  loss_dice_4: 0.176  loss_ce_5: 0.1005  loss_mask_5: 0.2393  loss_dice_5: 0.1765  loss_ce_6: 0.1001  loss_mask_6: 0.2295  loss_dice_6: 0.1777  loss_ce_7: 0.1007  loss_mask_7: 0.2294  loss_dice_7: 0.1771  loss_ce_8: 0.09999  loss_mask_8: 0.2291  loss_dice_8: 0.1712  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:53] d2.utils.events INFO:  eta: 0:53:13  iter: 24679  total_loss: 5.697  loss_ce: 0.1557  loss_mask: 0.1949  loss_dice: 0.2024  loss_ce_0: 0.0736  loss_mask_0: 0.2069  loss_dice_0: 0.1947  loss_ce_1: 0.1536  loss_mask_1: 0.2065  loss_dice_1: 0.2005  loss_ce_2: 0.1545  loss_mask_2: 0.197  loss_dice_2: 0.2015  loss_ce_3: 0.1545  loss_mask_3: 0.2088  loss_dice_3: 0.2041  loss_ce_4: 0.1538  loss_mask_4: 0.2036  loss_dice_4: 0.204  loss_ce_5: 0.154  loss_mask_5: 0.2035  loss_dice_5: 0.2008  loss_ce_6: 0.1542  loss_mask_6: 0.1998  loss_dice_6: 0.2054  loss_ce_7: 0.1535  loss_mask_7: 0.2077  loss_dice_7: 0.2102  loss_ce_8: 0.155  loss_mask_8: 0.2027  loss_dice_8: 0.1995  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:41:55] d2.utils.events INFO:  eta: 0:53:12  iter: 24699  total_loss: 5.119  loss_ce: 0.1038  loss_mask: 0.193  loss_dice: 0.1537  loss_ce_0: 0.05406  loss_mask_0: 0.1959  loss_dice_0: 0.1499  loss_ce_1: 0.1053  loss_mask_1: 0.1914  loss_dice_1: 0.149  loss_ce_2: 0.1046  loss_mask_2: 0.1939  loss_dice_2: 0.1563  loss_ce_3: 0.1047  loss_mask_3: 0.1995  loss_dice_3: 0.1577  loss_ce_4: 0.1052  loss_mask_4: 0.196  loss_dice_4: 0.1566  loss_ce_5: 0.1051  loss_mask_5: 0.193  loss_dice_5: 0.1598  loss_ce_6: 0.105  loss_mask_6: 0.2002  loss_dice_6: 0.157  loss_ce_7: 0.1054  loss_mask_7: 0.1944  loss_dice_7: 0.1559  loss_ce_8: 0.1043  loss_mask_8: 0.1945  loss_dice_8: 0.1572  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:41:58] d2.utils.events INFO:  eta: 0:53:10  iter: 24719  total_loss: 5.236  loss_ce: 0.1011  loss_mask: 0.1737  loss_dice: 0.1805  loss_ce_0: 0.05282  loss_mask_0: 0.1858  loss_dice_0: 0.1712  loss_ce_1: 0.1011  loss_mask_1: 0.1759  loss_dice_1: 0.1739  loss_ce_2: 0.1009  loss_mask_2: 0.1786  loss_dice_2: 0.1711  loss_ce_3: 0.1009  loss_mask_3: 0.1704  loss_dice_3: 0.1823  loss_ce_4: 0.1006  loss_mask_4: 0.1747  loss_dice_4: 0.1827  loss_ce_5: 0.1008  loss_mask_5: 0.1868  loss_dice_5: 0.1733  loss_ce_6: 0.1009  loss_mask_6: 0.196  loss_dice_6: 0.1765  loss_ce_7: 0.101  loss_mask_7: 0.1794  loss_dice_7: 0.1701  loss_ce_8: 0.1011  loss_mask_8: 0.1713  loss_dice_8: 0.1769  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:00] d2.utils.events INFO:  eta: 0:53:07  iter: 24739  total_loss: 5.725  loss_ce: 0.1602  loss_mask: 0.2247  loss_dice: 0.1736  loss_ce_0: 0.07514  loss_mask_0: 0.2289  loss_dice_0: 0.1742  loss_ce_1: 0.1583  loss_mask_1: 0.2274  loss_dice_1: 0.1799  loss_ce_2: 0.1598  loss_mask_2: 0.2274  loss_dice_2: 0.1775  loss_ce_3: 0.1597  loss_mask_3: 0.2202  loss_dice_3: 0.174  loss_ce_4: 0.1597  loss_mask_4: 0.229  loss_dice_4: 0.1776  loss_ce_5: 0.1596  loss_mask_5: 0.2214  loss_dice_5: 0.171  loss_ce_6: 0.1599  loss_mask_6: 0.2222  loss_dice_6: 0.1792  loss_ce_7: 0.1597  loss_mask_7: 0.2287  loss_dice_7: 0.1761  loss_ce_8: 0.1601  loss_mask_8: 0.2209  loss_dice_8: 0.1782  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:42:03] d2.utils.events INFO:  eta: 0:53:06  iter: 24759  total_loss: 4.71  loss_ce: 0.09884  loss_mask: 0.1965  loss_dice: 0.1554  loss_ce_0: 0.05223  loss_mask_0: 0.1966  loss_dice_0: 0.1553  loss_ce_1: 0.09884  loss_mask_1: 0.1952  loss_dice_1: 0.158  loss_ce_2: 0.09897  loss_mask_2: 0.1969  loss_dice_2: 0.1553  loss_ce_3: 0.09907  loss_mask_3: 0.1929  loss_dice_3: 0.1562  loss_ce_4: 0.0991  loss_mask_4: 0.1942  loss_dice_4: 0.1555  loss_ce_5: 0.09923  loss_mask_5: 0.1979  loss_dice_5: 0.1573  loss_ce_6: 0.09894  loss_mask_6: 0.1959  loss_dice_6: 0.1567  loss_ce_7: 0.09892  loss_mask_7: 0.1913  loss_dice_7: 0.1524  loss_ce_8: 0.09889  loss_mask_8: 0.1879  loss_dice_8: 0.1488  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:06] d2.utils.events INFO:  eta: 0:53:04  iter: 24779  total_loss: 5.032  loss_ce: 0.09734  loss_mask: 0.2321  loss_dice: 0.1365  loss_ce_0: 0.05162  loss_mask_0: 0.2333  loss_dice_0: 0.1405  loss_ce_1: 0.09754  loss_mask_1: 0.2325  loss_dice_1: 0.1357  loss_ce_2: 0.09726  loss_mask_2: 0.2372  loss_dice_2: 0.1372  loss_ce_3: 0.09725  loss_mask_3: 0.219  loss_dice_3: 0.139  loss_ce_4: 0.09702  loss_mask_4: 0.2181  loss_dice_4: 0.136  loss_ce_5: 0.09732  loss_mask_5: 0.2241  loss_dice_5: 0.1357  loss_ce_6: 0.09714  loss_mask_6: 0.2198  loss_dice_6: 0.1425  loss_ce_7: 0.09695  loss_mask_7: 0.2303  loss_dice_7: 0.1401  loss_ce_8: 0.09729  loss_mask_8: 0.2328  loss_dice_8: 0.1396  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:08] d2.utils.events INFO:  eta: 0:53:00  iter: 24799  total_loss: 4.721  loss_ce: 0.0943  loss_mask: 0.1732  loss_dice: 0.1626  loss_ce_0: 0.05053  loss_mask_0: 0.1719  loss_dice_0: 0.1713  loss_ce_1: 0.09404  loss_mask_1: 0.1707  loss_dice_1: 0.1681  loss_ce_2: 0.09433  loss_mask_2: 0.1712  loss_dice_2: 0.1629  loss_ce_3: 0.09423  loss_mask_3: 0.1724  loss_dice_3: 0.1656  loss_ce_4: 0.09402  loss_mask_4: 0.173  loss_dice_4: 0.1654  loss_ce_5: 0.09424  loss_mask_5: 0.1697  loss_dice_5: 0.1697  loss_ce_6: 0.09404  loss_mask_6: 0.1707  loss_dice_6: 0.1645  loss_ce_7: 0.09408  loss_mask_7: 0.1648  loss_dice_7: 0.1732  loss_ce_8: 0.09418  loss_mask_8: 0.1784  loss_dice_8: 0.1701  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:11] d2.utils.events INFO:  eta: 0:52:57  iter: 24819  total_loss: 5.326  loss_ce: 0.08773  loss_mask: 0.161  loss_dice: 0.1978  loss_ce_0: 0.04836  loss_mask_0: 0.1556  loss_dice_0: 0.1992  loss_ce_1: 0.0874  loss_mask_1: 0.1577  loss_dice_1: 0.1954  loss_ce_2: 0.08755  loss_mask_2: 0.1542  loss_dice_2: 0.1915  loss_ce_3: 0.0873  loss_mask_3: 0.1606  loss_dice_3: 0.1939  loss_ce_4: 0.08668  loss_mask_4: 0.1696  loss_dice_4: 0.2046  loss_ce_5: 0.08716  loss_mask_5: 0.1738  loss_dice_5: 0.1954  loss_ce_6: 0.0868  loss_mask_6: 0.16  loss_dice_6: 0.1942  loss_ce_7: 0.08639  loss_mask_7: 0.1598  loss_dice_7: 0.1898  loss_ce_8: 0.08738  loss_mask_8: 0.1616  loss_dice_8: 0.1916  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:13] d2.utils.events INFO:  eta: 0:52:54  iter: 24839  total_loss: 4.822  loss_ce: 0.08462  loss_mask: 0.1678  loss_dice: 0.1536  loss_ce_0: 0.04719  loss_mask_0: 0.1763  loss_dice_0: 0.1462  loss_ce_1: 0.08409  loss_mask_1: 0.176  loss_dice_1: 0.1606  loss_ce_2: 0.08448  loss_mask_2: 0.1638  loss_dice_2: 0.1565  loss_ce_3: 0.08436  loss_mask_3: 0.1756  loss_dice_3: 0.1579  loss_ce_4: 0.08389  loss_mask_4: 0.17  loss_dice_4: 0.1505  loss_ce_5: 0.08439  loss_mask_5: 0.1706  loss_dice_5: 0.1525  loss_ce_6: 0.08399  loss_mask_6: 0.1813  loss_dice_6: 0.1534  loss_ce_7: 0.08373  loss_mask_7: 0.1796  loss_dice_7: 0.1528  loss_ce_8: 0.08442  loss_mask_8: 0.1722  loss_dice_8: 0.1495  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:16] d2.utils.events INFO:  eta: 0:52:53  iter: 24859  total_loss: 5.425  loss_ce: 0.1885  loss_mask: 0.1765  loss_dice: 0.1566  loss_ce_0: 0.08486  loss_mask_0: 0.1644  loss_dice_0: 0.152  loss_ce_1: 0.1887  loss_mask_1: 0.1726  loss_dice_1: 0.1605  loss_ce_2: 0.1885  loss_mask_2: 0.1718  loss_dice_2: 0.1465  loss_ce_3: 0.1887  loss_mask_3: 0.1718  loss_dice_3: 0.1544  loss_ce_4: 0.1893  loss_mask_4: 0.1773  loss_dice_4: 0.1537  loss_ce_5: 0.1886  loss_mask_5: 0.1688  loss_dice_5: 0.1563  loss_ce_6: 0.1889  loss_mask_6: 0.1749  loss_dice_6: 0.1525  loss_ce_7: 0.189  loss_mask_7: 0.1736  loss_dice_7: 0.1567  loss_ce_8: 0.1884  loss_mask_8: 0.1771  loss_dice_8: 0.1569  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:18] d2.utils.events INFO:  eta: 0:52:51  iter: 24879  total_loss: 5.212  loss_ce: 0.08523  loss_mask: 0.1879  loss_dice: 0.1816  loss_ce_0: 0.04646  loss_mask_0: 0.1896  loss_dice_0: 0.1771  loss_ce_1: 0.08565  loss_mask_1: 0.1726  loss_dice_1: 0.1747  loss_ce_2: 0.0851  loss_mask_2: 0.1872  loss_dice_2: 0.1734  loss_ce_3: 0.08476  loss_mask_3: 0.1825  loss_dice_3: 0.177  loss_ce_4: 0.08494  loss_mask_4: 0.1815  loss_dice_4: 0.1763  loss_ce_5: 0.08503  loss_mask_5: 0.1996  loss_dice_5: 0.1701  loss_ce_6: 0.08522  loss_mask_6: 0.1881  loss_dice_6: 0.1742  loss_ce_7: 0.08562  loss_mask_7: 0.1819  loss_dice_7: 0.1756  loss_ce_8: 0.08557  loss_mask_8: 0.181  loss_dice_8: 0.1802  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:21] d2.utils.events INFO:  eta: 0:52:46  iter: 24899  total_loss: 5.184  loss_ce: 0.1807  loss_mask: 0.1975  loss_dice: 0.1552  loss_ce_0: 0.0834  loss_mask_0: 0.1938  loss_dice_0: 0.1554  loss_ce_1: 0.178  loss_mask_1: 0.1936  loss_dice_1: 0.1546  loss_ce_2: 0.1803  loss_mask_2: 0.2004  loss_dice_2: 0.148  loss_ce_3: 0.1804  loss_mask_3: 0.201  loss_dice_3: 0.1524  loss_ce_4: 0.1795  loss_mask_4: 0.2036  loss_dice_4: 0.1485  loss_ce_5: 0.1803  loss_mask_5: 0.2042  loss_dice_5: 0.1573  loss_ce_6: 0.1804  loss_mask_6: 0.1996  loss_dice_6: 0.1509  loss_ce_7: 0.18  loss_mask_7: 0.1963  loss_dice_7: 0.1588  loss_ce_8: 0.1804  loss_mask_8: 0.195  loss_dice_8: 0.1482  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:23] d2.utils.events INFO:  eta: 0:52:44  iter: 24919  total_loss: 5.199  loss_ce: 0.08982  loss_mask: 0.1882  loss_dice: 0.1715  loss_ce_0: 0.04814  loss_mask_0: 0.1756  loss_dice_0: 0.1765  loss_ce_1: 0.09127  loss_mask_1: 0.1761  loss_dice_1: 0.1621  loss_ce_2: 0.08996  loss_mask_2: 0.1746  loss_dice_2: 0.1713  loss_ce_3: 0.09005  loss_mask_3: 0.183  loss_dice_3: 0.1647  loss_ce_4: 0.09044  loss_mask_4: 0.175  loss_dice_4: 0.1693  loss_ce_5: 0.09029  loss_mask_5: 0.1712  loss_dice_5: 0.1608  loss_ce_6: 0.09033  loss_mask_6: 0.1784  loss_dice_6: 0.1693  loss_ce_7: 0.09078  loss_mask_7: 0.1699  loss_dice_7: 0.1596  loss_ce_8: 0.09013  loss_mask_8: 0.1776  loss_dice_8: 0.1697  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:26] d2.utils.events INFO:  eta: 0:52:40  iter: 24939  total_loss: 5.371  loss_ce: 0.09185  loss_mask: 0.2181  loss_dice: 0.1833  loss_ce_0: 0.04904  loss_mask_0: 0.219  loss_dice_0: 0.1829  loss_ce_1: 0.09318  loss_mask_1: 0.2248  loss_dice_1: 0.183  loss_ce_2: 0.09201  loss_mask_2: 0.2218  loss_dice_2: 0.1855  loss_ce_3: 0.09198  loss_mask_3: 0.2157  loss_dice_3: 0.182  loss_ce_4: 0.09284  loss_mask_4: 0.2216  loss_dice_4: 0.1912  loss_ce_5: 0.09199  loss_mask_5: 0.2214  loss_dice_5: 0.1815  loss_ce_6: 0.09199  loss_mask_6: 0.2139  loss_dice_6: 0.18  loss_ce_7: 0.09263  loss_mask_7: 0.2275  loss_dice_7: 0.1871  loss_ce_8: 0.09205  loss_mask_8: 0.226  loss_dice_8: 0.193  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:28] d2.utils.events INFO:  eta: 0:52:40  iter: 24959  total_loss: 5.021  loss_ce: 0.09208  loss_mask: 0.1765  loss_dice: 0.1564  loss_ce_0: 0.04924  loss_mask_0: 0.1846  loss_dice_0: 0.1527  loss_ce_1: 0.09314  loss_mask_1: 0.1755  loss_dice_1: 0.1569  loss_ce_2: 0.09227  loss_mask_2: 0.1899  loss_dice_2: 0.1577  loss_ce_3: 0.09208  loss_mask_3: 0.1893  loss_dice_3: 0.1539  loss_ce_4: 0.09219  loss_mask_4: 0.1848  loss_dice_4: 0.1552  loss_ce_5: 0.09206  loss_mask_5: 0.1933  loss_dice_5: 0.1581  loss_ce_6: 0.09213  loss_mask_6: 0.1943  loss_dice_6: 0.1562  loss_ce_7: 0.09236  loss_mask_7: 0.186  loss_dice_7: 0.1559  loss_ce_8: 0.09227  loss_mask_8: 0.1841  loss_dice_8: 0.1624  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:31] d2.utils.events INFO:  eta: 0:52:38  iter: 24979  total_loss: 5.024  loss_ce: 0.1611  loss_mask: 0.1542  loss_dice: 0.1722  loss_ce_0: 0.07608  loss_mask_0: 0.1587  loss_dice_0: 0.1546  loss_ce_1: 0.1592  loss_mask_1: 0.1494  loss_dice_1: 0.1588  loss_ce_2: 0.1606  loss_mask_2: 0.1607  loss_dice_2: 0.1598  loss_ce_3: 0.1604  loss_mask_3: 0.1629  loss_dice_3: 0.1599  loss_ce_4: 0.1596  loss_mask_4: 0.1487  loss_dice_4: 0.1638  loss_ce_5: 0.1603  loss_mask_5: 0.1466  loss_dice_5: 0.1674  loss_ce_6: 0.1599  loss_mask_6: 0.1574  loss_dice_6: 0.1574  loss_ce_7: 0.1592  loss_mask_7: 0.1528  loss_dice_7: 0.1662  loss_ce_8: 0.1606  loss_mask_8: 0.1582  loss_dice_8: 0.1654  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:42:33] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0024999.pth
[04/13 15:42:33] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 15:42:33] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 15:42:33] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 15:42:33] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 15:42:34] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 15:42:37] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0530 s/iter. Eval: 0.2345 s/iter. Total: 0.2881 s/iter. ETA=0:04:03
[04/13 15:42:42] d2.evaluation.evaluator INFO: Inference done 29/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2336 s/iter. Total: 0.2874 s/iter. ETA=0:03:57
[04/13 15:42:47] d2.evaluation.evaluator INFO: Inference done 47/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2335 s/iter. Total: 0.2872 s/iter. ETA=0:03:52
[04/13 15:42:52] d2.evaluation.evaluator INFO: Inference done 65/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2332 s/iter. Total: 0.2869 s/iter. ETA=0:03:46
[04/13 15:42:58] d2.evaluation.evaluator INFO: Inference done 83/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2332 s/iter. Total: 0.2869 s/iter. ETA=0:03:41
[04/13 15:43:03] d2.evaluation.evaluator INFO: Inference done 101/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2331 s/iter. Total: 0.2869 s/iter. ETA=0:03:36
[04/13 15:43:08] d2.evaluation.evaluator INFO: Inference done 119/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2343 s/iter. Total: 0.2883 s/iter. ETA=0:03:32
[04/13 15:43:13] d2.evaluation.evaluator INFO: Inference done 137/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2343 s/iter. Total: 0.2882 s/iter. ETA=0:03:27
[04/13 15:43:18] d2.evaluation.evaluator INFO: Inference done 155/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2344 s/iter. Total: 0.2884 s/iter. ETA=0:03:22
[04/13 15:43:24] d2.evaluation.evaluator INFO: Inference done 173/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2345 s/iter. Total: 0.2884 s/iter. ETA=0:03:16
[04/13 15:43:29] d2.evaluation.evaluator INFO: Inference done 191/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2345 s/iter. Total: 0.2884 s/iter. ETA=0:03:11
[04/13 15:43:34] d2.evaluation.evaluator INFO: Inference done 209/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2345 s/iter. Total: 0.2884 s/iter. ETA=0:03:06
[04/13 15:43:39] d2.evaluation.evaluator INFO: Inference done 227/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2346 s/iter. Total: 0.2885 s/iter. ETA=0:03:01
[04/13 15:43:44] d2.evaluation.evaluator INFO: Inference done 245/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2345 s/iter. Total: 0.2883 s/iter. ETA=0:02:56
[04/13 15:43:50] d2.evaluation.evaluator INFO: Inference done 263/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2344 s/iter. Total: 0.2883 s/iter. ETA=0:02:50
[04/13 15:43:55] d2.evaluation.evaluator INFO: Inference done 281/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2345 s/iter. Total: 0.2883 s/iter. ETA=0:02:45
[04/13 15:44:00] d2.evaluation.evaluator INFO: Inference done 299/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2345 s/iter. Total: 0.2883 s/iter. ETA=0:02:40
[04/13 15:44:05] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2345 s/iter. Total: 0.2883 s/iter. ETA=0:02:35
[04/13 15:44:10] d2.evaluation.evaluator INFO: Inference done 335/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2347 s/iter. Total: 0.2885 s/iter. ETA=0:02:30
[04/13 15:44:15] d2.evaluation.evaluator INFO: Inference done 352/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2352 s/iter. Total: 0.2890 s/iter. ETA=0:02:25
[04/13 15:44:21] d2.evaluation.evaluator INFO: Inference done 370/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2352 s/iter. Total: 0.2890 s/iter. ETA=0:02:20
[04/13 15:44:26] d2.evaluation.evaluator INFO: Inference done 388/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2352 s/iter. Total: 0.2891 s/iter. ETA=0:02:15
[04/13 15:44:31] d2.evaluation.evaluator INFO: Inference done 406/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2353 s/iter. Total: 0.2891 s/iter. ETA=0:02:10
[04/13 15:44:36] d2.evaluation.evaluator INFO: Inference done 424/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2353 s/iter. Total: 0.2891 s/iter. ETA=0:02:04
[04/13 15:44:42] d2.evaluation.evaluator INFO: Inference done 442/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2353 s/iter. Total: 0.2891 s/iter. ETA=0:01:59
[04/13 15:44:47] d2.evaluation.evaluator INFO: Inference done 460/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2354 s/iter. Total: 0.2892 s/iter. ETA=0:01:54
[04/13 15:44:52] d2.evaluation.evaluator INFO: Inference done 478/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2354 s/iter. Total: 0.2892 s/iter. ETA=0:01:49
[04/13 15:44:57] d2.evaluation.evaluator INFO: Inference done 496/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2354 s/iter. Total: 0.2892 s/iter. ETA=0:01:44
[04/13 15:45:02] d2.evaluation.evaluator INFO: Inference done 514/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2354 s/iter. Total: 0.2892 s/iter. ETA=0:01:38
[04/13 15:45:08] d2.evaluation.evaluator INFO: Inference done 532/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2354 s/iter. Total: 0.2892 s/iter. ETA=0:01:33
[04/13 15:45:13] d2.evaluation.evaluator INFO: Inference done 550/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2355 s/iter. Total: 0.2893 s/iter. ETA=0:01:28
[04/13 15:45:18] d2.evaluation.evaluator INFO: Inference done 567/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2356 s/iter. Total: 0.2895 s/iter. ETA=0:01:23
[04/13 15:45:23] d2.evaluation.evaluator INFO: Inference done 585/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2357 s/iter. Total: 0.2895 s/iter. ETA=0:01:18
[04/13 15:45:28] d2.evaluation.evaluator INFO: Inference done 603/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2357 s/iter. Total: 0.2895 s/iter. ETA=0:01:13
[04/13 15:45:34] d2.evaluation.evaluator INFO: Inference done 621/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2357 s/iter. Total: 0.2895 s/iter. ETA=0:01:08
[04/13 15:45:39] d2.evaluation.evaluator INFO: Inference done 639/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2357 s/iter. Total: 0.2895 s/iter. ETA=0:01:02
[04/13 15:45:44] d2.evaluation.evaluator INFO: Inference done 657/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2356 s/iter. Total: 0.2894 s/iter. ETA=0:00:57
[04/13 15:45:49] d2.evaluation.evaluator INFO: Inference done 675/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2355 s/iter. Total: 0.2893 s/iter. ETA=0:00:52
[04/13 15:45:54] d2.evaluation.evaluator INFO: Inference done 693/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2355 s/iter. Total: 0.2893 s/iter. ETA=0:00:47
[04/13 15:45:59] d2.evaluation.evaluator INFO: Inference done 711/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2356 s/iter. Total: 0.2893 s/iter. ETA=0:00:41
[04/13 15:46:05] d2.evaluation.evaluator INFO: Inference done 729/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2356 s/iter. Total: 0.2894 s/iter. ETA=0:00:36
[04/13 15:46:10] d2.evaluation.evaluator INFO: Inference done 747/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2357 s/iter. Total: 0.2895 s/iter. ETA=0:00:31
[04/13 15:46:15] d2.evaluation.evaluator INFO: Inference done 764/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2359 s/iter. Total: 0.2896 s/iter. ETA=0:00:26
[04/13 15:46:20] d2.evaluation.evaluator INFO: Inference done 781/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2360 s/iter. Total: 0.2898 s/iter. ETA=0:00:21
[04/13 15:46:25] d2.evaluation.evaluator INFO: Inference done 798/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2361 s/iter. Total: 0.2899 s/iter. ETA=0:00:16
[04/13 15:46:30] d2.evaluation.evaluator INFO: Inference done 815/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2363 s/iter. Total: 0.2901 s/iter. ETA=0:00:11
[04/13 15:46:35] d2.evaluation.evaluator INFO: Inference done 832/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2366 s/iter. Total: 0.2904 s/iter. ETA=0:00:06
[04/13 15:46:40] d2.evaluation.evaluator INFO: Inference done 849/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2368 s/iter. Total: 0.2906 s/iter. ETA=0:00:02
[04/13 15:46:43] d2.evaluation.evaluator INFO: Total inference time: 0:04:07.434608 (0.290757 s / iter per device, on 1 devices)
[04/13 15:46:43] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052872 s / iter per device, on 1 devices)
[04/13 15:46:44] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 15:46:44] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 15:46:45] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 15:46:45] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 15:46:45] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.27 seconds.
[04/13 15:46:45] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 15:46:45] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 15:46:45] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 15:46:45] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 15:46:45] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 15:46:49] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 15:46:51] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 1.87 seconds.
[04/13 15:46:51] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 15:46:51] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 15:46:51] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 60.213 | 60.213 | 60.213 |  nan  |  nan  | 60.213 |
[04/13 15:46:51] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 15:46:51] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 67.973 | defect     | 52.453 |
[04/13 15:46:51] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 15:46:51] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 15:46:51] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 15:46:51] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 15:46:51] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 15:46:51] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 15:46:51] d2.evaluation.testing INFO: copypaste: 60.2129,60.2129,60.2129,nan,nan,60.2129
[04/13 15:46:51] d2.utils.events INFO:  eta: 0:52:35  iter: 24999  total_loss: 5.71  loss_ce: 0.1059  loss_mask: 0.1767  loss_dice: 0.2282  loss_ce_0: 0.05428  loss_mask_0: 0.1763  loss_dice_0: 0.2208  loss_ce_1: 0.107  loss_mask_1: 0.1778  loss_dice_1: 0.234  loss_ce_2: 0.106  loss_mask_2: 0.1685  loss_dice_2: 0.2249  loss_ce_3: 0.1062  loss_mask_3: 0.1779  loss_dice_3: 0.2305  loss_ce_4: 0.1073  loss_mask_4: 0.1692  loss_dice_4: 0.2205  loss_ce_5: 0.1067  loss_mask_5: 0.1743  loss_dice_5: 0.2311  loss_ce_6: 0.1072  loss_mask_6: 0.1744  loss_dice_6: 0.2272  loss_ce_7: 0.1079  loss_mask_7: 0.1712  loss_dice_7: 0.2301  loss_ce_8: 0.1064  loss_mask_8: 0.1707  loss_dice_8: 0.2307  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:46:54] d2.utils.events INFO:  eta: 0:52:31  iter: 25019  total_loss: 5.734  loss_ce: 0.1291  loss_mask: 0.1962  loss_dice: 0.1933  loss_ce_0: 0.0637  loss_mask_0: 0.1833  loss_dice_0: 0.1854  loss_ce_1: 0.1288  loss_mask_1: 0.1888  loss_dice_1: 0.2004  loss_ce_2: 0.1292  loss_mask_2: 0.1904  loss_dice_2: 0.1902  loss_ce_3: 0.1292  loss_mask_3: 0.1897  loss_dice_3: 0.1907  loss_ce_4: 0.1292  loss_mask_4: 0.1861  loss_dice_4: 0.1927  loss_ce_5: 0.1292  loss_mask_5: 0.1848  loss_dice_5: 0.1883  loss_ce_6: 0.1291  loss_mask_6: 0.1953  loss_dice_6: 0.1915  loss_ce_7: 0.1291  loss_mask_7: 0.1867  loss_dice_7: 0.1925  loss_ce_8: 0.129  loss_mask_8: 0.1887  loss_dice_8: 0.1911  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:46:57] d2.utils.events INFO:  eta: 0:52:30  iter: 25039  total_loss: 5.683  loss_ce: 0.1042  loss_mask: 0.2218  loss_dice: 0.207  loss_ce_0: 0.05397  loss_mask_0: 0.2292  loss_dice_0: 0.2104  loss_ce_1: 0.1048  loss_mask_1: 0.2233  loss_dice_1: 0.2104  loss_ce_2: 0.1043  loss_mask_2: 0.2208  loss_dice_2: 0.2121  loss_ce_3: 0.1045  loss_mask_3: 0.2239  loss_dice_3: 0.2025  loss_ce_4: 0.1048  loss_mask_4: 0.2264  loss_dice_4: 0.2068  loss_ce_5: 0.1045  loss_mask_5: 0.2267  loss_dice_5: 0.2061  loss_ce_6: 0.1045  loss_mask_6: 0.2233  loss_dice_6: 0.2067  loss_ce_7: 0.1045  loss_mask_7: 0.2247  loss_dice_7: 0.2078  loss_ce_8: 0.1041  loss_mask_8: 0.2238  loss_dice_8: 0.1997  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:46:59] d2.utils.events INFO:  eta: 0:52:26  iter: 25059  total_loss: 5.097  loss_ce: 0.09963  loss_mask: 0.2286  loss_dice: 0.1312  loss_ce_0: 0.05213  loss_mask_0: 0.2335  loss_dice_0: 0.1288  loss_ce_1: 0.09929  loss_mask_1: 0.225  loss_dice_1: 0.1355  loss_ce_2: 0.09917  loss_mask_2: 0.2179  loss_dice_2: 0.1304  loss_ce_3: 0.09903  loss_mask_3: 0.2299  loss_dice_3: 0.1349  loss_ce_4: 0.09883  loss_mask_4: 0.2206  loss_dice_4: 0.1324  loss_ce_5: 0.09891  loss_mask_5: 0.2307  loss_dice_5: 0.132  loss_ce_6: 0.09905  loss_mask_6: 0.2268  loss_dice_6: 0.1383  loss_ce_7: 0.09899  loss_mask_7: 0.235  loss_dice_7: 0.1338  loss_ce_8: 0.0995  loss_mask_8: 0.238  loss_dice_8: 0.1322  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:02] d2.utils.events INFO:  eta: 0:52:27  iter: 25079  total_loss: 5.42  loss_ce: 0.1562  loss_mask: 0.1904  loss_dice: 0.1909  loss_ce_0: 0.07385  loss_mask_0: 0.1976  loss_dice_0: 0.1875  loss_ce_1: 0.1547  loss_mask_1: 0.1922  loss_dice_1: 0.1861  loss_ce_2: 0.1558  loss_mask_2: 0.1976  loss_dice_2: 0.1912  loss_ce_3: 0.1554  loss_mask_3: 0.195  loss_dice_3: 0.1888  loss_ce_4: 0.1553  loss_mask_4: 0.194  loss_dice_4: 0.186  loss_ce_5: 0.1557  loss_mask_5: 0.1973  loss_dice_5: 0.189  loss_ce_6: 0.1558  loss_mask_6: 0.1943  loss_dice_6: 0.1881  loss_ce_7: 0.1554  loss_mask_7: 0.1899  loss_dice_7: 0.1958  loss_ce_8: 0.1559  loss_mask_8: 0.1942  loss_dice_8: 0.1883  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:04] d2.utils.events INFO:  eta: 0:52:26  iter: 25099  total_loss: 4.764  loss_ce: 0.1285  loss_mask: 0.1709  loss_dice: 0.1774  loss_ce_0: 0.06325  loss_mask_0: 0.166  loss_dice_0: 0.1562  loss_ce_1: 0.1284  loss_mask_1: 0.1613  loss_dice_1: 0.1742  loss_ce_2: 0.1285  loss_mask_2: 0.1651  loss_dice_2: 0.1633  loss_ce_3: 0.1286  loss_mask_3: 0.1715  loss_dice_3: 0.1708  loss_ce_4: 0.1284  loss_mask_4: 0.1711  loss_dice_4: 0.1684  loss_ce_5: 0.1284  loss_mask_5: 0.174  loss_dice_5: 0.1622  loss_ce_6: 0.1284  loss_mask_6: 0.1665  loss_dice_6: 0.1624  loss_ce_7: 0.1284  loss_mask_7: 0.1711  loss_dice_7: 0.1746  loss_ce_8: 0.1286  loss_mask_8: 0.17  loss_dice_8: 0.1743  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:47:07] d2.utils.events INFO:  eta: 0:52:22  iter: 25119  total_loss: 4.955  loss_ce: 0.1278  loss_mask: 0.1697  loss_dice: 0.1675  loss_ce_0: 0.06301  loss_mask_0: 0.1691  loss_dice_0: 0.1718  loss_ce_1: 0.1279  loss_mask_1: 0.1631  loss_dice_1: 0.1676  loss_ce_2: 0.1275  loss_mask_2: 0.1758  loss_dice_2: 0.1689  loss_ce_3: 0.1273  loss_mask_3: 0.1748  loss_dice_3: 0.1721  loss_ce_4: 0.1275  loss_mask_4: 0.1684  loss_dice_4: 0.1719  loss_ce_5: 0.1275  loss_mask_5: 0.1719  loss_dice_5: 0.1682  loss_ce_6: 0.1275  loss_mask_6: 0.1716  loss_dice_6: 0.1714  loss_ce_7: 0.1274  loss_mask_7: 0.1767  loss_dice_7: 0.1736  loss_ce_8: 0.1277  loss_mask_8: 0.1733  loss_dice_8: 0.1684  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:09] d2.utils.events INFO:  eta: 0:52:18  iter: 25139  total_loss: 4.837  loss_ce: 0.1187  loss_mask: 0.2037  loss_dice: 0.1356  loss_ce_0: 0.05968  loss_mask_0: 0.1995  loss_dice_0: 0.1311  loss_ce_1: 0.12  loss_mask_1: 0.1974  loss_dice_1: 0.1374  loss_ce_2: 0.1185  loss_mask_2: 0.1936  loss_dice_2: 0.1353  loss_ce_3: 0.1189  loss_mask_3: 0.2007  loss_dice_3: 0.138  loss_ce_4: 0.1192  loss_mask_4: 0.2028  loss_dice_4: 0.1313  loss_ce_5: 0.1186  loss_mask_5: 0.1999  loss_dice_5: 0.1405  loss_ce_6: 0.1192  loss_mask_6: 0.2142  loss_dice_6: 0.1375  loss_ce_7: 0.1191  loss_mask_7: 0.2003  loss_dice_7: 0.1321  loss_ce_8: 0.1186  loss_mask_8: 0.2031  loss_dice_8: 0.1366  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:12] d2.utils.events INFO:  eta: 0:52:13  iter: 25159  total_loss: 5.431  loss_ce: 0.1132  loss_mask: 0.2243  loss_dice: 0.1605  loss_ce_0: 0.05742  loss_mask_0: 0.2279  loss_dice_0: 0.1586  loss_ce_1: 0.1129  loss_mask_1: 0.2428  loss_dice_1: 0.1622  loss_ce_2: 0.1124  loss_mask_2: 0.2294  loss_dice_2: 0.1643  loss_ce_3: 0.1121  loss_mask_3: 0.2431  loss_dice_3: 0.1585  loss_ce_4: 0.1122  loss_mask_4: 0.2211  loss_dice_4: 0.1576  loss_ce_5: 0.1124  loss_mask_5: 0.2382  loss_dice_5: 0.162  loss_ce_6: 0.1127  loss_mask_6: 0.2293  loss_dice_6: 0.1642  loss_ce_7: 0.1126  loss_mask_7: 0.2322  loss_dice_7: 0.1632  loss_ce_8: 0.1132  loss_mask_8: 0.235  loss_dice_8: 0.1643  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:14] d2.utils.events INFO:  eta: 0:52:10  iter: 25179  total_loss: 5.201  loss_ce: 0.1062  loss_mask: 0.2482  loss_dice: 0.1569  loss_ce_0: 0.05488  loss_mask_0: 0.2324  loss_dice_0: 0.153  loss_ce_1: 0.1056  loss_mask_1: 0.2405  loss_dice_1: 0.1519  loss_ce_2: 0.1057  loss_mask_2: 0.2482  loss_dice_2: 0.1589  loss_ce_3: 0.1053  loss_mask_3: 0.2389  loss_dice_3: 0.1505  loss_ce_4: 0.1049  loss_mask_4: 0.2485  loss_dice_4: 0.1565  loss_ce_5: 0.1054  loss_mask_5: 0.2357  loss_dice_5: 0.1505  loss_ce_6: 0.1053  loss_mask_6: 0.2438  loss_dice_6: 0.1581  loss_ce_7: 0.1051  loss_mask_7: 0.2382  loss_dice_7: 0.1481  loss_ce_8: 0.106  loss_mask_8: 0.248  loss_dice_8: 0.1496  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:17] d2.utils.events INFO:  eta: 0:52:07  iter: 25199  total_loss: 4.733  loss_ce: 0.1011  loss_mask: 0.2001  loss_dice: 0.137  loss_ce_0: 0.05309  loss_mask_0: 0.1946  loss_dice_0: 0.1383  loss_ce_1: 0.1002  loss_mask_1: 0.1934  loss_dice_1: 0.1447  loss_ce_2: 0.1011  loss_mask_2: 0.1918  loss_dice_2: 0.1405  loss_ce_3: 0.1008  loss_mask_3: 0.1973  loss_dice_3: 0.1408  loss_ce_4: 0.1003  loss_mask_4: 0.1876  loss_dice_4: 0.14  loss_ce_5: 0.1008  loss_mask_5: 0.1943  loss_dice_5: 0.1392  loss_ce_6: 0.1004  loss_mask_6: 0.1913  loss_dice_6: 0.1377  loss_ce_7: 0.1001  loss_mask_7: 0.1928  loss_dice_7: 0.1418  loss_ce_8: 0.1009  loss_mask_8: 0.1956  loss_dice_8: 0.1417  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:19] d2.utils.events INFO:  eta: 0:52:05  iter: 25219  total_loss: 4.898  loss_ce: 0.09991  loss_mask: 0.188  loss_dice: 0.1504  loss_ce_0: 0.0528  loss_mask_0: 0.1954  loss_dice_0: 0.144  loss_ce_1: 0.1012  loss_mask_1: 0.2045  loss_dice_1: 0.1508  loss_ce_2: 0.1005  loss_mask_2: 0.1976  loss_dice_2: 0.1464  loss_ce_3: 0.1004  loss_mask_3: 0.1982  loss_dice_3: 0.1505  loss_ce_4: 0.1003  loss_mask_4: 0.2017  loss_dice_4: 0.1532  loss_ce_5: 0.1004  loss_mask_5: 0.1885  loss_dice_5: 0.1499  loss_ce_6: 0.1  loss_mask_6: 0.1971  loss_dice_6: 0.1449  loss_ce_7: 0.1001  loss_mask_7: 0.196  loss_dice_7: 0.146  loss_ce_8: 0.1002  loss_mask_8: 0.1946  loss_dice_8: 0.1458  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:22] d2.utils.events INFO:  eta: 0:52:03  iter: 25239  total_loss: 4.752  loss_ce: 0.1578  loss_mask: 0.1842  loss_dice: 0.1244  loss_ce_0: 0.07395  loss_mask_0: 0.1863  loss_dice_0: 0.1306  loss_ce_1: 0.1569  loss_mask_1: 0.194  loss_dice_1: 0.1262  loss_ce_2: 0.1573  loss_mask_2: 0.1929  loss_dice_2: 0.1268  loss_ce_3: 0.1573  loss_mask_3: 0.1993  loss_dice_3: 0.1343  loss_ce_4: 0.1572  loss_mask_4: 0.1823  loss_dice_4: 0.1232  loss_ce_5: 0.1572  loss_mask_5: 0.1837  loss_dice_5: 0.1276  loss_ce_6: 0.1573  loss_mask_6: 0.1844  loss_dice_6: 0.1305  loss_ce_7: 0.1572  loss_mask_7: 0.1948  loss_dice_7: 0.1293  loss_ce_8: 0.1575  loss_mask_8: 0.1871  loss_dice_8: 0.1258  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:47:25] d2.utils.events INFO:  eta: 0:52:02  iter: 25259  total_loss: 5.355  loss_ce: 0.1456  loss_mask: 0.1475  loss_dice: 0.2536  loss_ce_0: 0.06932  loss_mask_0: 0.1519  loss_dice_0: 0.2542  loss_ce_1: 0.1436  loss_mask_1: 0.1468  loss_dice_1: 0.234  loss_ce_2: 0.1445  loss_mask_2: 0.1403  loss_dice_2: 0.2515  loss_ce_3: 0.144  loss_mask_3: 0.1418  loss_dice_3: 0.2458  loss_ce_4: 0.1432  loss_mask_4: 0.1464  loss_dice_4: 0.2448  loss_ce_5: 0.1442  loss_mask_5: 0.1412  loss_dice_5: 0.2653  loss_ce_6: 0.1442  loss_mask_6: 0.15  loss_dice_6: 0.2608  loss_ce_7: 0.1437  loss_mask_7: 0.1521  loss_dice_7: 0.2571  loss_ce_8: 0.145  loss_mask_8: 0.146  loss_dice_8: 0.2629  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:27] d2.utils.events INFO:  eta: 0:51:58  iter: 25279  total_loss: 5.158  loss_ce: 0.1233  loss_mask: 0.2234  loss_dice: 0.1641  loss_ce_0: 0.06148  loss_mask_0: 0.2301  loss_dice_0: 0.1602  loss_ce_1: 0.1262  loss_mask_1: 0.2234  loss_dice_1: 0.1615  loss_ce_2: 0.1235  loss_mask_2: 0.2212  loss_dice_2: 0.158  loss_ce_3: 0.1242  loss_mask_3: 0.2316  loss_dice_3: 0.1673  loss_ce_4: 0.1256  loss_mask_4: 0.2255  loss_dice_4: 0.1644  loss_ce_5: 0.1238  loss_mask_5: 0.2165  loss_dice_5: 0.1657  loss_ce_6: 0.1243  loss_mask_6: 0.2169  loss_dice_6: 0.1711  loss_ce_7: 0.1252  loss_mask_7: 0.2381  loss_dice_7: 0.1662  loss_ce_8: 0.1238  loss_mask_8: 0.2132  loss_dice_8: 0.1642  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:47:30] d2.utils.events INFO:  eta: 0:51:52  iter: 25299  total_loss: 5.178  loss_ce: 0.1317  loss_mask: 0.2032  loss_dice: 0.1519  loss_ce_0: 0.06456  loss_mask_0: 0.2084  loss_dice_0: 0.1514  loss_ce_1: 0.1292  loss_mask_1: 0.2059  loss_dice_1: 0.1503  loss_ce_2: 0.1322  loss_mask_2: 0.2091  loss_dice_2: 0.1509  loss_ce_3: 0.1314  loss_mask_3: 0.201  loss_dice_3: 0.1557  loss_ce_4: 0.1303  loss_mask_4: 0.2099  loss_dice_4: 0.1534  loss_ce_5: 0.1317  loss_mask_5: 0.2093  loss_dice_5: 0.1565  loss_ce_6: 0.1308  loss_mask_6: 0.2071  loss_dice_6: 0.1454  loss_ce_7: 0.1302  loss_mask_7: 0.2071  loss_dice_7: 0.1532  loss_ce_8: 0.1314  loss_mask_8: 0.21  loss_dice_8: 0.1573  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:32] d2.utils.events INFO:  eta: 0:51:45  iter: 25319  total_loss: 5.633  loss_ce: 0.1294  loss_mask: 0.1447  loss_dice: 0.2889  loss_ce_0: 0.06364  loss_mask_0: 0.1406  loss_dice_0: 0.2905  loss_ce_1: 0.1283  loss_mask_1: 0.1416  loss_dice_1: 0.2984  loss_ce_2: 0.1298  loss_mask_2: 0.1463  loss_dice_2: 0.3069  loss_ce_3: 0.1298  loss_mask_3: 0.1401  loss_dice_3: 0.2828  loss_ce_4: 0.129  loss_mask_4: 0.1406  loss_dice_4: 0.3003  loss_ce_5: 0.1295  loss_mask_5: 0.1432  loss_dice_5: 0.2934  loss_ce_6: 0.1295  loss_mask_6: 0.1337  loss_dice_6: 0.2924  loss_ce_7: 0.1296  loss_mask_7: 0.137  loss_dice_7: 0.2924  loss_ce_8: 0.1292  loss_mask_8: 0.1414  loss_dice_8: 0.2958  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:47:35] d2.utils.events INFO:  eta: 0:51:40  iter: 25339  total_loss: 4.753  loss_ce: 0.1276  loss_mask: 0.2083  loss_dice: 0.1542  loss_ce_0: 0.06301  loss_mask_0: 0.2118  loss_dice_0: 0.1502  loss_ce_1: 0.1271  loss_mask_1: 0.2025  loss_dice_1: 0.1449  loss_ce_2: 0.1285  loss_mask_2: 0.2066  loss_dice_2: 0.1504  loss_ce_3: 0.1282  loss_mask_3: 0.1971  loss_dice_3: 0.1475  loss_ce_4: 0.1277  loss_mask_4: 0.2073  loss_dice_4: 0.1461  loss_ce_5: 0.1279  loss_mask_5: 0.1996  loss_dice_5: 0.1505  loss_ce_6: 0.1274  loss_mask_6: 0.2072  loss_dice_6: 0.1557  loss_ce_7: 0.1276  loss_mask_7: 0.2073  loss_dice_7: 0.1519  loss_ce_8: 0.1277  loss_mask_8: 0.2001  loss_dice_8: 0.1524  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:37] d2.utils.events INFO:  eta: 0:51:34  iter: 25359  total_loss: 5.037  loss_ce: 0.1282  loss_mask: 0.1735  loss_dice: 0.1808  loss_ce_0: 0.06312  loss_mask_0: 0.1774  loss_dice_0: 0.1798  loss_ce_1: 0.1276  loss_mask_1: 0.1813  loss_dice_1: 0.1811  loss_ce_2: 0.1281  loss_mask_2: 0.184  loss_dice_2: 0.1734  loss_ce_3: 0.1282  loss_mask_3: 0.176  loss_dice_3: 0.1751  loss_ce_4: 0.1283  loss_mask_4: 0.1838  loss_dice_4: 0.178  loss_ce_5: 0.1282  loss_mask_5: 0.1693  loss_dice_5: 0.1736  loss_ce_6: 0.1281  loss_mask_6: 0.1765  loss_dice_6: 0.183  loss_ce_7: 0.128  loss_mask_7: 0.1809  loss_dice_7: 0.183  loss_ce_8: 0.1278  loss_mask_8: 0.1864  loss_dice_8: 0.1812  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:40] d2.utils.events INFO:  eta: 0:51:30  iter: 25379  total_loss: 5.324  loss_ce: 0.1276  loss_mask: 0.2012  loss_dice: 0.1857  loss_ce_0: 0.06294  loss_mask_0: 0.1958  loss_dice_0: 0.1885  loss_ce_1: 0.1277  loss_mask_1: 0.1893  loss_dice_1: 0.1867  loss_ce_2: 0.1275  loss_mask_2: 0.1933  loss_dice_2: 0.1836  loss_ce_3: 0.1277  loss_mask_3: 0.1955  loss_dice_3: 0.1753  loss_ce_4: 0.1296  loss_mask_4: 0.191  loss_dice_4: 0.186  loss_ce_5: 0.1276  loss_mask_5: 0.1966  loss_dice_5: 0.1762  loss_ce_6: 0.1277  loss_mask_6: 0.2051  loss_dice_6: 0.1782  loss_ce_7: 0.1281  loss_mask_7: 0.1987  loss_dice_7: 0.1913  loss_ce_8: 0.1276  loss_mask_8: 0.1923  loss_dice_8: 0.1792  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:42] d2.utils.events INFO:  eta: 0:51:28  iter: 25399  total_loss: 4.919  loss_ce: 0.1404  loss_mask: 0.1869  loss_dice: 0.1584  loss_ce_0: 0.06771  loss_mask_0: 0.1872  loss_dice_0: 0.1615  loss_ce_1: 0.1389  loss_mask_1: 0.194  loss_dice_1: 0.1579  loss_ce_2: 0.1396  loss_mask_2: 0.1933  loss_dice_2: 0.1557  loss_ce_3: 0.1403  loss_mask_3: 0.1945  loss_dice_3: 0.1548  loss_ce_4: 0.1405  loss_mask_4: 0.1967  loss_dice_4: 0.1607  loss_ce_5: 0.1397  loss_mask_5: 0.1998  loss_dice_5: 0.1569  loss_ce_6: 0.1405  loss_mask_6: 0.2023  loss_dice_6: 0.1631  loss_ce_7: 0.1398  loss_mask_7: 0.1943  loss_dice_7: 0.16  loss_ce_8: 0.1396  loss_mask_8: 0.1954  loss_dice_8: 0.1575  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:45] d2.utils.events INFO:  eta: 0:51:25  iter: 25419  total_loss: 4.665  loss_ce: 0.1277  loss_mask: 0.1833  loss_dice: 0.1575  loss_ce_0: 0.06292  loss_mask_0: 0.1776  loss_dice_0: 0.1615  loss_ce_1: 0.1278  loss_mask_1: 0.1818  loss_dice_1: 0.1636  loss_ce_2: 0.1271  loss_mask_2: 0.1798  loss_dice_2: 0.1587  loss_ce_3: 0.1276  loss_mask_3: 0.1779  loss_dice_3: 0.1594  loss_ce_4: 0.1278  loss_mask_4: 0.1772  loss_dice_4: 0.1565  loss_ce_5: 0.1272  loss_mask_5: 0.1697  loss_dice_5: 0.1574  loss_ce_6: 0.1277  loss_mask_6: 0.1834  loss_dice_6: 0.1643  loss_ce_7: 0.1276  loss_mask_7: 0.1731  loss_dice_7: 0.1589  loss_ce_8: 0.1259  loss_mask_8: 0.1851  loss_dice_8: 0.1534  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:47] d2.utils.events INFO:  eta: 0:51:25  iter: 25439  total_loss: 4.64  loss_ce: 0.1284  loss_mask: 0.1797  loss_dice: 0.1491  loss_ce_0: 0.06306  loss_mask_0: 0.1842  loss_dice_0: 0.1567  loss_ce_1: 0.1276  loss_mask_1: 0.1824  loss_dice_1: 0.1621  loss_ce_2: 0.1284  loss_mask_2: 0.1901  loss_dice_2: 0.1546  loss_ce_3: 0.1284  loss_mask_3: 0.1926  loss_dice_3: 0.1552  loss_ce_4: 0.1285  loss_mask_4: 0.1848  loss_dice_4: 0.1622  loss_ce_5: 0.1282  loss_mask_5: 0.1762  loss_dice_5: 0.1649  loss_ce_6: 0.1283  loss_mask_6: 0.1901  loss_dice_6: 0.1558  loss_ce_7: 0.1284  loss_mask_7: 0.1799  loss_dice_7: 0.1569  loss_ce_8: 0.1284  loss_mask_8: 0.1835  loss_dice_8: 0.1603  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:50] d2.utils.events INFO:  eta: 0:51:25  iter: 25459  total_loss: 5.277  loss_ce: 0.1115  loss_mask: 0.1931  loss_dice: 0.143  loss_ce_0: 0.05658  loss_mask_0: 0.2017  loss_dice_0: 0.1497  loss_ce_1: 0.1144  loss_mask_1: 0.1998  loss_dice_1: 0.1485  loss_ce_2: 0.1115  loss_mask_2: 0.2083  loss_dice_2: 0.1482  loss_ce_3: 0.1115  loss_mask_3: 0.1943  loss_dice_3: 0.1489  loss_ce_4: 0.1125  loss_mask_4: 0.2015  loss_dice_4: 0.1564  loss_ce_5: 0.1112  loss_mask_5: 0.1974  loss_dice_5: 0.1489  loss_ce_6: 0.111  loss_mask_6: 0.1934  loss_dice_6: 0.146  loss_ce_7: 0.1117  loss_mask_7: 0.203  loss_dice_7: 0.1548  loss_ce_8: 0.1116  loss_mask_8: 0.2039  loss_dice_8: 0.1498  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:52] d2.utils.events INFO:  eta: 0:51:25  iter: 25479  total_loss: 5.309  loss_ce: 0.1494  loss_mask: 0.2003  loss_dice: 0.1872  loss_ce_0: 0.07119  loss_mask_0: 0.1956  loss_dice_0: 0.1896  loss_ce_1: 0.1479  loss_mask_1: 0.1941  loss_dice_1: 0.1827  loss_ce_2: 0.1501  loss_mask_2: 0.1861  loss_dice_2: 0.1783  loss_ce_3: 0.1505  loss_mask_3: 0.1904  loss_dice_3: 0.1842  loss_ce_4: 0.1509  loss_mask_4: 0.1948  loss_dice_4: 0.1846  loss_ce_5: 0.1502  loss_mask_5: 0.1968  loss_dice_5: 0.1959  loss_ce_6: 0.15  loss_mask_6: 0.1839  loss_dice_6: 0.1883  loss_ce_7: 0.1504  loss_mask_7: 0.1898  loss_dice_7: 0.1806  loss_ce_8: 0.15  loss_mask_8: 0.1895  loss_dice_8: 0.1796  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:55] d2.utils.events INFO:  eta: 0:51:27  iter: 25499  total_loss: 5.045  loss_ce: 0.1279  loss_mask: 0.1715  loss_dice: 0.1909  loss_ce_0: 0.06303  loss_mask_0: 0.1717  loss_dice_0: 0.1905  loss_ce_1: 0.1269  loss_mask_1: 0.1585  loss_dice_1: 0.1815  loss_ce_2: 0.1286  loss_mask_2: 0.1726  loss_dice_2: 0.1824  loss_ce_3: 0.128  loss_mask_3: 0.16  loss_dice_3: 0.1863  loss_ce_4: 0.1278  loss_mask_4: 0.154  loss_dice_4: 0.1825  loss_ce_5: 0.1284  loss_mask_5: 0.1628  loss_dice_5: 0.1821  loss_ce_6: 0.1279  loss_mask_6: 0.162  loss_dice_6: 0.1842  loss_ce_7: 0.1277  loss_mask_7: 0.1637  loss_dice_7: 0.1785  loss_ce_8: 0.129  loss_mask_8: 0.1634  loss_dice_8: 0.19  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:47:57] d2.utils.events INFO:  eta: 0:51:26  iter: 25519  total_loss: 5.172  loss_ce: 0.1276  loss_mask: 0.2106  loss_dice: 0.1446  loss_ce_0: 0.06291  loss_mask_0: 0.2076  loss_dice_0: 0.144  loss_ce_1: 0.126  loss_mask_1: 0.2108  loss_dice_1: 0.1435  loss_ce_2: 0.1276  loss_mask_2: 0.2093  loss_dice_2: 0.144  loss_ce_3: 0.1275  loss_mask_3: 0.2028  loss_dice_3: 0.138  loss_ce_4: 0.1277  loss_mask_4: 0.2077  loss_dice_4: 0.1443  loss_ce_5: 0.1276  loss_mask_5: 0.2072  loss_dice_5: 0.1422  loss_ce_6: 0.1276  loss_mask_6: 0.2055  loss_dice_6: 0.1427  loss_ce_7: 0.1279  loss_mask_7: 0.2181  loss_dice_7: 0.1422  loss_ce_8: 0.1279  loss_mask_8: 0.2095  loss_dice_8: 0.1461  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:00] d2.utils.events INFO:  eta: 0:51:26  iter: 25539  total_loss: 5.786  loss_ce: 0.1162  loss_mask: 0.1833  loss_dice: 0.2609  loss_ce_0: 0.05789  loss_mask_0: 0.1728  loss_dice_0: 0.2342  loss_ce_1: 0.1163  loss_mask_1: 0.1788  loss_dice_1: 0.2383  loss_ce_2: 0.1154  loss_mask_2: 0.1756  loss_dice_2: 0.2456  loss_ce_3: 0.1158  loss_mask_3: 0.1784  loss_dice_3: 0.2459  loss_ce_4: 0.1162  loss_mask_4: 0.1747  loss_dice_4: 0.2566  loss_ce_5: 0.1155  loss_mask_5: 0.1801  loss_dice_5: 0.243  loss_ce_6: 0.1154  loss_mask_6: 0.1751  loss_dice_6: 0.2528  loss_ce_7: 0.1158  loss_mask_7: 0.1813  loss_dice_7: 0.2418  loss_ce_8: 0.1159  loss_mask_8: 0.1842  loss_dice_8: 0.2324  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:03] d2.utils.events INFO:  eta: 0:51:28  iter: 25559  total_loss: 4.894  loss_ce: 0.1102  loss_mask: 0.1839  loss_dice: 0.141  loss_ce_0: 0.05607  loss_mask_0: 0.1876  loss_dice_0: 0.1356  loss_ce_1: 0.1073  loss_mask_1: 0.185  loss_dice_1: 0.1344  loss_ce_2: 0.1096  loss_mask_2: 0.1845  loss_dice_2: 0.137  loss_ce_3: 0.1094  loss_mask_3: 0.1887  loss_dice_3: 0.1395  loss_ce_4: 0.1093  loss_mask_4: 0.1898  loss_dice_4: 0.1387  loss_ce_5: 0.1099  loss_mask_5: 0.1904  loss_dice_5: 0.14  loss_ce_6: 0.11  loss_mask_6: 0.1813  loss_dice_6: 0.1427  loss_ce_7: 0.1097  loss_mask_7: 0.1881  loss_dice_7: 0.1402  loss_ce_8: 0.1098  loss_mask_8: 0.1866  loss_dice_8: 0.1378  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:48:05] d2.utils.events INFO:  eta: 0:51:23  iter: 25579  total_loss: 5.554  loss_ce: 0.1572  loss_mask: 0.241  loss_dice: 0.1428  loss_ce_0: 0.07356  loss_mask_0: 0.2437  loss_dice_0: 0.1443  loss_ce_1: 0.1498  loss_mask_1: 0.2413  loss_dice_1: 0.1434  loss_ce_2: 0.1568  loss_mask_2: 0.2468  loss_dice_2: 0.147  loss_ce_3: 0.1576  loss_mask_3: 0.2405  loss_dice_3: 0.1487  loss_ce_4: 0.1582  loss_mask_4: 0.2349  loss_dice_4: 0.1461  loss_ce_5: 0.1568  loss_mask_5: 0.245  loss_dice_5: 0.1488  loss_ce_6: 0.1573  loss_mask_6: 0.2365  loss_dice_6: 0.1413  loss_ce_7: 0.1573  loss_mask_7: 0.2396  loss_dice_7: 0.1517  loss_ce_8: 0.1571  loss_mask_8: 0.2383  loss_dice_8: 0.1443  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:48:08] d2.utils.events INFO:  eta: 0:51:22  iter: 25599  total_loss: 5.252  loss_ce: 0.1068  loss_mask: 0.163  loss_dice: 0.2146  loss_ce_0: 0.05472  loss_mask_0: 0.1705  loss_dice_0: 0.211  loss_ce_1: 0.1136  loss_mask_1: 0.1657  loss_dice_1: 0.2178  loss_ce_2: 0.107  loss_mask_2: 0.1653  loss_dice_2: 0.2134  loss_ce_3: 0.1071  loss_mask_3: 0.1593  loss_dice_3: 0.219  loss_ce_4: 0.1093  loss_mask_4: 0.1665  loss_dice_4: 0.2066  loss_ce_5: 0.1061  loss_mask_5: 0.1739  loss_dice_5: 0.2149  loss_ce_6: 0.1057  loss_mask_6: 0.1639  loss_dice_6: 0.2124  loss_ce_7: 0.1066  loss_mask_7: 0.1732  loss_dice_7: 0.2063  loss_ce_8: 0.1066  loss_mask_8: 0.1747  loss_dice_8: 0.2226  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:10] d2.utils.events INFO:  eta: 0:51:20  iter: 25619  total_loss: 5.19  loss_ce: 0.1502  loss_mask: 0.1717  loss_dice: 0.1786  loss_ce_0: 0.07199  loss_mask_0: 0.1718  loss_dice_0: 0.1758  loss_ce_1: 0.1492  loss_mask_1: 0.1686  loss_dice_1: 0.1812  loss_ce_2: 0.1502  loss_mask_2: 0.1672  loss_dice_2: 0.174  loss_ce_3: 0.1506  loss_mask_3: 0.1649  loss_dice_3: 0.1792  loss_ce_4: 0.1507  loss_mask_4: 0.1673  loss_dice_4: 0.1793  loss_ce_5: 0.1509  loss_mask_5: 0.1613  loss_dice_5: 0.1788  loss_ce_6: 0.1515  loss_mask_6: 0.1655  loss_dice_6: 0.1734  loss_ce_7: 0.1508  loss_mask_7: 0.1669  loss_dice_7: 0.1763  loss_ce_8: 0.1492  loss_mask_8: 0.1758  loss_dice_8: 0.1855  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:13] d2.utils.events INFO:  eta: 0:51:18  iter: 25639  total_loss: 4.738  loss_ce: 0.1062  loss_mask: 0.1935  loss_dice: 0.133  loss_ce_0: 0.05455  loss_mask_0: 0.1961  loss_dice_0: 0.1394  loss_ce_1: 0.1057  loss_mask_1: 0.1946  loss_dice_1: 0.1412  loss_ce_2: 0.1059  loss_mask_2: 0.1976  loss_dice_2: 0.1399  loss_ce_3: 0.1058  loss_mask_3: 0.1975  loss_dice_3: 0.1368  loss_ce_4: 0.106  loss_mask_4: 0.1974  loss_dice_4: 0.1445  loss_ce_5: 0.1059  loss_mask_5: 0.193  loss_dice_5: 0.1346  loss_ce_6: 0.1057  loss_mask_6: 0.1853  loss_dice_6: 0.1354  loss_ce_7: 0.1058  loss_mask_7: 0.1895  loss_dice_7: 0.1384  loss_ce_8: 0.106  loss_mask_8: 0.1922  loss_dice_8: 0.1315  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:48:15] d2.utils.events INFO:  eta: 0:51:16  iter: 25659  total_loss: 5.384  loss_ce: 0.1302  loss_mask: 0.1755  loss_dice: 0.2375  loss_ce_0: 0.0636  loss_mask_0: 0.1838  loss_dice_0: 0.2344  loss_ce_1: 0.1263  loss_mask_1: 0.1836  loss_dice_1: 0.2184  loss_ce_2: 0.1303  loss_mask_2: 0.1805  loss_dice_2: 0.2236  loss_ce_3: 0.1304  loss_mask_3: 0.1811  loss_dice_3: 0.2316  loss_ce_4: 0.1278  loss_mask_4: 0.1873  loss_dice_4: 0.2298  loss_ce_5: 0.13  loss_mask_5: 0.1819  loss_dice_5: 0.2267  loss_ce_6: 0.1301  loss_mask_6: 0.1709  loss_dice_6: 0.2315  loss_ce_7: 0.1292  loss_mask_7: 0.1817  loss_dice_7: 0.2385  loss_ce_8: 0.1305  loss_mask_8: 0.1846  loss_dice_8: 0.2184  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:48:18] d2.utils.events INFO:  eta: 0:51:13  iter: 25679  total_loss: 5.236  loss_ce: 0.09542  loss_mask: 0.1964  loss_dice: 0.1382  loss_ce_0: 0.05102  loss_mask_0: 0.1945  loss_dice_0: 0.136  loss_ce_1: 0.09521  loss_mask_1: 0.2037  loss_dice_1: 0.1429  loss_ce_2: 0.09579  loss_mask_2: 0.1983  loss_dice_2: 0.1359  loss_ce_3: 0.09539  loss_mask_3: 0.1797  loss_dice_3: 0.1395  loss_ce_4: 0.09519  loss_mask_4: 0.1982  loss_dice_4: 0.1402  loss_ce_5: 0.09566  loss_mask_5: 0.1875  loss_dice_5: 0.1382  loss_ce_6: 0.09532  loss_mask_6: 0.193  loss_dice_6: 0.1381  loss_ce_7: 0.09546  loss_mask_7: 0.196  loss_dice_7: 0.1417  loss_ce_8: 0.09539  loss_mask_8: 0.1935  loss_dice_8: 0.1359  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:48:20] d2.utils.events INFO:  eta: 0:51:10  iter: 25699  total_loss: 4.547  loss_ce: 0.09237  loss_mask: 0.2022  loss_dice: 0.1607  loss_ce_0: 0.04999  loss_mask_0: 0.1996  loss_dice_0: 0.162  loss_ce_1: 0.09163  loss_mask_1: 0.1988  loss_dice_1: 0.1618  loss_ce_2: 0.09259  loss_mask_2: 0.2009  loss_dice_2: 0.1532  loss_ce_3: 0.09223  loss_mask_3: 0.1951  loss_dice_3: 0.1586  loss_ce_4: 0.09183  loss_mask_4: 0.1889  loss_dice_4: 0.1506  loss_ce_5: 0.09278  loss_mask_5: 0.1976  loss_dice_5: 0.1584  loss_ce_6: 0.09245  loss_mask_6: 0.1974  loss_dice_6: 0.1584  loss_ce_7: 0.09255  loss_mask_7: 0.1967  loss_dice_7: 0.1585  loss_ce_8: 0.09244  loss_mask_8: 0.1976  loss_dice_8: 0.164  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:48:23] d2.utils.events INFO:  eta: 0:51:09  iter: 25719  total_loss: 5.158  loss_ce: 0.08715  loss_mask: 0.2164  loss_dice: 0.1239  loss_ce_0: 0.04769  loss_mask_0: 0.2076  loss_dice_0: 0.1251  loss_ce_1: 0.08806  loss_mask_1: 0.2119  loss_dice_1: 0.1238  loss_ce_2: 0.08741  loss_mask_2: 0.2056  loss_dice_2: 0.124  loss_ce_3: 0.08728  loss_mask_3: 0.2066  loss_dice_3: 0.1231  loss_ce_4: 0.08673  loss_mask_4: 0.2028  loss_dice_4: 0.1277  loss_ce_5: 0.08682  loss_mask_5: 0.2013  loss_dice_5: 0.1181  loss_ce_6: 0.08643  loss_mask_6: 0.2039  loss_dice_6: 0.1208  loss_ce_7: 0.0866  loss_mask_7: 0.2021  loss_dice_7: 0.1216  loss_ce_8: 0.08687  loss_mask_8: 0.2018  loss_dice_8: 0.1266  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:26] d2.utils.events INFO:  eta: 0:51:08  iter: 25739  total_loss: 5.13  loss_ce: 0.09124  loss_mask: 0.1873  loss_dice: 0.1948  loss_ce_0: 0.04844  loss_mask_0: 0.19  loss_dice_0: 0.1987  loss_ce_1: 0.09216  loss_mask_1: 0.1813  loss_dice_1: 0.1916  loss_ce_2: 0.09132  loss_mask_2: 0.1876  loss_dice_2: 0.2004  loss_ce_3: 0.09146  loss_mask_3: 0.1805  loss_dice_3: 0.2033  loss_ce_4: 0.09181  loss_mask_4: 0.1835  loss_dice_4: 0.2116  loss_ce_5: 0.09023  loss_mask_5: 0.1837  loss_dice_5: 0.2054  loss_ce_6: 0.09002  loss_mask_6: 0.1879  loss_dice_6: 0.195  loss_ce_7: 0.09079  loss_mask_7: 0.1823  loss_dice_7: 0.196  loss_ce_8: 0.09128  loss_mask_8: 0.1839  loss_dice_8: 0.2017  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:28] d2.utils.events INFO:  eta: 0:51:06  iter: 25759  total_loss: 5.189  loss_ce: 0.08978  loss_mask: 0.2125  loss_dice: 0.1429  loss_ce_0: 0.0481  loss_mask_0: 0.2041  loss_dice_0: 0.1473  loss_ce_1: 0.09298  loss_mask_1: 0.2051  loss_dice_1: 0.145  loss_ce_2: 0.08971  loss_mask_2: 0.2163  loss_dice_2: 0.1498  loss_ce_3: 0.08968  loss_mask_3: 0.2061  loss_dice_3: 0.1535  loss_ce_4: 0.09072  loss_mask_4: 0.2053  loss_dice_4: 0.145  loss_ce_5: 0.08902  loss_mask_5: 0.2011  loss_dice_5: 0.1472  loss_ce_6: 0.08881  loss_mask_6: 0.2115  loss_dice_6: 0.145  loss_ce_7: 0.08919  loss_mask_7: 0.2087  loss_dice_7: 0.1436  loss_ce_8: 0.08968  loss_mask_8: 0.2012  loss_dice_8: 0.1384  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:31] d2.utils.events INFO:  eta: 0:51:02  iter: 25779  total_loss: 4.916  loss_ce: 0.177  loss_mask: 0.1979  loss_dice: 0.1561  loss_ce_0: 0.08199  loss_mask_0: 0.1887  loss_dice_0: 0.1594  loss_ce_1: 0.1723  loss_mask_1: 0.1924  loss_dice_1: 0.1594  loss_ce_2: 0.177  loss_mask_2: 0.199  loss_dice_2: 0.1538  loss_ce_3: 0.1771  loss_mask_3: 0.1935  loss_dice_3: 0.1579  loss_ce_4: 0.1768  loss_mask_4: 0.1957  loss_dice_4: 0.1617  loss_ce_5: 0.1783  loss_mask_5: 0.1952  loss_dice_5: 0.1552  loss_ce_6: 0.1784  loss_mask_6: 0.2013  loss_dice_6: 0.155  loss_ce_7: 0.1774  loss_mask_7: 0.1949  loss_dice_7: 0.155  loss_ce_8: 0.1767  loss_mask_8: 0.1959  loss_dice_8: 0.1573  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:33] d2.utils.events INFO:  eta: 0:50:58  iter: 25799  total_loss: 5.285  loss_ce: 0.1622  loss_mask: 0.1935  loss_dice: 0.1476  loss_ce_0: 0.07757  loss_mask_0: 0.1833  loss_dice_0: 0.1502  loss_ce_1: 0.1472  loss_mask_1: 0.1814  loss_dice_1: 0.1523  loss_ce_2: 0.1634  loss_mask_2: 0.1872  loss_dice_2: 0.146  loss_ce_3: 0.1627  loss_mask_3: 0.1858  loss_dice_3: 0.1465  loss_ce_4: 0.1614  loss_mask_4: 0.1888  loss_dice_4: 0.1537  loss_ce_5: 0.1641  loss_mask_5: 0.1916  loss_dice_5: 0.1515  loss_ce_6: 0.1639  loss_mask_6: 0.1837  loss_dice_6: 0.1497  loss_ce_7: 0.1638  loss_mask_7: 0.1828  loss_dice_7: 0.1477  loss_ce_8: 0.1628  loss_mask_8: 0.1911  loss_dice_8: 0.1495  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:36] d2.utils.events INFO:  eta: 0:50:57  iter: 25819  total_loss: 5.096  loss_ce: 0.1017  loss_mask: 0.21  loss_dice: 0.1356  loss_ce_0: 0.05173  loss_mask_0: 0.2215  loss_dice_0: 0.143  loss_ce_1: 0.1096  loss_mask_1: 0.2134  loss_dice_1: 0.1367  loss_ce_2: 0.1005  loss_mask_2: 0.2203  loss_dice_2: 0.1413  loss_ce_3: 0.1016  loss_mask_3: 0.209  loss_dice_3: 0.1361  loss_ce_4: 0.1035  loss_mask_4: 0.2168  loss_dice_4: 0.1377  loss_ce_5: 0.1003  loss_mask_5: 0.2131  loss_dice_5: 0.1427  loss_ce_6: 0.1008  loss_mask_6: 0.2091  loss_dice_6: 0.1422  loss_ce_7: 0.1016  loss_mask_7: 0.2144  loss_dice_7: 0.1418  loss_ce_8: 0.1014  loss_mask_8: 0.2151  loss_dice_8: 0.1378  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:38] d2.utils.events INFO:  eta: 0:50:54  iter: 25839  total_loss: 5.211  loss_ce: 0.1298  loss_mask: 0.1796  loss_dice: 0.1337  loss_ce_0: 0.06375  loss_mask_0: 0.1896  loss_dice_0: 0.1409  loss_ce_1: 0.1261  loss_mask_1: 0.1945  loss_dice_1: 0.1407  loss_ce_2: 0.1303  loss_mask_2: 0.1913  loss_dice_2: 0.1371  loss_ce_3: 0.1301  loss_mask_3: 0.2098  loss_dice_3: 0.1392  loss_ce_4: 0.1314  loss_mask_4: 0.2073  loss_dice_4: 0.1366  loss_ce_5: 0.13  loss_mask_5: 0.2045  loss_dice_5: 0.1387  loss_ce_6: 0.1298  loss_mask_6: 0.1953  loss_dice_6: 0.1406  loss_ce_7: 0.1305  loss_mask_7: 0.1975  loss_dice_7: 0.1357  loss_ce_8: 0.13  loss_mask_8: 0.1868  loss_dice_8: 0.1385  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:41] d2.utils.events INFO:  eta: 0:50:53  iter: 25859  total_loss: 5.15  loss_ce: 0.0965  loss_mask: 0.1815  loss_dice: 0.1443  loss_ce_0: 0.05067  loss_mask_0: 0.1796  loss_dice_0: 0.149  loss_ce_1: 0.09424  loss_mask_1: 0.1778  loss_dice_1: 0.1506  loss_ce_2: 0.09496  loss_mask_2: 0.1813  loss_dice_2: 0.1488  loss_ce_3: 0.09569  loss_mask_3: 0.1813  loss_dice_3: 0.1515  loss_ce_4: 0.09545  loss_mask_4: 0.1897  loss_dice_4: 0.1526  loss_ce_5: 0.09592  loss_mask_5: 0.1765  loss_dice_5: 0.1478  loss_ce_6: 0.09623  loss_mask_6: 0.1853  loss_dice_6: 0.1542  loss_ce_7: 0.09579  loss_mask_7: 0.1814  loss_dice_7: 0.1472  loss_ce_8: 0.09583  loss_mask_8: 0.1842  loss_dice_8: 0.1499  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:43] d2.utils.events INFO:  eta: 0:50:50  iter: 25879  total_loss: 5.102  loss_ce: 0.1322  loss_mask: 0.1738  loss_dice: 0.1691  loss_ce_0: 0.06436  loss_mask_0: 0.1791  loss_dice_0: 0.176  loss_ce_1: 0.1256  loss_mask_1: 0.1802  loss_dice_1: 0.1849  loss_ce_2: 0.1311  loss_mask_2: 0.1841  loss_dice_2: 0.1753  loss_ce_3: 0.1321  loss_mask_3: 0.1751  loss_dice_3: 0.1763  loss_ce_4: 0.1312  loss_mask_4: 0.1809  loss_dice_4: 0.1854  loss_ce_5: 0.1316  loss_mask_5: 0.1836  loss_dice_5: 0.1804  loss_ce_6: 0.1322  loss_mask_6: 0.1749  loss_dice_6: 0.1778  loss_ce_7: 0.1318  loss_mask_7: 0.1841  loss_dice_7: 0.1759  loss_ce_8: 0.1299  loss_mask_8: 0.1819  loss_dice_8: 0.1773  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:46] d2.utils.events INFO:  eta: 0:50:49  iter: 25899  total_loss: 4.89  loss_ce: 0.08969  loss_mask: 0.1887  loss_dice: 0.1326  loss_ce_0: 0.04888  loss_mask_0: 0.19  loss_dice_0: 0.1281  loss_ce_1: 0.08834  loss_mask_1: 0.1975  loss_dice_1: 0.1319  loss_ce_2: 0.09016  loss_mask_2: 0.1948  loss_dice_2: 0.1313  loss_ce_3: 0.08999  loss_mask_3: 0.1933  loss_dice_3: 0.1326  loss_ce_4: 0.08921  loss_mask_4: 0.1881  loss_dice_4: 0.1366  loss_ce_5: 0.09004  loss_mask_5: 0.197  loss_dice_5: 0.1398  loss_ce_6: 0.08968  loss_mask_6: 0.2008  loss_dice_6: 0.1365  loss_ce_7: 0.08954  loss_mask_7: 0.1954  loss_dice_7: 0.1321  loss_ce_8: 0.08956  loss_mask_8: 0.195  loss_dice_8: 0.1279  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:48] d2.utils.events INFO:  eta: 0:50:47  iter: 25919  total_loss: 5.392  loss_ce: 0.1747  loss_mask: 0.1984  loss_dice: 0.1703  loss_ce_0: 0.08011  loss_mask_0: 0.1991  loss_dice_0: 0.1784  loss_ce_1: 0.1582  loss_mask_1: 0.2079  loss_dice_1: 0.1759  loss_ce_2: 0.172  loss_mask_2: 0.1975  loss_dice_2: 0.1693  loss_ce_3: 0.173  loss_mask_3: 0.1997  loss_dice_3: 0.1708  loss_ce_4: 0.1729  loss_mask_4: 0.1969  loss_dice_4: 0.1681  loss_ce_5: 0.1745  loss_mask_5: 0.2072  loss_dice_5: 0.1698  loss_ce_6: 0.1753  loss_mask_6: 0.2103  loss_dice_6: 0.1727  loss_ce_7: 0.1742  loss_mask_7: 0.2009  loss_dice_7: 0.1732  loss_ce_8: 0.1737  loss_mask_8: 0.2011  loss_dice_8: 0.1719  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:48:51] d2.utils.events INFO:  eta: 0:50:45  iter: 25939  total_loss: 5.473  loss_ce: 0.1566  loss_mask: 0.1708  loss_dice: 0.1802  loss_ce_0: 0.07489  loss_mask_0: 0.1882  loss_dice_0: 0.1809  loss_ce_1: 0.1367  loss_mask_1: 0.1829  loss_dice_1: 0.1789  loss_ce_2: 0.1554  loss_mask_2: 0.1806  loss_dice_2: 0.179  loss_ce_3: 0.1557  loss_mask_3: 0.1761  loss_dice_3: 0.1799  loss_ce_4: 0.1546  loss_mask_4: 0.1722  loss_dice_4: 0.1775  loss_ce_5: 0.1575  loss_mask_5: 0.1827  loss_dice_5: 0.1771  loss_ce_6: 0.1577  loss_mask_6: 0.1771  loss_dice_6: 0.1791  loss_ce_7: 0.1564  loss_mask_7: 0.1891  loss_dice_7: 0.1843  loss_ce_8: 0.1559  loss_mask_8: 0.1785  loss_dice_8: 0.1866  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:48:53] d2.utils.events INFO:  eta: 0:50:42  iter: 25959  total_loss: 4.491  loss_ce: 0.1482  loss_mask: 0.1885  loss_dice: 0.1357  loss_ce_0: 0.07227  loss_mask_0: 0.1902  loss_dice_0: 0.1284  loss_ce_1: 0.1263  loss_mask_1: 0.189  loss_dice_1: 0.1377  loss_ce_2: 0.1542  loss_mask_2: 0.1861  loss_dice_2: 0.1376  loss_ce_3: 0.1513  loss_mask_3: 0.184  loss_dice_3: 0.1379  loss_ce_4: 0.1491  loss_mask_4: 0.1832  loss_dice_4: 0.1334  loss_ce_5: 0.1503  loss_mask_5: 0.1858  loss_dice_5: 0.1329  loss_ce_6: 0.1489  loss_mask_6: 0.1889  loss_dice_6: 0.1336  loss_ce_7: 0.1492  loss_mask_7: 0.1887  loss_dice_7: 0.1406  loss_ce_8: 0.1495  loss_mask_8: 0.1941  loss_dice_8: 0.1375  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:48:56] d2.utils.events INFO:  eta: 0:50:38  iter: 25979  total_loss: 5.192  loss_ce: 0.1346  loss_mask: 0.2236  loss_dice: 0.1543  loss_ce_0: 0.06694  loss_mask_0: 0.2274  loss_dice_0: 0.1542  loss_ce_1: 0.1251  loss_mask_1: 0.2272  loss_dice_1: 0.1609  loss_ce_2: 0.1398  loss_mask_2: 0.2241  loss_dice_2: 0.1528  loss_ce_3: 0.1358  loss_mask_3: 0.2301  loss_dice_3: 0.1547  loss_ce_4: 0.1324  loss_mask_4: 0.2242  loss_dice_4: 0.1577  loss_ce_5: 0.1352  loss_mask_5: 0.2292  loss_dice_5: 0.1649  loss_ce_6: 0.1338  loss_mask_6: 0.2199  loss_dice_6: 0.1546  loss_ce_7: 0.1342  loss_mask_7: 0.2308  loss_dice_7: 0.1563  loss_ce_8: 0.1361  loss_mask_8: 0.2329  loss_dice_8: 0.1547  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:48:58] d2.utils.events INFO:  eta: 0:50:38  iter: 25999  total_loss: 5.468  loss_ce: 0.1234  loss_mask: 0.1879  loss_dice: 0.1949  loss_ce_0: 0.06223  loss_mask_0: 0.1828  loss_dice_0: 0.2077  loss_ce_1: 0.1257  loss_mask_1: 0.1837  loss_dice_1: 0.1964  loss_ce_2: 0.1301  loss_mask_2: 0.2059  loss_dice_2: 0.2061  loss_ce_3: 0.1262  loss_mask_3: 0.1823  loss_dice_3: 0.2028  loss_ce_4: 0.1218  loss_mask_4: 0.1973  loss_dice_4: 0.2042  loss_ce_5: 0.1246  loss_mask_5: 0.1891  loss_dice_5: 0.1968  loss_ce_6: 0.1221  loss_mask_6: 0.1932  loss_dice_6: 0.1995  loss_ce_7: 0.1229  loss_mask_7: 0.1844  loss_dice_7: 0.2015  loss_ce_8: 0.1253  loss_mask_8: 0.1872  loss_dice_8: 0.2021  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:01] d2.utils.events INFO:  eta: 0:50:37  iter: 26019  total_loss: 5.6  loss_ce: 0.1279  loss_mask: 0.1722  loss_dice: 0.2362  loss_ce_0: 0.06283  loss_mask_0: 0.1804  loss_dice_0: 0.2207  loss_ce_1: 0.1283  loss_mask_1: 0.192  loss_dice_1: 0.2193  loss_ce_2: 0.1278  loss_mask_2: 0.1744  loss_dice_2: 0.2418  loss_ce_3: 0.1279  loss_mask_3: 0.1759  loss_dice_3: 0.2362  loss_ce_4: 0.1284  loss_mask_4: 0.1751  loss_dice_4: 0.2222  loss_ce_5: 0.1279  loss_mask_5: 0.1725  loss_dice_5: 0.2181  loss_ce_6: 0.1281  loss_mask_6: 0.1754  loss_dice_6: 0.2413  loss_ce_7: 0.1281  loss_mask_7: 0.1653  loss_dice_7: 0.2283  loss_ce_8: 0.1278  loss_mask_8: 0.1759  loss_dice_8: 0.2252  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:04] d2.utils.events INFO:  eta: 0:50:35  iter: 26039  total_loss: 5.401  loss_ce: 0.1133  loss_mask: 0.2061  loss_dice: 0.1663  loss_ce_0: 0.05902  loss_mask_0: 0.2115  loss_dice_0: 0.1667  loss_ce_1: 0.1131  loss_mask_1: 0.2101  loss_dice_1: 0.1656  loss_ce_2: 0.1193  loss_mask_2: 0.2092  loss_dice_2: 0.168  loss_ce_3: 0.1138  loss_mask_3: 0.2059  loss_dice_3: 0.1734  loss_ce_4: 0.1113  loss_mask_4: 0.2093  loss_dice_4: 0.1699  loss_ce_5: 0.1151  loss_mask_5: 0.2048  loss_dice_5: 0.1712  loss_ce_6: 0.1121  loss_mask_6: 0.2123  loss_dice_6: 0.1691  loss_ce_7: 0.1138  loss_mask_7: 0.2143  loss_dice_7: 0.1658  loss_ce_8: 0.1152  loss_mask_8: 0.2046  loss_dice_8: 0.1734  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:06] d2.utils.events INFO:  eta: 0:50:34  iter: 26059  total_loss: 5.789  loss_ce: 0.1077  loss_mask: 0.1877  loss_dice: 0.2294  loss_ce_0: 0.05684  loss_mask_0: 0.1933  loss_dice_0: 0.2273  loss_ce_1: 0.111  loss_mask_1: 0.1884  loss_dice_1: 0.2256  loss_ce_2: 0.1104  loss_mask_2: 0.1808  loss_dice_2: 0.2335  loss_ce_3: 0.1074  loss_mask_3: 0.1912  loss_dice_3: 0.2182  loss_ce_4: 0.1059  loss_mask_4: 0.1876  loss_dice_4: 0.2233  loss_ce_5: 0.1091  loss_mask_5: 0.1815  loss_dice_5: 0.2196  loss_ce_6: 0.1065  loss_mask_6: 0.1905  loss_dice_6: 0.2276  loss_ce_7: 0.1076  loss_mask_7: 0.1819  loss_dice_7: 0.221  loss_ce_8: 0.1093  loss_mask_8: 0.1859  loss_dice_8: 0.2141  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:09] d2.utils.events INFO:  eta: 0:50:29  iter: 26079  total_loss: 5.933  loss_ce: 0.1087  loss_mask: 0.1985  loss_dice: 0.2004  loss_ce_0: 0.05651  loss_mask_0: 0.196  loss_dice_0: 0.2194  loss_ce_1: 0.1231  loss_mask_1: 0.1949  loss_dice_1: 0.2118  loss_ce_2: 0.1107  loss_mask_2: 0.195  loss_dice_2: 0.2242  loss_ce_3: 0.109  loss_mask_3: 0.2028  loss_dice_3: 0.2025  loss_ce_4: 0.1095  loss_mask_4: 0.1987  loss_dice_4: 0.2143  loss_ce_5: 0.1103  loss_mask_5: 0.203  loss_dice_5: 0.2036  loss_ce_6: 0.1089  loss_mask_6: 0.193  loss_dice_6: 0.2115  loss_ce_7: 0.1101  loss_mask_7: 0.2071  loss_dice_7: 0.2121  loss_ce_8: 0.1094  loss_mask_8: 0.2019  loss_dice_8: 0.2106  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:49:11] d2.utils.events INFO:  eta: 0:50:26  iter: 26099  total_loss: 4.925  loss_ce: 0.1114  loss_mask: 0.1969  loss_dice: 0.1325  loss_ce_0: 0.05674  loss_mask_0: 0.1978  loss_dice_0: 0.1389  loss_ce_1: 0.1212  loss_mask_1: 0.1895  loss_dice_1: 0.1387  loss_ce_2: 0.1112  loss_mask_2: 0.1913  loss_dice_2: 0.136  loss_ce_3: 0.1106  loss_mask_3: 0.1924  loss_dice_3: 0.1422  loss_ce_4: 0.1116  loss_mask_4: 0.1952  loss_dice_4: 0.14  loss_ce_5: 0.1119  loss_mask_5: 0.1919  loss_dice_5: 0.1313  loss_ce_6: 0.1114  loss_mask_6: 0.1964  loss_dice_6: 0.1402  loss_ce_7: 0.1124  loss_mask_7: 0.1929  loss_dice_7: 0.1318  loss_ce_8: 0.112  loss_mask_8: 0.193  loss_dice_8: 0.1345  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:49:14] d2.utils.events INFO:  eta: 0:50:21  iter: 26119  total_loss: 5.223  loss_ce: 0.137  loss_mask: 0.2261  loss_dice: 0.1663  loss_ce_0: 0.06681  loss_mask_0: 0.2194  loss_dice_0: 0.1634  loss_ce_1: 0.1314  loss_mask_1: 0.217  loss_dice_1: 0.1628  loss_ce_2: 0.1387  loss_mask_2: 0.2148  loss_dice_2: 0.1727  loss_ce_3: 0.1376  loss_mask_3: 0.2149  loss_dice_3: 0.1654  loss_ce_4: 0.1353  loss_mask_4: 0.219  loss_dice_4: 0.1737  loss_ce_5: 0.1361  loss_mask_5: 0.215  loss_dice_5: 0.1652  loss_ce_6: 0.1358  loss_mask_6: 0.227  loss_dice_6: 0.1636  loss_ce_7: 0.1351  loss_mask_7: 0.2268  loss_dice_7: 0.1755  loss_ce_8: 0.1367  loss_mask_8: 0.2243  loss_dice_8: 0.1733  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:16] d2.utils.events INFO:  eta: 0:50:18  iter: 26139  total_loss: 5.381  loss_ce: 0.1287  loss_mask: 0.1895  loss_dice: 0.2206  loss_ce_0: 0.06234  loss_mask_0: 0.182  loss_dice_0: 0.2173  loss_ce_1: 0.1231  loss_mask_1: 0.1824  loss_dice_1: 0.215  loss_ce_2: 0.1257  loss_mask_2: 0.1894  loss_dice_2: 0.22  loss_ce_3: 0.128  loss_mask_3: 0.1875  loss_dice_3: 0.2232  loss_ce_4: 0.128  loss_mask_4: 0.1871  loss_dice_4: 0.2156  loss_ce_5: 0.1284  loss_mask_5: 0.1955  loss_dice_5: 0.2359  loss_ce_6: 0.1285  loss_mask_6: 0.1871  loss_dice_6: 0.2229  loss_ce_7: 0.1284  loss_mask_7: 0.1943  loss_dice_7: 0.2199  loss_ce_8: 0.1282  loss_mask_8: 0.1843  loss_dice_8: 0.2212  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:19] d2.utils.events INFO:  eta: 0:50:18  iter: 26159  total_loss: 5.232  loss_ce: 0.1293  loss_mask: 0.1814  loss_dice: 0.1565  loss_ce_0: 0.06448  loss_mask_0: 0.1864  loss_dice_0: 0.1574  loss_ce_1: 0.1279  loss_mask_1: 0.1871  loss_dice_1: 0.1598  loss_ce_2: 0.1318  loss_mask_2: 0.1852  loss_dice_2: 0.159  loss_ce_3: 0.1305  loss_mask_3: 0.1832  loss_dice_3: 0.156  loss_ce_4: 0.1294  loss_mask_4: 0.1905  loss_dice_4: 0.1571  loss_ce_5: 0.1305  loss_mask_5: 0.1805  loss_dice_5: 0.1615  loss_ce_6: 0.1293  loss_mask_6: 0.1836  loss_dice_6: 0.1521  loss_ce_7: 0.1298  loss_mask_7: 0.1825  loss_dice_7: 0.16  loss_ce_8: 0.1303  loss_mask_8: 0.1812  loss_dice_8: 0.1517  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:49:21] d2.utils.events INFO:  eta: 0:50:16  iter: 26179  total_loss: 5.358  loss_ce: 0.1273  loss_mask: 0.1893  loss_dice: 0.2091  loss_ce_0: 0.06278  loss_mask_0: 0.1872  loss_dice_0: 0.2112  loss_ce_1: 0.1237  loss_mask_1: 0.1832  loss_dice_1: 0.2107  loss_ce_2: 0.1272  loss_mask_2: 0.185  loss_dice_2: 0.2032  loss_ce_3: 0.1271  loss_mask_3: 0.1925  loss_dice_3: 0.2145  loss_ce_4: 0.1273  loss_mask_4: 0.1908  loss_dice_4: 0.2184  loss_ce_5: 0.1274  loss_mask_5: 0.1796  loss_dice_5: 0.211  loss_ce_6: 0.127  loss_mask_6: 0.1973  loss_dice_6: 0.2153  loss_ce_7: 0.1271  loss_mask_7: 0.175  loss_dice_7: 0.205  loss_ce_8: 0.1272  loss_mask_8: 0.1862  loss_dice_8: 0.2194  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:49:24] d2.utils.events INFO:  eta: 0:50:13  iter: 26199  total_loss: 5.052  loss_ce: 0.1329  loss_mask: 0.1972  loss_dice: 0.1581  loss_ce_0: 0.06417  loss_mask_0: 0.202  loss_dice_0: 0.1447  loss_ce_1: 0.1174  loss_mask_1: 0.199  loss_dice_1: 0.1477  loss_ce_2: 0.1301  loss_mask_2: 0.2047  loss_dice_2: 0.1491  loss_ce_3: 0.1325  loss_mask_3: 0.1998  loss_dice_3: 0.1464  loss_ce_4: 0.1315  loss_mask_4: 0.1928  loss_dice_4: 0.1449  loss_ce_5: 0.1325  loss_mask_5: 0.1898  loss_dice_5: 0.1473  loss_ce_6: 0.1339  loss_mask_6: 0.1935  loss_dice_6: 0.1453  loss_ce_7: 0.133  loss_mask_7: 0.1948  loss_dice_7: 0.1414  loss_ce_8: 0.1322  loss_mask_8: 0.1927  loss_dice_8: 0.1567  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:26] d2.utils.events INFO:  eta: 0:50:09  iter: 26219  total_loss: 5.694  loss_ce: 0.1328  loss_mask: 0.2319  loss_dice: 0.1861  loss_ce_0: 0.06403  loss_mask_0: 0.2189  loss_dice_0: 0.1957  loss_ce_1: 0.1275  loss_mask_1: 0.2256  loss_dice_1: 0.1891  loss_ce_2: 0.1298  loss_mask_2: 0.2279  loss_dice_2: 0.1926  loss_ce_3: 0.1323  loss_mask_3: 0.2189  loss_dice_3: 0.1915  loss_ce_4: 0.1316  loss_mask_4: 0.2275  loss_dice_4: 0.1855  loss_ce_5: 0.1317  loss_mask_5: 0.2221  loss_dice_5: 0.1947  loss_ce_6: 0.1326  loss_mask_6: 0.2178  loss_dice_6: 0.1872  loss_ce_7: 0.1323  loss_mask_7: 0.2191  loss_dice_7: 0.1877  loss_ce_8: 0.1322  loss_mask_8: 0.2256  loss_dice_8: 0.1865  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:29] d2.utils.events INFO:  eta: 0:50:04  iter: 26239  total_loss: 5.181  loss_ce: 0.1274  loss_mask: 0.2071  loss_dice: 0.1657  loss_ce_0: 0.06271  loss_mask_0: 0.2045  loss_dice_0: 0.1641  loss_ce_1: 0.1396  loss_mask_1: 0.2081  loss_dice_1: 0.1678  loss_ce_2: 0.1272  loss_mask_2: 0.2015  loss_dice_2: 0.1666  loss_ce_3: 0.1276  loss_mask_3: 0.2035  loss_dice_3: 0.1625  loss_ce_4: 0.1282  loss_mask_4: 0.2005  loss_dice_4: 0.1632  loss_ce_5: 0.1275  loss_mask_5: 0.2088  loss_dice_5: 0.1656  loss_ce_6: 0.1274  loss_mask_6: 0.212  loss_dice_6: 0.1648  loss_ce_7: 0.1277  loss_mask_7: 0.2006  loss_dice_7: 0.1709  loss_ce_8: 0.1266  loss_mask_8: 0.2046  loss_dice_8: 0.1627  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:31] d2.utils.events INFO:  eta: 0:50:00  iter: 26259  total_loss: 5.305  loss_ce: 0.1302  loss_mask: 0.1427  loss_dice: 0.2199  loss_ce_0: 0.06349  loss_mask_0: 0.1353  loss_dice_0: 0.2145  loss_ce_1: 0.1275  loss_mask_1: 0.1297  loss_dice_1: 0.2202  loss_ce_2: 0.1292  loss_mask_2: 0.1325  loss_dice_2: 0.2228  loss_ce_3: 0.1296  loss_mask_3: 0.1306  loss_dice_3: 0.2114  loss_ce_4: 0.1304  loss_mask_4: 0.1359  loss_dice_4: 0.2306  loss_ce_5: 0.1299  loss_mask_5: 0.1317  loss_dice_5: 0.2215  loss_ce_6: 0.1304  loss_mask_6: 0.1304  loss_dice_6: 0.2277  loss_ce_7: 0.1307  loss_mask_7: 0.1282  loss_dice_7: 0.2185  loss_ce_8: 0.1303  loss_mask_8: 0.1399  loss_dice_8: 0.2282  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:34] d2.utils.events INFO:  eta: 0:49:56  iter: 26279  total_loss: 5.484  loss_ce: 0.12  loss_mask: 0.2236  loss_dice: 0.1661  loss_ce_0: 0.06017  loss_mask_0: 0.2409  loss_dice_0: 0.1631  loss_ce_1: 0.1219  loss_mask_1: 0.2262  loss_dice_1: 0.1631  loss_ce_2: 0.1222  loss_mask_2: 0.2322  loss_dice_2: 0.1655  loss_ce_3: 0.1215  loss_mask_3: 0.2341  loss_dice_3: 0.1623  loss_ce_4: 0.1197  loss_mask_4: 0.2343  loss_dice_4: 0.1591  loss_ce_5: 0.1199  loss_mask_5: 0.2454  loss_dice_5: 0.1618  loss_ce_6: 0.119  loss_mask_6: 0.2262  loss_dice_6: 0.1642  loss_ce_7: 0.1188  loss_mask_7: 0.2287  loss_dice_7: 0.1643  loss_ce_8: 0.1198  loss_mask_8: 0.2311  loss_dice_8: 0.163  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:36] d2.utils.events INFO:  eta: 0:49:55  iter: 26299  total_loss: 4.805  loss_ce: 0.1138  loss_mask: 0.2235  loss_dice: 0.118  loss_ce_0: 0.0582  loss_mask_0: 0.222  loss_dice_0: 0.1207  loss_ce_1: 0.1171  loss_mask_1: 0.2277  loss_dice_1: 0.1257  loss_ce_2: 0.1174  loss_mask_2: 0.2144  loss_dice_2: 0.1203  loss_ce_3: 0.1157  loss_mask_3: 0.2215  loss_dice_3: 0.1239  loss_ce_4: 0.1132  loss_mask_4: 0.2289  loss_dice_4: 0.1243  loss_ce_5: 0.1138  loss_mask_5: 0.2273  loss_dice_5: 0.1265  loss_ce_6: 0.1121  loss_mask_6: 0.2238  loss_dice_6: 0.12  loss_ce_7: 0.1121  loss_mask_7: 0.2322  loss_dice_7: 0.1258  loss_ce_8: 0.1135  loss_mask_8: 0.2232  loss_dice_8: 0.1193  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:39] d2.utils.events INFO:  eta: 0:49:55  iter: 26319  total_loss: 5.442  loss_ce: 0.1066  loss_mask: 0.2072  loss_dice: 0.1838  loss_ce_0: 0.05541  loss_mask_0: 0.2146  loss_dice_0: 0.1908  loss_ce_1: 0.1246  loss_mask_1: 0.2073  loss_dice_1: 0.1908  loss_ce_2: 0.1107  loss_mask_2: 0.2123  loss_dice_2: 0.1925  loss_ce_3: 0.1081  loss_mask_3: 0.2075  loss_dice_3: 0.1944  loss_ce_4: 0.1062  loss_mask_4: 0.1918  loss_dice_4: 0.1975  loss_ce_5: 0.1073  loss_mask_5: 0.2124  loss_dice_5: 0.194  loss_ce_6: 0.1056  loss_mask_6: 0.2209  loss_dice_6: 0.1916  loss_ce_7: 0.1066  loss_mask_7: 0.1978  loss_dice_7: 0.19  loss_ce_8: 0.1078  loss_mask_8: 0.2028  loss_dice_8: 0.1958  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:49:41] d2.utils.events INFO:  eta: 0:49:54  iter: 26339  total_loss: 5.206  loss_ce: 0.1422  loss_mask: 0.214  loss_dice: 0.1578  loss_ce_0: 0.06744  loss_mask_0: 0.2012  loss_dice_0: 0.151  loss_ce_1: 0.1326  loss_mask_1: 0.2206  loss_dice_1: 0.1633  loss_ce_2: 0.1363  loss_mask_2: 0.2079  loss_dice_2: 0.1539  loss_ce_3: 0.14  loss_mask_3: 0.2118  loss_dice_3: 0.1661  loss_ce_4: 0.1395  loss_mask_4: 0.199  loss_dice_4: 0.1608  loss_ce_5: 0.139  loss_mask_5: 0.2123  loss_dice_5: 0.1604  loss_ce_6: 0.1407  loss_mask_6: 0.2136  loss_dice_6: 0.1526  loss_ce_7: 0.139  loss_mask_7: 0.2003  loss_dice_7: 0.1572  loss_ce_8: 0.1405  loss_mask_8: 0.2222  loss_dice_8: 0.1552  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:49:44] d2.utils.events INFO:  eta: 0:49:53  iter: 26359  total_loss: 6.703  loss_ce: 0.1264  loss_mask: 0.1758  loss_dice: 0.3434  loss_ce_0: 0.06227  loss_mask_0: 0.1698  loss_dice_0: 0.3479  loss_ce_1: 0.13  loss_mask_1: 0.1712  loss_dice_1: 0.3569  loss_ce_2: 0.1265  loss_mask_2: 0.1688  loss_dice_2: 0.3349  loss_ce_3: 0.1254  loss_mask_3: 0.1691  loss_dice_3: 0.3481  loss_ce_4: 0.1286  loss_mask_4: 0.1768  loss_dice_4: 0.3531  loss_ce_5: 0.1284  loss_mask_5: 0.1689  loss_dice_5: 0.3304  loss_ce_6: 0.1284  loss_mask_6: 0.1779  loss_dice_6: 0.3582  loss_ce_7: 0.1274  loss_mask_7: 0.1724  loss_dice_7: 0.3319  loss_ce_8: 0.1279  loss_mask_8: 0.1591  loss_dice_8: 0.3483  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:49:47] d2.utils.events INFO:  eta: 0:49:52  iter: 26379  total_loss: 4.883  loss_ce: 0.1264  loss_mask: 0.1883  loss_dice: 0.1684  loss_ce_0: 0.06216  loss_mask_0: 0.1947  loss_dice_0: 0.1714  loss_ce_1: 0.1288  loss_mask_1: 0.196  loss_dice_1: 0.1653  loss_ce_2: 0.1257  loss_mask_2: 0.1926  loss_dice_2: 0.1648  loss_ce_3: 0.1247  loss_mask_3: 0.1904  loss_dice_3: 0.1615  loss_ce_4: 0.1282  loss_mask_4: 0.1893  loss_dice_4: 0.166  loss_ce_5: 0.128  loss_mask_5: 0.1981  loss_dice_5: 0.1731  loss_ce_6: 0.1286  loss_mask_6: 0.1933  loss_dice_6: 0.1644  loss_ce_7: 0.1293  loss_mask_7: 0.1923  loss_dice_7: 0.1704  loss_ce_8: 0.1275  loss_mask_8: 0.1895  loss_dice_8: 0.1655  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:49:49] d2.utils.events INFO:  eta: 0:49:49  iter: 26399  total_loss: 5.115  loss_ce: 0.1227  loss_mask: 0.1596  loss_dice: 0.162  loss_ce_0: 0.06027  loss_mask_0: 0.1578  loss_dice_0: 0.163  loss_ce_1: 0.1186  loss_mask_1: 0.1597  loss_dice_1: 0.1624  loss_ce_2: 0.1187  loss_mask_2: 0.1429  loss_dice_2: 0.1656  loss_ce_3: 0.1205  loss_mask_3: 0.1403  loss_dice_3: 0.1648  loss_ce_4: 0.1211  loss_mask_4: 0.1621  loss_dice_4: 0.1663  loss_ce_5: 0.1221  loss_mask_5: 0.1546  loss_dice_5: 0.1632  loss_ce_6: 0.1228  loss_mask_6: 0.1482  loss_dice_6: 0.1622  loss_ce_7: 0.1223  loss_mask_7: 0.1497  loss_dice_7: 0.1668  loss_ce_8: 0.1216  loss_mask_8: 0.1492  loss_dice_8: 0.1569  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:51] d2.utils.events INFO:  eta: 0:49:44  iter: 26419  total_loss: 4.929  loss_ce: 0.1398  loss_mask: 0.2255  loss_dice: 0.1455  loss_ce_0: 0.06809  loss_mask_0: 0.2312  loss_dice_0: 0.1369  loss_ce_1: 0.1267  loss_mask_1: 0.2227  loss_dice_1: 0.1426  loss_ce_2: 0.1423  loss_mask_2: 0.2193  loss_dice_2: 0.1423  loss_ce_3: 0.1407  loss_mask_3: 0.2164  loss_dice_3: 0.1443  loss_ce_4: 0.1411  loss_mask_4: 0.2182  loss_dice_4: 0.1429  loss_ce_5: 0.1414  loss_mask_5: 0.225  loss_dice_5: 0.1392  loss_ce_6: 0.1415  loss_mask_6: 0.2189  loss_dice_6: 0.1464  loss_ce_7: 0.142  loss_mask_7: 0.2257  loss_dice_7: 0.1413  loss_ce_8: 0.1415  loss_mask_8: 0.2295  loss_dice_8: 0.1476  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:54] d2.utils.events INFO:  eta: 0:49:40  iter: 26439  total_loss: 5.042  loss_ce: 0.1156  loss_mask: 0.1938  loss_dice: 0.1979  loss_ce_0: 0.05842  loss_mask_0: 0.1954  loss_dice_0: 0.1954  loss_ce_1: 0.1238  loss_mask_1: 0.1987  loss_dice_1: 0.1941  loss_ce_2: 0.1177  loss_mask_2: 0.1952  loss_dice_2: 0.1912  loss_ce_3: 0.1173  loss_mask_3: 0.1995  loss_dice_3: 0.1896  loss_ce_4: 0.1172  loss_mask_4: 0.1882  loss_dice_4: 0.1991  loss_ce_5: 0.1161  loss_mask_5: 0.2072  loss_dice_5: 0.1903  loss_ce_6: 0.1157  loss_mask_6: 0.1998  loss_dice_6: 0.2002  loss_ce_7: 0.1162  loss_mask_7: 0.2015  loss_dice_7: 0.1911  loss_ce_8: 0.1159  loss_mask_8: 0.2014  loss_dice_8: 0.1947  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:56] d2.utils.events INFO:  eta: 0:49:36  iter: 26459  total_loss: 4.456  loss_ce: 0.1349  loss_mask: 0.1822  loss_dice: 0.1172  loss_ce_0: 0.06575  loss_mask_0: 0.1785  loss_dice_0: 0.1147  loss_ce_1: 0.1309  loss_mask_1: 0.183  loss_dice_1: 0.1184  loss_ce_2: 0.1333  loss_mask_2: 0.1838  loss_dice_2: 0.124  loss_ce_3: 0.1341  loss_mask_3: 0.1779  loss_dice_3: 0.1206  loss_ce_4: 0.1331  loss_mask_4: 0.1776  loss_dice_4: 0.1156  loss_ce_5: 0.1343  loss_mask_5: 0.1812  loss_dice_5: 0.1184  loss_ce_6: 0.1341  loss_mask_6: 0.1774  loss_dice_6: 0.1189  loss_ce_7: 0.1334  loss_mask_7: 0.1839  loss_dice_7: 0.1192  loss_ce_8: 0.1342  loss_mask_8: 0.1812  loss_dice_8: 0.1195  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:49:59] d2.utils.events INFO:  eta: 0:49:33  iter: 26479  total_loss: 4.899  loss_ce: 0.1334  loss_mask: 0.1521  loss_dice: 0.1475  loss_ce_0: 0.06545  loss_mask_0: 0.1567  loss_dice_0: 0.1437  loss_ce_1: 0.1244  loss_mask_1: 0.149  loss_dice_1: 0.1449  loss_ce_2: 0.1351  loss_mask_2: 0.1513  loss_dice_2: 0.1486  loss_ce_3: 0.1345  loss_mask_3: 0.1665  loss_dice_3: 0.1464  loss_ce_4: 0.1331  loss_mask_4: 0.1601  loss_dice_4: 0.1447  loss_ce_5: 0.1334  loss_mask_5: 0.1556  loss_dice_5: 0.1394  loss_ce_6: 0.1325  loss_mask_6: 0.151  loss_dice_6: 0.1447  loss_ce_7: 0.1325  loss_mask_7: 0.1509  loss_dice_7: 0.1449  loss_ce_8: 0.1336  loss_mask_8: 0.1497  loss_dice_8: 0.147  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:02] d2.utils.events INFO:  eta: 0:49:29  iter: 26499  total_loss: 5.168  loss_ce: 0.1233  loss_mask: 0.227  loss_dice: 0.1396  loss_ce_0: 0.06085  loss_mask_0: 0.2154  loss_dice_0: 0.1444  loss_ce_1: 0.1275  loss_mask_1: 0.2267  loss_dice_1: 0.1401  loss_ce_2: 0.1214  loss_mask_2: 0.2191  loss_dice_2: 0.1387  loss_ce_3: 0.1228  loss_mask_3: 0.226  loss_dice_3: 0.1426  loss_ce_4: 0.124  loss_mask_4: 0.2224  loss_dice_4: 0.1415  loss_ce_5: 0.1236  loss_mask_5: 0.2274  loss_dice_5: 0.1447  loss_ce_6: 0.1242  loss_mask_6: 0.2217  loss_dice_6: 0.1425  loss_ce_7: 0.124  loss_mask_7: 0.2256  loss_dice_7: 0.1403  loss_ce_8: 0.1229  loss_mask_8: 0.2142  loss_dice_8: 0.1436  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:04] d2.utils.events INFO:  eta: 0:49:26  iter: 26519  total_loss: 4.67  loss_ce: 0.123  loss_mask: 0.1954  loss_dice: 0.13  loss_ce_0: 0.06069  loss_mask_0: 0.2065  loss_dice_0: 0.1325  loss_ce_1: 0.1173  loss_mask_1: 0.1994  loss_dice_1: 0.1321  loss_ce_2: 0.1195  loss_mask_2: 0.1955  loss_dice_2: 0.1275  loss_ce_3: 0.1218  loss_mask_3: 0.2035  loss_dice_3: 0.1272  loss_ce_4: 0.1231  loss_mask_4: 0.2016  loss_dice_4: 0.1263  loss_ce_5: 0.1229  loss_mask_5: 0.2001  loss_dice_5: 0.1254  loss_ce_6: 0.1237  loss_mask_6: 0.1947  loss_dice_6: 0.1268  loss_ce_7: 0.1234  loss_mask_7: 0.199  loss_dice_7: 0.1312  loss_ce_8: 0.1222  loss_mask_8: 0.2046  loss_dice_8: 0.1294  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:07] d2.utils.events INFO:  eta: 0:49:23  iter: 26539  total_loss: 5.088  loss_ce: 0.1319  loss_mask: 0.1636  loss_dice: 0.1516  loss_ce_0: 0.06478  loss_mask_0: 0.1622  loss_dice_0: 0.1436  loss_ce_1: 0.1268  loss_mask_1: 0.1531  loss_dice_1: 0.1421  loss_ce_2: 0.1321  loss_mask_2: 0.1613  loss_dice_2: 0.1457  loss_ce_3: 0.1314  loss_mask_3: 0.1635  loss_dice_3: 0.1502  loss_ce_4: 0.1308  loss_mask_4: 0.1535  loss_dice_4: 0.1453  loss_ce_5: 0.132  loss_mask_5: 0.1601  loss_dice_5: 0.1452  loss_ce_6: 0.1315  loss_mask_6: 0.1646  loss_dice_6: 0.1472  loss_ce_7: 0.1315  loss_mask_7: 0.1603  loss_dice_7: 0.1473  loss_ce_8: 0.1319  loss_mask_8: 0.1571  loss_dice_8: 0.1495  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:09] d2.utils.events INFO:  eta: 0:49:19  iter: 26559  total_loss: 5.306  loss_ce: 0.1266  loss_mask: 0.2208  loss_dice: 0.1542  loss_ce_0: 0.06223  loss_mask_0: 0.2266  loss_dice_0: 0.152  loss_ce_1: 0.1128  loss_mask_1: 0.2332  loss_dice_1: 0.1529  loss_ce_2: 0.1252  loss_mask_2: 0.2261  loss_dice_2: 0.154  loss_ce_3: 0.1263  loss_mask_3: 0.2223  loss_dice_3: 0.1573  loss_ce_4: 0.1272  loss_mask_4: 0.2273  loss_dice_4: 0.1576  loss_ce_5: 0.1267  loss_mask_5: 0.2307  loss_dice_5: 0.1525  loss_ce_6: 0.1275  loss_mask_6: 0.2167  loss_dice_6: 0.1574  loss_ce_7: 0.1273  loss_mask_7: 0.2163  loss_dice_7: 0.151  loss_ce_8: 0.1264  loss_mask_8: 0.2166  loss_dice_8: 0.1531  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:12] d2.utils.events INFO:  eta: 0:49:16  iter: 26579  total_loss: 5.243  loss_ce: 0.1354  loss_mask: 0.1967  loss_dice: 0.1355  loss_ce_0: 0.06706  loss_mask_0: 0.1974  loss_dice_0: 0.1395  loss_ce_1: 0.1227  loss_mask_1: 0.1974  loss_dice_1: 0.1369  loss_ce_2: 0.138  loss_mask_2: 0.1902  loss_dice_2: 0.1381  loss_ce_3: 0.138  loss_mask_3: 0.2013  loss_dice_3: 0.1457  loss_ce_4: 0.1382  loss_mask_4: 0.1905  loss_dice_4: 0.1399  loss_ce_5: 0.1377  loss_mask_5: 0.1943  loss_dice_5: 0.1452  loss_ce_6: 0.1359  loss_mask_6: 0.1987  loss_dice_6: 0.1453  loss_ce_7: 0.1364  loss_mask_7: 0.1965  loss_dice_7: 0.1392  loss_ce_8: 0.1365  loss_mask_8: 0.1889  loss_dice_8: 0.1408  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:14] d2.utils.events INFO:  eta: 0:49:14  iter: 26599  total_loss: 5.969  loss_ce: 0.1139  loss_mask: 0.2854  loss_dice: 0.1689  loss_ce_0: 0.05851  loss_mask_0: 0.2896  loss_dice_0: 0.1642  loss_ce_1: 0.1159  loss_mask_1: 0.2826  loss_dice_1: 0.161  loss_ce_2: 0.1193  loss_mask_2: 0.2921  loss_dice_2: 0.1628  loss_ce_3: 0.1187  loss_mask_3: 0.2802  loss_dice_3: 0.1597  loss_ce_4: 0.1173  loss_mask_4: 0.278  loss_dice_4: 0.1644  loss_ce_5: 0.1152  loss_mask_5: 0.2706  loss_dice_5: 0.1618  loss_ce_6: 0.1139  loss_mask_6: 0.2886  loss_dice_6: 0.1645  loss_ce_7: 0.1142  loss_mask_7: 0.29  loss_dice_7: 0.1656  loss_ce_8: 0.1143  loss_mask_8: 0.2721  loss_dice_8: 0.1607  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:17] d2.utils.events INFO:  eta: 0:49:11  iter: 26619  total_loss: 4.995  loss_ce: 0.1466  loss_mask: 0.2023  loss_dice: 0.1474  loss_ce_0: 0.07001  loss_mask_0: 0.2133  loss_dice_0: 0.1492  loss_ce_1: 0.138  loss_mask_1: 0.2056  loss_dice_1: 0.1467  loss_ce_2: 0.1451  loss_mask_2: 0.2084  loss_dice_2: 0.1478  loss_ce_3: 0.1462  loss_mask_3: 0.206  loss_dice_3: 0.1546  loss_ce_4: 0.1476  loss_mask_4: 0.2077  loss_dice_4: 0.1459  loss_ce_5: 0.1475  loss_mask_5: 0.2021  loss_dice_5: 0.1466  loss_ce_6: 0.1479  loss_mask_6: 0.1886  loss_dice_6: 0.1466  loss_ce_7: 0.1476  loss_mask_7: 0.2  loss_dice_7: 0.1442  loss_ce_8: 0.1467  loss_mask_8: 0.2052  loss_dice_8: 0.1477  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:19] d2.utils.events INFO:  eta: 0:49:08  iter: 26639  total_loss: 4.891  loss_ce: 0.1118  loss_mask: 0.1927  loss_dice: 0.1528  loss_ce_0: 0.05708  loss_mask_0: 0.1866  loss_dice_0: 0.1491  loss_ce_1: 0.1104  loss_mask_1: 0.1872  loss_dice_1: 0.1513  loss_ce_2: 0.1137  loss_mask_2: 0.1832  loss_dice_2: 0.1556  loss_ce_3: 0.1129  loss_mask_3: 0.1881  loss_dice_3: 0.1515  loss_ce_4: 0.1119  loss_mask_4: 0.189  loss_dice_4: 0.1555  loss_ce_5: 0.1123  loss_mask_5: 0.1862  loss_dice_5: 0.1503  loss_ce_6: 0.1118  loss_mask_6: 0.1809  loss_dice_6: 0.1395  loss_ce_7: 0.1119  loss_mask_7: 0.1854  loss_dice_7: 0.1596  loss_ce_8: 0.112  loss_mask_8: 0.1833  loss_dice_8: 0.1547  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:22] d2.utils.events INFO:  eta: 0:49:06  iter: 26659  total_loss: 4.762  loss_ce: 0.1051  loss_mask: 0.188  loss_dice: 0.142  loss_ce_0: 0.05429  loss_mask_0: 0.1974  loss_dice_0: 0.1445  loss_ce_1: 0.1153  loss_mask_1: 0.1962  loss_dice_1: 0.1455  loss_ce_2: 0.1039  loss_mask_2: 0.1874  loss_dice_2: 0.1433  loss_ce_3: 0.1048  loss_mask_3: 0.1894  loss_dice_3: 0.1499  loss_ce_4: 0.1038  loss_mask_4: 0.1897  loss_dice_4: 0.1475  loss_ce_5: 0.1047  loss_mask_5: 0.1872  loss_dice_5: 0.1456  loss_ce_6: 0.1042  loss_mask_6: 0.1796  loss_dice_6: 0.1413  loss_ce_7: 0.1042  loss_mask_7: 0.187  loss_dice_7: 0.1423  loss_ce_8: 0.1047  loss_mask_8: 0.1854  loss_dice_8: 0.1414  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:24] d2.utils.events INFO:  eta: 0:49:03  iter: 26679  total_loss: 4.863  loss_ce: 0.1526  loss_mask: 0.1712  loss_dice: 0.1516  loss_ce_0: 0.07175  loss_mask_0: 0.1764  loss_dice_0: 0.155  loss_ce_1: 0.1368  loss_mask_1: 0.1819  loss_dice_1: 0.1591  loss_ce_2: 0.1452  loss_mask_2: 0.1784  loss_dice_2: 0.154  loss_ce_3: 0.1499  loss_mask_3: 0.1693  loss_dice_3: 0.1579  loss_ce_4: 0.151  loss_mask_4: 0.1784  loss_dice_4: 0.1578  loss_ce_5: 0.1519  loss_mask_5: 0.1704  loss_dice_5: 0.1611  loss_ce_6: 0.1528  loss_mask_6: 0.1807  loss_dice_6: 0.163  loss_ce_7: 0.1523  loss_mask_7: 0.1801  loss_dice_7: 0.1611  loss_ce_8: 0.1514  loss_mask_8: 0.1777  loss_dice_8: 0.1551  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:27] d2.utils.events INFO:  eta: 0:48:58  iter: 26699  total_loss: 5.058  loss_ce: 0.1242  loss_mask: 0.2168  loss_dice: 0.1494  loss_ce_0: 0.06112  loss_mask_0: 0.2224  loss_dice_0: 0.146  loss_ce_1: 0.1235  loss_mask_1: 0.2104  loss_dice_1: 0.1464  loss_ce_2: 0.1269  loss_mask_2: 0.2193  loss_dice_2: 0.1515  loss_ce_3: 0.1221  loss_mask_3: 0.2251  loss_dice_3: 0.1531  loss_ce_4: 0.1257  loss_mask_4: 0.2165  loss_dice_4: 0.1411  loss_ce_5: 0.1244  loss_mask_5: 0.2098  loss_dice_5: 0.1519  loss_ce_6: 0.126  loss_mask_6: 0.2166  loss_dice_6: 0.1477  loss_ce_7: 0.1271  loss_mask_7: 0.2137  loss_dice_7: 0.149  loss_ce_8: 0.1259  loss_mask_8: 0.2154  loss_dice_8: 0.1507  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:29] d2.utils.events INFO:  eta: 0:48:55  iter: 26719  total_loss: 5.635  loss_ce: 0.1261  loss_mask: 0.1973  loss_dice: 0.1977  loss_ce_0: 0.06179  loss_mask_0: 0.1901  loss_dice_0: 0.2027  loss_ce_1: 0.1187  loss_mask_1: 0.1923  loss_dice_1: 0.2009  loss_ce_2: 0.1249  loss_mask_2: 0.1919  loss_dice_2: 0.203  loss_ce_3: 0.1224  loss_mask_3: 0.1903  loss_dice_3: 0.2083  loss_ce_4: 0.126  loss_mask_4: 0.1868  loss_dice_4: 0.2052  loss_ce_5: 0.1261  loss_mask_5: 0.1881  loss_dice_5: 0.2086  loss_ce_6: 0.1274  loss_mask_6: 0.1948  loss_dice_6: 0.215  loss_ce_7: 0.1273  loss_mask_7: 0.1895  loss_dice_7: 0.206  loss_ce_8: 0.1264  loss_mask_8: 0.1945  loss_dice_8: 0.2015  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:32] d2.utils.events INFO:  eta: 0:48:52  iter: 26739  total_loss: 4.949  loss_ce: 0.1248  loss_mask: 0.2273  loss_dice: 0.1512  loss_ce_0: 0.06099  loss_mask_0: 0.2176  loss_dice_0: 0.1443  loss_ce_1: 0.1047  loss_mask_1: 0.2288  loss_dice_1: 0.1479  loss_ce_2: 0.1183  loss_mask_2: 0.2196  loss_dice_2: 0.1405  loss_ce_3: 0.1205  loss_mask_3: 0.2223  loss_dice_3: 0.1473  loss_ce_4: 0.123  loss_mask_4: 0.213  loss_dice_4: 0.1442  loss_ce_5: 0.1241  loss_mask_5: 0.222  loss_dice_5: 0.1466  loss_ce_6: 0.1256  loss_mask_6: 0.2215  loss_dice_6: 0.1466  loss_ce_7: 0.125  loss_mask_7: 0.2173  loss_dice_7: 0.1425  loss_ce_8: 0.1238  loss_mask_8: 0.2263  loss_dice_8: 0.1488  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:50:34] d2.utils.events INFO:  eta: 0:48:49  iter: 26759  total_loss: 5.035  loss_ce: 0.1388  loss_mask: 0.1643  loss_dice: 0.1512  loss_ce_0: 0.06709  loss_mask_0: 0.1747  loss_dice_0: 0.1576  loss_ce_1: 0.1296  loss_mask_1: 0.1745  loss_dice_1: 0.1499  loss_ce_2: 0.1294  loss_mask_2: 0.1766  loss_dice_2: 0.1518  loss_ce_3: 0.135  loss_mask_3: 0.1623  loss_dice_3: 0.1535  loss_ce_4: 0.1359  loss_mask_4: 0.1692  loss_dice_4: 0.1509  loss_ce_5: 0.1392  loss_mask_5: 0.1699  loss_dice_5: 0.1585  loss_ce_6: 0.1393  loss_mask_6: 0.1585  loss_dice_6: 0.154  loss_ce_7: 0.1403  loss_mask_7: 0.1724  loss_dice_7: 0.1503  loss_ce_8: 0.1391  loss_mask_8: 0.1577  loss_dice_8: 0.1518  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:37] d2.utils.events INFO:  eta: 0:48:47  iter: 26779  total_loss: 4.595  loss_ce: 0.1154  loss_mask: 0.1842  loss_dice: 0.126  loss_ce_0: 0.0578  loss_mask_0: 0.1796  loss_dice_0: 0.1275  loss_ce_1: 0.1195  loss_mask_1: 0.1844  loss_dice_1: 0.1245  loss_ce_2: 0.1182  loss_mask_2: 0.1803  loss_dice_2: 0.128  loss_ce_3: 0.115  loss_mask_3: 0.1765  loss_dice_3: 0.1275  loss_ce_4: 0.1132  loss_mask_4: 0.1855  loss_dice_4: 0.1303  loss_ce_5: 0.1139  loss_mask_5: 0.1861  loss_dice_5: 0.1243  loss_ce_6: 0.1146  loss_mask_6: 0.1809  loss_dice_6: 0.1245  loss_ce_7: 0.1136  loss_mask_7: 0.1848  loss_dice_7: 0.1268  loss_ce_8: 0.1147  loss_mask_8: 0.1825  loss_dice_8: 0.1231  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:39] d2.utils.events INFO:  eta: 0:48:46  iter: 26799  total_loss: 4.494  loss_ce: 0.1465  loss_mask: 0.1532  loss_dice: 0.1237  loss_ce_0: 0.07014  loss_mask_0: 0.157  loss_dice_0: 0.1189  loss_ce_1: 0.1436  loss_mask_1: 0.1575  loss_dice_1: 0.1149  loss_ce_2: 0.1426  loss_mask_2: 0.1562  loss_dice_2: 0.1166  loss_ce_3: 0.1454  loss_mask_3: 0.1612  loss_dice_3: 0.1169  loss_ce_4: 0.1464  loss_mask_4: 0.1555  loss_dice_4: 0.1188  loss_ce_5: 0.1463  loss_mask_5: 0.1507  loss_dice_5: 0.117  loss_ce_6: 0.1467  loss_mask_6: 0.1565  loss_dice_6: 0.1218  loss_ce_7: 0.1472  loss_mask_7: 0.1544  loss_dice_7: 0.1186  loss_ce_8: 0.1488  loss_mask_8: 0.1448  loss_dice_8: 0.1131  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:42] d2.utils.events INFO:  eta: 0:48:43  iter: 26819  total_loss: 4.589  loss_ce: 0.1183  loss_mask: 0.22  loss_dice: 0.1407  loss_ce_0: 0.06102  loss_mask_0: 0.2093  loss_dice_0: 0.1391  loss_ce_1: 0.1249  loss_mask_1: 0.2077  loss_dice_1: 0.1405  loss_ce_2: 0.1255  loss_mask_2: 0.2173  loss_dice_2: 0.1391  loss_ce_3: 0.1205  loss_mask_3: 0.2119  loss_dice_3: 0.1401  loss_ce_4: 0.1199  loss_mask_4: 0.2193  loss_dice_4: 0.1441  loss_ce_5: 0.119  loss_mask_5: 0.2123  loss_dice_5: 0.1404  loss_ce_6: 0.1203  loss_mask_6: 0.2201  loss_dice_6: 0.1393  loss_ce_7: 0.1208  loss_mask_7: 0.2119  loss_dice_7: 0.1381  loss_ce_8: 0.1205  loss_mask_8: 0.213  loss_dice_8: 0.1469  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:50:44] d2.utils.events INFO:  eta: 0:48:40  iter: 26839  total_loss: 5.153  loss_ce: 0.1181  loss_mask: 0.2095  loss_dice: 0.1791  loss_ce_0: 0.05988  loss_mask_0: 0.2147  loss_dice_0: 0.1836  loss_ce_1: 0.1296  loss_mask_1: 0.2175  loss_dice_1: 0.1701  loss_ce_2: 0.1206  loss_mask_2: 0.2101  loss_dice_2: 0.1861  loss_ce_3: 0.1174  loss_mask_3: 0.2162  loss_dice_3: 0.1675  loss_ce_4: 0.1174  loss_mask_4: 0.2146  loss_dice_4: 0.1726  loss_ce_5: 0.1156  loss_mask_5: 0.2159  loss_dice_5: 0.1724  loss_ce_6: 0.1188  loss_mask_6: 0.2174  loss_dice_6: 0.1815  loss_ce_7: 0.1184  loss_mask_7: 0.2119  loss_dice_7: 0.1813  loss_ce_8: 0.1195  loss_mask_8: 0.2181  loss_dice_8: 0.1856  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:47] d2.utils.events INFO:  eta: 0:48:37  iter: 26859  total_loss: 5.362  loss_ce: 0.1135  loss_mask: 0.2008  loss_dice: 0.1616  loss_ce_0: 0.05838  loss_mask_0: 0.1923  loss_dice_0: 0.1577  loss_ce_1: 0.121  loss_mask_1: 0.2002  loss_dice_1: 0.1565  loss_ce_2: 0.1181  loss_mask_2: 0.194  loss_dice_2: 0.1651  loss_ce_3: 0.1159  loss_mask_3: 0.202  loss_dice_3: 0.1661  loss_ce_4: 0.1151  loss_mask_4: 0.2072  loss_dice_4: 0.1643  loss_ce_5: 0.1143  loss_mask_5: 0.1932  loss_dice_5: 0.1552  loss_ce_6: 0.1136  loss_mask_6: 0.1987  loss_dice_6: 0.1633  loss_ce_7: 0.1141  loss_mask_7: 0.2028  loss_dice_7: 0.1638  loss_ce_8: 0.1141  loss_mask_8: 0.2051  loss_dice_8: 0.1608  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:50] d2.utils.events INFO:  eta: 0:48:35  iter: 26879  total_loss: 5.311  loss_ce: 0.1284  loss_mask: 0.2053  loss_dice: 0.2284  loss_ce_0: 0.06308  loss_mask_0: 0.2062  loss_dice_0: 0.2231  loss_ce_1: 0.1208  loss_mask_1: 0.2025  loss_dice_1: 0.242  loss_ce_2: 0.1276  loss_mask_2: 0.202  loss_dice_2: 0.2311  loss_ce_3: 0.1281  loss_mask_3: 0.214  loss_dice_3: 0.2204  loss_ce_4: 0.1274  loss_mask_4: 0.1994  loss_dice_4: 0.2346  loss_ce_5: 0.1282  loss_mask_5: 0.2033  loss_dice_5: 0.2256  loss_ce_6: 0.1284  loss_mask_6: 0.2099  loss_dice_6: 0.2238  loss_ce_7: 0.1282  loss_mask_7: 0.2132  loss_dice_7: 0.2332  loss_ce_8: 0.1283  loss_mask_8: 0.2059  loss_dice_8: 0.2335  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:52] d2.utils.events INFO:  eta: 0:48:34  iter: 26899  total_loss: 5.694  loss_ce: 0.1141  loss_mask: 0.2087  loss_dice: 0.18  loss_ce_0: 0.05709  loss_mask_0: 0.2015  loss_dice_0: 0.1697  loss_ce_1: 0.1231  loss_mask_1: 0.21  loss_dice_1: 0.1727  loss_ce_2: 0.1163  loss_mask_2: 0.1926  loss_dice_2: 0.1719  loss_ce_3: 0.113  loss_mask_3: 0.2023  loss_dice_3: 0.1752  loss_ce_4: 0.1209  loss_mask_4: 0.2086  loss_dice_4: 0.1722  loss_ce_5: 0.1144  loss_mask_5: 0.2117  loss_dice_5: 0.1696  loss_ce_6: 0.1134  loss_mask_6: 0.1991  loss_dice_6: 0.1721  loss_ce_7: 0.115  loss_mask_7: 0.2081  loss_dice_7: 0.1654  loss_ce_8: 0.1145  loss_mask_8: 0.1989  loss_dice_8: 0.1667  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:55] d2.utils.events INFO:  eta: 0:48:31  iter: 26919  total_loss: 5.391  loss_ce: 0.115  loss_mask: 0.193  loss_dice: 0.1902  loss_ce_0: 0.05713  loss_mask_0: 0.1901  loss_dice_0: 0.1953  loss_ce_1: 0.1224  loss_mask_1: 0.1889  loss_dice_1: 0.1932  loss_ce_2: 0.1167  loss_mask_2: 0.1849  loss_dice_2: 0.193  loss_ce_3: 0.1127  loss_mask_3: 0.1847  loss_dice_3: 0.1983  loss_ce_4: 0.1224  loss_mask_4: 0.1867  loss_dice_4: 0.1873  loss_ce_5: 0.1143  loss_mask_5: 0.1863  loss_dice_5: 0.1898  loss_ce_6: 0.1141  loss_mask_6: 0.1788  loss_dice_6: 0.1868  loss_ce_7: 0.1155  loss_mask_7: 0.1797  loss_dice_7: 0.1898  loss_ce_8: 0.1152  loss_mask_8: 0.1877  loss_dice_8: 0.1928  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:50:57] d2.utils.events INFO:  eta: 0:48:31  iter: 26939  total_loss: 5.38  loss_ce: 0.1144  loss_mask: 0.2242  loss_dice: 0.1523  loss_ce_0: 0.05718  loss_mask_0: 0.22  loss_dice_0: 0.1571  loss_ce_1: 0.1284  loss_mask_1: 0.2161  loss_dice_1: 0.1573  loss_ce_2: 0.1157  loss_mask_2: 0.2169  loss_dice_2: 0.1552  loss_ce_3: 0.1135  loss_mask_3: 0.2217  loss_dice_3: 0.1554  loss_ce_4: 0.121  loss_mask_4: 0.2223  loss_dice_4: 0.1568  loss_ce_5: 0.114  loss_mask_5: 0.2164  loss_dice_5: 0.154  loss_ce_6: 0.114  loss_mask_6: 0.2158  loss_dice_6: 0.154  loss_ce_7: 0.1153  loss_mask_7: 0.2166  loss_dice_7: 0.1562  loss_ce_8: 0.1141  loss_mask_8: 0.217  loss_dice_8: 0.157  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:00] d2.utils.events INFO:  eta: 0:48:29  iter: 26959  total_loss: 5.41  loss_ce: 0.1132  loss_mask: 0.1903  loss_dice: 0.222  loss_ce_0: 0.05658  loss_mask_0: 0.188  loss_dice_0: 0.2214  loss_ce_1: 0.1277  loss_mask_1: 0.1869  loss_dice_1: 0.223  loss_ce_2: 0.113  loss_mask_2: 0.1952  loss_dice_2: 0.2205  loss_ce_3: 0.1112  loss_mask_3: 0.1873  loss_dice_3: 0.2218  loss_ce_4: 0.1162  loss_mask_4: 0.1826  loss_dice_4: 0.222  loss_ce_5: 0.1112  loss_mask_5: 0.1869  loss_dice_5: 0.2256  loss_ce_6: 0.1124  loss_mask_6: 0.1815  loss_dice_6: 0.2301  loss_ce_7: 0.1125  loss_mask_7: 0.185  loss_dice_7: 0.217  loss_ce_8: 0.1123  loss_mask_8: 0.1865  loss_dice_8: 0.2156  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:02] d2.utils.events INFO:  eta: 0:48:27  iter: 26979  total_loss: 5.299  loss_ce: 0.1054  loss_mask: 0.2115  loss_dice: 0.1989  loss_ce_0: 0.05497  loss_mask_0: 0.2167  loss_dice_0: 0.1937  loss_ce_1: 0.07515  loss_mask_1: 0.2167  loss_dice_1: 0.2044  loss_ce_2: 0.09541  loss_mask_2: 0.2108  loss_dice_2: 0.1895  loss_ce_3: 0.1077  loss_mask_3: 0.208  loss_dice_3: 0.1919  loss_ce_4: 0.09467  loss_mask_4: 0.2191  loss_dice_4: 0.1964  loss_ce_5: 0.1026  loss_mask_5: 0.2086  loss_dice_5: 0.1997  loss_ce_6: 0.1039  loss_mask_6: 0.2215  loss_dice_6: 0.1951  loss_ce_7: 0.1002  loss_mask_7: 0.2028  loss_dice_7: 0.1979  loss_ce_8: 0.102  loss_mask_8: 0.2146  loss_dice_8: 0.1824  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:05] d2.utils.events INFO:  eta: 0:48:25  iter: 26999  total_loss: 5.564  loss_ce: 0.09398  loss_mask: 0.2021  loss_dice: 0.1814  loss_ce_0: 0.05238  loss_mask_0: 0.1993  loss_dice_0: 0.1816  loss_ce_1: 0.1116  loss_mask_1: 0.2007  loss_dice_1: 0.1785  loss_ce_2: 0.1238  loss_mask_2: 0.2051  loss_dice_2: 0.1813  loss_ce_3: 0.103  loss_mask_3: 0.1974  loss_dice_3: 0.174  loss_ce_4: 0.09404  loss_mask_4: 0.1972  loss_dice_4: 0.1821  loss_ce_5: 0.0907  loss_mask_5: 0.1958  loss_dice_5: 0.1844  loss_ce_6: 0.09209  loss_mask_6: 0.2074  loss_dice_6: 0.1821  loss_ce_7: 0.08947  loss_mask_7: 0.2014  loss_dice_7: 0.1765  loss_ce_8: 0.09258  loss_mask_8: 0.2025  loss_dice_8: 0.1842  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:07] d2.utils.events INFO:  eta: 0:48:22  iter: 27019  total_loss: 6.178  loss_ce: 0.09122  loss_mask: 0.1879  loss_dice: 0.2723  loss_ce_0: 0.05013  loss_mask_0: 0.1853  loss_dice_0: 0.2686  loss_ce_1: 0.1201  loss_mask_1: 0.1976  loss_dice_1: 0.2796  loss_ce_2: 0.1271  loss_mask_2: 0.1956  loss_dice_2: 0.2804  loss_ce_3: 0.09921  loss_mask_3: 0.1988  loss_dice_3: 0.2737  loss_ce_4: 0.1078  loss_mask_4: 0.1914  loss_dice_4: 0.2701  loss_ce_5: 0.09637  loss_mask_5: 0.2066  loss_dice_5: 0.2769  loss_ce_6: 0.09062  loss_mask_6: 0.189  loss_dice_6: 0.2735  loss_ce_7: 0.09311  loss_mask_7: 0.2002  loss_dice_7: 0.2771  loss_ce_8: 0.09217  loss_mask_8: 0.1928  loss_dice_8: 0.2834  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:10] d2.utils.events INFO:  eta: 0:48:18  iter: 27039  total_loss: 5.401  loss_ce: 0.1297  loss_mask: 0.1984  loss_dice: 0.1441  loss_ce_0: 0.06381  loss_mask_0: 0.199  loss_dice_0: 0.1354  loss_ce_1: 0.1124  loss_mask_1: 0.2002  loss_dice_1: 0.1406  loss_ce_2: 0.1254  loss_mask_2: 0.2025  loss_dice_2: 0.1408  loss_ce_3: 0.1308  loss_mask_3: 0.2009  loss_dice_3: 0.1399  loss_ce_4: 0.1287  loss_mask_4: 0.2043  loss_dice_4: 0.1365  loss_ce_5: 0.1294  loss_mask_5: 0.2072  loss_dice_5: 0.1455  loss_ce_6: 0.1298  loss_mask_6: 0.1922  loss_dice_6: 0.1408  loss_ce_7: 0.129  loss_mask_7: 0.1984  loss_dice_7: 0.1466  loss_ce_8: 0.1293  loss_mask_8: 0.2034  loss_dice_8: 0.1391  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:13] d2.utils.events INFO:  eta: 0:48:15  iter: 27059  total_loss: 5.462  loss_ce: 0.1294  loss_mask: 0.1826  loss_dice: 0.2024  loss_ce_0: 0.06343  loss_mask_0: 0.1789  loss_dice_0: 0.2076  loss_ce_1: 0.1211  loss_mask_1: 0.1872  loss_dice_1: 0.2076  loss_ce_2: 0.1302  loss_mask_2: 0.1841  loss_dice_2: 0.1951  loss_ce_3: 0.1309  loss_mask_3: 0.1863  loss_dice_3: 0.2012  loss_ce_4: 0.1315  loss_mask_4: 0.1847  loss_dice_4: 0.2082  loss_ce_5: 0.1295  loss_mask_5: 0.1815  loss_dice_5: 0.2064  loss_ce_6: 0.1291  loss_mask_6: 0.1891  loss_dice_6: 0.2088  loss_ce_7: 0.1295  loss_mask_7: 0.1785  loss_dice_7: 0.2055  loss_ce_8: 0.1298  loss_mask_8: 0.1798  loss_dice_8: 0.2071  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:15] d2.utils.events INFO:  eta: 0:48:13  iter: 27079  total_loss: 5.124  loss_ce: 0.1137  loss_mask: 0.2135  loss_dice: 0.1382  loss_ce_0: 0.0562  loss_mask_0: 0.2022  loss_dice_0: 0.1421  loss_ce_1: 0.09748  loss_mask_1: 0.2147  loss_dice_1: 0.1349  loss_ce_2: 0.1245  loss_mask_2: 0.2204  loss_dice_2: 0.1383  loss_ce_3: 0.1118  loss_mask_3: 0.2184  loss_dice_3: 0.1418  loss_ce_4: 0.1166  loss_mask_4: 0.2083  loss_dice_4: 0.1466  loss_ce_5: 0.1154  loss_mask_5: 0.2107  loss_dice_5: 0.1338  loss_ce_6: 0.1145  loss_mask_6: 0.2126  loss_dice_6: 0.1419  loss_ce_7: 0.1174  loss_mask_7: 0.2154  loss_dice_7: 0.1389  loss_ce_8: 0.1148  loss_mask_8: 0.2111  loss_dice_8: 0.1384  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:18] d2.utils.events INFO:  eta: 0:48:12  iter: 27099  total_loss: 5.125  loss_ce: 0.1115  loss_mask: 0.21  loss_dice: 0.203  loss_ce_0: 0.05543  loss_mask_0: 0.2089  loss_dice_0: 0.1986  loss_ce_1: 0.1151  loss_mask_1: 0.2109  loss_dice_1: 0.1982  loss_ce_2: 0.121  loss_mask_2: 0.2094  loss_dice_2: 0.1998  loss_ce_3: 0.1091  loss_mask_3: 0.2043  loss_dice_3: 0.1988  loss_ce_4: 0.1192  loss_mask_4: 0.2072  loss_dice_4: 0.1988  loss_ce_5: 0.11  loss_mask_5: 0.2035  loss_dice_5: 0.193  loss_ce_6: 0.1109  loss_mask_6: 0.2118  loss_dice_6: 0.2002  loss_ce_7: 0.1116  loss_mask_7: 0.2187  loss_dice_7: 0.2033  loss_ce_8: 0.1128  loss_mask_8: 0.2132  loss_dice_8: 0.2009  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:20] d2.utils.events INFO:  eta: 0:48:10  iter: 27119  total_loss: 6.082  loss_ce: 0.1037  loss_mask: 0.2243  loss_dice: 0.2272  loss_ce_0: 0.05415  loss_mask_0: 0.2259  loss_dice_0: 0.2283  loss_ce_1: 0.1227  loss_mask_1: 0.2255  loss_dice_1: 0.2233  loss_ce_2: 0.1287  loss_mask_2: 0.2293  loss_dice_2: 0.2239  loss_ce_3: 0.115  loss_mask_3: 0.227  loss_dice_3: 0.2354  loss_ce_4: 0.1166  loss_mask_4: 0.2203  loss_dice_4: 0.2254  loss_ce_5: 0.106  loss_mask_5: 0.2271  loss_dice_5: 0.2317  loss_ce_6: 0.1031  loss_mask_6: 0.2269  loss_dice_6: 0.2297  loss_ce_7: 0.1064  loss_mask_7: 0.2262  loss_dice_7: 0.2282  loss_ce_8: 0.1036  loss_mask_8: 0.2301  loss_dice_8: 0.2337  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:23] d2.utils.events INFO:  eta: 0:48:08  iter: 27139  total_loss: 5.657  loss_ce: 0.1542  loss_mask: 0.2191  loss_dice: 0.2083  loss_ce_0: 0.07128  loss_mask_0: 0.2167  loss_dice_0: 0.2048  loss_ce_1: 0.1331  loss_mask_1: 0.2154  loss_dice_1: 0.2048  loss_ce_2: 0.13  loss_mask_2: 0.2212  loss_dice_2: 0.2122  loss_ce_3: 0.1337  loss_mask_3: 0.218  loss_dice_3: 0.2047  loss_ce_4: 0.1327  loss_mask_4: 0.2219  loss_dice_4: 0.2073  loss_ce_5: 0.1419  loss_mask_5: 0.2152  loss_dice_5: 0.2046  loss_ce_6: 0.1497  loss_mask_6: 0.2253  loss_dice_6: 0.2126  loss_ce_7: 0.1378  loss_mask_7: 0.2168  loss_dice_7: 0.2054  loss_ce_8: 0.1465  loss_mask_8: 0.2254  loss_dice_8: 0.2074  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:25] d2.utils.events INFO:  eta: 0:48:06  iter: 27159  total_loss: 5.54  loss_ce: 0.1154  loss_mask: 0.2219  loss_dice: 0.1768  loss_ce_0: 0.0567  loss_mask_0: 0.2107  loss_dice_0: 0.1781  loss_ce_1: 0.1302  loss_mask_1: 0.2117  loss_dice_1: 0.1799  loss_ce_2: 0.1315  loss_mask_2: 0.2132  loss_dice_2: 0.1796  loss_ce_3: 0.1266  loss_mask_3: 0.2169  loss_dice_3: 0.1833  loss_ce_4: 0.1294  loss_mask_4: 0.2158  loss_dice_4: 0.1777  loss_ce_5: 0.1201  loss_mask_5: 0.2144  loss_dice_5: 0.1832  loss_ce_6: 0.1176  loss_mask_6: 0.213  loss_dice_6: 0.1935  loss_ce_7: 0.1273  loss_mask_7: 0.2287  loss_dice_7: 0.184  loss_ce_8: 0.1207  loss_mask_8: 0.2096  loss_dice_8: 0.1786  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:28] d2.utils.events INFO:  eta: 0:48:03  iter: 27179  total_loss: 5.584  loss_ce: 0.129  loss_mask: 0.1871  loss_dice: 0.2203  loss_ce_0: 0.06604  loss_mask_0: 0.1864  loss_dice_0: 0.2127  loss_ce_1: 0.1168  loss_mask_1: 0.1921  loss_dice_1: 0.2027  loss_ce_2: 0.1214  loss_mask_2: 0.1931  loss_dice_2: 0.2106  loss_ce_3: 0.1303  loss_mask_3: 0.1856  loss_dice_3: 0.211  loss_ce_4: 0.1221  loss_mask_4: 0.1824  loss_dice_4: 0.2094  loss_ce_5: 0.1284  loss_mask_5: 0.1917  loss_dice_5: 0.2001  loss_ce_6: 0.1284  loss_mask_6: 0.1916  loss_dice_6: 0.2145  loss_ce_7: 0.1267  loss_mask_7: 0.1868  loss_dice_7: 0.2016  loss_ce_8: 0.1277  loss_mask_8: 0.19  loss_dice_8: 0.2035  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:30] d2.utils.events INFO:  eta: 0:48:01  iter: 27199  total_loss: 4.829  loss_ce: 0.1283  loss_mask: 0.213  loss_dice: 0.1627  loss_ce_0: 0.06284  loss_mask_0: 0.218  loss_dice_0: 0.1551  loss_ce_1: 0.1128  loss_mask_1: 0.205  loss_dice_1: 0.1521  loss_ce_2: 0.1263  loss_mask_2: 0.2034  loss_dice_2: 0.1557  loss_ce_3: 0.1221  loss_mask_3: 0.2094  loss_dice_3: 0.1558  loss_ce_4: 0.1183  loss_mask_4: 0.2128  loss_dice_4: 0.1585  loss_ce_5: 0.127  loss_mask_5: 0.2203  loss_dice_5: 0.165  loss_ce_6: 0.1284  loss_mask_6: 0.2138  loss_dice_6: 0.156  loss_ce_7: 0.1271  loss_mask_7: 0.2135  loss_dice_7: 0.1601  loss_ce_8: 0.1257  loss_mask_8: 0.2116  loss_dice_8: 0.1618  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:33] d2.utils.events INFO:  eta: 0:48:01  iter: 27219  total_loss: 5.144  loss_ce: 0.1267  loss_mask: 0.1534  loss_dice: 0.1795  loss_ce_0: 0.06256  loss_mask_0: 0.1523  loss_dice_0: 0.1748  loss_ce_1: 0.09849  loss_mask_1: 0.1577  loss_dice_1: 0.1811  loss_ce_2: 0.1095  loss_mask_2: 0.1577  loss_dice_2: 0.1801  loss_ce_3: 0.1187  loss_mask_3: 0.1513  loss_dice_3: 0.1739  loss_ce_4: 0.1079  loss_mask_4: 0.1567  loss_dice_4: 0.1839  loss_ce_5: 0.1219  loss_mask_5: 0.1612  loss_dice_5: 0.1794  loss_ce_6: 0.1274  loss_mask_6: 0.1626  loss_dice_6: 0.1856  loss_ce_7: 0.119  loss_mask_7: 0.1492  loss_dice_7: 0.1868  loss_ce_8: 0.1204  loss_mask_8: 0.1598  loss_dice_8: 0.1862  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:35] d2.utils.events INFO:  eta: 0:48:00  iter: 27239  total_loss: 4.714  loss_ce: 0.1265  loss_mask: 0.1847  loss_dice: 0.1514  loss_ce_0: 0.0623  loss_mask_0: 0.1738  loss_dice_0: 0.15  loss_ce_1: 0.1338  loss_mask_1: 0.1841  loss_dice_1: 0.1496  loss_ce_2: 0.1269  loss_mask_2: 0.1746  loss_dice_2: 0.1466  loss_ce_3: 0.1295  loss_mask_3: 0.1768  loss_dice_3: 0.1498  loss_ce_4: 0.1473  loss_mask_4: 0.1791  loss_dice_4: 0.1503  loss_ce_5: 0.1305  loss_mask_5: 0.1765  loss_dice_5: 0.1478  loss_ce_6: 0.132  loss_mask_6: 0.1748  loss_dice_6: 0.1518  loss_ce_7: 0.1294  loss_mask_7: 0.1783  loss_dice_7: 0.1516  loss_ce_8: 0.127  loss_mask_8: 0.1801  loss_dice_8: 0.1503  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:38] d2.utils.events INFO:  eta: 0:47:58  iter: 27259  total_loss: 4.628  loss_ce: 0.1438  loss_mask: 0.1832  loss_dice: 0.1587  loss_ce_0: 0.06993  loss_mask_0: 0.1859  loss_dice_0: 0.1547  loss_ce_1: 0.1151  loss_mask_1: 0.1792  loss_dice_1: 0.1547  loss_ce_2: 0.1174  loss_mask_2: 0.1769  loss_dice_2: 0.1532  loss_ce_3: 0.1404  loss_mask_3: 0.173  loss_dice_3: 0.1522  loss_ce_4: 0.1277  loss_mask_4: 0.1812  loss_dice_4: 0.1534  loss_ce_5: 0.1371  loss_mask_5: 0.1761  loss_dice_5: 0.1538  loss_ce_6: 0.1504  loss_mask_6: 0.1812  loss_dice_6: 0.1588  loss_ce_7: 0.1367  loss_mask_7: 0.186  loss_dice_7: 0.1528  loss_ce_8: 0.135  loss_mask_8: 0.1749  loss_dice_8: 0.1513  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:51:40] d2.utils.events INFO:  eta: 0:47:55  iter: 27279  total_loss: 5.309  loss_ce: 0.1152  loss_mask: 0.2242  loss_dice: 0.1908  loss_ce_0: 0.05682  loss_mask_0: 0.2234  loss_dice_0: 0.1859  loss_ce_1: 0.1118  loss_mask_1: 0.2269  loss_dice_1: 0.1948  loss_ce_2: 0.1153  loss_mask_2: 0.2358  loss_dice_2: 0.1885  loss_ce_3: 0.1058  loss_mask_3: 0.2339  loss_dice_3: 0.1848  loss_ce_4: 0.1183  loss_mask_4: 0.2369  loss_dice_4: 0.1938  loss_ce_5: 0.1133  loss_mask_5: 0.2268  loss_dice_5: 0.189  loss_ce_6: 0.1078  loss_mask_6: 0.2306  loss_dice_6: 0.1929  loss_ce_7: 0.1088  loss_mask_7: 0.2205  loss_dice_7: 0.1837  loss_ce_8: 0.1176  loss_mask_8: 0.2331  loss_dice_8: 0.1859  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:43] d2.utils.events INFO:  eta: 0:47:52  iter: 27299  total_loss: 4.907  loss_ce: 0.09592  loss_mask: 0.1879  loss_dice: 0.1536  loss_ce_0: 0.05461  loss_mask_0: 0.1822  loss_dice_0: 0.1591  loss_ce_1: 0.1033  loss_mask_1: 0.1925  loss_dice_1: 0.1598  loss_ce_2: 0.1087  loss_mask_2: 0.1914  loss_dice_2: 0.1583  loss_ce_3: 0.07773  loss_mask_3: 0.1791  loss_dice_3: 0.1443  loss_ce_4: 0.1052  loss_mask_4: 0.1855  loss_dice_4: 0.1475  loss_ce_5: 0.1026  loss_mask_5: 0.1911  loss_dice_5: 0.1574  loss_ce_6: 0.09769  loss_mask_6: 0.1886  loss_dice_6: 0.1523  loss_ce_7: 0.09185  loss_mask_7: 0.1925  loss_dice_7: 0.1557  loss_ce_8: 0.09047  loss_mask_8: 0.188  loss_dice_8: 0.1569  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:46] d2.utils.events INFO:  eta: 0:47:48  iter: 27319  total_loss: 4.756  loss_ce: 0.1817  loss_mask: 0.1893  loss_dice: 0.1476  loss_ce_0: 0.07532  loss_mask_0: 0.1832  loss_dice_0: 0.1475  loss_ce_1: 0.138  loss_mask_1: 0.1947  loss_dice_1: 0.1443  loss_ce_2: 0.133  loss_mask_2: 0.1906  loss_dice_2: 0.1494  loss_ce_3: 0.1507  loss_mask_3: 0.191  loss_dice_3: 0.1489  loss_ce_4: 0.1375  loss_mask_4: 0.1869  loss_dice_4: 0.1444  loss_ce_5: 0.1548  loss_mask_5: 0.1947  loss_dice_5: 0.1494  loss_ce_6: 0.178  loss_mask_6: 0.184  loss_dice_6: 0.1468  loss_ce_7: 0.1804  loss_mask_7: 0.1951  loss_dice_7: 0.1489  loss_ce_8: 0.1621  loss_mask_8: 0.1881  loss_dice_8: 0.1449  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:48] d2.utils.events INFO:  eta: 0:47:44  iter: 27339  total_loss: 5.089  loss_ce: 0.1496  loss_mask: 0.138  loss_dice: 0.2163  loss_ce_0: 0.07344  loss_mask_0: 0.1269  loss_dice_0: 0.2111  loss_ce_1: 0.12  loss_mask_1: 0.135  loss_dice_1: 0.2116  loss_ce_2: 0.114  loss_mask_2: 0.1229  loss_dice_2: 0.2129  loss_ce_3: 0.1258  loss_mask_3: 0.1288  loss_dice_3: 0.2153  loss_ce_4: 0.1277  loss_mask_4: 0.133  loss_dice_4: 0.2091  loss_ce_5: 0.1301  loss_mask_5: 0.148  loss_dice_5: 0.2031  loss_ce_6: 0.1499  loss_mask_6: 0.1379  loss_dice_6: 0.2134  loss_ce_7: 0.1365  loss_mask_7: 0.1245  loss_dice_7: 0.21  loss_ce_8: 0.1324  loss_mask_8: 0.132  loss_dice_8: 0.2088  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:51] d2.utils.events INFO:  eta: 0:47:41  iter: 27359  total_loss: 5.355  loss_ce: 0.132  loss_mask: 0.2167  loss_dice: 0.1789  loss_ce_0: 0.0676  loss_mask_0: 0.2079  loss_dice_0: 0.1706  loss_ce_1: 0.1102  loss_mask_1: 0.2101  loss_dice_1: 0.177  loss_ce_2: 0.1026  loss_mask_2: 0.209  loss_dice_2: 0.1656  loss_ce_3: 0.1114  loss_mask_3: 0.2129  loss_dice_3: 0.1776  loss_ce_4: 0.1282  loss_mask_4: 0.2092  loss_dice_4: 0.1802  loss_ce_5: 0.1134  loss_mask_5: 0.213  loss_dice_5: 0.1757  loss_ce_6: 0.1285  loss_mask_6: 0.208  loss_dice_6: 0.173  loss_ce_7: 0.1274  loss_mask_7: 0.2077  loss_dice_7: 0.1848  loss_ce_8: 0.1219  loss_mask_8: 0.2103  loss_dice_8: 0.1786  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:51:53] d2.utils.events INFO:  eta: 0:47:37  iter: 27379  total_loss: 5.495  loss_ce: 0.1314  loss_mask: 0.2505  loss_dice: 0.1485  loss_ce_0: 0.05892  loss_mask_0: 0.2557  loss_dice_0: 0.1469  loss_ce_1: 0.09552  loss_mask_1: 0.2549  loss_dice_1: 0.1499  loss_ce_2: 0.09133  loss_mask_2: 0.2555  loss_dice_2: 0.1559  loss_ce_3: 0.111  loss_mask_3: 0.2572  loss_dice_3: 0.1551  loss_ce_4: 0.112  loss_mask_4: 0.2528  loss_dice_4: 0.1565  loss_ce_5: 0.1075  loss_mask_5: 0.2577  loss_dice_5: 0.1503  loss_ce_6: 0.1287  loss_mask_6: 0.2434  loss_dice_6: 0.1555  loss_ce_7: 0.1338  loss_mask_7: 0.2579  loss_dice_7: 0.1582  loss_ce_8: 0.1126  loss_mask_8: 0.2571  loss_dice_8: 0.1555  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:56] d2.utils.events INFO:  eta: 0:47:34  iter: 27399  total_loss: 5.469  loss_ce: 0.1108  loss_mask: 0.2129  loss_dice: 0.1771  loss_ce_0: 0.0586  loss_mask_0: 0.2233  loss_dice_0: 0.1796  loss_ce_1: 0.1041  loss_mask_1: 0.2201  loss_dice_1: 0.1796  loss_ce_2: 0.09561  loss_mask_2: 0.2174  loss_dice_2: 0.1827  loss_ce_3: 0.1132  loss_mask_3: 0.2186  loss_dice_3: 0.1816  loss_ce_4: 0.1118  loss_mask_4: 0.2161  loss_dice_4: 0.1801  loss_ce_5: 0.1093  loss_mask_5: 0.2188  loss_dice_5: 0.1793  loss_ce_6: 0.118  loss_mask_6: 0.2224  loss_dice_6: 0.1735  loss_ce_7: 0.1152  loss_mask_7: 0.2288  loss_dice_7: 0.1813  loss_ce_8: 0.1142  loss_mask_8: 0.2218  loss_dice_8: 0.1819  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:51:58] d2.utils.events INFO:  eta: 0:47:32  iter: 27419  total_loss: 5.217  loss_ce: 0.1504  loss_mask: 0.1674  loss_dice: 0.1967  loss_ce_0: 0.07028  loss_mask_0: 0.1697  loss_dice_0: 0.1898  loss_ce_1: 0.129  loss_mask_1: 0.1736  loss_dice_1: 0.1927  loss_ce_2: 0.1079  loss_mask_2: 0.1729  loss_dice_2: 0.1998  loss_ce_3: 0.1264  loss_mask_3: 0.1772  loss_dice_3: 0.194  loss_ce_4: 0.137  loss_mask_4: 0.166  loss_dice_4: 0.189  loss_ce_5: 0.1291  loss_mask_5: 0.1683  loss_dice_5: 0.1903  loss_ce_6: 0.1516  loss_mask_6: 0.1804  loss_dice_6: 0.1966  loss_ce_7: 0.1364  loss_mask_7: 0.1722  loss_dice_7: 0.1873  loss_ce_8: 0.1299  loss_mask_8: 0.1782  loss_dice_8: 0.1859  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:52:01] d2.utils.events INFO:  eta: 0:47:30  iter: 27439  total_loss: 5.745  loss_ce: 0.1272  loss_mask: 0.167  loss_dice: 0.2073  loss_ce_0: 0.0665  loss_mask_0: 0.1726  loss_dice_0: 0.2097  loss_ce_1: 0.1038  loss_mask_1: 0.1612  loss_dice_1: 0.2122  loss_ce_2: 0.1045  loss_mask_2: 0.1702  loss_dice_2: 0.1951  loss_ce_3: 0.1151  loss_mask_3: 0.1714  loss_dice_3: 0.2122  loss_ce_4: 0.11  loss_mask_4: 0.1671  loss_dice_4: 0.2096  loss_ce_5: 0.1098  loss_mask_5: 0.169  loss_dice_5: 0.2008  loss_ce_6: 0.1302  loss_mask_6: 0.1749  loss_dice_6: 0.2077  loss_ce_7: 0.1247  loss_mask_7: 0.1678  loss_dice_7: 0.2062  loss_ce_8: 0.1174  loss_mask_8: 0.167  loss_dice_8: 0.2038  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:52:03] d2.utils.events INFO:  eta: 0:47:29  iter: 27459  total_loss: 5.53  loss_ce: 0.119  loss_mask: 0.1778  loss_dice: 0.2523  loss_ce_0: 0.06259  loss_mask_0: 0.1766  loss_dice_0: 0.2395  loss_ce_1: 0.0952  loss_mask_1: 0.1844  loss_dice_1: 0.2559  loss_ce_2: 0.09668  loss_mask_2: 0.1784  loss_dice_2: 0.2431  loss_ce_3: 0.09811  loss_mask_3: 0.1817  loss_dice_3: 0.2388  loss_ce_4: 0.09375  loss_mask_4: 0.1832  loss_dice_4: 0.2375  loss_ce_5: 0.09567  loss_mask_5: 0.1728  loss_dice_5: 0.2438  loss_ce_6: 0.1238  loss_mask_6: 0.1866  loss_dice_6: 0.2387  loss_ce_7: 0.1057  loss_mask_7: 0.1765  loss_dice_7: 0.2439  loss_ce_8: 0.1055  loss_mask_8: 0.1849  loss_dice_8: 0.2473  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:52:06] d2.utils.events INFO:  eta: 0:47:25  iter: 27479  total_loss: 4.579  loss_ce: 0.1082  loss_mask: 0.2011  loss_dice: 0.1719  loss_ce_0: 0.06059  loss_mask_0: 0.1985  loss_dice_0: 0.1715  loss_ce_1: 0.086  loss_mask_1: 0.1951  loss_dice_1: 0.1743  loss_ce_2: 0.07025  loss_mask_2: 0.2011  loss_dice_2: 0.1767  loss_ce_3: 0.06754  loss_mask_3: 0.2083  loss_dice_3: 0.1779  loss_ce_4: 0.06616  loss_mask_4: 0.2009  loss_dice_4: 0.1764  loss_ce_5: 0.06803  loss_mask_5: 0.1977  loss_dice_5: 0.1717  loss_ce_6: 0.1125  loss_mask_6: 0.1992  loss_dice_6: 0.1786  loss_ce_7: 0.0882  loss_mask_7: 0.2024  loss_dice_7: 0.1806  loss_ce_8: 0.08968  loss_mask_8: 0.2039  loss_dice_8: 0.1732  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:08] d2.utils.events INFO:  eta: 0:47:22  iter: 27499  total_loss: 5.087  loss_ce: 0.1401  loss_mask: 0.2245  loss_dice: 0.1455  loss_ce_0: 0.07045  loss_mask_0: 0.2333  loss_dice_0: 0.1533  loss_ce_1: 0.05135  loss_mask_1: 0.2337  loss_dice_1: 0.1523  loss_ce_2: 0.04467  loss_mask_2: 0.224  loss_dice_2: 0.1488  loss_ce_3: 0.04203  loss_mask_3: 0.2283  loss_dice_3: 0.1578  loss_ce_4: 0.07122  loss_mask_4: 0.2291  loss_dice_4: 0.1535  loss_ce_5: 0.04326  loss_mask_5: 0.2306  loss_dice_5: 0.1502  loss_ce_6: 0.1337  loss_mask_6: 0.235  loss_dice_6: 0.1513  loss_ce_7: 0.09778  loss_mask_7: 0.2422  loss_dice_7: 0.1555  loss_ce_8: 0.1053  loss_mask_8: 0.2362  loss_dice_8: 0.1493  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:11] d2.utils.events INFO:  eta: 0:47:19  iter: 27519  total_loss: 4.203  loss_ce: 0.09902  loss_mask: 0.2017  loss_dice: 0.1485  loss_ce_0: 0.05351  loss_mask_0: 0.1932  loss_dice_0: 0.1453  loss_ce_1: 0.04867  loss_mask_1: 0.1993  loss_dice_1: 0.1442  loss_ce_2: 0.05433  loss_mask_2: 0.1951  loss_dice_2: 0.1476  loss_ce_3: 0.05514  loss_mask_3: 0.1922  loss_dice_3: 0.1523  loss_ce_4: 0.06567  loss_mask_4: 0.1902  loss_dice_4: 0.1425  loss_ce_5: 0.05491  loss_mask_5: 0.184  loss_dice_5: 0.1468  loss_ce_6: 0.08237  loss_mask_6: 0.1953  loss_dice_6: 0.1523  loss_ce_7: 0.08278  loss_mask_7: 0.1891  loss_dice_7: 0.1545  loss_ce_8: 0.07032  loss_mask_8: 0.1925  loss_dice_8: 0.1525  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:13] d2.utils.events INFO:  eta: 0:47:15  iter: 27539  total_loss: 5.745  loss_ce: 0.1465  loss_mask: 0.1785  loss_dice: 0.1437  loss_ce_0: 0.07881  loss_mask_0: 0.1752  loss_dice_0: 0.1431  loss_ce_1: 0.2358  loss_mask_1: 0.1716  loss_dice_1: 0.1433  loss_ce_2: 0.2095  loss_mask_2: 0.1702  loss_dice_2: 0.1436  loss_ce_3: 0.1912  loss_mask_3: 0.1604  loss_dice_3: 0.1397  loss_ce_4: 0.2386  loss_mask_4: 0.1654  loss_dice_4: 0.1395  loss_ce_5: 0.1998  loss_mask_5: 0.1751  loss_dice_5: 0.151  loss_ce_6: 0.1702  loss_mask_6: 0.1686  loss_dice_6: 0.1484  loss_ce_7: 0.1836  loss_mask_7: 0.1698  loss_dice_7: 0.1424  loss_ce_8: 0.1747  loss_mask_8: 0.1765  loss_dice_8: 0.1424  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:52:16] d2.utils.events INFO:  eta: 0:47:13  iter: 27559  total_loss: 5.313  loss_ce: 0.1281  loss_mask: 0.2059  loss_dice: 0.1538  loss_ce_0: 0.05049  loss_mask_0: 0.2145  loss_dice_0: 0.1521  loss_ce_1: 0.09212  loss_mask_1: 0.2211  loss_dice_1: 0.1573  loss_ce_2: 0.1788  loss_mask_2: 0.2101  loss_dice_2: 0.1519  loss_ce_3: 0.09472  loss_mask_3: 0.2156  loss_dice_3: 0.1552  loss_ce_4: 0.1264  loss_mask_4: 0.2189  loss_dice_4: 0.1482  loss_ce_5: 0.1802  loss_mask_5: 0.2195  loss_dice_5: 0.1561  loss_ce_6: 0.1328  loss_mask_6: 0.2263  loss_dice_6: 0.1565  loss_ce_7: 0.1183  loss_mask_7: 0.2206  loss_dice_7: 0.1487  loss_ce_8: 0.1591  loss_mask_8: 0.2132  loss_dice_8: 0.1536  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:52:18] d2.utils.events INFO:  eta: 0:47:10  iter: 27579  total_loss: 4.576  loss_ce: 0.1213  loss_mask: 0.1962  loss_dice: 0.1416  loss_ce_0: 0.0522  loss_mask_0: 0.202  loss_dice_0: 0.1344  loss_ce_1: 0.0986  loss_mask_1: 0.2009  loss_dice_1: 0.1376  loss_ce_2: 0.1012  loss_mask_2: 0.1919  loss_dice_2: 0.1406  loss_ce_3: 0.1057  loss_mask_3: 0.2041  loss_dice_3: 0.1381  loss_ce_4: 0.09566  loss_mask_4: 0.2064  loss_dice_4: 0.1363  loss_ce_5: 0.09688  loss_mask_5: 0.1999  loss_dice_5: 0.1392  loss_ce_6: 0.09878  loss_mask_6: 0.1995  loss_dice_6: 0.1368  loss_ce_7: 0.09619  loss_mask_7: 0.2024  loss_dice_7: 0.1376  loss_ce_8: 0.1061  loss_mask_8: 0.1984  loss_dice_8: 0.1304  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:52:21] d2.utils.events INFO:  eta: 0:47:08  iter: 27599  total_loss: 4.979  loss_ce: 0.1172  loss_mask: 0.2096  loss_dice: 0.1819  loss_ce_0: 0.05319  loss_mask_0: 0.2155  loss_dice_0: 0.1742  loss_ce_1: 0.1491  loss_mask_1: 0.2142  loss_dice_1: 0.1779  loss_ce_2: 0.1065  loss_mask_2: 0.214  loss_dice_2: 0.1699  loss_ce_3: 0.1144  loss_mask_3: 0.2188  loss_dice_3: 0.172  loss_ce_4: 0.1439  loss_mask_4: 0.2195  loss_dice_4: 0.1773  loss_ce_5: 0.1193  loss_mask_5: 0.2089  loss_dice_5: 0.1664  loss_ce_6: 0.1188  loss_mask_6: 0.2166  loss_dice_6: 0.1732  loss_ce_7: 0.1475  loss_mask_7: 0.2135  loss_dice_7: 0.1767  loss_ce_8: 0.1079  loss_mask_8: 0.2208  loss_dice_8: 0.1791  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:23] d2.utils.events INFO:  eta: 0:47:06  iter: 27619  total_loss: 4.827  loss_ce: 0.1166  loss_mask: 0.1601  loss_dice: 0.1462  loss_ce_0: 0.06326  loss_mask_0: 0.1625  loss_dice_0: 0.1436  loss_ce_1: 0.1101  loss_mask_1: 0.1613  loss_dice_1: 0.1491  loss_ce_2: 0.1006  loss_mask_2: 0.1718  loss_dice_2: 0.1565  loss_ce_3: 0.09692  loss_mask_3: 0.1655  loss_dice_3: 0.1515  loss_ce_4: 0.1047  loss_mask_4: 0.1654  loss_dice_4: 0.1542  loss_ce_5: 0.1072  loss_mask_5: 0.1612  loss_dice_5: 0.148  loss_ce_6: 0.1125  loss_mask_6: 0.1664  loss_dice_6: 0.1625  loss_ce_7: 0.1097  loss_mask_7: 0.1584  loss_dice_7: 0.1544  loss_ce_8: 0.1032  loss_mask_8: 0.155  loss_dice_8: 0.1508  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:26] d2.utils.events INFO:  eta: 0:47:03  iter: 27639  total_loss: 5.147  loss_ce: 0.1275  loss_mask: 0.1814  loss_dice: 0.1594  loss_ce_0: 0.07148  loss_mask_0: 0.1909  loss_dice_0: 0.1633  loss_ce_1: 0.1299  loss_mask_1: 0.1832  loss_dice_1: 0.1643  loss_ce_2: 0.1441  loss_mask_2: 0.1792  loss_dice_2: 0.1621  loss_ce_3: 0.1273  loss_mask_3: 0.1858  loss_dice_3: 0.1626  loss_ce_4: 0.1335  loss_mask_4: 0.1631  loss_dice_4: 0.1616  loss_ce_5: 0.1455  loss_mask_5: 0.1743  loss_dice_5: 0.1705  loss_ce_6: 0.1305  loss_mask_6: 0.1707  loss_dice_6: 0.1636  loss_ce_7: 0.1289  loss_mask_7: 0.1789  loss_dice_7: 0.1629  loss_ce_8: 0.1375  loss_mask_8: 0.1743  loss_dice_8: 0.1601  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:28] d2.utils.events INFO:  eta: 0:47:00  iter: 27659  total_loss: 4.477  loss_ce: 0.1136  loss_mask: 0.1555  loss_dice: 0.1346  loss_ce_0: 0.06437  loss_mask_0: 0.16  loss_dice_0: 0.1328  loss_ce_1: 0.1221  loss_mask_1: 0.1663  loss_dice_1: 0.1377  loss_ce_2: 0.1279  loss_mask_2: 0.1591  loss_dice_2: 0.1434  loss_ce_3: 0.1348  loss_mask_3: 0.1586  loss_dice_3: 0.1372  loss_ce_4: 0.1324  loss_mask_4: 0.1575  loss_dice_4: 0.1257  loss_ce_5: 0.128  loss_mask_5: 0.1483  loss_dice_5: 0.1414  loss_ce_6: 0.07612  loss_mask_6: 0.1682  loss_dice_6: 0.1334  loss_ce_7: 0.1156  loss_mask_7: 0.1596  loss_dice_7: 0.1388  loss_ce_8: 0.1006  loss_mask_8: 0.1581  loss_dice_8: 0.1332  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:31] d2.utils.events INFO:  eta: 0:47:00  iter: 27679  total_loss: 4.746  loss_ce: 0.08938  loss_mask: 0.1941  loss_dice: 0.1679  loss_ce_0: 0.05871  loss_mask_0: 0.1862  loss_dice_0: 0.1798  loss_ce_1: 0.1013  loss_mask_1: 0.195  loss_dice_1: 0.1719  loss_ce_2: 0.1084  loss_mask_2: 0.1902  loss_dice_2: 0.1677  loss_ce_3: 0.109  loss_mask_3: 0.1999  loss_dice_3: 0.1641  loss_ce_4: 0.1122  loss_mask_4: 0.1886  loss_dice_4: 0.1728  loss_ce_5: 0.1006  loss_mask_5: 0.1889  loss_dice_5: 0.1678  loss_ce_6: 0.1058  loss_mask_6: 0.1871  loss_dice_6: 0.1667  loss_ce_7: 0.1093  loss_mask_7: 0.1933  loss_dice_7: 0.174  loss_ce_8: 0.08792  loss_mask_8: 0.1962  loss_dice_8: 0.1737  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:33] d2.utils.events INFO:  eta: 0:47:00  iter: 27699  total_loss: 4.689  loss_ce: 0.09806  loss_mask: 0.2013  loss_dice: 0.1295  loss_ce_0: 0.05505  loss_mask_0: 0.2093  loss_dice_0: 0.1335  loss_ce_1: 0.1037  loss_mask_1: 0.2064  loss_dice_1: 0.1347  loss_ce_2: 0.108  loss_mask_2: 0.2057  loss_dice_2: 0.1292  loss_ce_3: 0.1068  loss_mask_3: 0.1928  loss_dice_3: 0.1322  loss_ce_4: 0.105  loss_mask_4: 0.1986  loss_dice_4: 0.1292  loss_ce_5: 0.09358  loss_mask_5: 0.2025  loss_dice_5: 0.129  loss_ce_6: 0.1054  loss_mask_6: 0.2072  loss_dice_6: 0.1354  loss_ce_7: 0.1026  loss_mask_7: 0.2055  loss_dice_7: 0.1302  loss_ce_8: 0.09436  loss_mask_8: 0.1984  loss_dice_8: 0.1348  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:52:36] d2.utils.events INFO:  eta: 0:47:00  iter: 27719  total_loss: 4.701  loss_ce: 0.1185  loss_mask: 0.1912  loss_dice: 0.1579  loss_ce_0: 0.05255  loss_mask_0: 0.2009  loss_dice_0: 0.152  loss_ce_1: 0.1115  loss_mask_1: 0.1943  loss_dice_1: 0.1553  loss_ce_2: 0.12  loss_mask_2: 0.1923  loss_dice_2: 0.154  loss_ce_3: 0.1175  loss_mask_3: 0.194  loss_dice_3: 0.1576  loss_ce_4: 0.1111  loss_mask_4: 0.2013  loss_dice_4: 0.1544  loss_ce_5: 0.1019  loss_mask_5: 0.1946  loss_dice_5: 0.155  loss_ce_6: 0.1003  loss_mask_6: 0.1989  loss_dice_6: 0.1618  loss_ce_7: 0.1091  loss_mask_7: 0.2081  loss_dice_7: 0.1522  loss_ce_8: 0.1005  loss_mask_8: 0.1956  loss_dice_8: 0.1546  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:39] d2.utils.events INFO:  eta: 0:46:57  iter: 27739  total_loss: 5.202  loss_ce: 0.1157  loss_mask: 0.2143  loss_dice: 0.1798  loss_ce_0: 0.04985  loss_mask_0: 0.2178  loss_dice_0: 0.1838  loss_ce_1: 0.1035  loss_mask_1: 0.2197  loss_dice_1: 0.1787  loss_ce_2: 0.1201  loss_mask_2: 0.2242  loss_dice_2: 0.1811  loss_ce_3: 0.1207  loss_mask_3: 0.2229  loss_dice_3: 0.1878  loss_ce_4: 0.1049  loss_mask_4: 0.2067  loss_dice_4: 0.1845  loss_ce_5: 0.1003  loss_mask_5: 0.23  loss_dice_5: 0.1837  loss_ce_6: 0.09484  loss_mask_6: 0.2125  loss_dice_6: 0.1841  loss_ce_7: 0.1026  loss_mask_7: 0.2214  loss_dice_7: 0.1867  loss_ce_8: 0.1151  loss_mask_8: 0.215  loss_dice_8: 0.1861  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:41] d2.utils.events INFO:  eta: 0:46:55  iter: 27759  total_loss: 4.604  loss_ce: 0.02999  loss_mask: 0.1655  loss_dice: 0.1672  loss_ce_0: 0.04732  loss_mask_0: 0.1706  loss_dice_0: 0.1777  loss_ce_1: 0.0317  loss_mask_1: 0.1639  loss_dice_1: 0.1621  loss_ce_2: 0.02828  loss_mask_2: 0.1774  loss_dice_2: 0.1754  loss_ce_3: 0.02931  loss_mask_3: 0.1833  loss_dice_3: 0.1764  loss_ce_4: 0.03466  loss_mask_4: 0.1777  loss_dice_4: 0.1679  loss_ce_5: 0.03532  loss_mask_5: 0.1615  loss_dice_5: 0.1664  loss_ce_6: 0.03349  loss_mask_6: 0.1683  loss_dice_6: 0.1705  loss_ce_7: 0.03294  loss_mask_7: 0.1681  loss_dice_7: 0.1674  loss_ce_8: 0.0378  loss_mask_8: 0.1705  loss_dice_8: 0.1727  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:44] d2.utils.events INFO:  eta: 0:46:51  iter: 27779  total_loss: 5.189  loss_ce: 0.09821  loss_mask: 0.1844  loss_dice: 0.234  loss_ce_0: 0.04246  loss_mask_0: 0.1857  loss_dice_0: 0.2227  loss_ce_1: 0.0889  loss_mask_1: 0.1836  loss_dice_1: 0.2226  loss_ce_2: 0.09107  loss_mask_2: 0.1874  loss_dice_2: 0.2191  loss_ce_3: 0.1148  loss_mask_3: 0.1797  loss_dice_3: 0.2265  loss_ce_4: 0.07719  loss_mask_4: 0.1816  loss_dice_4: 0.2266  loss_ce_5: 0.0809  loss_mask_5: 0.1886  loss_dice_5: 0.2232  loss_ce_6: 0.08944  loss_mask_6: 0.1854  loss_dice_6: 0.2249  loss_ce_7: 0.08975  loss_mask_7: 0.1842  loss_dice_7: 0.2332  loss_ce_8: 0.1075  loss_mask_8: 0.1866  loss_dice_8: 0.2284  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:46] d2.utils.events INFO:  eta: 0:46:47  iter: 27799  total_loss: 5.004  loss_ce: 0.09721  loss_mask: 0.1887  loss_dice: 0.1662  loss_ce_0: 0.04107  loss_mask_0: 0.177  loss_dice_0: 0.1705  loss_ce_1: 0.116  loss_mask_1: 0.1837  loss_dice_1: 0.1621  loss_ce_2: 0.1064  loss_mask_2: 0.1688  loss_dice_2: 0.1642  loss_ce_3: 0.1139  loss_mask_3: 0.1842  loss_dice_3: 0.1627  loss_ce_4: 0.1045  loss_mask_4: 0.1815  loss_dice_4: 0.1662  loss_ce_5: 0.09669  loss_mask_5: 0.1724  loss_dice_5: 0.1667  loss_ce_6: 0.08914  loss_mask_6: 0.1787  loss_dice_6: 0.1669  loss_ce_7: 0.09551  loss_mask_7: 0.1799  loss_dice_7: 0.1713  loss_ce_8: 0.1004  loss_mask_8: 0.1793  loss_dice_8: 0.1654  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:49] d2.utils.events INFO:  eta: 0:46:44  iter: 27819  total_loss: 4.768  loss_ce: 0.1215  loss_mask: 0.2064  loss_dice: 0.1442  loss_ce_0: 0.04235  loss_mask_0: 0.2026  loss_dice_0: 0.1414  loss_ce_1: 0.1224  loss_mask_1: 0.2001  loss_dice_1: 0.1406  loss_ce_2: 0.1256  loss_mask_2: 0.1997  loss_dice_2: 0.1361  loss_ce_3: 0.1206  loss_mask_3: 0.2022  loss_dice_3: 0.1423  loss_ce_4: 0.1212  loss_mask_4: 0.2009  loss_dice_4: 0.1339  loss_ce_5: 0.114  loss_mask_5: 0.2012  loss_dice_5: 0.1387  loss_ce_6: 0.1245  loss_mask_6: 0.2043  loss_dice_6: 0.1399  loss_ce_7: 0.1172  loss_mask_7: 0.1945  loss_dice_7: 0.1461  loss_ce_8: 0.1179  loss_mask_8: 0.1957  loss_dice_8: 0.1377  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:51] d2.utils.events INFO:  eta: 0:46:42  iter: 27839  total_loss: 4.781  loss_ce: 0.1214  loss_mask: 0.1646  loss_dice: 0.1611  loss_ce_0: 0.04442  loss_mask_0: 0.175  loss_dice_0: 0.1638  loss_ce_1: 0.1154  loss_mask_1: 0.1654  loss_dice_1: 0.1701  loss_ce_2: 0.1207  loss_mask_2: 0.1607  loss_dice_2: 0.1649  loss_ce_3: 0.1238  loss_mask_3: 0.1758  loss_dice_3: 0.1657  loss_ce_4: 0.1225  loss_mask_4: 0.172  loss_dice_4: 0.1692  loss_ce_5: 0.1232  loss_mask_5: 0.1672  loss_dice_5: 0.1648  loss_ce_6: 0.116  loss_mask_6: 0.1709  loss_dice_6: 0.1694  loss_ce_7: 0.1159  loss_mask_7: 0.1675  loss_dice_7: 0.1705  loss_ce_8: 0.1211  loss_mask_8: 0.1663  loss_dice_8: 0.1594  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:54] d2.utils.events INFO:  eta: 0:46:37  iter: 27859  total_loss: 4.919  loss_ce: 0.09389  loss_mask: 0.1658  loss_dice: 0.1868  loss_ce_0: 0.04401  loss_mask_0: 0.1737  loss_dice_0: 0.19  loss_ce_1: 0.1026  loss_mask_1: 0.1645  loss_dice_1: 0.1875  loss_ce_2: 0.1091  loss_mask_2: 0.1763  loss_dice_2: 0.1876  loss_ce_3: 0.1077  loss_mask_3: 0.1791  loss_dice_3: 0.1914  loss_ce_4: 0.1062  loss_mask_4: 0.1757  loss_dice_4: 0.1882  loss_ce_5: 0.1093  loss_mask_5: 0.1615  loss_dice_5: 0.1949  loss_ce_6: 0.09176  loss_mask_6: 0.1633  loss_dice_6: 0.1833  loss_ce_7: 0.09765  loss_mask_7: 0.1758  loss_dice_7: 0.1869  loss_ce_8: 0.09565  loss_mask_8: 0.1677  loss_dice_8: 0.1895  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:52:56] d2.utils.events INFO:  eta: 0:46:33  iter: 27879  total_loss: 4.588  loss_ce: 0.03727  loss_mask: 0.2018  loss_dice: 0.2028  loss_ce_0: 0.04279  loss_mask_0: 0.202  loss_dice_0: 0.1972  loss_ce_1: 0.03912  loss_mask_1: 0.1958  loss_dice_1: 0.1954  loss_ce_2: 0.05576  loss_mask_2: 0.2027  loss_dice_2: 0.2019  loss_ce_3: 0.03458  loss_mask_3: 0.1982  loss_dice_3: 0.1942  loss_ce_4: 0.04208  loss_mask_4: 0.1973  loss_dice_4: 0.1982  loss_ce_5: 0.06507  loss_mask_5: 0.2092  loss_dice_5: 0.1935  loss_ce_6: 0.03585  loss_mask_6: 0.2127  loss_dice_6: 0.1903  loss_ce_7: 0.04569  loss_mask_7: 0.203  loss_dice_7: 0.1919  loss_ce_8: 0.05273  loss_mask_8: 0.205  loss_dice_8: 0.2063  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:52:59] d2.utils.events INFO:  eta: 0:46:31  iter: 27899  total_loss: 5.275  loss_ce: 0.04154  loss_mask: 0.2005  loss_dice: 0.2289  loss_ce_0: 0.03901  loss_mask_0: 0.2087  loss_dice_0: 0.2301  loss_ce_1: 0.04344  loss_mask_1: 0.1983  loss_dice_1: 0.2275  loss_ce_2: 0.03744  loss_mask_2: 0.2002  loss_dice_2: 0.227  loss_ce_3: 0.03765  loss_mask_3: 0.2021  loss_dice_3: 0.225  loss_ce_4: 0.03916  loss_mask_4: 0.2063  loss_dice_4: 0.2291  loss_ce_5: 0.03933  loss_mask_5: 0.2  loss_dice_5: 0.2324  loss_ce_6: 0.04038  loss_mask_6: 0.2019  loss_dice_6: 0.217  loss_ce_7: 0.04134  loss_mask_7: 0.2064  loss_dice_7: 0.2247  loss_ce_8: 0.04127  loss_mask_8: 0.1992  loss_dice_8: 0.2228  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:53:01] d2.utils.events INFO:  eta: 0:46:27  iter: 27919  total_loss: 5.364  loss_ce: 0.04271  loss_mask: 0.221  loss_dice: 0.1685  loss_ce_0: 0.03613  loss_mask_0: 0.2256  loss_dice_0: 0.1645  loss_ce_1: 0.04496  loss_mask_1: 0.2309  loss_dice_1: 0.1654  loss_ce_2: 0.04585  loss_mask_2: 0.222  loss_dice_2: 0.1664  loss_ce_3: 0.04558  loss_mask_3: 0.2279  loss_dice_3: 0.1675  loss_ce_4: 0.04304  loss_mask_4: 0.2256  loss_dice_4: 0.1579  loss_ce_5: 0.0475  loss_mask_5: 0.2276  loss_dice_5: 0.1706  loss_ce_6: 0.04234  loss_mask_6: 0.2253  loss_dice_6: 0.1668  loss_ce_7: 0.04571  loss_mask_7: 0.2262  loss_dice_7: 0.1679  loss_ce_8: 0.04905  loss_mask_8: 0.2181  loss_dice_8: 0.1601  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:04] d2.utils.events INFO:  eta: 0:46:23  iter: 27939  total_loss: 5.058  loss_ce: 0.1003  loss_mask: 0.1847  loss_dice: 0.1306  loss_ce_0: 0.03527  loss_mask_0: 0.1894  loss_dice_0: 0.1389  loss_ce_1: 0.09926  loss_mask_1: 0.1887  loss_dice_1: 0.1331  loss_ce_2: 0.08996  loss_mask_2: 0.1902  loss_dice_2: 0.1415  loss_ce_3: 0.1064  loss_mask_3: 0.1901  loss_dice_3: 0.1429  loss_ce_4: 0.08553  loss_mask_4: 0.1854  loss_dice_4: 0.1445  loss_ce_5: 0.0849  loss_mask_5: 0.196  loss_dice_5: 0.1342  loss_ce_6: 0.1032  loss_mask_6: 0.193  loss_dice_6: 0.1404  loss_ce_7: 0.09406  loss_mask_7: 0.195  loss_dice_7: 0.144  loss_ce_8: 0.1002  loss_mask_8: 0.1839  loss_dice_8: 0.1393  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:53:06] d2.utils.events INFO:  eta: 0:46:19  iter: 27959  total_loss: 5.334  loss_ce: 0.1109  loss_mask: 0.2193  loss_dice: 0.1689  loss_ce_0: 0.03876  loss_mask_0: 0.2294  loss_dice_0: 0.1722  loss_ce_1: 0.1153  loss_mask_1: 0.2076  loss_dice_1: 0.1662  loss_ce_2: 0.1046  loss_mask_2: 0.2186  loss_dice_2: 0.1671  loss_ce_3: 0.1177  loss_mask_3: 0.2151  loss_dice_3: 0.1668  loss_ce_4: 0.1088  loss_mask_4: 0.2152  loss_dice_4: 0.1662  loss_ce_5: 0.09884  loss_mask_5: 0.2198  loss_dice_5: 0.1702  loss_ce_6: 0.1149  loss_mask_6: 0.223  loss_dice_6: 0.1717  loss_ce_7: 0.1105  loss_mask_7: 0.2187  loss_dice_7: 0.1686  loss_ce_8: 0.1076  loss_mask_8: 0.2133  loss_dice_8: 0.1641  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:53:09] d2.utils.events INFO:  eta: 0:46:15  iter: 27979  total_loss: 4.878  loss_ce: 0.08498  loss_mask: 0.204  loss_dice: 0.1792  loss_ce_0: 0.0393  loss_mask_0: 0.2007  loss_dice_0: 0.1832  loss_ce_1: 0.07027  loss_mask_1: 0.197  loss_dice_1: 0.1761  loss_ce_2: 0.07909  loss_mask_2: 0.1944  loss_dice_2: 0.172  loss_ce_3: 0.0947  loss_mask_3: 0.201  loss_dice_3: 0.1817  loss_ce_4: 0.07373  loss_mask_4: 0.1978  loss_dice_4: 0.1834  loss_ce_5: 0.07955  loss_mask_5: 0.1986  loss_dice_5: 0.1778  loss_ce_6: 0.08794  loss_mask_6: 0.1972  loss_dice_6: 0.1816  loss_ce_7: 0.07645  loss_mask_7: 0.2015  loss_dice_7: 0.1774  loss_ce_8: 0.08021  loss_mask_8: 0.189  loss_dice_8: 0.1755  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:11] d2.utils.events INFO:  eta: 0:46:10  iter: 27999  total_loss: 5.312  loss_ce: 0.1055  loss_mask: 0.1332  loss_dice: 0.196  loss_ce_0: 0.06744  loss_mask_0: 0.1317  loss_dice_0: 0.1924  loss_ce_1: 0.1153  loss_mask_1: 0.1235  loss_dice_1: 0.1898  loss_ce_2: 0.1069  loss_mask_2: 0.1345  loss_dice_2: 0.2088  loss_ce_3: 0.1179  loss_mask_3: 0.1316  loss_dice_3: 0.1996  loss_ce_4: 0.1104  loss_mask_4: 0.1347  loss_dice_4: 0.1931  loss_ce_5: 0.12  loss_mask_5: 0.1389  loss_dice_5: 0.1961  loss_ce_6: 0.1065  loss_mask_6: 0.1347  loss_dice_6: 0.2046  loss_ce_7: 0.1141  loss_mask_7: 0.1312  loss_dice_7: 0.1998  loss_ce_8: 0.1144  loss_mask_8: 0.1356  loss_dice_8: 0.2045  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:14] d2.utils.events INFO:  eta: 0:46:07  iter: 28019  total_loss: 4.756  loss_ce: 0.1057  loss_mask: 0.1906  loss_dice: 0.1495  loss_ce_0: 0.06629  loss_mask_0: 0.1978  loss_dice_0: 0.1446  loss_ce_1: 0.1126  loss_mask_1: 0.1985  loss_dice_1: 0.15  loss_ce_2: 0.115  loss_mask_2: 0.1945  loss_dice_2: 0.1464  loss_ce_3: 0.116  loss_mask_3: 0.1977  loss_dice_3: 0.1505  loss_ce_4: 0.1133  loss_mask_4: 0.1944  loss_dice_4: 0.1504  loss_ce_5: 0.1235  loss_mask_5: 0.1982  loss_dice_5: 0.144  loss_ce_6: 0.1013  loss_mask_6: 0.1981  loss_dice_6: 0.1447  loss_ce_7: 0.1161  loss_mask_7: 0.1977  loss_dice_7: 0.1512  loss_ce_8: 0.1155  loss_mask_8: 0.1921  loss_dice_8: 0.1435  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:16] d2.utils.events INFO:  eta: 0:46:07  iter: 28039  total_loss: 4.505  loss_ce: 0.1076  loss_mask: 0.1938  loss_dice: 0.1298  loss_ce_0: 0.04539  loss_mask_0: 0.1955  loss_dice_0: 0.1312  loss_ce_1: 0.09273  loss_mask_1: 0.1888  loss_dice_1: 0.1347  loss_ce_2: 0.07813  loss_mask_2: 0.1836  loss_dice_2: 0.1301  loss_ce_3: 0.09958  loss_mask_3: 0.1937  loss_dice_3: 0.1328  loss_ce_4: 0.07553  loss_mask_4: 0.1947  loss_dice_4: 0.1297  loss_ce_5: 0.08092  loss_mask_5: 0.1907  loss_dice_5: 0.1305  loss_ce_6: 0.1017  loss_mask_6: 0.1943  loss_dice_6: 0.1307  loss_ce_7: 0.08778  loss_mask_7: 0.1859  loss_dice_7: 0.13  loss_ce_8: 0.07881  loss_mask_8: 0.1887  loss_dice_8: 0.1318  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:19] d2.utils.events INFO:  eta: 0:46:05  iter: 28059  total_loss: 4.992  loss_ce: 0.1112  loss_mask: 0.234  loss_dice: 0.1372  loss_ce_0: 0.04439  loss_mask_0: 0.2319  loss_dice_0: 0.1379  loss_ce_1: 0.09258  loss_mask_1: 0.2334  loss_dice_1: 0.136  loss_ce_2: 0.09419  loss_mask_2: 0.2269  loss_dice_2: 0.1402  loss_ce_3: 0.1139  loss_mask_3: 0.2289  loss_dice_3: 0.1399  loss_ce_4: 0.07863  loss_mask_4: 0.2289  loss_dice_4: 0.1347  loss_ce_5: 0.1011  loss_mask_5: 0.2325  loss_dice_5: 0.1337  loss_ce_6: 0.1084  loss_mask_6: 0.2255  loss_dice_6: 0.1428  loss_ce_7: 0.09708  loss_mask_7: 0.2322  loss_dice_7: 0.1398  loss_ce_8: 0.09561  loss_mask_8: 0.2283  loss_dice_8: 0.1428  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:21] d2.utils.events INFO:  eta: 0:46:02  iter: 28079  total_loss: 4.795  loss_ce: 0.09089  loss_mask: 0.1655  loss_dice: 0.1697  loss_ce_0: 0.07921  loss_mask_0: 0.1553  loss_dice_0: 0.1618  loss_ce_1: 0.08841  loss_mask_1: 0.1561  loss_dice_1: 0.1655  loss_ce_2: 0.07813  loss_mask_2: 0.1562  loss_dice_2: 0.1617  loss_ce_3: 0.09547  loss_mask_3: 0.149  loss_dice_3: 0.1551  loss_ce_4: 0.06951  loss_mask_4: 0.1584  loss_dice_4: 0.1613  loss_ce_5: 0.07506  loss_mask_5: 0.1608  loss_dice_5: 0.1628  loss_ce_6: 0.08319  loss_mask_6: 0.1695  loss_dice_6: 0.171  loss_ce_7: 0.08765  loss_mask_7: 0.1655  loss_dice_7: 0.1649  loss_ce_8: 0.07537  loss_mask_8: 0.1625  loss_dice_8: 0.1651  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:24] d2.utils.events INFO:  eta: 0:46:00  iter: 28099  total_loss: 5.061  loss_ce: 0.05713  loss_mask: 0.2164  loss_dice: 0.1676  loss_ce_0: 0.05137  loss_mask_0: 0.2116  loss_dice_0: 0.1715  loss_ce_1: 0.06823  loss_mask_1: 0.204  loss_dice_1: 0.166  loss_ce_2: 0.05358  loss_mask_2: 0.2104  loss_dice_2: 0.1747  loss_ce_3: 0.06339  loss_mask_3: 0.2089  loss_dice_3: 0.1762  loss_ce_4: 0.06738  loss_mask_4: 0.2108  loss_dice_4: 0.172  loss_ce_5: 0.04591  loss_mask_5: 0.2088  loss_dice_5: 0.1808  loss_ce_6: 0.05051  loss_mask_6: 0.211  loss_dice_6: 0.1689  loss_ce_7: 0.065  loss_mask_7: 0.207  loss_dice_7: 0.172  loss_ce_8: 0.04849  loss_mask_8: 0.2107  loss_dice_8: 0.1736  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:53:27] d2.utils.events INFO:  eta: 0:45:57  iter: 28119  total_loss: 5.467  loss_ce: 0.07287  loss_mask: 0.1988  loss_dice: 0.1401  loss_ce_0: 0.05001  loss_mask_0: 0.186  loss_dice_0: 0.1278  loss_ce_1: 0.09286  loss_mask_1: 0.1943  loss_dice_1: 0.1321  loss_ce_2: 0.03067  loss_mask_2: 0.1915  loss_dice_2: 0.1305  loss_ce_3: 0.0637  loss_mask_3: 0.204  loss_dice_3: 0.1361  loss_ce_4: 0.0993  loss_mask_4: 0.1945  loss_dice_4: 0.1411  loss_ce_5: 0.06949  loss_mask_5: 0.1994  loss_dice_5: 0.1357  loss_ce_6: 0.09536  loss_mask_6: 0.1985  loss_dice_6: 0.1358  loss_ce_7: 0.08377  loss_mask_7: 0.1965  loss_dice_7: 0.1378  loss_ce_8: 0.04716  loss_mask_8: 0.1978  loss_dice_8: 0.1371  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:53:29] d2.utils.events INFO:  eta: 0:45:54  iter: 28139  total_loss: 5.172  loss_ce: 0.08864  loss_mask: 0.1936  loss_dice: 0.1531  loss_ce_0: 0.08531  loss_mask_0: 0.1965  loss_dice_0: 0.1556  loss_ce_1: 0.08845  loss_mask_1: 0.1934  loss_dice_1: 0.1564  loss_ce_2: 0.1031  loss_mask_2: 0.1945  loss_dice_2: 0.1553  loss_ce_3: 0.09245  loss_mask_3: 0.1923  loss_dice_3: 0.1554  loss_ce_4: 0.1145  loss_mask_4: 0.1918  loss_dice_4: 0.1624  loss_ce_5: 0.1019  loss_mask_5: 0.1933  loss_dice_5: 0.1557  loss_ce_6: 0.09378  loss_mask_6: 0.1913  loss_dice_6: 0.1506  loss_ce_7: 0.1134  loss_mask_7: 0.1867  loss_dice_7: 0.1574  loss_ce_8: 0.08399  loss_mask_8: 0.1883  loss_dice_8: 0.1585  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:32] d2.utils.events INFO:  eta: 0:45:51  iter: 28159  total_loss: 5.414  loss_ce: 0.1369  loss_mask: 0.1646  loss_dice: 0.1924  loss_ce_0: 0.04744  loss_mask_0: 0.1719  loss_dice_0: 0.2028  loss_ce_1: 0.0818  loss_mask_1: 0.16  loss_dice_1: 0.1905  loss_ce_2: 0.09222  loss_mask_2: 0.1716  loss_dice_2: 0.2026  loss_ce_3: 0.1229  loss_mask_3: 0.164  loss_dice_3: 0.1934  loss_ce_4: 0.1018  loss_mask_4: 0.1653  loss_dice_4: 0.2064  loss_ce_5: 0.1494  loss_mask_5: 0.1742  loss_dice_5: 0.2017  loss_ce_6: 0.161  loss_mask_6: 0.1775  loss_dice_6: 0.1987  loss_ce_7: 0.09647  loss_mask_7: 0.1664  loss_dice_7: 0.1991  loss_ce_8: 0.1242  loss_mask_8: 0.1598  loss_dice_8: 0.2032  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:34] d2.utils.events INFO:  eta: 0:45:47  iter: 28179  total_loss: 4.74  loss_ce: 0.1172  loss_mask: 0.2191  loss_dice: 0.1258  loss_ce_0: 0.04778  loss_mask_0: 0.2199  loss_dice_0: 0.1338  loss_ce_1: 0.0942  loss_mask_1: 0.2182  loss_dice_1: 0.1304  loss_ce_2: 0.1319  loss_mask_2: 0.2212  loss_dice_2: 0.1308  loss_ce_3: 0.1117  loss_mask_3: 0.2256  loss_dice_3: 0.1277  loss_ce_4: 0.1076  loss_mask_4: 0.2222  loss_dice_4: 0.1354  loss_ce_5: 0.125  loss_mask_5: 0.2271  loss_dice_5: 0.1315  loss_ce_6: 0.1151  loss_mask_6: 0.2228  loss_dice_6: 0.1306  loss_ce_7: 0.1066  loss_mask_7: 0.2172  loss_dice_7: 0.1287  loss_ce_8: 0.1279  loss_mask_8: 0.2274  loss_dice_8: 0.1353  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:37] d2.utils.events INFO:  eta: 0:45:44  iter: 28199  total_loss: 4.666  loss_ce: 0.1183  loss_mask: 0.2047  loss_dice: 0.1504  loss_ce_0: 0.047  loss_mask_0: 0.2082  loss_dice_0: 0.1529  loss_ce_1: 0.09464  loss_mask_1: 0.2137  loss_dice_1: 0.1597  loss_ce_2: 0.1173  loss_mask_2: 0.2108  loss_dice_2: 0.1578  loss_ce_3: 0.1145  loss_mask_3: 0.2093  loss_dice_3: 0.1536  loss_ce_4: 0.1144  loss_mask_4: 0.209  loss_dice_4: 0.1541  loss_ce_5: 0.1178  loss_mask_5: 0.2163  loss_dice_5: 0.1563  loss_ce_6: 0.1218  loss_mask_6: 0.2189  loss_dice_6: 0.1548  loss_ce_7: 0.1157  loss_mask_7: 0.2143  loss_dice_7: 0.1508  loss_ce_8: 0.1206  loss_mask_8: 0.2114  loss_dice_8: 0.1575  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:39] d2.utils.events INFO:  eta: 0:45:41  iter: 28219  total_loss: 4.842  loss_ce: 0.128  loss_mask: 0.1691  loss_dice: 0.1509  loss_ce_0: 0.06485  loss_mask_0: 0.1734  loss_dice_0: 0.1527  loss_ce_1: 0.1035  loss_mask_1: 0.1652  loss_dice_1: 0.1499  loss_ce_2: 0.1183  loss_mask_2: 0.1664  loss_dice_2: 0.1469  loss_ce_3: 0.1324  loss_mask_3: 0.1701  loss_dice_3: 0.1446  loss_ce_4: 0.1053  loss_mask_4: 0.1738  loss_dice_4: 0.155  loss_ce_5: 0.1292  loss_mask_5: 0.1708  loss_dice_5: 0.1465  loss_ce_6: 0.1246  loss_mask_6: 0.1715  loss_dice_6: 0.1435  loss_ce_7: 0.1032  loss_mask_7: 0.1643  loss_dice_7: 0.1555  loss_ce_8: 0.1156  loss_mask_8: 0.1632  loss_dice_8: 0.1545  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:42] d2.utils.events INFO:  eta: 0:45:39  iter: 28239  total_loss: 4.985  loss_ce: 0.121  loss_mask: 0.1849  loss_dice: 0.1524  loss_ce_0: 0.06351  loss_mask_0: 0.1762  loss_dice_0: 0.1468  loss_ce_1: 0.1594  loss_mask_1: 0.1773  loss_dice_1: 0.153  loss_ce_2: 0.1316  loss_mask_2: 0.1882  loss_dice_2: 0.1552  loss_ce_3: 0.1229  loss_mask_3: 0.186  loss_dice_3: 0.1449  loss_ce_4: 0.1587  loss_mask_4: 0.182  loss_dice_4: 0.1514  loss_ce_5: 0.1282  loss_mask_5: 0.178  loss_dice_5: 0.1487  loss_ce_6: 0.112  loss_mask_6: 0.1844  loss_dice_6: 0.1487  loss_ce_7: 0.1612  loss_mask_7: 0.1887  loss_dice_7: 0.152  loss_ce_8: 0.1322  loss_mask_8: 0.178  loss_dice_8: 0.1513  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:44] d2.utils.events INFO:  eta: 0:45:37  iter: 28259  total_loss: 5.172  loss_ce: 0.1089  loss_mask: 0.2038  loss_dice: 0.2365  loss_ce_0: 0.05149  loss_mask_0: 0.2025  loss_dice_0: 0.2329  loss_ce_1: 0.1224  loss_mask_1: 0.2036  loss_dice_1: 0.2332  loss_ce_2: 0.1221  loss_mask_2: 0.1999  loss_dice_2: 0.224  loss_ce_3: 0.1146  loss_mask_3: 0.2105  loss_dice_3: 0.2284  loss_ce_4: 0.1164  loss_mask_4: 0.2094  loss_dice_4: 0.2272  loss_ce_5: 0.1234  loss_mask_5: 0.2085  loss_dice_5: 0.2212  loss_ce_6: 0.1078  loss_mask_6: 0.1969  loss_dice_6: 0.2191  loss_ce_7: 0.1145  loss_mask_7: 0.2069  loss_dice_7: 0.2298  loss_ce_8: 0.1128  loss_mask_8: 0.2145  loss_dice_8: 0.2374  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:47] d2.utils.events INFO:  eta: 0:45:37  iter: 28279  total_loss: 5.432  loss_ce: 0.1235  loss_mask: 0.1837  loss_dice: 0.2155  loss_ce_0: 0.05046  loss_mask_0: 0.1826  loss_dice_0: 0.2083  loss_ce_1: 0.1021  loss_mask_1: 0.1786  loss_dice_1: 0.2098  loss_ce_2: 0.09987  loss_mask_2: 0.1912  loss_dice_2: 0.2025  loss_ce_3: 0.1167  loss_mask_3: 0.1788  loss_dice_3: 0.2086  loss_ce_4: 0.08371  loss_mask_4: 0.1813  loss_dice_4: 0.2077  loss_ce_5: 0.1223  loss_mask_5: 0.1816  loss_dice_5: 0.2109  loss_ce_6: 0.1204  loss_mask_6: 0.185  loss_dice_6: 0.2119  loss_ce_7: 0.08406  loss_mask_7: 0.1919  loss_dice_7: 0.2083  loss_ce_8: 0.1019  loss_mask_8: 0.1938  loss_dice_8: 0.2121  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:53:49] d2.utils.events INFO:  eta: 0:45:35  iter: 28299  total_loss: 4.692  loss_ce: 0.1274  loss_mask: 0.1739  loss_dice: 0.1514  loss_ce_0: 0.07966  loss_mask_0: 0.1713  loss_dice_0: 0.1497  loss_ce_1: 0.145  loss_mask_1: 0.162  loss_dice_1: 0.142  loss_ce_2: 0.1267  loss_mask_2: 0.1631  loss_dice_2: 0.1494  loss_ce_3: 0.1256  loss_mask_3: 0.1698  loss_dice_3: 0.1438  loss_ce_4: 0.1413  loss_mask_4: 0.167  loss_dice_4: 0.1508  loss_ce_5: 0.1265  loss_mask_5: 0.168  loss_dice_5: 0.1481  loss_ce_6: 0.1216  loss_mask_6: 0.17  loss_dice_6: 0.1567  loss_ce_7: 0.1422  loss_mask_7: 0.168  loss_dice_7: 0.1529  loss_ce_8: 0.126  loss_mask_8: 0.167  loss_dice_8: 0.1517  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:52] d2.utils.events INFO:  eta: 0:45:32  iter: 28319  total_loss: 4.626  loss_ce: 0.1031  loss_mask: 0.1823  loss_dice: 0.1671  loss_ce_0: 0.07243  loss_mask_0: 0.1767  loss_dice_0: 0.1664  loss_ce_1: 0.1177  loss_mask_1: 0.1826  loss_dice_1: 0.1723  loss_ce_2: 0.1051  loss_mask_2: 0.1822  loss_dice_2: 0.1642  loss_ce_3: 0.09528  loss_mask_3: 0.1747  loss_dice_3: 0.1687  loss_ce_4: 0.1085  loss_mask_4: 0.1841  loss_dice_4: 0.1583  loss_ce_5: 0.1066  loss_mask_5: 0.1753  loss_dice_5: 0.1633  loss_ce_6: 0.09898  loss_mask_6: 0.1874  loss_dice_6: 0.1745  loss_ce_7: 0.1056  loss_mask_7: 0.1821  loss_dice_7: 0.165  loss_ce_8: 0.09694  loss_mask_8: 0.1851  loss_dice_8: 0.1664  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:54] d2.utils.events INFO:  eta: 0:45:31  iter: 28339  total_loss: 4.736  loss_ce: 0.08785  loss_mask: 0.1939  loss_dice: 0.1351  loss_ce_0: 0.05884  loss_mask_0: 0.1967  loss_dice_0: 0.1329  loss_ce_1: 0.0932  loss_mask_1: 0.1934  loss_dice_1: 0.1315  loss_ce_2: 0.08431  loss_mask_2: 0.1996  loss_dice_2: 0.1363  loss_ce_3: 0.07628  loss_mask_3: 0.199  loss_dice_3: 0.1386  loss_ce_4: 0.08849  loss_mask_4: 0.1958  loss_dice_4: 0.1368  loss_ce_5: 0.09182  loss_mask_5: 0.1969  loss_dice_5: 0.1365  loss_ce_6: 0.08551  loss_mask_6: 0.1895  loss_dice_6: 0.1377  loss_ce_7: 0.08897  loss_mask_7: 0.1964  loss_dice_7: 0.1364  loss_ce_8: 0.0846  loss_mask_8: 0.1985  loss_dice_8: 0.1426  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:53:57] d2.utils.events INFO:  eta: 0:45:28  iter: 28359  total_loss: 5.149  loss_ce: 0.08328  loss_mask: 0.1414  loss_dice: 0.2157  loss_ce_0: 0.05885  loss_mask_0: 0.1366  loss_dice_0: 0.2154  loss_ce_1: 0.1021  loss_mask_1: 0.1362  loss_dice_1: 0.2248  loss_ce_2: 0.07929  loss_mask_2: 0.1452  loss_dice_2: 0.203  loss_ce_3: 0.07918  loss_mask_3: 0.1375  loss_dice_3: 0.2101  loss_ce_4: 0.093  loss_mask_4: 0.1419  loss_dice_4: 0.2231  loss_ce_5: 0.08428  loss_mask_5: 0.1428  loss_dice_5: 0.2045  loss_ce_6: 0.08673  loss_mask_6: 0.136  loss_dice_6: 0.2107  loss_ce_7: 0.09788  loss_mask_7: 0.1337  loss_dice_7: 0.2062  loss_ce_8: 0.08177  loss_mask_8: 0.1391  loss_dice_8: 0.2066  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:00] d2.utils.events INFO:  eta: 0:45:26  iter: 28379  total_loss: 4.948  loss_ce: 0.08077  loss_mask: 0.2012  loss_dice: 0.1895  loss_ce_0: 0.05702  loss_mask_0: 0.2036  loss_dice_0: 0.1893  loss_ce_1: 0.09237  loss_mask_1: 0.2002  loss_dice_1: 0.1856  loss_ce_2: 0.07869  loss_mask_2: 0.199  loss_dice_2: 0.1805  loss_ce_3: 0.07871  loss_mask_3: 0.1995  loss_dice_3: 0.1887  loss_ce_4: 0.091  loss_mask_4: 0.2094  loss_dice_4: 0.1948  loss_ce_5: 0.08739  loss_mask_5: 0.207  loss_dice_5: 0.1878  loss_ce_6: 0.08241  loss_mask_6: 0.2057  loss_dice_6: 0.1971  loss_ce_7: 0.09233  loss_mask_7: 0.1998  loss_dice_7: 0.1791  loss_ce_8: 0.08391  loss_mask_8: 0.2039  loss_dice_8: 0.191  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:02] d2.utils.events INFO:  eta: 0:45:23  iter: 28399  total_loss: 4.832  loss_ce: 0.08275  loss_mask: 0.228  loss_dice: 0.1525  loss_ce_0: 0.06719  loss_mask_0: 0.2342  loss_dice_0: 0.1522  loss_ce_1: 0.09712  loss_mask_1: 0.2302  loss_dice_1: 0.1539  loss_ce_2: 0.08288  loss_mask_2: 0.2392  loss_dice_2: 0.1484  loss_ce_3: 0.08268  loss_mask_3: 0.2454  loss_dice_3: 0.1494  loss_ce_4: 0.09254  loss_mask_4: 0.2348  loss_dice_4: 0.1497  loss_ce_5: 0.08142  loss_mask_5: 0.2366  loss_dice_5: 0.1554  loss_ce_6: 0.0816  loss_mask_6: 0.2437  loss_dice_6: 0.1561  loss_ce_7: 0.09432  loss_mask_7: 0.2432  loss_dice_7: 0.157  loss_ce_8: 0.08027  loss_mask_8: 0.2343  loss_dice_8: 0.1518  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:05] d2.utils.events INFO:  eta: 0:45:22  iter: 28419  total_loss: 5.412  loss_ce: 0.0608  loss_mask: 0.2457  loss_dice: 0.2022  loss_ce_0: 0.06124  loss_mask_0: 0.24  loss_dice_0: 0.2049  loss_ce_1: 0.06907  loss_mask_1: 0.2475  loss_dice_1: 0.2079  loss_ce_2: 0.07171  loss_mask_2: 0.2462  loss_dice_2: 0.2095  loss_ce_3: 0.05518  loss_mask_3: 0.2486  loss_dice_3: 0.2072  loss_ce_4: 0.06179  loss_mask_4: 0.2486  loss_dice_4: 0.2116  loss_ce_5: 0.06793  loss_mask_5: 0.248  loss_dice_5: 0.2055  loss_ce_6: 0.05944  loss_mask_6: 0.2334  loss_dice_6: 0.2061  loss_ce_7: 0.06231  loss_mask_7: 0.2441  loss_dice_7: 0.2071  loss_ce_8: 0.07346  loss_mask_8: 0.2476  loss_dice_8: 0.2015  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:07] d2.utils.events INFO:  eta: 0:45:21  iter: 28439  total_loss: 5.92  loss_ce: 0.07602  loss_mask: 0.2312  loss_dice: 0.1759  loss_ce_0: 0.06325  loss_mask_0: 0.2209  loss_dice_0: 0.185  loss_ce_1: 0.07212  loss_mask_1: 0.2134  loss_dice_1: 0.1828  loss_ce_2: 0.1245  loss_mask_2: 0.2224  loss_dice_2: 0.1833  loss_ce_3: 0.09383  loss_mask_3: 0.2339  loss_dice_3: 0.1801  loss_ce_4: 0.1111  loss_mask_4: 0.2254  loss_dice_4: 0.1738  loss_ce_5: 0.08156  loss_mask_5: 0.2374  loss_dice_5: 0.1815  loss_ce_6: 0.05841  loss_mask_6: 0.2255  loss_dice_6: 0.1808  loss_ce_7: 0.08377  loss_mask_7: 0.2221  loss_dice_7: 0.1785  loss_ce_8: 0.1419  loss_mask_8: 0.2138  loss_dice_8: 0.1788  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:10] d2.utils.events INFO:  eta: 0:45:19  iter: 28459  total_loss: 4.902  loss_ce: 0.04303  loss_mask: 0.1839  loss_dice: 0.1741  loss_ce_0: 0.0623  loss_mask_0: 0.1922  loss_dice_0: 0.1763  loss_ce_1: 0.05018  loss_mask_1: 0.1837  loss_dice_1: 0.1775  loss_ce_2: 0.05753  loss_mask_2: 0.1905  loss_dice_2: 0.1653  loss_ce_3: 0.05558  loss_mask_3: 0.1864  loss_dice_3: 0.176  loss_ce_4: 0.04677  loss_mask_4: 0.1875  loss_dice_4: 0.1738  loss_ce_5: 0.04903  loss_mask_5: 0.1804  loss_dice_5: 0.1733  loss_ce_6: 0.04635  loss_mask_6: 0.1903  loss_dice_6: 0.1743  loss_ce_7: 0.04463  loss_mask_7: 0.1909  loss_dice_7: 0.1768  loss_ce_8: 0.0484  loss_mask_8: 0.1914  loss_dice_8: 0.1757  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:12] d2.utils.events INFO:  eta: 0:45:19  iter: 28479  total_loss: 4.702  loss_ce: 0.08091  loss_mask: 0.1921  loss_dice: 0.1431  loss_ce_0: 0.06061  loss_mask_0: 0.1831  loss_dice_0: 0.142  loss_ce_1: 0.0697  loss_mask_1: 0.1767  loss_dice_1: 0.1393  loss_ce_2: 0.07086  loss_mask_2: 0.1804  loss_dice_2: 0.1393  loss_ce_3: 0.08704  loss_mask_3: 0.1827  loss_dice_3: 0.1472  loss_ce_4: 0.0685  loss_mask_4: 0.1891  loss_dice_4: 0.1421  loss_ce_5: 0.07022  loss_mask_5: 0.1845  loss_dice_5: 0.1435  loss_ce_6: 0.07543  loss_mask_6: 0.1786  loss_dice_6: 0.1372  loss_ce_7: 0.06965  loss_mask_7: 0.1736  loss_dice_7: 0.1397  loss_ce_8: 0.06461  loss_mask_8: 0.181  loss_dice_8: 0.1403  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:15] d2.utils.events INFO:  eta: 0:45:19  iter: 28499  total_loss: 4.756  loss_ce: 0.07415  loss_mask: 0.1814  loss_dice: 0.1372  loss_ce_0: 0.05705  loss_mask_0: 0.1831  loss_dice_0: 0.1344  loss_ce_1: 0.0633  loss_mask_1: 0.185  loss_dice_1: 0.1303  loss_ce_2: 0.07082  loss_mask_2: 0.179  loss_dice_2: 0.137  loss_ce_3: 0.06632  loss_mask_3: 0.1914  loss_dice_3: 0.1413  loss_ce_4: 0.07281  loss_mask_4: 0.1844  loss_dice_4: 0.1386  loss_ce_5: 0.06899  loss_mask_5: 0.1796  loss_dice_5: 0.1327  loss_ce_6: 0.06905  loss_mask_6: 0.1808  loss_dice_6: 0.1352  loss_ce_7: 0.07129  loss_mask_7: 0.1859  loss_dice_7: 0.1371  loss_ce_8: 0.07512  loss_mask_8: 0.1793  loss_dice_8: 0.132  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:17] d2.utils.events INFO:  eta: 0:45:17  iter: 28519  total_loss: 4.185  loss_ce: 0.07254  loss_mask: 0.1848  loss_dice: 0.1326  loss_ce_0: 0.07055  loss_mask_0: 0.1862  loss_dice_0: 0.1326  loss_ce_1: 0.06411  loss_mask_1: 0.1803  loss_dice_1: 0.1317  loss_ce_2: 0.0695  loss_mask_2: 0.1798  loss_dice_2: 0.1355  loss_ce_3: 0.07126  loss_mask_3: 0.1779  loss_dice_3: 0.1319  loss_ce_4: 0.06917  loss_mask_4: 0.1778  loss_dice_4: 0.1302  loss_ce_5: 0.07336  loss_mask_5: 0.176  loss_dice_5: 0.1344  loss_ce_6: 0.07244  loss_mask_6: 0.1778  loss_dice_6: 0.1293  loss_ce_7: 0.071  loss_mask_7: 0.1884  loss_dice_7: 0.1299  loss_ce_8: 0.06993  loss_mask_8: 0.1791  loss_dice_8: 0.1318  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:20] d2.utils.events INFO:  eta: 0:45:15  iter: 28539  total_loss: 5.546  loss_ce: 0.1093  loss_mask: 0.1679  loss_dice: 0.1388  loss_ce_0: 0.05614  loss_mask_0: 0.1631  loss_dice_0: 0.1356  loss_ce_1: 0.07159  loss_mask_1: 0.1677  loss_dice_1: 0.1349  loss_ce_2: 0.07607  loss_mask_2: 0.1709  loss_dice_2: 0.137  loss_ce_3: 0.116  loss_mask_3: 0.1686  loss_dice_3: 0.1336  loss_ce_4: 0.07552  loss_mask_4: 0.1637  loss_dice_4: 0.135  loss_ce_5: 0.07983  loss_mask_5: 0.1635  loss_dice_5: 0.1351  loss_ce_6: 0.1136  loss_mask_6: 0.1674  loss_dice_6: 0.131  loss_ce_7: 0.07722  loss_mask_7: 0.1759  loss_dice_7: 0.1378  loss_ce_8: 0.08118  loss_mask_8: 0.1673  loss_dice_8: 0.1351  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:22] d2.utils.events INFO:  eta: 0:45:13  iter: 28559  total_loss: 4.284  loss_ce: 0.07048  loss_mask: 0.1717  loss_dice: 0.1172  loss_ce_0: 0.0562  loss_mask_0: 0.169  loss_dice_0: 0.1165  loss_ce_1: 0.08419  loss_mask_1: 0.1626  loss_dice_1: 0.1201  loss_ce_2: 0.07586  loss_mask_2: 0.1658  loss_dice_2: 0.1199  loss_ce_3: 0.07193  loss_mask_3: 0.1651  loss_dice_3: 0.1218  loss_ce_4: 0.07206  loss_mask_4: 0.1654  loss_dice_4: 0.1203  loss_ce_5: 0.06758  loss_mask_5: 0.1747  loss_dice_5: 0.116  loss_ce_6: 0.07325  loss_mask_6: 0.164  loss_dice_6: 0.1217  loss_ce_7: 0.07413  loss_mask_7: 0.1633  loss_dice_7: 0.1158  loss_ce_8: 0.06649  loss_mask_8: 0.1703  loss_dice_8: 0.116  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:25] d2.utils.events INFO:  eta: 0:45:09  iter: 28579  total_loss: 3.928  loss_ce: 0.04457  loss_mask: 0.179  loss_dice: 0.1348  loss_ce_0: 0.0527  loss_mask_0: 0.1724  loss_dice_0: 0.1274  loss_ce_1: 0.04674  loss_mask_1: 0.1806  loss_dice_1: 0.1302  loss_ce_2: 0.04631  loss_mask_2: 0.1719  loss_dice_2: 0.1305  loss_ce_3: 0.04456  loss_mask_3: 0.1766  loss_dice_3: 0.1343  loss_ce_4: 0.04517  loss_mask_4: 0.1765  loss_dice_4: 0.1316  loss_ce_5: 0.04429  loss_mask_5: 0.1722  loss_dice_5: 0.1319  loss_ce_6: 0.04496  loss_mask_6: 0.1739  loss_dice_6: 0.131  loss_ce_7: 0.04549  loss_mask_7: 0.1792  loss_dice_7: 0.1321  loss_ce_8: 0.04513  loss_mask_8: 0.1702  loss_dice_8: 0.1339  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:27] d2.utils.events INFO:  eta: 0:45:05  iter: 28599  total_loss: 4.331  loss_ce: 0.203  loss_mask: 0.1949  loss_dice: 0.1213  loss_ce_0: 0.08069  loss_mask_0: 0.1939  loss_dice_0: 0.1223  loss_ce_1: 0.158  loss_mask_1: 0.1988  loss_dice_1: 0.1268  loss_ce_2: 0.1609  loss_mask_2: 0.1953  loss_dice_2: 0.1231  loss_ce_3: 0.1733  loss_mask_3: 0.1927  loss_dice_3: 0.1239  loss_ce_4: 0.1617  loss_mask_4: 0.1968  loss_dice_4: 0.12  loss_ce_5: 0.1401  loss_mask_5: 0.1948  loss_dice_5: 0.1286  loss_ce_6: 0.1716  loss_mask_6: 0.1857  loss_dice_6: 0.1182  loss_ce_7: 0.1602  loss_mask_7: 0.1917  loss_dice_7: 0.125  loss_ce_8: 0.1281  loss_mask_8: 0.1963  loss_dice_8: 0.1252  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:30] d2.utils.events INFO:  eta: 0:45:03  iter: 28619  total_loss: 4.864  loss_ce: 0.0685  loss_mask: 0.1968  loss_dice: 0.1564  loss_ce_0: 0.05172  loss_mask_0: 0.2029  loss_dice_0: 0.1544  loss_ce_1: 0.06334  loss_mask_1: 0.2017  loss_dice_1: 0.1565  loss_ce_2: 0.05679  loss_mask_2: 0.1954  loss_dice_2: 0.1474  loss_ce_3: 0.05442  loss_mask_3: 0.1973  loss_dice_3: 0.1566  loss_ce_4: 0.0637  loss_mask_4: 0.2033  loss_dice_4: 0.1564  loss_ce_5: 0.06935  loss_mask_5: 0.1929  loss_dice_5: 0.1594  loss_ce_6: 0.06322  loss_mask_6: 0.2003  loss_dice_6: 0.154  loss_ce_7: 0.0704  loss_mask_7: 0.2054  loss_dice_7: 0.1594  loss_ce_8: 0.06504  loss_mask_8: 0.1973  loss_dice_8: 0.15  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:33] d2.utils.events INFO:  eta: 0:45:00  iter: 28639  total_loss: 4.698  loss_ce: 0.06097  loss_mask: 0.208  loss_dice: 0.1768  loss_ce_0: 0.07335  loss_mask_0: 0.2031  loss_dice_0: 0.1713  loss_ce_1: 0.051  loss_mask_1: 0.1929  loss_dice_1: 0.1723  loss_ce_2: 0.0509  loss_mask_2: 0.1964  loss_dice_2: 0.1761  loss_ce_3: 0.05067  loss_mask_3: 0.2112  loss_dice_3: 0.1794  loss_ce_4: 0.05568  loss_mask_4: 0.1971  loss_dice_4: 0.1768  loss_ce_5: 0.05846  loss_mask_5: 0.2091  loss_dice_5: 0.1828  loss_ce_6: 0.05694  loss_mask_6: 0.2096  loss_dice_6: 0.1803  loss_ce_7: 0.05623  loss_mask_7: 0.1957  loss_dice_7: 0.1803  loss_ce_8: 0.05492  loss_mask_8: 0.1972  loss_dice_8: 0.1706  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:35] d2.utils.events INFO:  eta: 0:44:58  iter: 28659  total_loss: 4.226  loss_ce: 0.05601  loss_mask: 0.1963  loss_dice: 0.1407  loss_ce_0: 0.05428  loss_mask_0: 0.1973  loss_dice_0: 0.1389  loss_ce_1: 0.04797  loss_mask_1: 0.1943  loss_dice_1: 0.1431  loss_ce_2: 0.05225  loss_mask_2: 0.1993  loss_dice_2: 0.1456  loss_ce_3: 0.05184  loss_mask_3: 0.201  loss_dice_3: 0.1438  loss_ce_4: 0.05315  loss_mask_4: 0.1973  loss_dice_4: 0.1419  loss_ce_5: 0.05476  loss_mask_5: 0.1984  loss_dice_5: 0.1401  loss_ce_6: 0.05417  loss_mask_6: 0.1987  loss_dice_6: 0.1437  loss_ce_7: 0.05127  loss_mask_7: 0.1984  loss_dice_7: 0.1393  loss_ce_8: 0.05376  loss_mask_8: 0.1993  loss_dice_8: 0.1418  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:38] d2.utils.events INFO:  eta: 0:44:52  iter: 28679  total_loss: 4.63  loss_ce: 0.05821  loss_mask: 0.1783  loss_dice: 0.1508  loss_ce_0: 0.05438  loss_mask_0: 0.1848  loss_dice_0: 0.1436  loss_ce_1: 0.05366  loss_mask_1: 0.1866  loss_dice_1: 0.1516  loss_ce_2: 0.05724  loss_mask_2: 0.1868  loss_dice_2: 0.1497  loss_ce_3: 0.05793  loss_mask_3: 0.1832  loss_dice_3: 0.1487  loss_ce_4: 0.05254  loss_mask_4: 0.187  loss_dice_4: 0.1531  loss_ce_5: 0.05702  loss_mask_5: 0.1908  loss_dice_5: 0.148  loss_ce_6: 0.05686  loss_mask_6: 0.1889  loss_dice_6: 0.1497  loss_ce_7: 0.05186  loss_mask_7: 0.181  loss_dice_7: 0.1527  loss_ce_8: 0.05931  loss_mask_8: 0.1792  loss_dice_8: 0.1522  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:54:40] d2.utils.events INFO:  eta: 0:44:47  iter: 28699  total_loss: 3.794  loss_ce: 0.05038  loss_mask: 0.189  loss_dice: 0.15  loss_ce_0: 0.05287  loss_mask_0: 0.1858  loss_dice_0: 0.1457  loss_ce_1: 0.04188  loss_mask_1: 0.1848  loss_dice_1: 0.1543  loss_ce_2: 0.04773  loss_mask_2: 0.1865  loss_dice_2: 0.155  loss_ce_3: 0.04819  loss_mask_3: 0.1889  loss_dice_3: 0.154  loss_ce_4: 0.04498  loss_mask_4: 0.1847  loss_dice_4: 0.1502  loss_ce_5: 0.05144  loss_mask_5: 0.19  loss_dice_5: 0.1528  loss_ce_6: 0.04976  loss_mask_6: 0.1885  loss_dice_6: 0.1556  loss_ce_7: 0.0415  loss_mask_7: 0.1888  loss_dice_7: 0.1529  loss_ce_8: 0.04533  loss_mask_8: 0.1879  loss_dice_8: 0.1449  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:43] d2.utils.events INFO:  eta: 0:44:43  iter: 28719  total_loss: 5.219  loss_ce: 0.116  loss_mask: 0.1668  loss_dice: 0.2008  loss_ce_0: 0.07886  loss_mask_0: 0.1462  loss_dice_0: 0.195  loss_ce_1: 0.04955  loss_mask_1: 0.1615  loss_dice_1: 0.1987  loss_ce_2: 0.08605  loss_mask_2: 0.1609  loss_dice_2: 0.1962  loss_ce_3: 0.07076  loss_mask_3: 0.147  loss_dice_3: 0.1909  loss_ce_4: 0.1194  loss_mask_4: 0.1504  loss_dice_4: 0.1923  loss_ce_5: 0.1466  loss_mask_5: 0.1696  loss_dice_5: 0.199  loss_ce_6: 0.09251  loss_mask_6: 0.1554  loss_dice_6: 0.1905  loss_ce_7: 0.1147  loss_mask_7: 0.1538  loss_dice_7: 0.202  loss_ce_8: 0.1127  loss_mask_8: 0.1548  loss_dice_8: 0.1849  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:45] d2.utils.events INFO:  eta: 0:44:40  iter: 28739  total_loss: 4.841  loss_ce: 0.08764  loss_mask: 0.1823  loss_dice: 0.1512  loss_ce_0: 0.07011  loss_mask_0: 0.1816  loss_dice_0: 0.165  loss_ce_1: 0.04175  loss_mask_1: 0.1758  loss_dice_1: 0.1628  loss_ce_2: 0.1313  loss_mask_2: 0.1776  loss_dice_2: 0.1545  loss_ce_3: 0.1099  loss_mask_3: 0.1845  loss_dice_3: 0.1549  loss_ce_4: 0.1676  loss_mask_4: 0.1776  loss_dice_4: 0.1552  loss_ce_5: 0.09778  loss_mask_5: 0.1759  loss_dice_5: 0.1597  loss_ce_6: 0.08683  loss_mask_6: 0.1789  loss_dice_6: 0.1562  loss_ce_7: 0.1547  loss_mask_7: 0.1748  loss_dice_7: 0.1528  loss_ce_8: 0.1428  loss_mask_8: 0.1731  loss_dice_8: 0.1498  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:48] d2.utils.events INFO:  eta: 0:44:38  iter: 28759  total_loss: 5.011  loss_ce: 0.04182  loss_mask: 0.2181  loss_dice: 0.1598  loss_ce_0: 0.05555  loss_mask_0: 0.2109  loss_dice_0: 0.1593  loss_ce_1: 0.04279  loss_mask_1: 0.2283  loss_dice_1: 0.1611  loss_ce_2: 0.05374  loss_mask_2: 0.2134  loss_dice_2: 0.1537  loss_ce_3: 0.04207  loss_mask_3: 0.221  loss_dice_3: 0.1617  loss_ce_4: 0.06191  loss_mask_4: 0.221  loss_dice_4: 0.1561  loss_ce_5: 0.04243  loss_mask_5: 0.2168  loss_dice_5: 0.1618  loss_ce_6: 0.03948  loss_mask_6: 0.2264  loss_dice_6: 0.1651  loss_ce_7: 0.06099  loss_mask_7: 0.2208  loss_dice_7: 0.1628  loss_ce_8: 0.04847  loss_mask_8: 0.2183  loss_dice_8: 0.1594  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:54:50] d2.utils.events INFO:  eta: 0:44:37  iter: 28779  total_loss: 4.578  loss_ce: 0.05394  loss_mask: 0.209  loss_dice: 0.1372  loss_ce_0: 0.05496  loss_mask_0: 0.2057  loss_dice_0: 0.1473  loss_ce_1: 0.03532  loss_mask_1: 0.2079  loss_dice_1: 0.1441  loss_ce_2: 0.04653  loss_mask_2: 0.2099  loss_dice_2: 0.1444  loss_ce_3: 0.05583  loss_mask_3: 0.2123  loss_dice_3: 0.1407  loss_ce_4: 0.04116  loss_mask_4: 0.2072  loss_dice_4: 0.1448  loss_ce_5: 0.04614  loss_mask_5: 0.2074  loss_dice_5: 0.1401  loss_ce_6: 0.05724  loss_mask_6: 0.2071  loss_dice_6: 0.1389  loss_ce_7: 0.04143  loss_mask_7: 0.2076  loss_dice_7: 0.1423  loss_ce_8: 0.04883  loss_mask_8: 0.2137  loss_dice_8: 0.1446  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:53] d2.utils.events INFO:  eta: 0:44:36  iter: 28799  total_loss: 4.423  loss_ce: 0.02433  loss_mask: 0.1853  loss_dice: 0.2059  loss_ce_0: 0.06907  loss_mask_0: 0.18  loss_dice_0: 0.2049  loss_ce_1: 0.02047  loss_mask_1: 0.1876  loss_dice_1: 0.2134  loss_ce_2: 0.02231  loss_mask_2: 0.1795  loss_dice_2: 0.2103  loss_ce_3: 0.02631  loss_mask_3: 0.1834  loss_dice_3: 0.2115  loss_ce_4: 0.02545  loss_mask_4: 0.1899  loss_dice_4: 0.2121  loss_ce_5: 0.02609  loss_mask_5: 0.1891  loss_dice_5: 0.2138  loss_ce_6: 0.02544  loss_mask_6: 0.1796  loss_dice_6: 0.2122  loss_ce_7: 0.02311  loss_mask_7: 0.1791  loss_dice_7: 0.2138  loss_ce_8: 0.02375  loss_mask_8: 0.1772  loss_dice_8: 0.2132  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:55] d2.utils.events INFO:  eta: 0:44:34  iter: 28819  total_loss: 5.222  loss_ce: 0.0305  loss_mask: 0.1785  loss_dice: 0.3052  loss_ce_0: 0.06341  loss_mask_0: 0.1644  loss_dice_0: 0.3104  loss_ce_1: 0.02213  loss_mask_1: 0.1698  loss_dice_1: 0.2877  loss_ce_2: 0.02675  loss_mask_2: 0.1734  loss_dice_2: 0.3106  loss_ce_3: 0.03438  loss_mask_3: 0.1785  loss_dice_3: 0.3068  loss_ce_4: 0.02763  loss_mask_4: 0.1776  loss_dice_4: 0.3188  loss_ce_5: 0.02979  loss_mask_5: 0.1734  loss_dice_5: 0.3115  loss_ce_6: 0.03063  loss_mask_6: 0.1791  loss_dice_6: 0.3178  loss_ce_7: 0.02614  loss_mask_7: 0.166  loss_dice_7: 0.3058  loss_ce_8: 0.02682  loss_mask_8: 0.1668  loss_dice_8: 0.3133  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:54:58] d2.utils.events INFO:  eta: 0:44:33  iter: 28839  total_loss: 4.799  loss_ce: 0.03751  loss_mask: 0.2192  loss_dice: 0.1616  loss_ce_0: 0.06221  loss_mask_0: 0.2213  loss_dice_0: 0.1648  loss_ce_1: 0.0205  loss_mask_1: 0.2276  loss_dice_1: 0.1703  loss_ce_2: 0.03413  loss_mask_2: 0.2239  loss_dice_2: 0.1627  loss_ce_3: 0.03696  loss_mask_3: 0.2305  loss_dice_3: 0.1606  loss_ce_4: 0.03287  loss_mask_4: 0.2219  loss_dice_4: 0.1697  loss_ce_5: 0.03998  loss_mask_5: 0.2311  loss_dice_5: 0.1691  loss_ce_6: 0.03861  loss_mask_6: 0.223  loss_dice_6: 0.1686  loss_ce_7: 0.03573  loss_mask_7: 0.2198  loss_dice_7: 0.1616  loss_ce_8: 0.03632  loss_mask_8: 0.2178  loss_dice_8: 0.1598  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:55:00] d2.utils.events INFO:  eta: 0:44:32  iter: 28859  total_loss: 6.737  loss_ce: 0.1716  loss_mask: 0.1999  loss_dice: 0.2068  loss_ce_0: 0.06182  loss_mask_0: 0.1939  loss_dice_0: 0.2057  loss_ce_1: 0.1031  loss_mask_1: 0.2025  loss_dice_1: 0.2074  loss_ce_2: 0.1331  loss_mask_2: 0.1959  loss_dice_2: 0.1946  loss_ce_3: 0.1938  loss_mask_3: 0.2065  loss_dice_3: 0.1949  loss_ce_4: 0.1528  loss_mask_4: 0.2075  loss_dice_4: 0.2095  loss_ce_5: 0.2373  loss_mask_5: 0.2079  loss_dice_5: 0.1989  loss_ce_6: 0.192  loss_mask_6: 0.1901  loss_dice_6: 0.2121  loss_ce_7: 0.1677  loss_mask_7: 0.1933  loss_dice_7: 0.2049  loss_ce_8: 0.2113  loss_mask_8: 0.2006  loss_dice_8: 0.2023  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:55:03] d2.utils.events INFO:  eta: 0:44:31  iter: 28879  total_loss: 5.389  loss_ce: 0.06304  loss_mask: 0.1763  loss_dice: 0.2373  loss_ce_0: 0.06249  loss_mask_0: 0.1705  loss_dice_0: 0.2435  loss_ce_1: 0.04086  loss_mask_1: 0.1802  loss_dice_1: 0.235  loss_ce_2: 0.06775  loss_mask_2: 0.1787  loss_dice_2: 0.2337  loss_ce_3: 0.08583  loss_mask_3: 0.1763  loss_dice_3: 0.2353  loss_ce_4: 0.05972  loss_mask_4: 0.1812  loss_dice_4: 0.2395  loss_ce_5: 0.08411  loss_mask_5: 0.1702  loss_dice_5: 0.2473  loss_ce_6: 0.06965  loss_mask_6: 0.1746  loss_dice_6: 0.2278  loss_ce_7: 0.04917  loss_mask_7: 0.1752  loss_dice_7: 0.2387  loss_ce_8: 0.08659  loss_mask_8: 0.1772  loss_dice_8: 0.2253  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:06] d2.utils.events INFO:  eta: 0:44:29  iter: 28899  total_loss: 4.834  loss_ce: 0.0562  loss_mask: 0.2387  loss_dice: 0.1421  loss_ce_0: 0.06138  loss_mask_0: 0.2408  loss_dice_0: 0.1422  loss_ce_1: 0.03678  loss_mask_1: 0.24  loss_dice_1: 0.1414  loss_ce_2: 0.1057  loss_mask_2: 0.2393  loss_dice_2: 0.1426  loss_ce_3: 0.04799  loss_mask_3: 0.2324  loss_dice_3: 0.1359  loss_ce_4: 0.04805  loss_mask_4: 0.2285  loss_dice_4: 0.1405  loss_ce_5: 0.06242  loss_mask_5: 0.226  loss_dice_5: 0.1444  loss_ce_6: 0.04599  loss_mask_6: 0.2289  loss_dice_6: 0.1456  loss_ce_7: 0.04007  loss_mask_7: 0.2298  loss_dice_7: 0.1355  loss_ce_8: 0.08206  loss_mask_8: 0.237  loss_dice_8: 0.1397  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:08] d2.utils.events INFO:  eta: 0:44:29  iter: 28919  total_loss: 4.346  loss_ce: 0.02887  loss_mask: 0.191  loss_dice: 0.1757  loss_ce_0: 0.06247  loss_mask_0: 0.1952  loss_dice_0: 0.1788  loss_ce_1: 0.02863  loss_mask_1: 0.191  loss_dice_1: 0.1745  loss_ce_2: 0.03038  loss_mask_2: 0.1868  loss_dice_2: 0.1821  loss_ce_3: 0.03099  loss_mask_3: 0.1925  loss_dice_3: 0.1826  loss_ce_4: 0.03372  loss_mask_4: 0.1936  loss_dice_4: 0.1826  loss_ce_5: 0.03061  loss_mask_5: 0.1848  loss_dice_5: 0.1769  loss_ce_6: 0.0303  loss_mask_6: 0.1838  loss_dice_6: 0.1819  loss_ce_7: 0.03363  loss_mask_7: 0.1909  loss_dice_7: 0.1736  loss_ce_8: 0.0323  loss_mask_8: 0.1931  loss_dice_8: 0.1811  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:11] d2.utils.events INFO:  eta: 0:44:30  iter: 28939  total_loss: 4.756  loss_ce: 0.02304  loss_mask: 0.1831  loss_dice: 0.1506  loss_ce_0: 0.06184  loss_mask_0: 0.1805  loss_dice_0: 0.1562  loss_ce_1: 0.02804  loss_mask_1: 0.181  loss_dice_1: 0.1506  loss_ce_2: 0.02503  loss_mask_2: 0.185  loss_dice_2: 0.1695  loss_ce_3: 0.02392  loss_mask_3: 0.1845  loss_dice_3: 0.1502  loss_ce_4: 0.02521  loss_mask_4: 0.1783  loss_dice_4: 0.1449  loss_ce_5: 0.02499  loss_mask_5: 0.1815  loss_dice_5: 0.1497  loss_ce_6: 0.0241  loss_mask_6: 0.1744  loss_dice_6: 0.1485  loss_ce_7: 0.02452  loss_mask_7: 0.1766  loss_dice_7: 0.1496  loss_ce_8: 0.0264  loss_mask_8: 0.1839  loss_dice_8: 0.1513  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:13] d2.utils.events INFO:  eta: 0:44:29  iter: 28959  total_loss: 4.273  loss_ce: 0.01913  loss_mask: 0.195  loss_dice: 0.2013  loss_ce_0: 0.06183  loss_mask_0: 0.1948  loss_dice_0: 0.1933  loss_ce_1: 0.01919  loss_mask_1: 0.2069  loss_dice_1: 0.1927  loss_ce_2: 0.02167  loss_mask_2: 0.197  loss_dice_2: 0.2011  loss_ce_3: 0.02109  loss_mask_3: 0.2017  loss_dice_3: 0.2021  loss_ce_4: 0.02048  loss_mask_4: 0.2014  loss_dice_4: 0.1914  loss_ce_5: 0.02324  loss_mask_5: 0.1941  loss_dice_5: 0.1857  loss_ce_6: 0.02121  loss_mask_6: 0.1986  loss_dice_6: 0.1948  loss_ce_7: 0.02012  loss_mask_7: 0.2033  loss_dice_7: 0.1982  loss_ce_8: 0.02237  loss_mask_8: 0.1999  loss_dice_8: 0.2004  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:16] d2.utils.events INFO:  eta: 0:44:27  iter: 28979  total_loss: 4.686  loss_ce: 0.02561  loss_mask: 0.2033  loss_dice: 0.203  loss_ce_0: 0.06199  loss_mask_0: 0.1969  loss_dice_0: 0.197  loss_ce_1: 0.02869  loss_mask_1: 0.1965  loss_dice_1: 0.1936  loss_ce_2: 0.02681  loss_mask_2: 0.1948  loss_dice_2: 0.192  loss_ce_3: 0.02491  loss_mask_3: 0.2051  loss_dice_3: 0.2005  loss_ce_4: 0.02542  loss_mask_4: 0.2023  loss_dice_4: 0.2002  loss_ce_5: 0.02554  loss_mask_5: 0.1957  loss_dice_5: 0.2008  loss_ce_6: 0.02601  loss_mask_6: 0.1966  loss_dice_6: 0.1979  loss_ce_7: 0.02418  loss_mask_7: 0.1986  loss_dice_7: 0.2061  loss_ce_8: 0.02702  loss_mask_8: 0.1948  loss_dice_8: 0.1994  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:18] d2.utils.events INFO:  eta: 0:44:24  iter: 28999  total_loss: 4.397  loss_ce: 0.01647  loss_mask: 0.2101  loss_dice: 0.1673  loss_ce_0: 0.062  loss_mask_0: 0.2106  loss_dice_0: 0.1714  loss_ce_1: 0.01593  loss_mask_1: 0.218  loss_dice_1: 0.1776  loss_ce_2: 0.01613  loss_mask_2: 0.2138  loss_dice_2: 0.1721  loss_ce_3: 0.01784  loss_mask_3: 0.2162  loss_dice_3: 0.178  loss_ce_4: 0.01646  loss_mask_4: 0.2142  loss_dice_4: 0.1787  loss_ce_5: 0.01835  loss_mask_5: 0.2098  loss_dice_5: 0.1702  loss_ce_6: 0.01823  loss_mask_6: 0.2165  loss_dice_6: 0.1739  loss_ce_7: 0.01539  loss_mask_7: 0.2172  loss_dice_7: 0.1761  loss_ce_8: 0.01763  loss_mask_8: 0.2164  loss_dice_8: 0.1679  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:21] d2.utils.events INFO:  eta: 0:44:20  iter: 29019  total_loss: 5.032  loss_ce: 0.03399  loss_mask: 0.1743  loss_dice: 0.196  loss_ce_0: 0.06161  loss_mask_0: 0.1707  loss_dice_0: 0.193  loss_ce_1: 0.02341  loss_mask_1: 0.1742  loss_dice_1: 0.1881  loss_ce_2: 0.0619  loss_mask_2: 0.1768  loss_dice_2: 0.1996  loss_ce_3: 0.02998  loss_mask_3: 0.1731  loss_dice_3: 0.1833  loss_ce_4: 0.01999  loss_mask_4: 0.1941  loss_dice_4: 0.1944  loss_ce_5: 0.03694  loss_mask_5: 0.1756  loss_dice_5: 0.1912  loss_ce_6: 0.02956  loss_mask_6: 0.1729  loss_dice_6: 0.1843  loss_ce_7: 0.0201  loss_mask_7: 0.1765  loss_dice_7: 0.1951  loss_ce_8: 0.07889  loss_mask_8: 0.1775  loss_dice_8: 0.1919  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:23] d2.utils.events INFO:  eta: 0:44:16  iter: 29039  total_loss: 4.468  loss_ce: 0.0193  loss_mask: 0.2015  loss_dice: 0.157  loss_ce_0: 0.06435  loss_mask_0: 0.2016  loss_dice_0: 0.151  loss_ce_1: 0.01914  loss_mask_1: 0.2063  loss_dice_1: 0.1526  loss_ce_2: 0.0236  loss_mask_2: 0.2048  loss_dice_2: 0.1533  loss_ce_3: 0.01791  loss_mask_3: 0.2105  loss_dice_3: 0.1507  loss_ce_4: 0.01996  loss_mask_4: 0.2101  loss_dice_4: 0.1565  loss_ce_5: 0.02182  loss_mask_5: 0.2103  loss_dice_5: 0.1552  loss_ce_6: 0.02239  loss_mask_6: 0.2074  loss_dice_6: 0.1602  loss_ce_7: 0.02119  loss_mask_7: 0.2114  loss_dice_7: 0.1577  loss_ce_8: 0.02285  loss_mask_8: 0.2091  loss_dice_8: 0.1568  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:55:26] d2.utils.events INFO:  eta: 0:44:12  iter: 29059  total_loss: 5.495  loss_ce: 0.04339  loss_mask: 0.1835  loss_dice: 0.1534  loss_ce_0: 0.06256  loss_mask_0: 0.1864  loss_dice_0: 0.1546  loss_ce_1: 0.0534  loss_mask_1: 0.1916  loss_dice_1: 0.1534  loss_ce_2: 0.04524  loss_mask_2: 0.1768  loss_dice_2: 0.1496  loss_ce_3: 0.04279  loss_mask_3: 0.1829  loss_dice_3: 0.1483  loss_ce_4: 0.0448  loss_mask_4: 0.1987  loss_dice_4: 0.1547  loss_ce_5: 0.04302  loss_mask_5: 0.1805  loss_dice_5: 0.1583  loss_ce_6: 0.04672  loss_mask_6: 0.1787  loss_dice_6: 0.1609  loss_ce_7: 0.04682  loss_mask_7: 0.1854  loss_dice_7: 0.1513  loss_ce_8: 0.05127  loss_mask_8: 0.1859  loss_dice_8: 0.1523  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:55:28] d2.utils.events INFO:  eta: 0:44:09  iter: 29079  total_loss: 5.071  loss_ce: 0.06495  loss_mask: 0.1627  loss_dice: 0.1799  loss_ce_0: 0.06198  loss_mask_0: 0.1815  loss_dice_0: 0.1848  loss_ce_1: 0.1353  loss_mask_1: 0.178  loss_dice_1: 0.1784  loss_ce_2: 0.07707  loss_mask_2: 0.1683  loss_dice_2: 0.1824  loss_ce_3: 0.0717  loss_mask_3: 0.1736  loss_dice_3: 0.1828  loss_ce_4: 0.1201  loss_mask_4: 0.175  loss_dice_4: 0.1816  loss_ce_5: 0.04736  loss_mask_5: 0.1736  loss_dice_5: 0.1795  loss_ce_6: 0.0695  loss_mask_6: 0.1711  loss_dice_6: 0.1821  loss_ce_7: 0.1261  loss_mask_7: 0.1689  loss_dice_7: 0.1833  loss_ce_8: 0.05491  loss_mask_8: 0.1801  loss_dice_8: 0.1776  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:31] d2.utils.events INFO:  eta: 0:44:09  iter: 29099  total_loss: 4.289  loss_ce: 0.03474  loss_mask: 0.1968  loss_dice: 0.1615  loss_ce_0: 0.05898  loss_mask_0: 0.1996  loss_dice_0: 0.1548  loss_ce_1: 0.04  loss_mask_1: 0.2  loss_dice_1: 0.1579  loss_ce_2: 0.03471  loss_mask_2: 0.1902  loss_dice_2: 0.1541  loss_ce_3: 0.03281  loss_mask_3: 0.1987  loss_dice_3: 0.1623  loss_ce_4: 0.03786  loss_mask_4: 0.1944  loss_dice_4: 0.1567  loss_ce_5: 0.03664  loss_mask_5: 0.2066  loss_dice_5: 0.1583  loss_ce_6: 0.03246  loss_mask_6: 0.1952  loss_dice_6: 0.1633  loss_ce_7: 0.03848  loss_mask_7: 0.1988  loss_dice_7: 0.1673  loss_ce_8: 0.03577  loss_mask_8: 0.1996  loss_dice_8: 0.1586  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:33] d2.utils.events INFO:  eta: 0:44:06  iter: 29119  total_loss: 4.07  loss_ce: 0.0245  loss_mask: 0.1539  loss_dice: 0.1647  loss_ce_0: 0.06317  loss_mask_0: 0.1597  loss_dice_0: 0.1669  loss_ce_1: 0.03343  loss_mask_1: 0.1545  loss_dice_1: 0.1632  loss_ce_2: 0.02952  loss_mask_2: 0.1543  loss_dice_2: 0.1651  loss_ce_3: 0.0246  loss_mask_3: 0.1609  loss_dice_3: 0.1688  loss_ce_4: 0.02927  loss_mask_4: 0.1572  loss_dice_4: 0.1673  loss_ce_5: 0.02862  loss_mask_5: 0.1483  loss_dice_5: 0.1725  loss_ce_6: 0.02419  loss_mask_6: 0.1563  loss_dice_6: 0.1596  loss_ce_7: 0.02828  loss_mask_7: 0.1493  loss_dice_7: 0.1674  loss_ce_8: 0.02817  loss_mask_8: 0.166  loss_dice_8: 0.1668  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:36] d2.utils.events INFO:  eta: 0:44:03  iter: 29139  total_loss: 4.57  loss_ce: 0.02449  loss_mask: 0.2082  loss_dice: 0.1465  loss_ce_0: 0.07568  loss_mask_0: 0.2082  loss_dice_0: 0.1407  loss_ce_1: 0.03067  loss_mask_1: 0.2063  loss_dice_1: 0.1502  loss_ce_2: 0.0262  loss_mask_2: 0.2158  loss_dice_2: 0.1487  loss_ce_3: 0.02286  loss_mask_3: 0.2074  loss_dice_3: 0.151  loss_ce_4: 0.02832  loss_mask_4: 0.2209  loss_dice_4: 0.1494  loss_ce_5: 0.0263  loss_mask_5: 0.2128  loss_dice_5: 0.1443  loss_ce_6: 0.02312  loss_mask_6: 0.2116  loss_dice_6: 0.1452  loss_ce_7: 0.03005  loss_mask_7: 0.2217  loss_dice_7: 0.1507  loss_ce_8: 0.03018  loss_mask_8: 0.2077  loss_dice_8: 0.15  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:39] d2.utils.events INFO:  eta: 0:43:58  iter: 29159  total_loss: 4.286  loss_ce: 0.02488  loss_mask: 0.2112  loss_dice: 0.1474  loss_ce_0: 0.06812  loss_mask_0: 0.203  loss_dice_0: 0.1465  loss_ce_1: 0.03033  loss_mask_1: 0.2129  loss_dice_1: 0.1472  loss_ce_2: 0.02677  loss_mask_2: 0.2136  loss_dice_2: 0.1516  loss_ce_3: 0.02509  loss_mask_3: 0.2113  loss_dice_3: 0.1539  loss_ce_4: 0.03029  loss_mask_4: 0.2061  loss_dice_4: 0.1464  loss_ce_5: 0.02873  loss_mask_5: 0.2132  loss_dice_5: 0.1446  loss_ce_6: 0.02472  loss_mask_6: 0.2185  loss_dice_6: 0.1456  loss_ce_7: 0.03003  loss_mask_7: 0.2055  loss_dice_7: 0.1519  loss_ce_8: 0.03026  loss_mask_8: 0.2109  loss_dice_8: 0.1443  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:55:41] d2.utils.events INFO:  eta: 0:43:56  iter: 29179  total_loss: 4.665  loss_ce: 0.03706  loss_mask: 0.1868  loss_dice: 0.1506  loss_ce_0: 0.06367  loss_mask_0: 0.1812  loss_dice_0: 0.1491  loss_ce_1: 0.03729  loss_mask_1: 0.1789  loss_dice_1: 0.1525  loss_ce_2: 0.03609  loss_mask_2: 0.173  loss_dice_2: 0.1507  loss_ce_3: 0.03991  loss_mask_3: 0.1836  loss_dice_3: 0.1518  loss_ce_4: 0.03493  loss_mask_4: 0.188  loss_dice_4: 0.1461  loss_ce_5: 0.03279  loss_mask_5: 0.1899  loss_dice_5: 0.1502  loss_ce_6: 0.0355  loss_mask_6: 0.1865  loss_dice_6: 0.1524  loss_ce_7: 0.03686  loss_mask_7: 0.1867  loss_dice_7: 0.1545  loss_ce_8: 0.03084  loss_mask_8: 0.1815  loss_dice_8: 0.1519  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:44] d2.utils.events INFO:  eta: 0:43:52  iter: 29199  total_loss: 5.154  loss_ce: 0.06747  loss_mask: 0.2376  loss_dice: 0.162  loss_ce_0: 0.05762  loss_mask_0: 0.2274  loss_dice_0: 0.1581  loss_ce_1: 0.05807  loss_mask_1: 0.2337  loss_dice_1: 0.1545  loss_ce_2: 0.06355  loss_mask_2: 0.2339  loss_dice_2: 0.1632  loss_ce_3: 0.06034  loss_mask_3: 0.2354  loss_dice_3: 0.1618  loss_ce_4: 0.05586  loss_mask_4: 0.2442  loss_dice_4: 0.1649  loss_ce_5: 0.0673  loss_mask_5: 0.2287  loss_dice_5: 0.1614  loss_ce_6: 0.06941  loss_mask_6: 0.2327  loss_dice_6: 0.1589  loss_ce_7: 0.05269  loss_mask_7: 0.2233  loss_dice_7: 0.1566  loss_ce_8: 0.07791  loss_mask_8: 0.236  loss_dice_8: 0.1585  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:46] d2.utils.events INFO:  eta: 0:43:50  iter: 29219  total_loss: 5.51  loss_ce: 0.0461  loss_mask: 0.2093  loss_dice: 0.187  loss_ce_0: 0.05231  loss_mask_0: 0.2117  loss_dice_0: 0.1853  loss_ce_1: 0.06396  loss_mask_1: 0.2135  loss_dice_1: 0.182  loss_ce_2: 0.0546  loss_mask_2: 0.2212  loss_dice_2: 0.1859  loss_ce_3: 0.04844  loss_mask_3: 0.2204  loss_dice_3: 0.1901  loss_ce_4: 0.04973  loss_mask_4: 0.2211  loss_dice_4: 0.1819  loss_ce_5: 0.0542  loss_mask_5: 0.2078  loss_dice_5: 0.1801  loss_ce_6: 0.04904  loss_mask_6: 0.2072  loss_dice_6: 0.181  loss_ce_7: 0.04633  loss_mask_7: 0.2205  loss_dice_7: 0.1832  loss_ce_8: 0.05276  loss_mask_8: 0.2143  loss_dice_8: 0.1888  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:49] d2.utils.events INFO:  eta: 0:43:48  iter: 29239  total_loss: 4.645  loss_ce: 0.08802  loss_mask: 0.1572  loss_dice: 0.1333  loss_ce_0: 0.07786  loss_mask_0: 0.1597  loss_dice_0: 0.1298  loss_ce_1: 0.1268  loss_mask_1: 0.1605  loss_dice_1: 0.1301  loss_ce_2: 0.0939  loss_mask_2: 0.1661  loss_dice_2: 0.13  loss_ce_3: 0.0838  loss_mask_3: 0.1584  loss_dice_3: 0.1359  loss_ce_4: 0.1094  loss_mask_4: 0.1553  loss_dice_4: 0.1321  loss_ce_5: 0.08775  loss_mask_5: 0.1639  loss_dice_5: 0.1278  loss_ce_6: 0.08401  loss_mask_6: 0.159  loss_dice_6: 0.1358  loss_ce_7: 0.1051  loss_mask_7: 0.1717  loss_dice_7: 0.1343  loss_ce_8: 0.09698  loss_mask_8: 0.1572  loss_dice_8: 0.1359  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:51] d2.utils.events INFO:  eta: 0:43:46  iter: 29259  total_loss: 4.521  loss_ce: 0.03072  loss_mask: 0.2254  loss_dice: 0.1427  loss_ce_0: 0.06232  loss_mask_0: 0.2193  loss_dice_0: 0.1524  loss_ce_1: 0.03947  loss_mask_1: 0.2147  loss_dice_1: 0.1522  loss_ce_2: 0.03367  loss_mask_2: 0.2223  loss_dice_2: 0.1391  loss_ce_3: 0.03109  loss_mask_3: 0.2113  loss_dice_3: 0.1441  loss_ce_4: 0.04151  loss_mask_4: 0.218  loss_dice_4: 0.1445  loss_ce_5: 0.02309  loss_mask_5: 0.2175  loss_dice_5: 0.151  loss_ce_6: 0.02831  loss_mask_6: 0.2183  loss_dice_6: 0.152  loss_ce_7: 0.03397  loss_mask_7: 0.226  loss_dice_7: 0.1478  loss_ce_8: 0.02528  loss_mask_8: 0.2125  loss_dice_8: 0.1467  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:54] d2.utils.events INFO:  eta: 0:43:43  iter: 29279  total_loss: 4.215  loss_ce: 0.02984  loss_mask: 0.2052  loss_dice: 0.1217  loss_ce_0: 0.06204  loss_mask_0: 0.2144  loss_dice_0: 0.1223  loss_ce_1: 0.05986  loss_mask_1: 0.2172  loss_dice_1: 0.1238  loss_ce_2: 0.03019  loss_mask_2: 0.203  loss_dice_2: 0.1192  loss_ce_3: 0.02786  loss_mask_3: 0.2148  loss_dice_3: 0.1271  loss_ce_4: 0.04279  loss_mask_4: 0.2162  loss_dice_4: 0.1257  loss_ce_5: 0.03093  loss_mask_5: 0.2121  loss_dice_5: 0.1227  loss_ce_6: 0.0288  loss_mask_6: 0.2162  loss_dice_6: 0.1225  loss_ce_7: 0.05174  loss_mask_7: 0.2151  loss_dice_7: 0.1244  loss_ce_8: 0.03152  loss_mask_8: 0.2174  loss_dice_8: 0.1226  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:56] d2.utils.events INFO:  eta: 0:43:42  iter: 29299  total_loss: 5.106  loss_ce: 0.05857  loss_mask: 0.2242  loss_dice: 0.1685  loss_ce_0: 0.06205  loss_mask_0: 0.2009  loss_dice_0: 0.1732  loss_ce_1: 0.07244  loss_mask_1: 0.2117  loss_dice_1: 0.1762  loss_ce_2: 0.05489  loss_mask_2: 0.2226  loss_dice_2: 0.1748  loss_ce_3: 0.0472  loss_mask_3: 0.2189  loss_dice_3: 0.1774  loss_ce_4: 0.09079  loss_mask_4: 0.2113  loss_dice_4: 0.1725  loss_ce_5: 0.06802  loss_mask_5: 0.2164  loss_dice_5: 0.1685  loss_ce_6: 0.09103  loss_mask_6: 0.2139  loss_dice_6: 0.179  loss_ce_7: 0.097  loss_mask_7: 0.2178  loss_dice_7: 0.1696  loss_ce_8: 0.06219  loss_mask_8: 0.2195  loss_dice_8: 0.1634  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:55:59] d2.utils.events INFO:  eta: 0:43:41  iter: 29319  total_loss: 3.722  loss_ce: 0.02105  loss_mask: 0.1919  loss_dice: 0.1239  loss_ce_0: 0.06196  loss_mask_0: 0.1953  loss_dice_0: 0.1274  loss_ce_1: 0.02765  loss_mask_1: 0.1823  loss_dice_1: 0.1252  loss_ce_2: 0.02406  loss_mask_2: 0.1874  loss_dice_2: 0.1235  loss_ce_3: 0.02094  loss_mask_3: 0.1839  loss_dice_3: 0.1291  loss_ce_4: 0.02053  loss_mask_4: 0.1942  loss_dice_4: 0.1272  loss_ce_5: 0.03333  loss_mask_5: 0.1844  loss_dice_5: 0.1209  loss_ce_6: 0.03551  loss_mask_6: 0.1799  loss_dice_6: 0.1247  loss_ce_7: 0.03018  loss_mask_7: 0.1966  loss_dice_7: 0.1254  loss_ce_8: 0.02356  loss_mask_8: 0.1829  loss_dice_8: 0.1189  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:56:01] d2.utils.events INFO:  eta: 0:43:38  iter: 29339  total_loss: 5.131  loss_ce: 0.009932  loss_mask: 0.2052  loss_dice: 0.1531  loss_ce_0: 0.06247  loss_mask_0: 0.2031  loss_dice_0: 0.1619  loss_ce_1: 0.01907  loss_mask_1: 0.2012  loss_dice_1: 0.1603  loss_ce_2: 0.01579  loss_mask_2: 0.1921  loss_dice_2: 0.1548  loss_ce_3: 0.008857  loss_mask_3: 0.1962  loss_dice_3: 0.1609  loss_ce_4: 0.01141  loss_mask_4: 0.2029  loss_dice_4: 0.1632  loss_ce_5: 0.02272  loss_mask_5: 0.2058  loss_dice_5: 0.1596  loss_ce_6: 0.01804  loss_mask_6: 0.1997  loss_dice_6: 0.1602  loss_ce_7: 0.01166  loss_mask_7: 0.1842  loss_dice_7: 0.1536  loss_ce_8: 0.01566  loss_mask_8: 0.2078  loss_dice_8: 0.1604  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:04] d2.utils.events INFO:  eta: 0:43:34  iter: 29359  total_loss: 3.855  loss_ce: 0.01123  loss_mask: 0.2079  loss_dice: 0.1638  loss_ce_0: 0.06124  loss_mask_0: 0.213  loss_dice_0: 0.1602  loss_ce_1: 0.01797  loss_mask_1: 0.1985  loss_dice_1: 0.1548  loss_ce_2: 0.01457  loss_mask_2: 0.2023  loss_dice_2: 0.1626  loss_ce_3: 0.01357  loss_mask_3: 0.199  loss_dice_3: 0.1602  loss_ce_4: 0.01281  loss_mask_4: 0.2095  loss_dice_4: 0.1645  loss_ce_5: 0.01694  loss_mask_5: 0.2072  loss_dice_5: 0.1685  loss_ce_6: 0.0161  loss_mask_6: 0.2065  loss_dice_6: 0.1623  loss_ce_7: 0.01081  loss_mask_7: 0.2018  loss_dice_7: 0.153  loss_ce_8: 0.01297  loss_mask_8: 0.2041  loss_dice_8: 0.1633  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:06] d2.utils.events INFO:  eta: 0:43:31  iter: 29379  total_loss: 4.907  loss_ce: 0.008095  loss_mask: 0.2241  loss_dice: 0.2042  loss_ce_0: 0.06055  loss_mask_0: 0.2178  loss_dice_0: 0.1998  loss_ce_1: 0.01075  loss_mask_1: 0.2128  loss_dice_1: 0.2084  loss_ce_2: 0.01036  loss_mask_2: 0.2254  loss_dice_2: 0.2073  loss_ce_3: 0.008593  loss_mask_3: 0.2161  loss_dice_3: 0.2113  loss_ce_4: 0.008136  loss_mask_4: 0.2205  loss_dice_4: 0.2032  loss_ce_5: 0.01235  loss_mask_5: 0.2105  loss_dice_5: 0.1988  loss_ce_6: 0.009481  loss_mask_6: 0.2133  loss_dice_6: 0.2033  loss_ce_7: 0.007134  loss_mask_7: 0.2172  loss_dice_7: 0.2011  loss_ce_8: 0.01013  loss_mask_8: 0.2118  loss_dice_8: 0.2027  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:09] d2.utils.events INFO:  eta: 0:43:28  iter: 29399  total_loss: 5.435  loss_ce: 0.02985  loss_mask: 0.1863  loss_dice: 0.1784  loss_ce_0: 0.05714  loss_mask_0: 0.1881  loss_dice_0: 0.1836  loss_ce_1: 0.0453  loss_mask_1: 0.1927  loss_dice_1: 0.1845  loss_ce_2: 0.04018  loss_mask_2: 0.1928  loss_dice_2: 0.1854  loss_ce_3: 0.01834  loss_mask_3: 0.1849  loss_dice_3: 0.1808  loss_ce_4: 0.06135  loss_mask_4: 0.1847  loss_dice_4: 0.1873  loss_ce_5: 0.03489  loss_mask_5: 0.1855  loss_dice_5: 0.1788  loss_ce_6: 0.01453  loss_mask_6: 0.1792  loss_dice_6: 0.179  loss_ce_7: 0.04814  loss_mask_7: 0.1868  loss_dice_7: 0.1812  loss_ce_8: 0.04648  loss_mask_8: 0.184  loss_dice_8: 0.1822  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:11] d2.utils.events INFO:  eta: 0:43:24  iter: 29419  total_loss: 4.654  loss_ce: 0.02309  loss_mask: 0.1513  loss_dice: 0.1586  loss_ce_0: 0.07343  loss_mask_0: 0.1482  loss_dice_0: 0.1634  loss_ce_1: 0.02006  loss_mask_1: 0.1361  loss_dice_1: 0.1712  loss_ce_2: 0.0251  loss_mask_2: 0.1421  loss_dice_2: 0.1642  loss_ce_3: 0.02147  loss_mask_3: 0.1408  loss_dice_3: 0.1583  loss_ce_4: 0.01871  loss_mask_4: 0.1365  loss_dice_4: 0.166  loss_ce_5: 0.02245  loss_mask_5: 0.1488  loss_dice_5: 0.1815  loss_ce_6: 0.02689  loss_mask_6: 0.1398  loss_dice_6: 0.1742  loss_ce_7: 0.02008  loss_mask_7: 0.1364  loss_dice_7: 0.1618  loss_ce_8: 0.02319  loss_mask_8: 0.1273  loss_dice_8: 0.1593  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:14] d2.utils.events INFO:  eta: 0:43:21  iter: 29439  total_loss: 4.736  loss_ce: 0.01691  loss_mask: 0.2004  loss_dice: 0.1724  loss_ce_0: 0.057  loss_mask_0: 0.193  loss_dice_0: 0.1691  loss_ce_1: 0.02054  loss_mask_1: 0.1922  loss_dice_1: 0.167  loss_ce_2: 0.01526  loss_mask_2: 0.1949  loss_dice_2: 0.1763  loss_ce_3: 0.01463  loss_mask_3: 0.1944  loss_dice_3: 0.1697  loss_ce_4: 0.01983  loss_mask_4: 0.1962  loss_dice_4: 0.1727  loss_ce_5: 0.01418  loss_mask_5: 0.1918  loss_dice_5: 0.1648  loss_ce_6: 0.02252  loss_mask_6: 0.1943  loss_dice_6: 0.1701  loss_ce_7: 0.02967  loss_mask_7: 0.1915  loss_dice_7: 0.1652  loss_ce_8: 0.01421  loss_mask_8: 0.1945  loss_dice_8: 0.1703  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:16] d2.utils.events INFO:  eta: 0:43:16  iter: 29459  total_loss: 5.529  loss_ce: 0.0794  loss_mask: 0.2306  loss_dice: 0.1726  loss_ce_0: 0.06259  loss_mask_0: 0.2357  loss_dice_0: 0.169  loss_ce_1: 0.05554  loss_mask_1: 0.2306  loss_dice_1: 0.1755  loss_ce_2: 0.05248  loss_mask_2: 0.2343  loss_dice_2: 0.1794  loss_ce_3: 0.1013  loss_mask_3: 0.2281  loss_dice_3: 0.1794  loss_ce_4: 0.06883  loss_mask_4: 0.2263  loss_dice_4: 0.173  loss_ce_5: 0.05254  loss_mask_5: 0.2422  loss_dice_5: 0.1679  loss_ce_6: 0.05426  loss_mask_6: 0.2315  loss_dice_6: 0.1801  loss_ce_7: 0.05619  loss_mask_7: 0.2398  loss_dice_7: 0.1739  loss_ce_8: 0.06116  loss_mask_8: 0.2315  loss_dice_8: 0.1813  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:56:19] d2.utils.events INFO:  eta: 0:43:13  iter: 29479  total_loss: 4.314  loss_ce: 0.04565  loss_mask: 0.1872  loss_dice: 0.1678  loss_ce_0: 0.05074  loss_mask_0: 0.1813  loss_dice_0: 0.17  loss_ce_1: 0.04447  loss_mask_1: 0.1785  loss_dice_1: 0.1626  loss_ce_2: 0.0436  loss_mask_2: 0.1797  loss_dice_2: 0.1668  loss_ce_3: 0.04657  loss_mask_3: 0.1763  loss_dice_3: 0.1614  loss_ce_4: 0.04702  loss_mask_4: 0.18  loss_dice_4: 0.1636  loss_ce_5: 0.04641  loss_mask_5: 0.1775  loss_dice_5: 0.167  loss_ce_6: 0.03852  loss_mask_6: 0.1757  loss_dice_6: 0.1586  loss_ce_7: 0.04595  loss_mask_7: 0.1841  loss_dice_7: 0.171  loss_ce_8: 0.05054  loss_mask_8: 0.1862  loss_dice_8: 0.1666  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:22] d2.utils.events INFO:  eta: 0:43:11  iter: 29499  total_loss: 4.033  loss_ce: 0.01897  loss_mask: 0.1728  loss_dice: 0.1832  loss_ce_0: 0.04706  loss_mask_0: 0.174  loss_dice_0: 0.1841  loss_ce_1: 0.02417  loss_mask_1: 0.1727  loss_dice_1: 0.1784  loss_ce_2: 0.01734  loss_mask_2: 0.1764  loss_dice_2: 0.187  loss_ce_3: 0.01877  loss_mask_3: 0.1774  loss_dice_3: 0.1919  loss_ce_4: 0.01747  loss_mask_4: 0.1775  loss_dice_4: 0.1735  loss_ce_5: 0.01866  loss_mask_5: 0.1791  loss_dice_5: 0.1825  loss_ce_6: 0.0186  loss_mask_6: 0.1709  loss_dice_6: 0.1811  loss_ce_7: 0.02227  loss_mask_7: 0.1721  loss_dice_7: 0.1741  loss_ce_8: 0.01952  loss_mask_8: 0.1821  loss_dice_8: 0.1752  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:24] d2.utils.events INFO:  eta: 0:43:10  iter: 29519  total_loss: 5.133  loss_ce: 0.02689  loss_mask: 0.2156  loss_dice: 0.2131  loss_ce_0: 0.06508  loss_mask_0: 0.2281  loss_dice_0: 0.2033  loss_ce_1: 0.02797  loss_mask_1: 0.226  loss_dice_1: 0.2087  loss_ce_2: 0.02522  loss_mask_2: 0.2197  loss_dice_2: 0.2089  loss_ce_3: 0.02252  loss_mask_3: 0.2167  loss_dice_3: 0.2082  loss_ce_4: 0.02797  loss_mask_4: 0.2188  loss_dice_4: 0.206  loss_ce_5: 0.02408  loss_mask_5: 0.2206  loss_dice_5: 0.2129  loss_ce_6: 0.02647  loss_mask_6: 0.2187  loss_dice_6: 0.2065  loss_ce_7: 0.0308  loss_mask_7: 0.2164  loss_dice_7: 0.208  loss_ce_8: 0.02471  loss_mask_8: 0.2218  loss_dice_8: 0.217  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:56:27] d2.utils.events INFO:  eta: 0:43:08  iter: 29539  total_loss: 4.109  loss_ce: 0.01747  loss_mask: 0.2174  loss_dice: 0.1551  loss_ce_0: 0.04685  loss_mask_0: 0.2096  loss_dice_0: 0.1559  loss_ce_1: 0.03312  loss_mask_1: 0.2174  loss_dice_1: 0.1591  loss_ce_2: 0.02146  loss_mask_2: 0.2109  loss_dice_2: 0.1551  loss_ce_3: 0.0163  loss_mask_3: 0.2129  loss_dice_3: 0.1573  loss_ce_4: 0.0205  loss_mask_4: 0.2142  loss_dice_4: 0.1536  loss_ce_5: 0.0167  loss_mask_5: 0.2135  loss_dice_5: 0.1557  loss_ce_6: 0.01872  loss_mask_6: 0.205  loss_dice_6: 0.1581  loss_ce_7: 0.02532  loss_mask_7: 0.21  loss_dice_7: 0.1561  loss_ce_8: 0.01659  loss_mask_8: 0.2093  loss_dice_8: 0.1568  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:56:29] d2.utils.events INFO:  eta: 0:43:05  iter: 29559  total_loss: 3.846  loss_ce: 0.01407  loss_mask: 0.2133  loss_dice: 0.1423  loss_ce_0: 0.04865  loss_mask_0: 0.2083  loss_dice_0: 0.1366  loss_ce_1: 0.01184  loss_mask_1: 0.2222  loss_dice_1: 0.1369  loss_ce_2: 0.01116  loss_mask_2: 0.2122  loss_dice_2: 0.1402  loss_ce_3: 0.01171  loss_mask_3: 0.2097  loss_dice_3: 0.1421  loss_ce_4: 0.01192  loss_mask_4: 0.2114  loss_dice_4: 0.1371  loss_ce_5: 0.01159  loss_mask_5: 0.2116  loss_dice_5: 0.1416  loss_ce_6: 0.01211  loss_mask_6: 0.214  loss_dice_6: 0.1435  loss_ce_7: 0.009547  loss_mask_7: 0.2101  loss_dice_7: 0.1392  loss_ce_8: 0.01264  loss_mask_8: 0.2005  loss_dice_8: 0.1426  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:56:32] d2.utils.events INFO:  eta: 0:43:03  iter: 29579  total_loss: 5.552  loss_ce: 0.1296  loss_mask: 0.1647  loss_dice: 0.1793  loss_ce_0: 0.05206  loss_mask_0: 0.1451  loss_dice_0: 0.1676  loss_ce_1: 0.1002  loss_mask_1: 0.148  loss_dice_1: 0.1687  loss_ce_2: 0.1319  loss_mask_2: 0.1609  loss_dice_2: 0.1701  loss_ce_3: 0.125  loss_mask_3: 0.1482  loss_dice_3: 0.1572  loss_ce_4: 0.1137  loss_mask_4: 0.1433  loss_dice_4: 0.1688  loss_ce_5: 0.1317  loss_mask_5: 0.1489  loss_dice_5: 0.1711  loss_ce_6: 0.1307  loss_mask_6: 0.1572  loss_dice_6: 0.1657  loss_ce_7: 0.1159  loss_mask_7: 0.1514  loss_dice_7: 0.1608  loss_ce_8: 0.1307  loss_mask_8: 0.1502  loss_dice_8: 0.1679  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:56:34] d2.utils.events INFO:  eta: 0:43:01  iter: 29599  total_loss: 6.82  loss_ce: 0.08764  loss_mask: 0.2425  loss_dice: 0.1836  loss_ce_0: 0.05255  loss_mask_0: 0.2416  loss_dice_0: 0.1858  loss_ce_1: 0.08304  loss_mask_1: 0.2312  loss_dice_1: 0.1736  loss_ce_2: 0.0905  loss_mask_2: 0.2401  loss_dice_2: 0.1775  loss_ce_3: 0.08871  loss_mask_3: 0.2379  loss_dice_3: 0.1883  loss_ce_4: 0.08402  loss_mask_4: 0.2411  loss_dice_4: 0.1934  loss_ce_5: 0.09398  loss_mask_5: 0.2408  loss_dice_5: 0.1821  loss_ce_6: 0.07751  loss_mask_6: 0.241  loss_dice_6: 0.1902  loss_ce_7: 0.08172  loss_mask_7: 0.2393  loss_dice_7: 0.1844  loss_ce_8: 0.09645  loss_mask_8: 0.244  loss_dice_8: 0.189  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:56:37] d2.utils.events INFO:  eta: 0:42:57  iter: 29619  total_loss: 4.979  loss_ce: 0.06085  loss_mask: 0.2324  loss_dice: 0.1825  loss_ce_0: 0.0507  loss_mask_0: 0.2274  loss_dice_0: 0.1774  loss_ce_1: 0.04521  loss_mask_1: 0.2287  loss_dice_1: 0.1749  loss_ce_2: 0.07167  loss_mask_2: 0.2286  loss_dice_2: 0.1767  loss_ce_3: 0.05786  loss_mask_3: 0.2304  loss_dice_3: 0.1796  loss_ce_4: 0.05306  loss_mask_4: 0.2368  loss_dice_4: 0.1776  loss_ce_5: 0.07629  loss_mask_5: 0.2307  loss_dice_5: 0.175  loss_ce_6: 0.05761  loss_mask_6: 0.2265  loss_dice_6: 0.175  loss_ce_7: 0.05526  loss_mask_7: 0.2267  loss_dice_7: 0.1791  loss_ce_8: 0.07366  loss_mask_8: 0.2203  loss_dice_8: 0.1747  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:39] d2.utils.events INFO:  eta: 0:42:53  iter: 29639  total_loss: 4.943  loss_ce: 0.07555  loss_mask: 0.2188  loss_dice: 0.1701  loss_ce_0: 0.05082  loss_mask_0: 0.2148  loss_dice_0: 0.165  loss_ce_1: 0.06945  loss_mask_1: 0.2205  loss_dice_1: 0.1738  loss_ce_2: 0.09237  loss_mask_2: 0.2162  loss_dice_2: 0.1649  loss_ce_3: 0.08118  loss_mask_3: 0.2178  loss_dice_3: 0.1673  loss_ce_4: 0.07775  loss_mask_4: 0.2156  loss_dice_4: 0.1715  loss_ce_5: 0.0761  loss_mask_5: 0.2032  loss_dice_5: 0.1616  loss_ce_6: 0.09797  loss_mask_6: 0.2232  loss_dice_6: 0.1628  loss_ce_7: 0.07162  loss_mask_7: 0.2136  loss_dice_7: 0.161  loss_ce_8: 0.08447  loss_mask_8: 0.2137  loss_dice_8: 0.1637  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:56:42] d2.utils.events INFO:  eta: 0:42:50  iter: 29659  total_loss: 4.341  loss_ce: 0.02718  loss_mask: 0.1995  loss_dice: 0.1437  loss_ce_0: 0.05076  loss_mask_0: 0.1961  loss_dice_0: 0.1364  loss_ce_1: 0.02336  loss_mask_1: 0.2008  loss_dice_1: 0.1389  loss_ce_2: 0.029  loss_mask_2: 0.1903  loss_dice_2: 0.1374  loss_ce_3: 0.03164  loss_mask_3: 0.1895  loss_dice_3: 0.1391  loss_ce_4: 0.02408  loss_mask_4: 0.1876  loss_dice_4: 0.1413  loss_ce_5: 0.0338  loss_mask_5: 0.189  loss_dice_5: 0.142  loss_ce_6: 0.02823  loss_mask_6: 0.1987  loss_dice_6: 0.1406  loss_ce_7: 0.02617  loss_mask_7: 0.1909  loss_dice_7: 0.1384  loss_ce_8: 0.03113  loss_mask_8: 0.1972  loss_dice_8: 0.1406  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 15:56:44] d2.utils.events INFO:  eta: 0:42:49  iter: 29679  total_loss: 4.678  loss_ce: 0.0244  loss_mask: 0.1942  loss_dice: 0.1816  loss_ce_0: 0.06304  loss_mask_0: 0.1813  loss_dice_0: 0.1776  loss_ce_1: 0.02236  loss_mask_1: 0.1968  loss_dice_1: 0.1809  loss_ce_2: 0.02841  loss_mask_2: 0.1969  loss_dice_2: 0.1831  loss_ce_3: 0.02708  loss_mask_3: 0.1906  loss_dice_3: 0.1772  loss_ce_4: 0.02116  loss_mask_4: 0.1975  loss_dice_4: 0.1769  loss_ce_5: 0.02446  loss_mask_5: 0.2011  loss_dice_5: 0.184  loss_ce_6: 0.02578  loss_mask_6: 0.1906  loss_dice_6: 0.1767  loss_ce_7: 0.02114  loss_mask_7: 0.1934  loss_dice_7: 0.1787  loss_ce_8: 0.02627  loss_mask_8: 0.1922  loss_dice_8: 0.1826  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:47] d2.utils.events INFO:  eta: 0:42:47  iter: 29699  total_loss: 4.001  loss_ce: 0.008013  loss_mask: 0.1698  loss_dice: 0.1646  loss_ce_0: 0.06282  loss_mask_0: 0.1619  loss_dice_0: 0.1628  loss_ce_1: 0.01269  loss_mask_1: 0.1666  loss_dice_1: 0.1577  loss_ce_2: 0.007701  loss_mask_2: 0.16  loss_dice_2: 0.161  loss_ce_3: 0.01004  loss_mask_3: 0.1652  loss_dice_3: 0.1593  loss_ce_4: 0.005978  loss_mask_4: 0.1623  loss_dice_4: 0.1595  loss_ce_5: 0.01027  loss_mask_5: 0.1726  loss_dice_5: 0.1636  loss_ce_6: 0.007625  loss_mask_6: 0.1611  loss_dice_6: 0.1567  loss_ce_7: 0.006582  loss_mask_7: 0.1662  loss_dice_7: 0.163  loss_ce_8: 0.008273  loss_mask_8: 0.1576  loss_dice_8: 0.1558  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:49] d2.utils.events INFO:  eta: 0:42:45  iter: 29719  total_loss: 5.876  loss_ce: 0.01952  loss_mask: 0.2195  loss_dice: 0.1589  loss_ce_0: 0.05577  loss_mask_0: 0.2149  loss_dice_0: 0.1539  loss_ce_1: 0.02774  loss_mask_1: 0.2199  loss_dice_1: 0.1559  loss_ce_2: 0.05203  loss_mask_2: 0.2207  loss_dice_2: 0.1546  loss_ce_3: 0.02077  loss_mask_3: 0.2184  loss_dice_3: 0.1575  loss_ce_4: 0.02414  loss_mask_4: 0.2078  loss_dice_4: 0.1558  loss_ce_5: 0.04017  loss_mask_5: 0.2189  loss_dice_5: 0.1552  loss_ce_6: 0.02196  loss_mask_6: 0.2181  loss_dice_6: 0.1546  loss_ce_7: 0.02308  loss_mask_7: 0.2147  loss_dice_7: 0.1588  loss_ce_8: 0.05593  loss_mask_8: 0.2179  loss_dice_8: 0.1545  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:52] d2.utils.events INFO:  eta: 0:42:43  iter: 29739  total_loss: 4.963  loss_ce: 0.06061  loss_mask: 0.1683  loss_dice: 0.1541  loss_ce_0: 0.05603  loss_mask_0: 0.1717  loss_dice_0: 0.1563  loss_ce_1: 0.06303  loss_mask_1: 0.1716  loss_dice_1: 0.1503  loss_ce_2: 0.06369  loss_mask_2: 0.1804  loss_dice_2: 0.1603  loss_ce_3: 0.08379  loss_mask_3: 0.175  loss_dice_3: 0.1521  loss_ce_4: 0.05856  loss_mask_4: 0.1804  loss_dice_4: 0.1534  loss_ce_5: 0.05138  loss_mask_5: 0.1819  loss_dice_5: 0.1527  loss_ce_6: 0.07584  loss_mask_6: 0.1795  loss_dice_6: 0.1512  loss_ce_7: 0.0748  loss_mask_7: 0.1778  loss_dice_7: 0.1455  loss_ce_8: 0.04898  loss_mask_8: 0.1776  loss_dice_8: 0.1541  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:54] d2.utils.events INFO:  eta: 0:42:40  iter: 29759  total_loss: 4.708  loss_ce: 0.07423  loss_mask: 0.2054  loss_dice: 0.1372  loss_ce_0: 0.07026  loss_mask_0: 0.201  loss_dice_0: 0.1346  loss_ce_1: 0.08012  loss_mask_1: 0.2054  loss_dice_1: 0.1412  loss_ce_2: 0.07621  loss_mask_2: 0.2142  loss_dice_2: 0.131  loss_ce_3: 0.06893  loss_mask_3: 0.2133  loss_dice_3: 0.1387  loss_ce_4: 0.08307  loss_mask_4: 0.2096  loss_dice_4: 0.142  loss_ce_5: 0.06966  loss_mask_5: 0.2073  loss_dice_5: 0.1356  loss_ce_6: 0.06841  loss_mask_6: 0.2084  loss_dice_6: 0.1341  loss_ce_7: 0.1018  loss_mask_7: 0.2126  loss_dice_7: 0.1359  loss_ce_8: 0.08792  loss_mask_8: 0.2109  loss_dice_8: 0.1352  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:57] d2.utils.events INFO:  eta: 0:42:39  iter: 29779  total_loss: 4.384  loss_ce: 0.06177  loss_mask: 0.2086  loss_dice: 0.147  loss_ce_0: 0.06281  loss_mask_0: 0.2158  loss_dice_0: 0.148  loss_ce_1: 0.0654  loss_mask_1: 0.2127  loss_dice_1: 0.1433  loss_ce_2: 0.05805  loss_mask_2: 0.2104  loss_dice_2: 0.1485  loss_ce_3: 0.05913  loss_mask_3: 0.2103  loss_dice_3: 0.1431  loss_ce_4: 0.06399  loss_mask_4: 0.2147  loss_dice_4: 0.1448  loss_ce_5: 0.05584  loss_mask_5: 0.21  loss_dice_5: 0.1468  loss_ce_6: 0.05837  loss_mask_6: 0.2087  loss_dice_6: 0.1424  loss_ce_7: 0.06503  loss_mask_7: 0.2134  loss_dice_7: 0.1495  loss_ce_8: 0.06337  loss_mask_8: 0.2151  loss_dice_8: 0.1491  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:56:59] d2.utils.events INFO:  eta: 0:42:35  iter: 29799  total_loss: 4.31  loss_ce: 0.03851  loss_mask: 0.2205  loss_dice: 0.1586  loss_ce_0: 0.05671  loss_mask_0: 0.2124  loss_dice_0: 0.1545  loss_ce_1: 0.05305  loss_mask_1: 0.22  loss_dice_1: 0.1591  loss_ce_2: 0.04579  loss_mask_2: 0.2157  loss_dice_2: 0.1569  loss_ce_3: 0.03966  loss_mask_3: 0.2138  loss_dice_3: 0.153  loss_ce_4: 0.05443  loss_mask_4: 0.1986  loss_dice_4: 0.1518  loss_ce_5: 0.04578  loss_mask_5: 0.2088  loss_dice_5: 0.1594  loss_ce_6: 0.0447  loss_mask_6: 0.2198  loss_dice_6: 0.1552  loss_ce_7: 0.05156  loss_mask_7: 0.2149  loss_dice_7: 0.1569  loss_ce_8: 0.04621  loss_mask_8: 0.2135  loss_dice_8: 0.1533  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:57:02] d2.utils.events INFO:  eta: 0:42:32  iter: 29819  total_loss: 6.184  loss_ce: 0.04138  loss_mask: 0.1859  loss_dice: 0.1669  loss_ce_0: 0.06838  loss_mask_0: 0.1854  loss_dice_0: 0.1597  loss_ce_1: 0.0414  loss_mask_1: 0.1884  loss_dice_1: 0.1686  loss_ce_2: 0.04074  loss_mask_2: 0.1907  loss_dice_2: 0.1696  loss_ce_3: 0.0399  loss_mask_3: 0.1874  loss_dice_3: 0.1694  loss_ce_4: 0.04358  loss_mask_4: 0.1882  loss_dice_4: 0.1669  loss_ce_5: 0.04724  loss_mask_5: 0.1877  loss_dice_5: 0.1769  loss_ce_6: 0.04097  loss_mask_6: 0.1902  loss_dice_6: 0.1672  loss_ce_7: 0.04392  loss_mask_7: 0.1832  loss_dice_7: 0.1666  loss_ce_8: 0.04398  loss_mask_8: 0.1882  loss_dice_8: 0.1733  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:57:05] d2.utils.events INFO:  eta: 0:42:29  iter: 29839  total_loss: 5.456  loss_ce: 0.08777  loss_mask: 0.1882  loss_dice: 0.1601  loss_ce_0: 0.05842  loss_mask_0: 0.1751  loss_dice_0: 0.1561  loss_ce_1: 0.09039  loss_mask_1: 0.1777  loss_dice_1: 0.1575  loss_ce_2: 0.09708  loss_mask_2: 0.1831  loss_dice_2: 0.1579  loss_ce_3: 0.09532  loss_mask_3: 0.1852  loss_dice_3: 0.1599  loss_ce_4: 0.09202  loss_mask_4: 0.1917  loss_dice_4: 0.1541  loss_ce_5: 0.09224  loss_mask_5: 0.1859  loss_dice_5: 0.1643  loss_ce_6: 0.1054  loss_mask_6: 0.1959  loss_dice_6: 0.1593  loss_ce_7: 0.09801  loss_mask_7: 0.1849  loss_dice_7: 0.1633  loss_ce_8: 0.1117  loss_mask_8: 0.1925  loss_dice_8: 0.1582  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:57:07] d2.utils.events INFO:  eta: 0:42:25  iter: 29859  total_loss: 5.223  loss_ce: 0.05317  loss_mask: 0.1995  loss_dice: 0.1567  loss_ce_0: 0.05697  loss_mask_0: 0.2078  loss_dice_0: 0.1537  loss_ce_1: 0.05301  loss_mask_1: 0.2053  loss_dice_1: 0.1542  loss_ce_2: 0.05674  loss_mask_2: 0.2023  loss_dice_2: 0.1562  loss_ce_3: 0.05101  loss_mask_3: 0.2093  loss_dice_3: 0.1556  loss_ce_4: 0.04915  loss_mask_4: 0.2023  loss_dice_4: 0.1576  loss_ce_5: 0.05635  loss_mask_5: 0.2016  loss_dice_5: 0.1565  loss_ce_6: 0.0519  loss_mask_6: 0.2039  loss_dice_6: 0.1557  loss_ce_7: 0.04946  loss_mask_7: 0.2077  loss_dice_7: 0.1558  loss_ce_8: 0.06054  loss_mask_8: 0.1973  loss_dice_8: 0.1591  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:57:10] d2.utils.events INFO:  eta: 0:42:21  iter: 29879  total_loss: 4.231  loss_ce: 0.0379  loss_mask: 0.2021  loss_dice: 0.1277  loss_ce_0: 0.06817  loss_mask_0: 0.204  loss_dice_0: 0.1264  loss_ce_1: 0.03577  loss_mask_1: 0.2068  loss_dice_1: 0.1306  loss_ce_2: 0.03777  loss_mask_2: 0.1979  loss_dice_2: 0.1303  loss_ce_3: 0.03802  loss_mask_3: 0.1973  loss_dice_3: 0.1335  loss_ce_4: 0.03787  loss_mask_4: 0.2079  loss_dice_4: 0.1319  loss_ce_5: 0.03749  loss_mask_5: 0.2116  loss_dice_5: 0.1349  loss_ce_6: 0.0377  loss_mask_6: 0.2009  loss_dice_6: 0.1299  loss_ce_7: 0.03724  loss_mask_7: 0.2056  loss_dice_7: 0.1258  loss_ce_8: 0.04028  loss_mask_8: 0.1959  loss_dice_8: 0.1275  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:57:12] d2.utils.events INFO:  eta: 0:42:17  iter: 29899  total_loss: 4.275  loss_ce: 0.02632  loss_mask: 0.2037  loss_dice: 0.162  loss_ce_0: 0.05952  loss_mask_0: 0.2065  loss_dice_0: 0.155  loss_ce_1: 0.02822  loss_mask_1: 0.2134  loss_dice_1: 0.1656  loss_ce_2: 0.02864  loss_mask_2: 0.2095  loss_dice_2: 0.166  loss_ce_3: 0.02756  loss_mask_3: 0.2089  loss_dice_3: 0.1643  loss_ce_4: 0.02948  loss_mask_4: 0.2101  loss_dice_4: 0.1622  loss_ce_5: 0.02652  loss_mask_5: 0.1983  loss_dice_5: 0.1604  loss_ce_6: 0.02765  loss_mask_6: 0.208  loss_dice_6: 0.1619  loss_ce_7: 0.02708  loss_mask_7: 0.2148  loss_dice_7: 0.1627  loss_ce_8: 0.0298  loss_mask_8: 0.2142  loss_dice_8: 0.159  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:57:15] d2.utils.events INFO:  eta: 0:42:11  iter: 29919  total_loss: 5.281  loss_ce: 0.187  loss_mask: 0.197  loss_dice: 0.1741  loss_ce_0: 0.06426  loss_mask_0: 0.2006  loss_dice_0: 0.1835  loss_ce_1: 0.0991  loss_mask_1: 0.2009  loss_dice_1: 0.182  loss_ce_2: 0.2299  loss_mask_2: 0.1978  loss_dice_2: 0.1779  loss_ce_3: 0.1718  loss_mask_3: 0.2039  loss_dice_3: 0.1714  loss_ce_4: 0.1288  loss_mask_4: 0.1971  loss_dice_4: 0.1777  loss_ce_5: 0.2642  loss_mask_5: 0.203  loss_dice_5: 0.1765  loss_ce_6: 0.1722  loss_mask_6: 0.21  loss_dice_6: 0.1813  loss_ce_7: 0.1193  loss_mask_7: 0.1941  loss_dice_7: 0.1797  loss_ce_8: 0.1662  loss_mask_8: 0.191  loss_dice_8: 0.1753  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 15:57:17] d2.utils.events INFO:  eta: 0:42:07  iter: 29939  total_loss: 5.079  loss_ce: 0.03688  loss_mask: 0.2397  loss_dice: 0.1624  loss_ce_0: 0.05907  loss_mask_0: 0.2307  loss_dice_0: 0.1728  loss_ce_1: 0.03288  loss_mask_1: 0.2326  loss_dice_1: 0.1692  loss_ce_2: 0.03307  loss_mask_2: 0.2349  loss_dice_2: 0.1647  loss_ce_3: 0.03723  loss_mask_3: 0.2394  loss_dice_3: 0.1755  loss_ce_4: 0.03228  loss_mask_4: 0.2459  loss_dice_4: 0.1678  loss_ce_5: 0.03517  loss_mask_5: 0.2391  loss_dice_5: 0.1672  loss_ce_6: 0.03696  loss_mask_6: 0.2321  loss_dice_6: 0.1714  loss_ce_7: 0.03268  loss_mask_7: 0.2443  loss_dice_7: 0.1737  loss_ce_8: 0.03292  loss_mask_8: 0.2321  loss_dice_8: 0.1722  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:57:20] d2.utils.events INFO:  eta: 0:42:03  iter: 29959  total_loss: 5.475  loss_ce: 0.1143  loss_mask: 0.1653  loss_dice: 0.1663  loss_ce_0: 0.06756  loss_mask_0: 0.1573  loss_dice_0: 0.1653  loss_ce_1: 0.1031  loss_mask_1: 0.1497  loss_dice_1: 0.1613  loss_ce_2: 0.1148  loss_mask_2: 0.166  loss_dice_2: 0.1631  loss_ce_3: 0.1161  loss_mask_3: 0.1696  loss_dice_3: 0.1699  loss_ce_4: 0.1018  loss_mask_4: 0.1674  loss_dice_4: 0.1743  loss_ce_5: 0.122  loss_mask_5: 0.163  loss_dice_5: 0.1746  loss_ce_6: 0.1195  loss_mask_6: 0.165  loss_dice_6: 0.1717  loss_ce_7: 0.1105  loss_mask_7: 0.1579  loss_dice_7: 0.1736  loss_ce_8: 0.1235  loss_mask_8: 0.1517  loss_dice_8: 0.1599  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:57:22] d2.utils.events INFO:  eta: 0:41:59  iter: 29979  total_loss: 4.878  loss_ce: 0.0392  loss_mask: 0.1651  loss_dice: 0.2367  loss_ce_0: 0.06319  loss_mask_0: 0.1711  loss_dice_0: 0.2258  loss_ce_1: 0.05  loss_mask_1: 0.1586  loss_dice_1: 0.2308  loss_ce_2: 0.03838  loss_mask_2: 0.1751  loss_dice_2: 0.2279  loss_ce_3: 0.03474  loss_mask_3: 0.1716  loss_dice_3: 0.229  loss_ce_4: 0.04885  loss_mask_4: 0.1672  loss_dice_4: 0.2322  loss_ce_5: 0.03445  loss_mask_5: 0.1636  loss_dice_5: 0.2301  loss_ce_6: 0.0378  loss_mask_6: 0.1699  loss_dice_6: 0.2237  loss_ce_7: 0.04889  loss_mask_7: 0.172  loss_dice_7: 0.2207  loss_ce_8: 0.04097  loss_mask_8: 0.1688  loss_dice_8: 0.2257  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 15:57:25] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0029999.pth
[04/13 15:57:25] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 15:57:25] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 15:57:25] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 15:57:25] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 15:57:25] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 15:57:28] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0529 s/iter. Eval: 0.2280 s/iter. Total: 0.2815 s/iter. ETA=0:03:57
[04/13 15:57:33] d2.evaluation.evaluator INFO: Inference done 29/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2284 s/iter. Total: 0.2821 s/iter. ETA=0:03:53
[04/13 15:57:38] d2.evaluation.evaluator INFO: Inference done 47/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2296 s/iter. Total: 0.2833 s/iter. ETA=0:03:49
[04/13 15:57:43] d2.evaluation.evaluator INFO: Inference done 65/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2293 s/iter. Total: 0.2831 s/iter. ETA=0:03:43
[04/13 15:57:49] d2.evaluation.evaluator INFO: Inference done 83/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2294 s/iter. Total: 0.2831 s/iter. ETA=0:03:38
[04/13 15:57:54] d2.evaluation.evaluator INFO: Inference done 101/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2297 s/iter. Total: 0.2833 s/iter. ETA=0:03:33
[04/13 15:57:59] d2.evaluation.evaluator INFO: Inference done 119/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2299 s/iter. Total: 0.2836 s/iter. ETA=0:03:29
[04/13 15:58:04] d2.evaluation.evaluator INFO: Inference done 137/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2301 s/iter. Total: 0.2838 s/iter. ETA=0:03:24
[04/13 15:58:09] d2.evaluation.evaluator INFO: Inference done 155/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2302 s/iter. Total: 0.2839 s/iter. ETA=0:03:19
[04/13 15:58:14] d2.evaluation.evaluator INFO: Inference done 173/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2307 s/iter. Total: 0.2844 s/iter. ETA=0:03:14
[04/13 15:58:19] d2.evaluation.evaluator INFO: Inference done 191/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2307 s/iter. Total: 0.2845 s/iter. ETA=0:03:09
[04/13 15:58:25] d2.evaluation.evaluator INFO: Inference done 209/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2309 s/iter. Total: 0.2847 s/iter. ETA=0:03:04
[04/13 15:58:30] d2.evaluation.evaluator INFO: Inference done 227/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2309 s/iter. Total: 0.2847 s/iter. ETA=0:02:59
[04/13 15:58:35] d2.evaluation.evaluator INFO: Inference done 245/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2310 s/iter. Total: 0.2848 s/iter. ETA=0:02:54
[04/13 15:58:40] d2.evaluation.evaluator INFO: Inference done 263/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2310 s/iter. Total: 0.2848 s/iter. ETA=0:02:48
[04/13 15:58:45] d2.evaluation.evaluator INFO: Inference done 281/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2315 s/iter. Total: 0.2853 s/iter. ETA=0:02:44
[04/13 15:58:50] d2.evaluation.evaluator INFO: Inference done 299/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2316 s/iter. Total: 0.2854 s/iter. ETA=0:02:38
[04/13 15:58:56] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2317 s/iter. Total: 0.2854 s/iter. ETA=0:02:33
[04/13 15:59:01] d2.evaluation.evaluator INFO: Inference done 335/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2317 s/iter. Total: 0.2855 s/iter. ETA=0:02:28
[04/13 15:59:06] d2.evaluation.evaluator INFO: Inference done 353/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2318 s/iter. Total: 0.2856 s/iter. ETA=0:02:23
[04/13 15:59:11] d2.evaluation.evaluator INFO: Inference done 371/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2320 s/iter. Total: 0.2857 s/iter. ETA=0:02:18
[04/13 15:59:16] d2.evaluation.evaluator INFO: Inference done 389/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2321 s/iter. Total: 0.2859 s/iter. ETA=0:02:13
[04/13 15:59:21] d2.evaluation.evaluator INFO: Inference done 406/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2324 s/iter. Total: 0.2862 s/iter. ETA=0:02:08
[04/13 15:59:26] d2.evaluation.evaluator INFO: Inference done 424/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2324 s/iter. Total: 0.2863 s/iter. ETA=0:02:03
[04/13 15:59:32] d2.evaluation.evaluator INFO: Inference done 442/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2325 s/iter. Total: 0.2864 s/iter. ETA=0:01:58
[04/13 15:59:37] d2.evaluation.evaluator INFO: Inference done 460/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2326 s/iter. Total: 0.2865 s/iter. ETA=0:01:53
[04/13 15:59:42] d2.evaluation.evaluator INFO: Inference done 478/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2326 s/iter. Total: 0.2865 s/iter. ETA=0:01:48
[04/13 15:59:47] d2.evaluation.evaluator INFO: Inference done 496/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2328 s/iter. Total: 0.2867 s/iter. ETA=0:01:43
[04/13 15:59:52] d2.evaluation.evaluator INFO: Inference done 514/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2328 s/iter. Total: 0.2867 s/iter. ETA=0:01:38
[04/13 15:59:58] d2.evaluation.evaluator INFO: Inference done 532/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2868 s/iter. ETA=0:01:32
[04/13 16:00:03] d2.evaluation.evaluator INFO: Inference done 550/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2868 s/iter. ETA=0:01:27
[04/13 16:00:08] d2.evaluation.evaluator INFO: Inference done 568/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2868 s/iter. ETA=0:01:22
[04/13 16:00:13] d2.evaluation.evaluator INFO: Inference done 586/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2868 s/iter. ETA=0:01:17
[04/13 16:00:18] d2.evaluation.evaluator INFO: Inference done 604/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2868 s/iter. ETA=0:01:12
[04/13 16:00:23] d2.evaluation.evaluator INFO: Inference done 622/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2328 s/iter. Total: 0.2867 s/iter. ETA=0:01:07
[04/13 16:00:28] d2.evaluation.evaluator INFO: Inference done 640/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2328 s/iter. Total: 0.2867 s/iter. ETA=0:01:01
[04/13 16:00:34] d2.evaluation.evaluator INFO: Inference done 658/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2328 s/iter. Total: 0.2866 s/iter. ETA=0:00:56
[04/13 16:00:39] d2.evaluation.evaluator INFO: Inference done 676/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2328 s/iter. Total: 0.2866 s/iter. ETA=0:00:51
[04/13 16:00:44] d2.evaluation.evaluator INFO: Inference done 694/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2328 s/iter. Total: 0.2866 s/iter. ETA=0:00:46
[04/13 16:00:49] d2.evaluation.evaluator INFO: Inference done 712/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2328 s/iter. Total: 0.2867 s/iter. ETA=0:00:41
[04/13 16:00:54] d2.evaluation.evaluator INFO: Inference done 730/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2868 s/iter. ETA=0:00:36
[04/13 16:01:00] d2.evaluation.evaluator INFO: Inference done 748/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2330 s/iter. Total: 0.2869 s/iter. ETA=0:00:30
[04/13 16:01:05] d2.evaluation.evaluator INFO: Inference done 766/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2331 s/iter. Total: 0.2870 s/iter. ETA=0:00:25
[04/13 16:01:10] d2.evaluation.evaluator INFO: Inference done 784/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2331 s/iter. Total: 0.2870 s/iter. ETA=0:00:20
[04/13 16:01:15] d2.evaluation.evaluator INFO: Inference done 802/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2331 s/iter. Total: 0.2870 s/iter. ETA=0:00:15
[04/13 16:01:20] d2.evaluation.evaluator INFO: Inference done 820/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2331 s/iter. Total: 0.2870 s/iter. ETA=0:00:10
[04/13 16:01:26] d2.evaluation.evaluator INFO: Inference done 838/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2331 s/iter. Total: 0.2870 s/iter. ETA=0:00:05
[04/13 16:01:31] d2.evaluation.evaluator INFO: Inference done 856/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2331 s/iter. Total: 0.2870 s/iter. ETA=0:00:00
[04/13 16:01:31] d2.evaluation.evaluator INFO: Total inference time: 0:04:04.276772 (0.287047 s / iter per device, on 1 devices)
[04/13 16:01:31] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:45 (0.052946 s / iter per device, on 1 devices)
[04/13 16:01:32] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 16:01:32] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 16:01:33] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 16:01:33] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 16:01:34] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.28 seconds.
[04/13 16:01:34] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 16:01:34] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 16:01:34] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 16:01:34] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 16:01:34] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 16:01:38] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 16:01:40] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 2.14 seconds.
[04/13 16:01:40] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 16:01:40] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 16:01:40] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 80.720 | 80.720 | 80.720 |  nan  |  nan  | 80.720 |
[04/13 16:01:40] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 16:01:40] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 81.982 | defect     | 79.457 |
[04/13 16:01:40] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 16:01:40] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 16:01:40] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 16:01:40] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 16:01:40] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 16:01:40] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 16:01:40] d2.evaluation.testing INFO: copypaste: 80.7197,80.7197,80.7197,nan,nan,80.7197
[04/13 16:01:40] d2.utils.events INFO:  eta: 0:41:57  iter: 29999  total_loss: 4.723  loss_ce: 0.1274  loss_mask: 0.1935  loss_dice: 0.1525  loss_ce_0: 0.06197  loss_mask_0: 0.1934  loss_dice_0: 0.1541  loss_ce_1: 0.1117  loss_mask_1: 0.1959  loss_dice_1: 0.1545  loss_ce_2: 0.07511  loss_mask_2: 0.2046  loss_dice_2: 0.1546  loss_ce_3: 0.1277  loss_mask_3: 0.2011  loss_dice_3: 0.1563  loss_ce_4: 0.1268  loss_mask_4: 0.1986  loss_dice_4: 0.1554  loss_ce_5: 0.08502  loss_mask_5: 0.2032  loss_dice_5: 0.1638  loss_ce_6: 0.1296  loss_mask_6: 0.2083  loss_dice_6: 0.1588  loss_ce_7: 0.1209  loss_mask_7: 0.1928  loss_dice_7: 0.1522  loss_ce_8: 0.08641  loss_mask_8: 0.2045  loss_dice_8: 0.1536  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:01:43] d2.utils.events INFO:  eta: 0:41:57  iter: 30019  total_loss: 4.522  loss_ce: 0.04249  loss_mask: 0.1877  loss_dice: 0.1857  loss_ce_0: 0.06321  loss_mask_0: 0.1868  loss_dice_0: 0.1902  loss_ce_1: 0.03651  loss_mask_1: 0.1907  loss_dice_1: 0.1994  loss_ce_2: 0.04144  loss_mask_2: 0.1898  loss_dice_2: 0.1937  loss_ce_3: 0.03804  loss_mask_3: 0.1971  loss_dice_3: 0.2022  loss_ce_4: 0.03456  loss_mask_4: 0.1948  loss_dice_4: 0.1983  loss_ce_5: 0.03951  loss_mask_5: 0.1907  loss_dice_5: 0.201  loss_ce_6: 0.03523  loss_mask_6: 0.1825  loss_dice_6: 0.1826  loss_ce_7: 0.03711  loss_mask_7: 0.1932  loss_dice_7: 0.1892  loss_ce_8: 0.03723  loss_mask_8: 0.1962  loss_dice_8: 0.1971  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:01:45] d2.utils.events INFO:  eta: 0:41:54  iter: 30039  total_loss: 5.306  loss_ce: 0.04509  loss_mask: 0.2094  loss_dice: 0.1972  loss_ce_0: 0.06056  loss_mask_0: 0.2032  loss_dice_0: 0.2043  loss_ce_1: 0.09229  loss_mask_1: 0.1996  loss_dice_1: 0.2063  loss_ce_2: 0.04912  loss_mask_2: 0.2039  loss_dice_2: 0.1997  loss_ce_3: 0.04324  loss_mask_3: 0.2071  loss_dice_3: 0.2101  loss_ce_4: 0.06836  loss_mask_4: 0.2093  loss_dice_4: 0.2095  loss_ce_5: 0.04468  loss_mask_5: 0.2095  loss_dice_5: 0.209  loss_ce_6: 0.04164  loss_mask_6: 0.1988  loss_dice_6: 0.1988  loss_ce_7: 0.07435  loss_mask_7: 0.2032  loss_dice_7: 0.1944  loss_ce_8: 0.04484  loss_mask_8: 0.2045  loss_dice_8: 0.2064  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:01:48] d2.utils.events INFO:  eta: 0:41:51  iter: 30059  total_loss: 4.567  loss_ce: 0.02172  loss_mask: 0.1778  loss_dice: 0.1794  loss_ce_0: 0.05653  loss_mask_0: 0.1859  loss_dice_0: 0.1798  loss_ce_1: 0.03928  loss_mask_1: 0.1826  loss_dice_1: 0.1839  loss_ce_2: 0.02182  loss_mask_2: 0.1878  loss_dice_2: 0.1824  loss_ce_3: 0.01964  loss_mask_3: 0.1775  loss_dice_3: 0.1884  loss_ce_4: 0.02106  loss_mask_4: 0.1833  loss_dice_4: 0.1823  loss_ce_5: 0.02073  loss_mask_5: 0.1779  loss_dice_5: 0.1772  loss_ce_6: 0.01876  loss_mask_6: 0.1849  loss_dice_6: 0.1883  loss_ce_7: 0.02579  loss_mask_7: 0.1897  loss_dice_7: 0.1871  loss_ce_8: 0.01987  loss_mask_8: 0.1865  loss_dice_8: 0.1871  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:01:50] d2.utils.events INFO:  eta: 0:41:49  iter: 30079  total_loss: 5.095  loss_ce: 0.03375  loss_mask: 0.2037  loss_dice: 0.1375  loss_ce_0: 0.06294  loss_mask_0: 0.2096  loss_dice_0: 0.1366  loss_ce_1: 0.03767  loss_mask_1: 0.2051  loss_dice_1: 0.1399  loss_ce_2: 0.03354  loss_mask_2: 0.2045  loss_dice_2: 0.1472  loss_ce_3: 0.03227  loss_mask_3: 0.2115  loss_dice_3: 0.1494  loss_ce_4: 0.03365  loss_mask_4: 0.2067  loss_dice_4: 0.1433  loss_ce_5: 0.03347  loss_mask_5: 0.2034  loss_dice_5: 0.1482  loss_ce_6: 0.03237  loss_mask_6: 0.2077  loss_dice_6: 0.1376  loss_ce_7: 0.03406  loss_mask_7: 0.2028  loss_dice_7: 0.1424  loss_ce_8: 0.03409  loss_mask_8: 0.2026  loss_dice_8: 0.1417  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:01:53] d2.utils.events INFO:  eta: 0:41:45  iter: 30099  total_loss: 5.121  loss_ce: 0.03091  loss_mask: 0.1936  loss_dice: 0.1657  loss_ce_0: 0.05218  loss_mask_0: 0.191  loss_dice_0: 0.1657  loss_ce_1: 0.03288  loss_mask_1: 0.1866  loss_dice_1: 0.1535  loss_ce_2: 0.02824  loss_mask_2: 0.1943  loss_dice_2: 0.1612  loss_ce_3: 0.03029  loss_mask_3: 0.1899  loss_dice_3: 0.1616  loss_ce_4: 0.03397  loss_mask_4: 0.1959  loss_dice_4: 0.1617  loss_ce_5: 0.0282  loss_mask_5: 0.1939  loss_dice_5: 0.1631  loss_ce_6: 0.03348  loss_mask_6: 0.1946  loss_dice_6: 0.1562  loss_ce_7: 0.03263  loss_mask_7: 0.1998  loss_dice_7: 0.1656  loss_ce_8: 0.02932  loss_mask_8: 0.1918  loss_dice_8: 0.161  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:01:55] d2.utils.events INFO:  eta: 0:41:42  iter: 30119  total_loss: 4.312  loss_ce: 0.02153  loss_mask: 0.2278  loss_dice: 0.1564  loss_ce_0: 0.06274  loss_mask_0: 0.232  loss_dice_0: 0.1573  loss_ce_1: 0.02087  loss_mask_1: 0.2355  loss_dice_1: 0.1606  loss_ce_2: 0.01831  loss_mask_2: 0.2318  loss_dice_2: 0.1598  loss_ce_3: 0.01926  loss_mask_3: 0.2351  loss_dice_3: 0.1607  loss_ce_4: 0.01983  loss_mask_4: 0.2256  loss_dice_4: 0.1544  loss_ce_5: 0.01841  loss_mask_5: 0.2281  loss_dice_5: 0.1545  loss_ce_6: 0.02124  loss_mask_6: 0.2352  loss_dice_6: 0.1571  loss_ce_7: 0.01799  loss_mask_7: 0.2285  loss_dice_7: 0.1618  loss_ce_8: 0.02059  loss_mask_8: 0.2337  loss_dice_8: 0.1554  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:01:58] d2.utils.events INFO:  eta: 0:41:40  iter: 30139  total_loss: 3.974  loss_ce: 0.01469  loss_mask: 0.2019  loss_dice: 0.1572  loss_ce_0: 0.06631  loss_mask_0: 0.2023  loss_dice_0: 0.1442  loss_ce_1: 0.01533  loss_mask_1: 0.2054  loss_dice_1: 0.1647  loss_ce_2: 0.01312  loss_mask_2: 0.1991  loss_dice_2: 0.1567  loss_ce_3: 0.01182  loss_mask_3: 0.1985  loss_dice_3: 0.1593  loss_ce_4: 0.01408  loss_mask_4: 0.1914  loss_dice_4: 0.1538  loss_ce_5: 0.01248  loss_mask_5: 0.1995  loss_dice_5: 0.1562  loss_ce_6: 0.01495  loss_mask_6: 0.1999  loss_dice_6: 0.1496  loss_ce_7: 0.01285  loss_mask_7: 0.199  loss_dice_7: 0.1604  loss_ce_8: 0.01507  loss_mask_8: 0.1984  loss_dice_8: 0.1577  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:00] d2.utils.events INFO:  eta: 0:41:36  iter: 30159  total_loss: 4.605  loss_ce: 0.01318  loss_mask: 0.2273  loss_dice: 0.1407  loss_ce_0: 0.06282  loss_mask_0: 0.2346  loss_dice_0: 0.1418  loss_ce_1: 0.01441  loss_mask_1: 0.231  loss_dice_1: 0.1394  loss_ce_2: 0.01215  loss_mask_2: 0.2352  loss_dice_2: 0.1426  loss_ce_3: 0.01165  loss_mask_3: 0.2288  loss_dice_3: 0.1423  loss_ce_4: 0.01842  loss_mask_4: 0.2335  loss_dice_4: 0.1342  loss_ce_5: 0.0118  loss_mask_5: 0.2275  loss_dice_5: 0.1387  loss_ce_6: 0.01459  loss_mask_6: 0.2289  loss_dice_6: 0.1389  loss_ce_7: 0.01685  loss_mask_7: 0.2329  loss_dice_7: 0.1399  loss_ce_8: 0.01522  loss_mask_8: 0.2358  loss_dice_8: 0.1393  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:03] d2.utils.events INFO:  eta: 0:41:33  iter: 30179  total_loss: 4.707  loss_ce: 0.01057  loss_mask: 0.2062  loss_dice: 0.1665  loss_ce_0: 0.06466  loss_mask_0: 0.2119  loss_dice_0: 0.1672  loss_ce_1: 0.01072  loss_mask_1: 0.2054  loss_dice_1: 0.1743  loss_ce_2: 0.01003  loss_mask_2: 0.2142  loss_dice_2: 0.1705  loss_ce_3: 0.009546  loss_mask_3: 0.212  loss_dice_3: 0.17  loss_ce_4: 0.01064  loss_mask_4: 0.2083  loss_dice_4: 0.1656  loss_ce_5: 0.01022  loss_mask_5: 0.203  loss_dice_5: 0.1649  loss_ce_6: 0.01076  loss_mask_6: 0.2018  loss_dice_6: 0.1694  loss_ce_7: 0.00986  loss_mask_7: 0.203  loss_dice_7: 0.1634  loss_ce_8: 0.0109  loss_mask_8: 0.1971  loss_dice_8: 0.1709  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:05] d2.utils.events INFO:  eta: 0:41:31  iter: 30199  total_loss: 5.261  loss_ce: 0.01041  loss_mask: 0.2037  loss_dice: 0.2127  loss_ce_0: 0.06226  loss_mask_0: 0.2074  loss_dice_0: 0.222  loss_ce_1: 0.01167  loss_mask_1: 0.206  loss_dice_1: 0.2172  loss_ce_2: 0.01072  loss_mask_2: 0.2047  loss_dice_2: 0.2136  loss_ce_3: 0.009983  loss_mask_3: 0.1942  loss_dice_3: 0.2137  loss_ce_4: 0.01084  loss_mask_4: 0.201  loss_dice_4: 0.2068  loss_ce_5: 0.01046  loss_mask_5: 0.1933  loss_dice_5: 0.2164  loss_ce_6: 0.0105  loss_mask_6: 0.2051  loss_dice_6: 0.2069  loss_ce_7: 0.009763  loss_mask_7: 0.2121  loss_dice_7: 0.2083  loss_ce_8: 0.01094  loss_mask_8: 0.2049  loss_dice_8: 0.2052  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:08] d2.utils.events INFO:  eta: 0:41:27  iter: 30219  total_loss: 5.047  loss_ce: 0.02995  loss_mask: 0.2382  loss_dice: 0.1606  loss_ce_0: 0.06213  loss_mask_0: 0.2152  loss_dice_0: 0.159  loss_ce_1: 0.0308  loss_mask_1: 0.2347  loss_dice_1: 0.157  loss_ce_2: 0.02461  loss_mask_2: 0.2285  loss_dice_2: 0.1552  loss_ce_3: 0.03289  loss_mask_3: 0.228  loss_dice_3: 0.1587  loss_ce_4: 0.0452  loss_mask_4: 0.23  loss_dice_4: 0.158  loss_ce_5: 0.01744  loss_mask_5: 0.2291  loss_dice_5: 0.1555  loss_ce_6: 0.03213  loss_mask_6: 0.231  loss_dice_6: 0.1547  loss_ce_7: 0.03896  loss_mask_7: 0.2261  loss_dice_7: 0.1585  loss_ce_8: 0.0385  loss_mask_8: 0.2272  loss_dice_8: 0.1494  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:10] d2.utils.events INFO:  eta: 0:41:22  iter: 30239  total_loss: 5.27  loss_ce: 0.01803  loss_mask: 0.2071  loss_dice: 0.1924  loss_ce_0: 0.06101  loss_mask_0: 0.1914  loss_dice_0: 0.1952  loss_ce_1: 0.01646  loss_mask_1: 0.217  loss_dice_1: 0.1962  loss_ce_2: 0.01713  loss_mask_2: 0.2054  loss_dice_2: 0.1936  loss_ce_3: 0.01739  loss_mask_3: 0.1954  loss_dice_3: 0.2032  loss_ce_4: 0.01754  loss_mask_4: 0.2047  loss_dice_4: 0.1893  loss_ce_5: 0.01576  loss_mask_5: 0.2013  loss_dice_5: 0.2002  loss_ce_6: 0.01732  loss_mask_6: 0.2011  loss_dice_6: 0.1965  loss_ce_7: 0.01492  loss_mask_7: 0.2  loss_dice_7: 0.1919  loss_ce_8: 0.01752  loss_mask_8: 0.2035  loss_dice_8: 0.1934  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:13] d2.utils.events INFO:  eta: 0:41:20  iter: 30259  total_loss: 5.795  loss_ce: 0.07936  loss_mask: 0.1834  loss_dice: 0.174  loss_ce_0: 0.06691  loss_mask_0: 0.194  loss_dice_0: 0.177  loss_ce_1: 0.06712  loss_mask_1: 0.1921  loss_dice_1: 0.1815  loss_ce_2: 0.05432  loss_mask_2: 0.1895  loss_dice_2: 0.1726  loss_ce_3: 0.08353  loss_mask_3: 0.1835  loss_dice_3: 0.1799  loss_ce_4: 0.05851  loss_mask_4: 0.1823  loss_dice_4: 0.175  loss_ce_5: 0.0533  loss_mask_5: 0.1817  loss_dice_5: 0.1762  loss_ce_6: 0.07192  loss_mask_6: 0.1875  loss_dice_6: 0.1831  loss_ce_7: 0.05843  loss_mask_7: 0.1831  loss_dice_7: 0.1771  loss_ce_8: 0.05482  loss_mask_8: 0.1915  loss_dice_8: 0.1838  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:15] d2.utils.events INFO:  eta: 0:41:17  iter: 30279  total_loss: 4.189  loss_ce: 0.0112  loss_mask: 0.1943  loss_dice: 0.1349  loss_ce_0: 0.06266  loss_mask_0: 0.1924  loss_dice_0: 0.1304  loss_ce_1: 0.01968  loss_mask_1: 0.1876  loss_dice_1: 0.138  loss_ce_2: 0.01711  loss_mask_2: 0.1865  loss_dice_2: 0.1304  loss_ce_3: 0.01528  loss_mask_3: 0.1968  loss_dice_3: 0.132  loss_ce_4: 0.01422  loss_mask_4: 0.1938  loss_dice_4: 0.1385  loss_ce_5: 0.01147  loss_mask_5: 0.196  loss_dice_5: 0.1376  loss_ce_6: 0.01448  loss_mask_6: 0.1916  loss_dice_6: 0.1342  loss_ce_7: 0.01572  loss_mask_7: 0.1997  loss_dice_7: 0.1295  loss_ce_8: 0.01585  loss_mask_8: 0.194  loss_dice_8: 0.1419  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:18] d2.utils.events INFO:  eta: 0:41:14  iter: 30299  total_loss: 5.383  loss_ce: 0.04811  loss_mask: 0.1644  loss_dice: 0.1823  loss_ce_0: 0.06064  loss_mask_0: 0.1761  loss_dice_0: 0.1781  loss_ce_1: 0.02648  loss_mask_1: 0.1752  loss_dice_1: 0.1783  loss_ce_2: 0.06579  loss_mask_2: 0.1655  loss_dice_2: 0.1716  loss_ce_3: 0.05871  loss_mask_3: 0.1816  loss_dice_3: 0.1768  loss_ce_4: 0.038  loss_mask_4: 0.1744  loss_dice_4: 0.171  loss_ce_5: 0.0698  loss_mask_5: 0.1711  loss_dice_5: 0.1721  loss_ce_6: 0.0508  loss_mask_6: 0.1789  loss_dice_6: 0.1753  loss_ce_7: 0.03155  loss_mask_7: 0.174  loss_dice_7: 0.1811  loss_ce_8: 0.05851  loss_mask_8: 0.1785  loss_dice_8: 0.1769  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:20] d2.utils.events INFO:  eta: 0:41:12  iter: 30319  total_loss: 4.306  loss_ce: 0.01496  loss_mask: 0.2166  loss_dice: 0.1513  loss_ce_0: 0.05579  loss_mask_0: 0.2144  loss_dice_0: 0.1435  loss_ce_1: 0.01738  loss_mask_1: 0.2198  loss_dice_1: 0.1438  loss_ce_2: 0.02077  loss_mask_2: 0.2054  loss_dice_2: 0.1419  loss_ce_3: 0.01922  loss_mask_3: 0.2181  loss_dice_3: 0.1544  loss_ce_4: 0.01981  loss_mask_4: 0.2207  loss_dice_4: 0.1518  loss_ce_5: 0.02231  loss_mask_5: 0.2179  loss_dice_5: 0.1513  loss_ce_6: 0.0172  loss_mask_6: 0.2232  loss_dice_6: 0.1531  loss_ce_7: 0.01771  loss_mask_7: 0.2151  loss_dice_7: 0.1557  loss_ce_8: 0.01692  loss_mask_8: 0.2249  loss_dice_8: 0.1483  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:23] d2.utils.events INFO:  eta: 0:41:09  iter: 30339  total_loss: 5.56  loss_ce: 0.09325  loss_mask: 0.1837  loss_dice: 0.1922  loss_ce_0: 0.05082  loss_mask_0: 0.1901  loss_dice_0: 0.1968  loss_ce_1: 0.1202  loss_mask_1: 0.1867  loss_dice_1: 0.1893  loss_ce_2: 0.09401  loss_mask_2: 0.1879  loss_dice_2: 0.189  loss_ce_3: 0.116  loss_mask_3: 0.192  loss_dice_3: 0.1868  loss_ce_4: 0.07242  loss_mask_4: 0.1954  loss_dice_4: 0.185  loss_ce_5: 0.04916  loss_mask_5: 0.1864  loss_dice_5: 0.1931  loss_ce_6: 0.08712  loss_mask_6: 0.1885  loss_dice_6: 0.1876  loss_ce_7: 0.05802  loss_mask_7: 0.1931  loss_dice_7: 0.1949  loss_ce_8: 0.04535  loss_mask_8: 0.1924  loss_dice_8: 0.1923  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:25] d2.utils.events INFO:  eta: 0:41:08  iter: 30359  total_loss: 4.654  loss_ce: 0.01831  loss_mask: 0.1719  loss_dice: 0.2061  loss_ce_0: 0.04686  loss_mask_0: 0.1905  loss_dice_0: 0.2158  loss_ce_1: 0.03208  loss_mask_1: 0.1858  loss_dice_1: 0.2021  loss_ce_2: 0.01834  loss_mask_2: 0.1785  loss_dice_2: 0.2062  loss_ce_3: 0.01731  loss_mask_3: 0.1709  loss_dice_3: 0.1988  loss_ce_4: 0.0165  loss_mask_4: 0.1781  loss_dice_4: 0.2104  loss_ce_5: 0.01622  loss_mask_5: 0.1813  loss_dice_5: 0.2058  loss_ce_6: 0.01634  loss_mask_6: 0.189  loss_dice_6: 0.2051  loss_ce_7: 0.01773  loss_mask_7: 0.1786  loss_dice_7: 0.2088  loss_ce_8: 0.01639  loss_mask_8: 0.1801  loss_dice_8: 0.1969  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:28] d2.utils.events INFO:  eta: 0:41:08  iter: 30379  total_loss: 6.415  loss_ce: 0.07971  loss_mask: 0.2018  loss_dice: 0.1797  loss_ce_0: 0.08445  loss_mask_0: 0.2045  loss_dice_0: 0.1822  loss_ce_1: 0.1603  loss_mask_1: 0.2055  loss_dice_1: 0.1831  loss_ce_2: 0.08393  loss_mask_2: 0.2032  loss_dice_2: 0.177  loss_ce_3: 0.07861  loss_mask_3: 0.2026  loss_dice_3: 0.1701  loss_ce_4: 0.09716  loss_mask_4: 0.209  loss_dice_4: 0.1718  loss_ce_5: 0.1567  loss_mask_5: 0.2069  loss_dice_5: 0.1719  loss_ce_6: 0.06596  loss_mask_6: 0.2042  loss_dice_6: 0.1768  loss_ce_7: 0.141  loss_mask_7: 0.2126  loss_dice_7: 0.1779  loss_ce_8: 0.1317  loss_mask_8: 0.2001  loss_dice_8: 0.1775  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:30] d2.utils.events INFO:  eta: 0:41:05  iter: 30399  total_loss: 4.957  loss_ce: 0.02087  loss_mask: 0.2146  loss_dice: 0.1398  loss_ce_0: 0.06433  loss_mask_0: 0.2148  loss_dice_0: 0.1349  loss_ce_1: 0.03209  loss_mask_1: 0.2158  loss_dice_1: 0.1389  loss_ce_2: 0.0299  loss_mask_2: 0.2218  loss_dice_2: 0.1393  loss_ce_3: 0.02285  loss_mask_3: 0.2173  loss_dice_3: 0.1359  loss_ce_4: 0.02378  loss_mask_4: 0.226  loss_dice_4: 0.1377  loss_ce_5: 0.02172  loss_mask_5: 0.2265  loss_dice_5: 0.1406  loss_ce_6: 0.02151  loss_mask_6: 0.2156  loss_dice_6: 0.1351  loss_ce_7: 0.02927  loss_mask_7: 0.2157  loss_dice_7: 0.1321  loss_ce_8: 0.02357  loss_mask_8: 0.224  loss_dice_8: 0.1366  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 16:02:33] d2.utils.events INFO:  eta: 0:41:04  iter: 30419  total_loss: 4.618  loss_ce: 0.06723  loss_mask: 0.1664  loss_dice: 0.1659  loss_ce_0: 0.04751  loss_mask_0: 0.1791  loss_dice_0: 0.1635  loss_ce_1: 0.09345  loss_mask_1: 0.1679  loss_dice_1: 0.1599  loss_ce_2: 0.07652  loss_mask_2: 0.1692  loss_dice_2: 0.1578  loss_ce_3: 0.07815  loss_mask_3: 0.1735  loss_dice_3: 0.1668  loss_ce_4: 0.07199  loss_mask_4: 0.1672  loss_dice_4: 0.1703  loss_ce_5: 0.06163  loss_mask_5: 0.1767  loss_dice_5: 0.1614  loss_ce_6: 0.06802  loss_mask_6: 0.1781  loss_dice_6: 0.1705  loss_ce_7: 0.07332  loss_mask_7: 0.1554  loss_dice_7: 0.1661  loss_ce_8: 0.06663  loss_mask_8: 0.1695  loss_dice_8: 0.1722  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:36] d2.utils.events INFO:  eta: 0:41:01  iter: 30439  total_loss: 4.768  loss_ce: 0.02115  loss_mask: 0.2332  loss_dice: 0.1588  loss_ce_0: 0.07772  loss_mask_0: 0.2309  loss_dice_0: 0.1664  loss_ce_1: 0.06475  loss_mask_1: 0.2271  loss_dice_1: 0.1569  loss_ce_2: 0.0238  loss_mask_2: 0.2327  loss_dice_2: 0.1631  loss_ce_3: 0.02336  loss_mask_3: 0.2315  loss_dice_3: 0.1608  loss_ce_4: 0.02434  loss_mask_4: 0.2273  loss_dice_4: 0.1582  loss_ce_5: 0.02106  loss_mask_5: 0.2252  loss_dice_5: 0.1611  loss_ce_6: 0.02242  loss_mask_6: 0.2225  loss_dice_6: 0.1559  loss_ce_7: 0.02763  loss_mask_7: 0.2231  loss_dice_7: 0.1645  loss_ce_8: 0.02332  loss_mask_8: 0.2263  loss_dice_8: 0.1586  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 16:02:38] d2.utils.events INFO:  eta: 0:41:00  iter: 30459  total_loss: 5.986  loss_ce: 0.06586  loss_mask: 0.2199  loss_dice: 0.1907  loss_ce_0: 0.07219  loss_mask_0: 0.2116  loss_dice_0: 0.1832  loss_ce_1: 0.1022  loss_mask_1: 0.212  loss_dice_1: 0.1855  loss_ce_2: 0.08378  loss_mask_2: 0.2131  loss_dice_2: 0.1887  loss_ce_3: 0.07956  loss_mask_3: 0.2137  loss_dice_3: 0.1896  loss_ce_4: 0.08899  loss_mask_4: 0.2137  loss_dice_4: 0.1892  loss_ce_5: 0.05672  loss_mask_5: 0.211  loss_dice_5: 0.1861  loss_ce_6: 0.06163  loss_mask_6: 0.2137  loss_dice_6: 0.1846  loss_ce_7: 0.08524  loss_mask_7: 0.2171  loss_dice_7: 0.1844  loss_ce_8: 0.06955  loss_mask_8: 0.2157  loss_dice_8: 0.1896  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:41] d2.utils.events INFO:  eta: 0:40:58  iter: 30479  total_loss: 5.212  loss_ce: 0.02391  loss_mask: 0.1839  loss_dice: 0.1862  loss_ce_0: 0.05455  loss_mask_0: 0.1879  loss_dice_0: 0.1855  loss_ce_1: 0.07501  loss_mask_1: 0.1885  loss_dice_1: 0.1858  loss_ce_2: 0.02684  loss_mask_2: 0.1798  loss_dice_2: 0.1873  loss_ce_3: 0.02416  loss_mask_3: 0.1888  loss_dice_3: 0.184  loss_ce_4: 0.04311  loss_mask_4: 0.1833  loss_dice_4: 0.1791  loss_ce_5: 0.02194  loss_mask_5: 0.1853  loss_dice_5: 0.1845  loss_ce_6: 0.02439  loss_mask_6: 0.1792  loss_dice_6: 0.1804  loss_ce_7: 0.05005  loss_mask_7: 0.1891  loss_dice_7: 0.1807  loss_ce_8: 0.02275  loss_mask_8: 0.1895  loss_dice_8: 0.1825  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:43] d2.utils.events INFO:  eta: 0:40:55  iter: 30499  total_loss: 4.907  loss_ce: 0.01764  loss_mask: 0.2039  loss_dice: 0.1747  loss_ce_0: 0.06757  loss_mask_0: 0.199  loss_dice_0: 0.1748  loss_ce_1: 0.02083  loss_mask_1: 0.2002  loss_dice_1: 0.1748  loss_ce_2: 0.0202  loss_mask_2: 0.197  loss_dice_2: 0.171  loss_ce_3: 0.01721  loss_mask_3: 0.2051  loss_dice_3: 0.1797  loss_ce_4: 0.01823  loss_mask_4: 0.1887  loss_dice_4: 0.1633  loss_ce_5: 0.01849  loss_mask_5: 0.1941  loss_dice_5: 0.1631  loss_ce_6: 0.01768  loss_mask_6: 0.195  loss_dice_6: 0.1693  loss_ce_7: 0.01813  loss_mask_7: 0.2013  loss_dice_7: 0.1738  loss_ce_8: 0.02001  loss_mask_8: 0.1993  loss_dice_8: 0.1712  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:46] d2.utils.events INFO:  eta: 0:40:52  iter: 30519  total_loss: 4.916  loss_ce: 0.01538  loss_mask: 0.2031  loss_dice: 0.1927  loss_ce_0: 0.06436  loss_mask_0: 0.2107  loss_dice_0: 0.1863  loss_ce_1: 0.01898  loss_mask_1: 0.2152  loss_dice_1: 0.1897  loss_ce_2: 0.01423  loss_mask_2: 0.2192  loss_dice_2: 0.1843  loss_ce_3: 0.01444  loss_mask_3: 0.2112  loss_dice_3: 0.1881  loss_ce_4: 0.01439  loss_mask_4: 0.2072  loss_dice_4: 0.1926  loss_ce_5: 0.01491  loss_mask_5: 0.212  loss_dice_5: 0.1926  loss_ce_6: 0.0146  loss_mask_6: 0.205  loss_dice_6: 0.1873  loss_ce_7: 0.01324  loss_mask_7: 0.2079  loss_dice_7: 0.1888  loss_ce_8: 0.01472  loss_mask_8: 0.2111  loss_dice_8: 0.1886  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:48] d2.utils.events INFO:  eta: 0:40:48  iter: 30539  total_loss: 6.436  loss_ce: 0.02045  loss_mask: 0.1846  loss_dice: 0.1734  loss_ce_0: 0.0619  loss_mask_0: 0.1833  loss_dice_0: 0.1757  loss_ce_1: 0.09462  loss_mask_1: 0.1884  loss_dice_1: 0.184  loss_ce_2: 0.01672  loss_mask_2: 0.1824  loss_dice_2: 0.1847  loss_ce_3: 0.01921  loss_mask_3: 0.1774  loss_dice_3: 0.1751  loss_ce_4: 0.01546  loss_mask_4: 0.1918  loss_dice_4: 0.1774  loss_ce_5: 0.0144  loss_mask_5: 0.1931  loss_dice_5: 0.1786  loss_ce_6: 0.0244  loss_mask_6: 0.1921  loss_dice_6: 0.1781  loss_ce_7: 0.0261  loss_mask_7: 0.1942  loss_dice_7: 0.1799  loss_ce_8: 0.01524  loss_mask_8: 0.1895  loss_dice_8: 0.1756  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:02:51] d2.utils.events INFO:  eta: 0:40:45  iter: 30559  total_loss: 4.938  loss_ce: 0.01552  loss_mask: 0.1937  loss_dice: 0.1835  loss_ce_0: 0.06172  loss_mask_0: 0.195  loss_dice_0: 0.165  loss_ce_1: 0.02595  loss_mask_1: 0.194  loss_dice_1: 0.1757  loss_ce_2: 0.01475  loss_mask_2: 0.2009  loss_dice_2: 0.1776  loss_ce_3: 0.01576  loss_mask_3: 0.2012  loss_dice_3: 0.1754  loss_ce_4: 0.01402  loss_mask_4: 0.2004  loss_dice_4: 0.1746  loss_ce_5: 0.01423  loss_mask_5: 0.1955  loss_dice_5: 0.1743  loss_ce_6: 0.0153  loss_mask_6: 0.1991  loss_dice_6: 0.1723  loss_ce_7: 0.01379  loss_mask_7: 0.1966  loss_dice_7: 0.1797  loss_ce_8: 0.01487  loss_mask_8: 0.188  loss_dice_8: 0.1781  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:53] d2.utils.events INFO:  eta: 0:40:44  iter: 30579  total_loss: 4.904  loss_ce: 0.0132  loss_mask: 0.2122  loss_dice: 0.1508  loss_ce_0: 0.0587  loss_mask_0: 0.2047  loss_dice_0: 0.1529  loss_ce_1: 0.01954  loss_mask_1: 0.2005  loss_dice_1: 0.1506  loss_ce_2: 0.01641  loss_mask_2: 0.206  loss_dice_2: 0.1551  loss_ce_3: 0.01359  loss_mask_3: 0.2087  loss_dice_3: 0.1558  loss_ce_4: 0.01034  loss_mask_4: 0.2253  loss_dice_4: 0.1508  loss_ce_5: 0.01475  loss_mask_5: 0.2144  loss_dice_5: 0.1539  loss_ce_6: 0.01305  loss_mask_6: 0.2138  loss_dice_6: 0.1541  loss_ce_7: 0.009741  loss_mask_7: 0.2043  loss_dice_7: 0.1611  loss_ce_8: 0.0154  loss_mask_8: 0.2078  loss_dice_8: 0.1472  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:56] d2.utils.events INFO:  eta: 0:40:42  iter: 30599  total_loss: 6.454  loss_ce: 0.0316  loss_mask: 0.13  loss_dice: 0.3406  loss_ce_0: 0.07151  loss_mask_0: 0.1278  loss_dice_0: 0.3404  loss_ce_1: 0.06206  loss_mask_1: 0.1244  loss_dice_1: 0.3013  loss_ce_2: 0.04985  loss_mask_2: 0.1314  loss_dice_2: 0.3459  loss_ce_3: 0.0388  loss_mask_3: 0.1261  loss_dice_3: 0.3333  loss_ce_4: 0.06187  loss_mask_4: 0.1265  loss_dice_4: 0.3281  loss_ce_5: 0.03313  loss_mask_5: 0.1269  loss_dice_5: 0.35  loss_ce_6: 0.03339  loss_mask_6: 0.1227  loss_dice_6: 0.3418  loss_ce_7: 0.07259  loss_mask_7: 0.1322  loss_dice_7: 0.3281  loss_ce_8: 0.03122  loss_mask_8: 0.1251  loss_dice_8: 0.3471  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:02:58] d2.utils.events INFO:  eta: 0:40:39  iter: 30619  total_loss: 4.821  loss_ce: 0.01146  loss_mask: 0.2256  loss_dice: 0.1666  loss_ce_0: 0.05713  loss_mask_0: 0.2261  loss_dice_0: 0.1644  loss_ce_1: 0.01287  loss_mask_1: 0.2297  loss_dice_1: 0.1622  loss_ce_2: 0.008803  loss_mask_2: 0.2324  loss_dice_2: 0.1675  loss_ce_3: 0.0107  loss_mask_3: 0.2214  loss_dice_3: 0.1679  loss_ce_4: 0.01149  loss_mask_4: 0.2277  loss_dice_4: 0.166  loss_ce_5: 0.01006  loss_mask_5: 0.2201  loss_dice_5: 0.1597  loss_ce_6: 0.0107  loss_mask_6: 0.2211  loss_dice_6: 0.1586  loss_ce_7: 0.01183  loss_mask_7: 0.2264  loss_dice_7: 0.168  loss_ce_8: 0.01037  loss_mask_8: 0.219  loss_dice_8: 0.1702  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:01] d2.utils.events INFO:  eta: 0:40:36  iter: 30639  total_loss: 4.159  loss_ce: 0.01007  loss_mask: 0.198  loss_dice: 0.1257  loss_ce_0: 0.05852  loss_mask_0: 0.1991  loss_dice_0: 0.1261  loss_ce_1: 0.01492  loss_mask_1: 0.2005  loss_dice_1: 0.1261  loss_ce_2: 0.00812  loss_mask_2: 0.2019  loss_dice_2: 0.1279  loss_ce_3: 0.009337  loss_mask_3: 0.2039  loss_dice_3: 0.1268  loss_ce_4: 0.008421  loss_mask_4: 0.1947  loss_dice_4: 0.1243  loss_ce_5: 0.008809  loss_mask_5: 0.205  loss_dice_5: 0.1214  loss_ce_6: 0.009645  loss_mask_6: 0.2009  loss_dice_6: 0.1232  loss_ce_7: 0.009304  loss_mask_7: 0.2089  loss_dice_7: 0.1262  loss_ce_8: 0.0087  loss_mask_8: 0.1976  loss_dice_8: 0.1282  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:03] d2.utils.events INFO:  eta: 0:40:33  iter: 30659  total_loss: 5.273  loss_ce: 0.01737  loss_mask: 0.1515  loss_dice: 0.2404  loss_ce_0: 0.06742  loss_mask_0: 0.1803  loss_dice_0: 0.2511  loss_ce_1: 0.02143  loss_mask_1: 0.1483  loss_dice_1: 0.2408  loss_ce_2: 0.01474  loss_mask_2: 0.1557  loss_dice_2: 0.2519  loss_ce_3: 0.01694  loss_mask_3: 0.156  loss_dice_3: 0.2652  loss_ce_4: 0.01712  loss_mask_4: 0.1519  loss_dice_4: 0.2604  loss_ce_5: 0.01567  loss_mask_5: 0.1596  loss_dice_5: 0.2552  loss_ce_6: 0.0163  loss_mask_6: 0.1534  loss_dice_6: 0.2423  loss_ce_7: 0.01826  loss_mask_7: 0.1613  loss_dice_7: 0.2487  loss_ce_8: 0.01646  loss_mask_8: 0.15  loss_dice_8: 0.2482  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:03:06] d2.utils.events INFO:  eta: 0:40:30  iter: 30679  total_loss: 4.277  loss_ce: 0.01737  loss_mask: 0.1995  loss_dice: 0.1392  loss_ce_0: 0.06239  loss_mask_0: 0.2024  loss_dice_0: 0.1421  loss_ce_1: 0.01874  loss_mask_1: 0.2054  loss_dice_1: 0.1391  loss_ce_2: 0.01586  loss_mask_2: 0.2067  loss_dice_2: 0.1412  loss_ce_3: 0.01564  loss_mask_3: 0.2064  loss_dice_3: 0.1447  loss_ce_4: 0.0218  loss_mask_4: 0.2058  loss_dice_4: 0.136  loss_ce_5: 0.0171  loss_mask_5: 0.2072  loss_dice_5: 0.1371  loss_ce_6: 0.0153  loss_mask_6: 0.2054  loss_dice_6: 0.144  loss_ce_7: 0.01916  loss_mask_7: 0.2158  loss_dice_7: 0.1438  loss_ce_8: 0.01797  loss_mask_8: 0.2118  loss_dice_8: 0.1406  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:03:08] d2.utils.events INFO:  eta: 0:40:28  iter: 30699  total_loss: 4.526  loss_ce: 0.01896  loss_mask: 0.1684  loss_dice: 0.1534  loss_ce_0: 0.0646  loss_mask_0: 0.1855  loss_dice_0: 0.1595  loss_ce_1: 0.02166  loss_mask_1: 0.1765  loss_dice_1: 0.1576  loss_ce_2: 0.01786  loss_mask_2: 0.1763  loss_dice_2: 0.1592  loss_ce_3: 0.01704  loss_mask_3: 0.179  loss_dice_3: 0.1555  loss_ce_4: 0.01995  loss_mask_4: 0.1764  loss_dice_4: 0.1623  loss_ce_5: 0.01852  loss_mask_5: 0.1739  loss_dice_5: 0.1607  loss_ce_6: 0.01707  loss_mask_6: 0.1782  loss_dice_6: 0.1548  loss_ce_7: 0.0217  loss_mask_7: 0.1772  loss_dice_7: 0.155  loss_ce_8: 0.01892  loss_mask_8: 0.1737  loss_dice_8: 0.1559  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:11] d2.utils.events INFO:  eta: 0:40:26  iter: 30719  total_loss: 3.714  loss_ce: 0.0181  loss_mask: 0.1833  loss_dice: 0.1416  loss_ce_0: 0.05918  loss_mask_0: 0.187  loss_dice_0: 0.1398  loss_ce_1: 0.01904  loss_mask_1: 0.1794  loss_dice_1: 0.1452  loss_ce_2: 0.01728  loss_mask_2: 0.1882  loss_dice_2: 0.1476  loss_ce_3: 0.01659  loss_mask_3: 0.1871  loss_dice_3: 0.1431  loss_ce_4: 0.01705  loss_mask_4: 0.1922  loss_dice_4: 0.1417  loss_ce_5: 0.01827  loss_mask_5: 0.1821  loss_dice_5: 0.1391  loss_ce_6: 0.01624  loss_mask_6: 0.1875  loss_dice_6: 0.1495  loss_ce_7: 0.01758  loss_mask_7: 0.184  loss_dice_7: 0.1383  loss_ce_8: 0.01761  loss_mask_8: 0.1814  loss_dice_8: 0.1393  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:14] d2.utils.events INFO:  eta: 0:40:25  iter: 30739  total_loss: 4.95  loss_ce: 0.0159  loss_mask: 0.2076  loss_dice: 0.2208  loss_ce_0: 0.07303  loss_mask_0: 0.2069  loss_dice_0: 0.2267  loss_ce_1: 0.01659  loss_mask_1: 0.2124  loss_dice_1: 0.2163  loss_ce_2: 0.01537  loss_mask_2: 0.2048  loss_dice_2: 0.2056  loss_ce_3: 0.014  loss_mask_3: 0.2075  loss_dice_3: 0.213  loss_ce_4: 0.009271  loss_mask_4: 0.2065  loss_dice_4: 0.2131  loss_ce_5: 0.01574  loss_mask_5: 0.2117  loss_dice_5: 0.2154  loss_ce_6: 0.01435  loss_mask_6: 0.2101  loss_dice_6: 0.2085  loss_ce_7: 0.01233  loss_mask_7: 0.2048  loss_dice_7: 0.215  loss_ce_8: 0.01469  loss_mask_8: 0.2053  loss_dice_8: 0.2136  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:16] d2.utils.events INFO:  eta: 0:40:22  iter: 30759  total_loss: 6.159  loss_ce: 0.02262  loss_mask: 0.1901  loss_dice: 0.189  loss_ce_0: 0.06252  loss_mask_0: 0.2063  loss_dice_0: 0.1858  loss_ce_1: 0.06596  loss_mask_1: 0.2007  loss_dice_1: 0.1914  loss_ce_2: 0.05455  loss_mask_2: 0.1989  loss_dice_2: 0.1794  loss_ce_3: 0.01879  loss_mask_3: 0.1973  loss_dice_3: 0.1901  loss_ce_4: 0.03672  loss_mask_4: 0.1914  loss_dice_4: 0.1845  loss_ce_5: 0.05253  loss_mask_5: 0.1964  loss_dice_5: 0.1898  loss_ce_6: 0.02282  loss_mask_6: 0.1924  loss_dice_6: 0.1958  loss_ce_7: 0.05647  loss_mask_7: 0.1974  loss_dice_7: 0.1873  loss_ce_8: 0.0609  loss_mask_8: 0.1965  loss_dice_8: 0.1869  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:03:19] d2.utils.events INFO:  eta: 0:40:19  iter: 30779  total_loss: 4.739  loss_ce: 0.0393  loss_mask: 0.2036  loss_dice: 0.1649  loss_ce_0: 0.0635  loss_mask_0: 0.2111  loss_dice_0: 0.1643  loss_ce_1: 0.04232  loss_mask_1: 0.2117  loss_dice_1: 0.1662  loss_ce_2: 0.04524  loss_mask_2: 0.2069  loss_dice_2: 0.1667  loss_ce_3: 0.0314  loss_mask_3: 0.209  loss_dice_3: 0.1682  loss_ce_4: 0.03008  loss_mask_4: 0.2226  loss_dice_4: 0.1653  loss_ce_5: 0.03717  loss_mask_5: 0.2092  loss_dice_5: 0.1677  loss_ce_6: 0.02995  loss_mask_6: 0.2061  loss_dice_6: 0.1708  loss_ce_7: 0.03559  loss_mask_7: 0.1987  loss_dice_7: 0.1628  loss_ce_8: 0.0448  loss_mask_8: 0.2068  loss_dice_8: 0.1641  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:21] d2.utils.events INFO:  eta: 0:40:17  iter: 30799  total_loss: 5.928  loss_ce: 0.04399  loss_mask: 0.182  loss_dice: 0.1759  loss_ce_0: 0.05908  loss_mask_0: 0.1735  loss_dice_0: 0.1709  loss_ce_1: 0.04311  loss_mask_1: 0.1828  loss_dice_1: 0.1714  loss_ce_2: 0.09437  loss_mask_2: 0.1904  loss_dice_2: 0.1793  loss_ce_3: 0.0354  loss_mask_3: 0.1955  loss_dice_3: 0.1705  loss_ce_4: 0.03798  loss_mask_4: 0.1866  loss_dice_4: 0.1712  loss_ce_5: 0.08883  loss_mask_5: 0.1892  loss_dice_5: 0.1753  loss_ce_6: 0.04532  loss_mask_6: 0.1886  loss_dice_6: 0.1755  loss_ce_7: 0.05584  loss_mask_7: 0.1952  loss_dice_7: 0.1704  loss_ce_8: 0.1026  loss_mask_8: 0.1846  loss_dice_8: 0.1748  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:24] d2.utils.events INFO:  eta: 0:40:14  iter: 30819  total_loss: 6.04  loss_ce: 0.1026  loss_mask: 0.1761  loss_dice: 0.1964  loss_ce_0: 0.06994  loss_mask_0: 0.18  loss_dice_0: 0.197  loss_ce_1: 0.08075  loss_mask_1: 0.1759  loss_dice_1: 0.2005  loss_ce_2: 0.1182  loss_mask_2: 0.1735  loss_dice_2: 0.1879  loss_ce_3: 0.1182  loss_mask_3: 0.1805  loss_dice_3: 0.1866  loss_ce_4: 0.1154  loss_mask_4: 0.1749  loss_dice_4: 0.1846  loss_ce_5: 0.1106  loss_mask_5: 0.1706  loss_dice_5: 0.186  loss_ce_6: 0.1073  loss_mask_6: 0.1747  loss_dice_6: 0.1922  loss_ce_7: 0.08801  loss_mask_7: 0.1846  loss_dice_7: 0.1911  loss_ce_8: 0.1213  loss_mask_8: 0.1795  loss_dice_8: 0.1866  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:26] d2.utils.events INFO:  eta: 0:40:12  iter: 30839  total_loss: 5.53  loss_ce: 0.02386  loss_mask: 0.1986  loss_dice: 0.1977  loss_ce_0: 0.05768  loss_mask_0: 0.1946  loss_dice_0: 0.1956  loss_ce_1: 0.01807  loss_mask_1: 0.1893  loss_dice_1: 0.1848  loss_ce_2: 0.02851  loss_mask_2: 0.196  loss_dice_2: 0.1933  loss_ce_3: 0.02614  loss_mask_3: 0.1896  loss_dice_3: 0.1889  loss_ce_4: 0.02338  loss_mask_4: 0.2008  loss_dice_4: 0.2044  loss_ce_5: 0.03292  loss_mask_5: 0.1911  loss_dice_5: 0.1997  loss_ce_6: 0.02279  loss_mask_6: 0.1928  loss_dice_6: 0.1919  loss_ce_7: 0.01707  loss_mask_7: 0.1843  loss_dice_7: 0.1897  loss_ce_8: 0.02563  loss_mask_8: 0.1862  loss_dice_8: 0.1832  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:29] d2.utils.events INFO:  eta: 0:40:08  iter: 30859  total_loss: 5.13  loss_ce: 0.0259  loss_mask: 0.1792  loss_dice: 0.1922  loss_ce_0: 0.06215  loss_mask_0: 0.1838  loss_dice_0: 0.2038  loss_ce_1: 0.01624  loss_mask_1: 0.1875  loss_dice_1: 0.1944  loss_ce_2: 0.03068  loss_mask_2: 0.1855  loss_dice_2: 0.1925  loss_ce_3: 0.02536  loss_mask_3: 0.1881  loss_dice_3: 0.1986  loss_ce_4: 0.01797  loss_mask_4: 0.1818  loss_dice_4: 0.1952  loss_ce_5: 0.03512  loss_mask_5: 0.1882  loss_dice_5: 0.1934  loss_ce_6: 0.02433  loss_mask_6: 0.1852  loss_dice_6: 0.193  loss_ce_7: 0.01405  loss_mask_7: 0.1862  loss_dice_7: 0.204  loss_ce_8: 0.02467  loss_mask_8: 0.1873  loss_dice_8: 0.1899  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:31] d2.utils.events INFO:  eta: 0:40:06  iter: 30879  total_loss: 4.952  loss_ce: 0.01727  loss_mask: 0.1999  loss_dice: 0.1861  loss_ce_0: 0.0621  loss_mask_0: 0.1964  loss_dice_0: 0.1866  loss_ce_1: 0.01646  loss_mask_1: 0.1932  loss_dice_1: 0.1845  loss_ce_2: 0.02248  loss_mask_2: 0.1934  loss_dice_2: 0.1823  loss_ce_3: 0.01768  loss_mask_3: 0.1852  loss_dice_3: 0.1925  loss_ce_4: 0.01484  loss_mask_4: 0.1979  loss_dice_4: 0.1808  loss_ce_5: 0.02119  loss_mask_5: 0.1992  loss_dice_5: 0.1814  loss_ce_6: 0.01724  loss_mask_6: 0.2057  loss_dice_6: 0.1844  loss_ce_7: 0.01405  loss_mask_7: 0.196  loss_dice_7: 0.1845  loss_ce_8: 0.01897  loss_mask_8: 0.2017  loss_dice_8: 0.1828  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:03:34] d2.utils.events INFO:  eta: 0:40:04  iter: 30899  total_loss: 5.758  loss_ce: 0.07453  loss_mask: 0.2084  loss_dice: 0.1687  loss_ce_0: 0.06095  loss_mask_0: 0.2064  loss_dice_0: 0.1685  loss_ce_1: 0.06358  loss_mask_1: 0.2096  loss_dice_1: 0.1687  loss_ce_2: 0.1092  loss_mask_2: 0.2059  loss_dice_2: 0.1649  loss_ce_3: 0.06629  loss_mask_3: 0.2119  loss_dice_3: 0.1702  loss_ce_4: 0.06755  loss_mask_4: 0.2145  loss_dice_4: 0.1686  loss_ce_5: 0.1074  loss_mask_5: 0.2143  loss_dice_5: 0.1676  loss_ce_6: 0.07358  loss_mask_6: 0.2153  loss_dice_6: 0.1671  loss_ce_7: 0.09496  loss_mask_7: 0.2121  loss_dice_7: 0.1673  loss_ce_8: 0.1026  loss_mask_8: 0.1998  loss_dice_8: 0.17  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:36] d2.utils.events INFO:  eta: 0:40:02  iter: 30919  total_loss: 4.571  loss_ce: 0.05352  loss_mask: 0.1747  loss_dice: 0.1292  loss_ce_0: 0.0621  loss_mask_0: 0.1734  loss_dice_0: 0.1336  loss_ce_1: 0.02153  loss_mask_1: 0.1685  loss_dice_1: 0.1342  loss_ce_2: 0.04387  loss_mask_2: 0.1711  loss_dice_2: 0.1315  loss_ce_3: 0.04305  loss_mask_3: 0.1857  loss_dice_3: 0.1365  loss_ce_4: 0.03329  loss_mask_4: 0.1762  loss_dice_4: 0.1363  loss_ce_5: 0.04595  loss_mask_5: 0.1716  loss_dice_5: 0.135  loss_ce_6: 0.05806  loss_mask_6: 0.1679  loss_dice_6: 0.138  loss_ce_7: 0.03657  loss_mask_7: 0.168  loss_dice_7: 0.1316  loss_ce_8: 0.04698  loss_mask_8: 0.1743  loss_dice_8: 0.1327  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:39] d2.utils.events INFO:  eta: 0:40:00  iter: 30939  total_loss: 5.223  loss_ce: 0.08591  loss_mask: 0.2091  loss_dice: 0.1791  loss_ce_0: 0.06549  loss_mask_0: 0.2115  loss_dice_0: 0.178  loss_ce_1: 0.09176  loss_mask_1: 0.2055  loss_dice_1: 0.178  loss_ce_2: 0.0369  loss_mask_2: 0.2006  loss_dice_2: 0.1778  loss_ce_3: 0.07517  loss_mask_3: 0.2071  loss_dice_3: 0.1763  loss_ce_4: 0.07851  loss_mask_4: 0.2156  loss_dice_4: 0.1764  loss_ce_5: 0.03714  loss_mask_5: 0.2124  loss_dice_5: 0.1792  loss_ce_6: 0.09195  loss_mask_6: 0.224  loss_dice_6: 0.1766  loss_ce_7: 0.07535  loss_mask_7: 0.2097  loss_dice_7: 0.1764  loss_ce_8: 0.04962  loss_mask_8: 0.2092  loss_dice_8: 0.1769  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:41] d2.utils.events INFO:  eta: 0:39:59  iter: 30959  total_loss: 4.767  loss_ce: 0.04983  loss_mask: 0.2122  loss_dice: 0.1669  loss_ce_0: 0.06021  loss_mask_0: 0.2087  loss_dice_0: 0.159  loss_ce_1: 0.05217  loss_mask_1: 0.2216  loss_dice_1: 0.1607  loss_ce_2: 0.04186  loss_mask_2: 0.2125  loss_dice_2: 0.1671  loss_ce_3: 0.03766  loss_mask_3: 0.2138  loss_dice_3: 0.1641  loss_ce_4: 0.1134  loss_mask_4: 0.2116  loss_dice_4: 0.1619  loss_ce_5: 0.04698  loss_mask_5: 0.2135  loss_dice_5: 0.1687  loss_ce_6: 0.04781  loss_mask_6: 0.2122  loss_dice_6: 0.1608  loss_ce_7: 0.09777  loss_mask_7: 0.2022  loss_dice_7: 0.1626  loss_ce_8: 0.04688  loss_mask_8: 0.2165  loss_dice_8: 0.1622  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:44] d2.utils.events INFO:  eta: 0:39:58  iter: 30979  total_loss: 4.886  loss_ce: 0.05631  loss_mask: 0.1901  loss_dice: 0.1499  loss_ce_0: 0.0593  loss_mask_0: 0.1909  loss_dice_0: 0.1499  loss_ce_1: 0.07393  loss_mask_1: 0.1919  loss_dice_1: 0.1438  loss_ce_2: 0.04738  loss_mask_2: 0.1843  loss_dice_2: 0.1428  loss_ce_3: 0.06046  loss_mask_3: 0.1956  loss_dice_3: 0.1399  loss_ce_4: 0.05726  loss_mask_4: 0.1943  loss_dice_4: 0.1392  loss_ce_5: 0.04435  loss_mask_5: 0.1962  loss_dice_5: 0.1488  loss_ce_6: 0.05392  loss_mask_6: 0.1974  loss_dice_6: 0.1454  loss_ce_7: 0.04789  loss_mask_7: 0.1971  loss_dice_7: 0.1557  loss_ce_8: 0.0473  loss_mask_8: 0.1879  loss_dice_8: 0.1453  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:47] d2.utils.events INFO:  eta: 0:39:57  iter: 30999  total_loss: 4.465  loss_ce: 0.02262  loss_mask: 0.1963  loss_dice: 0.1892  loss_ce_0: 0.06271  loss_mask_0: 0.1903  loss_dice_0: 0.1794  loss_ce_1: 0.02056  loss_mask_1: 0.1907  loss_dice_1: 0.1886  loss_ce_2: 0.02373  loss_mask_2: 0.2004  loss_dice_2: 0.1866  loss_ce_3: 0.02152  loss_mask_3: 0.1873  loss_dice_3: 0.1788  loss_ce_4: 0.02105  loss_mask_4: 0.1927  loss_dice_4: 0.1819  loss_ce_5: 0.02577  loss_mask_5: 0.1814  loss_dice_5: 0.1793  loss_ce_6: 0.02204  loss_mask_6: 0.1798  loss_dice_6: 0.1846  loss_ce_7: 0.02041  loss_mask_7: 0.1884  loss_dice_7: 0.183  loss_ce_8: 0.02508  loss_mask_8: 0.1928  loss_dice_8: 0.1937  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:03:49] d2.utils.events INFO:  eta: 0:39:52  iter: 31019  total_loss: 6.067  loss_ce: 0.07128  loss_mask: 0.1998  loss_dice: 0.1872  loss_ce_0: 0.05293  loss_mask_0: 0.1939  loss_dice_0: 0.196  loss_ce_1: 0.09561  loss_mask_1: 0.1921  loss_dice_1: 0.1913  loss_ce_2: 0.07786  loss_mask_2: 0.1856  loss_dice_2: 0.199  loss_ce_3: 0.06822  loss_mask_3: 0.1929  loss_dice_3: 0.2074  loss_ce_4: 0.08941  loss_mask_4: 0.1922  loss_dice_4: 0.192  loss_ce_5: 0.09135  loss_mask_5: 0.1833  loss_dice_5: 0.1902  loss_ce_6: 0.07145  loss_mask_6: 0.1911  loss_dice_6: 0.195  loss_ce_7: 0.08171  loss_mask_7: 0.1919  loss_dice_7: 0.1954  loss_ce_8: 0.06854  loss_mask_8: 0.1931  loss_dice_8: 0.1957  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 16:03:52] d2.utils.events INFO:  eta: 0:39:52  iter: 31039  total_loss: 5.299  loss_ce: 0.02681  loss_mask: 0.1917  loss_dice: 0.203  loss_ce_0: 0.0522  loss_mask_0: 0.1872  loss_dice_0: 0.2123  loss_ce_1: 0.0259  loss_mask_1: 0.2094  loss_dice_1: 0.2207  loss_ce_2: 0.02706  loss_mask_2: 0.1946  loss_dice_2: 0.2084  loss_ce_3: 0.0264  loss_mask_3: 0.197  loss_dice_3: 0.2189  loss_ce_4: 0.03178  loss_mask_4: 0.2004  loss_dice_4: 0.2142  loss_ce_5: 0.03073  loss_mask_5: 0.1981  loss_dice_5: 0.2018  loss_ce_6: 0.02701  loss_mask_6: 0.1974  loss_dice_6: 0.2071  loss_ce_7: 0.03569  loss_mask_7: 0.1954  loss_dice_7: 0.2133  loss_ce_8: 0.02903  loss_mask_8: 0.1889  loss_dice_8: 0.2059  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:54] d2.utils.events INFO:  eta: 0:39:49  iter: 31059  total_loss: 4.367  loss_ce: 0.02412  loss_mask: 0.2063  loss_dice: 0.1516  loss_ce_0: 0.0492  loss_mask_0: 0.2003  loss_dice_0: 0.1488  loss_ce_1: 0.02664  loss_mask_1: 0.2048  loss_dice_1: 0.1528  loss_ce_2: 0.02442  loss_mask_2: 0.2038  loss_dice_2: 0.1533  loss_ce_3: 0.02405  loss_mask_3: 0.2037  loss_dice_3: 0.1546  loss_ce_4: 0.02419  loss_mask_4: 0.2045  loss_dice_4: 0.15  loss_ce_5: 0.02741  loss_mask_5: 0.2029  loss_dice_5: 0.1487  loss_ce_6: 0.02411  loss_mask_6: 0.2084  loss_dice_6: 0.1525  loss_ce_7: 0.02432  loss_mask_7: 0.2078  loss_dice_7: 0.1541  loss_ce_8: 0.02486  loss_mask_8: 0.1988  loss_dice_8: 0.1594  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:03:57] d2.utils.events INFO:  eta: 0:39:46  iter: 31079  total_loss: 4.959  loss_ce: 0.08636  loss_mask: 0.2066  loss_dice: 0.1561  loss_ce_0: 0.07167  loss_mask_0: 0.2111  loss_dice_0: 0.1527  loss_ce_1: 0.05837  loss_mask_1: 0.2071  loss_dice_1: 0.1518  loss_ce_2: 0.07509  loss_mask_2: 0.2032  loss_dice_2: 0.1499  loss_ce_3: 0.07925  loss_mask_3: 0.2052  loss_dice_3: 0.1528  loss_ce_4: 0.07987  loss_mask_4: 0.2014  loss_dice_4: 0.1522  loss_ce_5: 0.06817  loss_mask_5: 0.2094  loss_dice_5: 0.1533  loss_ce_6: 0.08545  loss_mask_6: 0.2055  loss_dice_6: 0.1524  loss_ce_7: 0.08007  loss_mask_7: 0.2037  loss_dice_7: 0.1489  loss_ce_8: 0.07799  loss_mask_8: 0.2082  loss_dice_8: 0.1561  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:03:59] d2.utils.events INFO:  eta: 0:39:44  iter: 31099  total_loss: 4.811  loss_ce: 0.02637  loss_mask: 0.2076  loss_dice: 0.1871  loss_ce_0: 0.05482  loss_mask_0: 0.2079  loss_dice_0: 0.1961  loss_ce_1: 0.005314  loss_mask_1: 0.206  loss_dice_1: 0.1982  loss_ce_2: 0.008934  loss_mask_2: 0.1996  loss_dice_2: 0.1937  loss_ce_3: 0.02524  loss_mask_3: 0.2058  loss_dice_3: 0.1911  loss_ce_4: 0.01242  loss_mask_4: 0.2113  loss_dice_4: 0.1914  loss_ce_5: 0.009144  loss_mask_5: 0.2063  loss_dice_5: 0.193  loss_ce_6: 0.02664  loss_mask_6: 0.2025  loss_dice_6: 0.1898  loss_ce_7: 0.0116  loss_mask_7: 0.2088  loss_dice_7: 0.1995  loss_ce_8: 0.01028  loss_mask_8: 0.2019  loss_dice_8: 0.1841  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:02] d2.utils.events INFO:  eta: 0:39:42  iter: 31119  total_loss: 5.571  loss_ce: 0.03051  loss_mask: 0.1953  loss_dice: 0.1456  loss_ce_0: 0.05284  loss_mask_0: 0.1904  loss_dice_0: 0.1364  loss_ce_1: 0.05375  loss_mask_1: 0.1894  loss_dice_1: 0.1385  loss_ce_2: 0.0391  loss_mask_2: 0.2017  loss_dice_2: 0.1417  loss_ce_3: 0.02817  loss_mask_3: 0.1994  loss_dice_3: 0.1431  loss_ce_4: 0.02824  loss_mask_4: 0.2023  loss_dice_4: 0.1402  loss_ce_5: 0.03218  loss_mask_5: 0.193  loss_dice_5: 0.1368  loss_ce_6: 0.02741  loss_mask_6: 0.1967  loss_dice_6: 0.1419  loss_ce_7: 0.03726  loss_mask_7: 0.1922  loss_dice_7: 0.1385  loss_ce_8: 0.03329  loss_mask_8: 0.1962  loss_dice_8: 0.14  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:04] d2.utils.events INFO:  eta: 0:39:38  iter: 31139  total_loss: 4.712  loss_ce: 0.04707  loss_mask: 0.157  loss_dice: 0.243  loss_ce_0: 0.06382  loss_mask_0: 0.1469  loss_dice_0: 0.2283  loss_ce_1: 0.0793  loss_mask_1: 0.1436  loss_dice_1: 0.2316  loss_ce_2: 0.05385  loss_mask_2: 0.1514  loss_dice_2: 0.2205  loss_ce_3: 0.04238  loss_mask_3: 0.1431  loss_dice_3: 0.2413  loss_ce_4: 0.03789  loss_mask_4: 0.1508  loss_dice_4: 0.2204  loss_ce_5: 0.04155  loss_mask_5: 0.1454  loss_dice_5: 0.2377  loss_ce_6: 0.04332  loss_mask_6: 0.1483  loss_dice_6: 0.2434  loss_ce_7: 0.05341  loss_mask_7: 0.1537  loss_dice_7: 0.2481  loss_ce_8: 0.04225  loss_mask_8: 0.144  loss_dice_8: 0.2229  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:07] d2.utils.events INFO:  eta: 0:39:37  iter: 31159  total_loss: 4.396  loss_ce: 0.02687  loss_mask: 0.1722  loss_dice: 0.1673  loss_ce_0: 0.06454  loss_mask_0: 0.1693  loss_dice_0: 0.1606  loss_ce_1: 0.02987  loss_mask_1: 0.1694  loss_dice_1: 0.1631  loss_ce_2: 0.02648  loss_mask_2: 0.1704  loss_dice_2: 0.1602  loss_ce_3: 0.02619  loss_mask_3: 0.1758  loss_dice_3: 0.1641  loss_ce_4: 0.03139  loss_mask_4: 0.1696  loss_dice_4: 0.1555  loss_ce_5: 0.02794  loss_mask_5: 0.1729  loss_dice_5: 0.1612  loss_ce_6: 0.02691  loss_mask_6: 0.1728  loss_dice_6: 0.1654  loss_ce_7: 0.03157  loss_mask_7: 0.171  loss_dice_7: 0.1611  loss_ce_8: 0.02448  loss_mask_8: 0.1679  loss_dice_8: 0.1554  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 16:04:09] d2.utils.events INFO:  eta: 0:39:35  iter: 31179  total_loss: 5.119  loss_ce: 0.02586  loss_mask: 0.1758  loss_dice: 0.1622  loss_ce_0: 0.0628  loss_mask_0: 0.1723  loss_dice_0: 0.1826  loss_ce_1: 0.019  loss_mask_1: 0.1794  loss_dice_1: 0.1745  loss_ce_2: 0.0243  loss_mask_2: 0.1768  loss_dice_2: 0.1683  loss_ce_3: 0.02477  loss_mask_3: 0.1793  loss_dice_3: 0.1733  loss_ce_4: 0.02771  loss_mask_4: 0.1883  loss_dice_4: 0.1821  loss_ce_5: 0.0283  loss_mask_5: 0.1743  loss_dice_5: 0.1719  loss_ce_6: 0.0248  loss_mask_6: 0.1765  loss_dice_6: 0.1633  loss_ce_7: 0.02492  loss_mask_7: 0.1775  loss_dice_7: 0.17  loss_ce_8: 0.02424  loss_mask_8: 0.1793  loss_dice_8: 0.1631  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:12] d2.utils.events INFO:  eta: 0:39:32  iter: 31199  total_loss: 4.18  loss_ce: 0.0145  loss_mask: 0.2074  loss_dice: 0.1629  loss_ce_0: 0.05273  loss_mask_0: 0.1916  loss_dice_0: 0.1585  loss_ce_1: 0.007504  loss_mask_1: 0.1889  loss_dice_1: 0.1567  loss_ce_2: 0.01135  loss_mask_2: 0.1925  loss_dice_2: 0.1508  loss_ce_3: 0.01516  loss_mask_3: 0.1937  loss_dice_3: 0.1625  loss_ce_4: 0.01056  loss_mask_4: 0.1963  loss_dice_4: 0.1524  loss_ce_5: 0.01155  loss_mask_5: 0.192  loss_dice_5: 0.1583  loss_ce_6: 0.01557  loss_mask_6: 0.1941  loss_dice_6: 0.1611  loss_ce_7: 0.008508  loss_mask_7: 0.1901  loss_dice_7: 0.1603  loss_ce_8: 0.01048  loss_mask_8: 0.1999  loss_dice_8: 0.1568  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:14] d2.utils.events INFO:  eta: 0:39:33  iter: 31219  total_loss: 3.736  loss_ce: 0.01452  loss_mask: 0.1999  loss_dice: 0.1255  loss_ce_0: 0.06864  loss_mask_0: 0.2062  loss_dice_0: 0.1272  loss_ce_1: 0.01087  loss_mask_1: 0.1898  loss_dice_1: 0.1208  loss_ce_2: 0.01496  loss_mask_2: 0.1956  loss_dice_2: 0.1233  loss_ce_3: 0.01606  loss_mask_3: 0.2041  loss_dice_3: 0.1276  loss_ce_4: 0.01714  loss_mask_4: 0.1924  loss_dice_4: 0.1236  loss_ce_5: 0.01882  loss_mask_5: 0.1928  loss_dice_5: 0.1246  loss_ce_6: 0.01659  loss_mask_6: 0.1962  loss_dice_6: 0.1232  loss_ce_7: 0.01084  loss_mask_7: 0.187  loss_dice_7: 0.1213  loss_ce_8: 0.01357  loss_mask_8: 0.2  loss_dice_8: 0.1256  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:17] d2.utils.events INFO:  eta: 0:39:31  iter: 31239  total_loss: 4.213  loss_ce: 0.00894  loss_mask: 0.2046  loss_dice: 0.1373  loss_ce_0: 0.06651  loss_mask_0: 0.2098  loss_dice_0: 0.1424  loss_ce_1: 0.00555  loss_mask_1: 0.2126  loss_dice_1: 0.1378  loss_ce_2: 0.00921  loss_mask_2: 0.2131  loss_dice_2: 0.134  loss_ce_3: 0.008181  loss_mask_3: 0.2139  loss_dice_3: 0.1342  loss_ce_4: 0.01104  loss_mask_4: 0.2106  loss_dice_4: 0.1357  loss_ce_5: 0.01125  loss_mask_5: 0.2164  loss_dice_5: 0.1355  loss_ce_6: 0.009079  loss_mask_6: 0.2218  loss_dice_6: 0.1395  loss_ce_7: 0.00721  loss_mask_7: 0.2059  loss_dice_7: 0.1346  loss_ce_8: 0.008218  loss_mask_8: 0.2139  loss_dice_8: 0.1297  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:19] d2.utils.events INFO:  eta: 0:39:28  iter: 31259  total_loss: 4.109  loss_ce: 0.009387  loss_mask: 0.1987  loss_dice: 0.142  loss_ce_0: 0.0588  loss_mask_0: 0.1873  loss_dice_0: 0.1396  loss_ce_1: 0.00835  loss_mask_1: 0.1813  loss_dice_1: 0.1416  loss_ce_2: 0.01314  loss_mask_2: 0.201  loss_dice_2: 0.1376  loss_ce_3: 0.009678  loss_mask_3: 0.1942  loss_dice_3: 0.1366  loss_ce_4: 0.01061  loss_mask_4: 0.1965  loss_dice_4: 0.1396  loss_ce_5: 0.01485  loss_mask_5: 0.1954  loss_dice_5: 0.1438  loss_ce_6: 0.009989  loss_mask_6: 0.1892  loss_dice_6: 0.1355  loss_ce_7: 0.0107  loss_mask_7: 0.1999  loss_dice_7: 0.1407  loss_ce_8: 0.01411  loss_mask_8: 0.1911  loss_dice_8: 0.14  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:22] d2.utils.events INFO:  eta: 0:39:25  iter: 31279  total_loss: 5.568  loss_ce: 0.01065  loss_mask: 0.2047  loss_dice: 0.2065  loss_ce_0: 0.05795  loss_mask_0: 0.2087  loss_dice_0: 0.2085  loss_ce_1: 0.004579  loss_mask_1: 0.2013  loss_dice_1: 0.2052  loss_ce_2: 0.01343  loss_mask_2: 0.2087  loss_dice_2: 0.1983  loss_ce_3: 0.007552  loss_mask_3: 0.2041  loss_dice_3: 0.1937  loss_ce_4: 0.01176  loss_mask_4: 0.2026  loss_dice_4: 0.1989  loss_ce_5: 0.02144  loss_mask_5: 0.1999  loss_dice_5: 0.1983  loss_ce_6: 0.009637  loss_mask_6: 0.2052  loss_dice_6: 0.2059  loss_ce_7: 0.007411  loss_mask_7: 0.2076  loss_dice_7: 0.2028  loss_ce_8: 0.01413  loss_mask_8: 0.2013  loss_dice_8: 0.2021  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:24] d2.utils.events INFO:  eta: 0:39:21  iter: 31299  total_loss: 5.247  loss_ce: 0.1064  loss_mask: 0.1605  loss_dice: 0.1463  loss_ce_0: 0.05203  loss_mask_0: 0.1519  loss_dice_0: 0.1408  loss_ce_1: 0.1188  loss_mask_1: 0.1581  loss_dice_1: 0.1387  loss_ce_2: 0.07985  loss_mask_2: 0.1511  loss_dice_2: 0.1459  loss_ce_3: 0.09379  loss_mask_3: 0.1499  loss_dice_3: 0.1529  loss_ce_4: 0.1005  loss_mask_4: 0.1555  loss_dice_4: 0.139  loss_ce_5: 0.07932  loss_mask_5: 0.1534  loss_dice_5: 0.1484  loss_ce_6: 0.09682  loss_mask_6: 0.1564  loss_dice_6: 0.1388  loss_ce_7: 0.08727  loss_mask_7: 0.1519  loss_dice_7: 0.1496  loss_ce_8: 0.07806  loss_mask_8: 0.152  loss_dice_8: 0.1521  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:27] d2.utils.events INFO:  eta: 0:39:17  iter: 31319  total_loss: 4.534  loss_ce: 0.01918  loss_mask: 0.2211  loss_dice: 0.1645  loss_ce_0: 0.04881  loss_mask_0: 0.2178  loss_dice_0: 0.1653  loss_ce_1: 0.01867  loss_mask_1: 0.2203  loss_dice_1: 0.1644  loss_ce_2: 0.02425  loss_mask_2: 0.2299  loss_dice_2: 0.163  loss_ce_3: 0.01329  loss_mask_3: 0.2174  loss_dice_3: 0.1609  loss_ce_4: 0.01444  loss_mask_4: 0.2199  loss_dice_4: 0.1606  loss_ce_5: 0.02497  loss_mask_5: 0.2235  loss_dice_5: 0.1668  loss_ce_6: 0.01742  loss_mask_6: 0.214  loss_dice_6: 0.1579  loss_ce_7: 0.01515  loss_mask_7: 0.2179  loss_dice_7: 0.1597  loss_ce_8: 0.02553  loss_mask_8: 0.2274  loss_dice_8: 0.1597  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:04:30] d2.utils.events INFO:  eta: 0:39:15  iter: 31339  total_loss: 5.044  loss_ce: 0.08469  loss_mask: 0.1512  loss_dice: 0.1831  loss_ce_0: 0.0651  loss_mask_0: 0.153  loss_dice_0: 0.1879  loss_ce_1: 0.09273  loss_mask_1: 0.15  loss_dice_1: 0.1835  loss_ce_2: 0.09729  loss_mask_2: 0.1489  loss_dice_2: 0.1898  loss_ce_3: 0.07672  loss_mask_3: 0.1566  loss_dice_3: 0.1841  loss_ce_4: 0.07059  loss_mask_4: 0.1528  loss_dice_4: 0.1915  loss_ce_5: 0.07005  loss_mask_5: 0.1562  loss_dice_5: 0.1799  loss_ce_6: 0.1043  loss_mask_6: 0.1467  loss_dice_6: 0.1823  loss_ce_7: 0.06075  loss_mask_7: 0.1538  loss_dice_7: 0.1926  loss_ce_8: 0.09481  loss_mask_8: 0.1471  loss_dice_8: 0.173  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:32] d2.utils.events INFO:  eta: 0:39:11  iter: 31359  total_loss: 5.99  loss_ce: 0.06636  loss_mask: 0.2373  loss_dice: 0.1436  loss_ce_0: 0.04728  loss_mask_0: 0.2342  loss_dice_0: 0.1504  loss_ce_1: 0.08211  loss_mask_1: 0.2291  loss_dice_1: 0.1397  loss_ce_2: 0.08936  loss_mask_2: 0.2424  loss_dice_2: 0.1451  loss_ce_3: 0.06907  loss_mask_3: 0.2321  loss_dice_3: 0.1428  loss_ce_4: 0.07318  loss_mask_4: 0.2359  loss_dice_4: 0.1429  loss_ce_5: 0.09046  loss_mask_5: 0.2383  loss_dice_5: 0.1405  loss_ce_6: 0.06637  loss_mask_6: 0.2366  loss_dice_6: 0.1466  loss_ce_7: 0.07364  loss_mask_7: 0.239  loss_dice_7: 0.15  loss_ce_8: 0.08644  loss_mask_8: 0.2358  loss_dice_8: 0.1421  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:35] d2.utils.events INFO:  eta: 0:39:07  iter: 31379  total_loss: 5.071  loss_ce: 0.03699  loss_mask: 0.2139  loss_dice: 0.1586  loss_ce_0: 0.04678  loss_mask_0: 0.2025  loss_dice_0: 0.1661  loss_ce_1: 0.02303  loss_mask_1: 0.2053  loss_dice_1: 0.1644  loss_ce_2: 0.02947  loss_mask_2: 0.2045  loss_dice_2: 0.1655  loss_ce_3: 0.05233  loss_mask_3: 0.2113  loss_dice_3: 0.1691  loss_ce_4: 0.04112  loss_mask_4: 0.2053  loss_dice_4: 0.1555  loss_ce_5: 0.03606  loss_mask_5: 0.1964  loss_dice_5: 0.161  loss_ce_6: 0.05065  loss_mask_6: 0.195  loss_dice_6: 0.1641  loss_ce_7: 0.03071  loss_mask_7: 0.1935  loss_dice_7: 0.1597  loss_ce_8: 0.03547  loss_mask_8: 0.2068  loss_dice_8: 0.1713  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:37] d2.utils.events INFO:  eta: 0:39:05  iter: 31399  total_loss: 4.655  loss_ce: 0.03196  loss_mask: 0.1615  loss_dice: 0.1919  loss_ce_0: 0.08307  loss_mask_0: 0.1454  loss_dice_0: 0.1868  loss_ce_1: 0.025  loss_mask_1: 0.1471  loss_dice_1: 0.1903  loss_ce_2: 0.02567  loss_mask_2: 0.1562  loss_dice_2: 0.1957  loss_ce_3: 0.0534  loss_mask_3: 0.1596  loss_dice_3: 0.1885  loss_ce_4: 0.02831  loss_mask_4: 0.1544  loss_dice_4: 0.1849  loss_ce_5: 0.0289  loss_mask_5: 0.152  loss_dice_5: 0.1902  loss_ce_6: 0.05681  loss_mask_6: 0.1494  loss_dice_6: 0.1906  loss_ce_7: 0.02664  loss_mask_7: 0.1565  loss_dice_7: 0.1925  loss_ce_8: 0.03212  loss_mask_8: 0.1522  loss_dice_8: 0.1894  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:40] d2.utils.events INFO:  eta: 0:39:00  iter: 31419  total_loss: 4.845  loss_ce: 0.01482  loss_mask: 0.1963  loss_dice: 0.1417  loss_ce_0: 0.05294  loss_mask_0: 0.1997  loss_dice_0: 0.144  loss_ce_1: 0.05959  loss_mask_1: 0.2002  loss_dice_1: 0.1508  loss_ce_2: 0.03489  loss_mask_2: 0.1995  loss_dice_2: 0.1519  loss_ce_3: 0.01814  loss_mask_3: 0.1954  loss_dice_3: 0.139  loss_ce_4: 0.02932  loss_mask_4: 0.1957  loss_dice_4: 0.146  loss_ce_5: 0.03631  loss_mask_5: 0.2009  loss_dice_5: 0.1417  loss_ce_6: 0.01647  loss_mask_6: 0.1912  loss_dice_6: 0.1402  loss_ce_7: 0.03963  loss_mask_7: 0.1928  loss_dice_7: 0.1417  loss_ce_8: 0.03278  loss_mask_8: 0.2098  loss_dice_8: 0.1467  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:42] d2.utils.events INFO:  eta: 0:38:56  iter: 31439  total_loss: 4.034  loss_ce: 0.01204  loss_mask: 0.1933  loss_dice: 0.1348  loss_ce_0: 0.06632  loss_mask_0: 0.1963  loss_dice_0: 0.1373  loss_ce_1: 0.01894  loss_mask_1: 0.1905  loss_dice_1: 0.1357  loss_ce_2: 0.0191  loss_mask_2: 0.1934  loss_dice_2: 0.1337  loss_ce_3: 0.01535  loss_mask_3: 0.1876  loss_dice_3: 0.1376  loss_ce_4: 0.01312  loss_mask_4: 0.1928  loss_dice_4: 0.1306  loss_ce_5: 0.01318  loss_mask_5: 0.1874  loss_dice_5: 0.1378  loss_ce_6: 0.01392  loss_mask_6: 0.1856  loss_dice_6: 0.137  loss_ce_7: 0.01886  loss_mask_7: 0.1929  loss_dice_7: 0.1392  loss_ce_8: 0.01542  loss_mask_8: 0.187  loss_dice_8: 0.139  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:45] d2.utils.events INFO:  eta: 0:38:53  iter: 31459  total_loss: 4.693  loss_ce: 0.005123  loss_mask: 0.1913  loss_dice: 0.1978  loss_ce_0: 0.05912  loss_mask_0: 0.191  loss_dice_0: 0.1977  loss_ce_1: 0.01186  loss_mask_1: 0.1881  loss_dice_1: 0.1962  loss_ce_2: 0.00642  loss_mask_2: 0.1882  loss_dice_2: 0.2037  loss_ce_3: 0.004451  loss_mask_3: 0.1856  loss_dice_3: 0.2002  loss_ce_4: 0.006251  loss_mask_4: 0.19  loss_dice_4: 0.2058  loss_ce_5: 0.004449  loss_mask_5: 0.1905  loss_dice_5: 0.1987  loss_ce_6: 0.003528  loss_mask_6: 0.1881  loss_dice_6: 0.2013  loss_ce_7: 0.008876  loss_mask_7: 0.1921  loss_dice_7: 0.1914  loss_ce_8: 0.005386  loss_mask_8: 0.1883  loss_dice_8: 0.1979  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:04:47] d2.utils.events INFO:  eta: 0:38:49  iter: 31479  total_loss: 4.962  loss_ce: 0.007169  loss_mask: 0.2523  loss_dice: 0.1908  loss_ce_0: 0.06243  loss_mask_0: 0.2521  loss_dice_0: 0.1824  loss_ce_1: 0.009832  loss_mask_1: 0.2467  loss_dice_1: 0.1893  loss_ce_2: 0.007228  loss_mask_2: 0.2484  loss_dice_2: 0.1917  loss_ce_3: 0.00636  loss_mask_3: 0.247  loss_dice_3: 0.1867  loss_ce_4: 0.007126  loss_mask_4: 0.2481  loss_dice_4: 0.1906  loss_ce_5: 0.006341  loss_mask_5: 0.2489  loss_dice_5: 0.1904  loss_ce_6: 0.006331  loss_mask_6: 0.2472  loss_dice_6: 0.1889  loss_ce_7: 0.009247  loss_mask_7: 0.2483  loss_dice_7: 0.1925  loss_ce_8: 0.008493  loss_mask_8: 0.2526  loss_dice_8: 0.1995  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:50] d2.utils.events INFO:  eta: 0:38:46  iter: 31499  total_loss: 4.434  loss_ce: 0.00673  loss_mask: 0.1744  loss_dice: 0.1593  loss_ce_0: 0.05536  loss_mask_0: 0.1712  loss_dice_0: 0.1533  loss_ce_1: 0.01079  loss_mask_1: 0.1855  loss_dice_1: 0.1586  loss_ce_2: 0.007478  loss_mask_2: 0.174  loss_dice_2: 0.1607  loss_ce_3: 0.005673  loss_mask_3: 0.165  loss_dice_3: 0.1565  loss_ce_4: 0.006139  loss_mask_4: 0.173  loss_dice_4: 0.1622  loss_ce_5: 0.005934  loss_mask_5: 0.1758  loss_dice_5: 0.1633  loss_ce_6: 0.005918  loss_mask_6: 0.1772  loss_dice_6: 0.1656  loss_ce_7: 0.008784  loss_mask_7: 0.1741  loss_dice_7: 0.159  loss_ce_8: 0.008383  loss_mask_8: 0.1727  loss_dice_8: 0.1606  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:52] d2.utils.events INFO:  eta: 0:38:45  iter: 31519  total_loss: 4.698  loss_ce: 0.005739  loss_mask: 0.1555  loss_dice: 0.1884  loss_ce_0: 0.05954  loss_mask_0: 0.1563  loss_dice_0: 0.1759  loss_ce_1: 0.00664  loss_mask_1: 0.1581  loss_dice_1: 0.1854  loss_ce_2: 0.004908  loss_mask_2: 0.1594  loss_dice_2: 0.1892  loss_ce_3: 0.004507  loss_mask_3: 0.1549  loss_dice_3: 0.1857  loss_ce_4: 0.004944  loss_mask_4: 0.156  loss_dice_4: 0.1878  loss_ce_5: 0.004553  loss_mask_5: 0.1564  loss_dice_5: 0.1836  loss_ce_6: 0.004284  loss_mask_6: 0.1567  loss_dice_6: 0.1873  loss_ce_7: 0.005704  loss_mask_7: 0.1541  loss_dice_7: 0.18  loss_ce_8: 0.005949  loss_mask_8: 0.1691  loss_dice_8: 0.1855  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:55] d2.utils.events INFO:  eta: 0:38:43  iter: 31539  total_loss: 5.291  loss_ce: 0.01146  loss_mask: 0.222  loss_dice: 0.1651  loss_ce_0: 0.0594  loss_mask_0: 0.2299  loss_dice_0: 0.152  loss_ce_1: 0.007382  loss_mask_1: 0.2284  loss_dice_1: 0.1582  loss_ce_2: 0.01364  loss_mask_2: 0.2221  loss_dice_2: 0.1579  loss_ce_3: 0.03269  loss_mask_3: 0.2179  loss_dice_3: 0.1618  loss_ce_4: 0.007382  loss_mask_4: 0.2344  loss_dice_4: 0.16  loss_ce_5: 0.0073  loss_mask_5: 0.226  loss_dice_5: 0.1621  loss_ce_6: 0.009496  loss_mask_6: 0.225  loss_dice_6: 0.161  loss_ce_7: 0.006622  loss_mask_7: 0.2233  loss_dice_7: 0.1515  loss_ce_8: 0.01277  loss_mask_8: 0.2285  loss_dice_8: 0.1581  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:04:57] d2.utils.events INFO:  eta: 0:38:41  iter: 31559  total_loss: 4.875  loss_ce: 0.007994  loss_mask: 0.2083  loss_dice: 0.1917  loss_ce_0: 0.06203  loss_mask_0: 0.2122  loss_dice_0: 0.1958  loss_ce_1: 0.003825  loss_mask_1: 0.21  loss_dice_1: 0.1991  loss_ce_2: 0.005338  loss_mask_2: 0.2047  loss_dice_2: 0.1884  loss_ce_3: 0.008147  loss_mask_3: 0.206  loss_dice_3: 0.1923  loss_ce_4: 0.006616  loss_mask_4: 0.2073  loss_dice_4: 0.1996  loss_ce_5: 0.006995  loss_mask_5: 0.2029  loss_dice_5: 0.1997  loss_ce_6: 0.006878  loss_mask_6: 0.2089  loss_dice_6: 0.2  loss_ce_7: 0.007265  loss_mask_7: 0.2057  loss_dice_7: 0.1992  loss_ce_8: 0.006408  loss_mask_8: 0.2106  loss_dice_8: 0.1987  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:00] d2.utils.events INFO:  eta: 0:38:38  iter: 31579  total_loss: 4.934  loss_ce: 0.01022  loss_mask: 0.251  loss_dice: 0.1678  loss_ce_0: 0.05943  loss_mask_0: 0.2671  loss_dice_0: 0.1667  loss_ce_1: 0.006536  loss_mask_1: 0.2479  loss_dice_1: 0.1676  loss_ce_2: 0.008616  loss_mask_2: 0.2611  loss_dice_2: 0.1655  loss_ce_3: 0.01009  loss_mask_3: 0.256  loss_dice_3: 0.1689  loss_ce_4: 0.009501  loss_mask_4: 0.2639  loss_dice_4: 0.1669  loss_ce_5: 0.01005  loss_mask_5: 0.2496  loss_dice_5: 0.1631  loss_ce_6: 0.008207  loss_mask_6: 0.2564  loss_dice_6: 0.1644  loss_ce_7: 0.01003  loss_mask_7: 0.2601  loss_dice_7: 0.1653  loss_ce_8: 0.01142  loss_mask_8: 0.2566  loss_dice_8: 0.1656  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:02] d2.utils.events INFO:  eta: 0:38:35  iter: 31599  total_loss: 5.146  loss_ce: 0.0929  loss_mask: 0.182  loss_dice: 0.1488  loss_ce_0: 0.06567  loss_mask_0: 0.1904  loss_dice_0: 0.1423  loss_ce_1: 0.1459  loss_mask_1: 0.1788  loss_dice_1: 0.1499  loss_ce_2: 0.08236  loss_mask_2: 0.1864  loss_dice_2: 0.1395  loss_ce_3: 0.07676  loss_mask_3: 0.1951  loss_dice_3: 0.1426  loss_ce_4: 0.05993  loss_mask_4: 0.1849  loss_dice_4: 0.1432  loss_ce_5: 0.1045  loss_mask_5: 0.1846  loss_dice_5: 0.1408  loss_ce_6: 0.07477  loss_mask_6: 0.1908  loss_dice_6: 0.1425  loss_ce_7: 0.05582  loss_mask_7: 0.1846  loss_dice_7: 0.1425  loss_ce_8: 0.07341  loss_mask_8: 0.1795  loss_dice_8: 0.1496  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:05:05] d2.utils.events INFO:  eta: 0:38:33  iter: 31619  total_loss: 4.825  loss_ce: 0.07658  loss_mask: 0.1523  loss_dice: 0.1492  loss_ce_0: 0.05983  loss_mask_0: 0.15  loss_dice_0: 0.1566  loss_ce_1: 0.0401  loss_mask_1: 0.1549  loss_dice_1: 0.1506  loss_ce_2: 0.05981  loss_mask_2: 0.1532  loss_dice_2: 0.1522  loss_ce_3: 0.09254  loss_mask_3: 0.1547  loss_dice_3: 0.1505  loss_ce_4: 0.06701  loss_mask_4: 0.1456  loss_dice_4: 0.1584  loss_ce_5: 0.05974  loss_mask_5: 0.1448  loss_dice_5: 0.1562  loss_ce_6: 0.07782  loss_mask_6: 0.1558  loss_dice_6: 0.1494  loss_ce_7: 0.06849  loss_mask_7: 0.1523  loss_dice_7: 0.1498  loss_ce_8: 0.05901  loss_mask_8: 0.156  loss_dice_8: 0.1524  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:05:07] d2.utils.events INFO:  eta: 0:38:31  iter: 31639  total_loss: 4.388  loss_ce: 0.01248  loss_mask: 0.2098  loss_dice: 0.1507  loss_ce_0: 0.05855  loss_mask_0: 0.2161  loss_dice_0: 0.1495  loss_ce_1: 0.01335  loss_mask_1: 0.216  loss_dice_1: 0.1465  loss_ce_2: 0.01506  loss_mask_2: 0.2197  loss_dice_2: 0.1515  loss_ce_3: 0.0154  loss_mask_3: 0.2142  loss_dice_3: 0.1512  loss_ce_4: 0.01631  loss_mask_4: 0.2165  loss_dice_4: 0.1547  loss_ce_5: 0.01527  loss_mask_5: 0.222  loss_dice_5: 0.1508  loss_ce_6: 0.01417  loss_mask_6: 0.2181  loss_dice_6: 0.1477  loss_ce_7: 0.0154  loss_mask_7: 0.2212  loss_dice_7: 0.1543  loss_ce_8: 0.01451  loss_mask_8: 0.2067  loss_dice_8: 0.1425  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:10] d2.utils.events INFO:  eta: 0:38:29  iter: 31659  total_loss: 4.388  loss_ce: 0.01073  loss_mask: 0.218  loss_dice: 0.1415  loss_ce_0: 0.06633  loss_mask_0: 0.2157  loss_dice_0: 0.1449  loss_ce_1: 0.01384  loss_mask_1: 0.2112  loss_dice_1: 0.1388  loss_ce_2: 0.03871  loss_mask_2: 0.216  loss_dice_2: 0.1422  loss_ce_3: 0.0132  loss_mask_3: 0.2231  loss_dice_3: 0.14  loss_ce_4: 0.01543  loss_mask_4: 0.2201  loss_dice_4: 0.1389  loss_ce_5: 0.01543  loss_mask_5: 0.2176  loss_dice_5: 0.1399  loss_ce_6: 0.01023  loss_mask_6: 0.2265  loss_dice_6: 0.1466  loss_ce_7: 0.01673  loss_mask_7: 0.2267  loss_dice_7: 0.1384  loss_ce_8: 0.01425  loss_mask_8: 0.2157  loss_dice_8: 0.1374  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:13] d2.utils.events INFO:  eta: 0:38:27  iter: 31679  total_loss: 5.665  loss_ce: 0.05323  loss_mask: 0.1942  loss_dice: 0.1401  loss_ce_0: 0.06116  loss_mask_0: 0.1906  loss_dice_0: 0.1482  loss_ce_1: 0.08068  loss_mask_1: 0.192  loss_dice_1: 0.1431  loss_ce_2: 0.03862  loss_mask_2: 0.1913  loss_dice_2: 0.1456  loss_ce_3: 0.05874  loss_mask_3: 0.1972  loss_dice_3: 0.1456  loss_ce_4: 0.09976  loss_mask_4: 0.1856  loss_dice_4: 0.1513  loss_ce_5: 0.0358  loss_mask_5: 0.1846  loss_dice_5: 0.1471  loss_ce_6: 0.04447  loss_mask_6: 0.1868  loss_dice_6: 0.1452  loss_ce_7: 0.1032  loss_mask_7: 0.1957  loss_dice_7: 0.1471  loss_ce_8: 0.03728  loss_mask_8: 0.1952  loss_dice_8: 0.1475  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:15] d2.utils.events INFO:  eta: 0:38:24  iter: 31699  total_loss: 4.074  loss_ce: 0.01585  loss_mask: 0.192  loss_dice: 0.1413  loss_ce_0: 0.06213  loss_mask_0: 0.1897  loss_dice_0: 0.1404  loss_ce_1: 0.01336  loss_mask_1: 0.1916  loss_dice_1: 0.1488  loss_ce_2: 0.01472  loss_mask_2: 0.1885  loss_dice_2: 0.1429  loss_ce_3: 0.0163  loss_mask_3: 0.1923  loss_dice_3: 0.1397  loss_ce_4: 0.01377  loss_mask_4: 0.1818  loss_dice_4: 0.1407  loss_ce_5: 0.01725  loss_mask_5: 0.1839  loss_dice_5: 0.1443  loss_ce_6: 0.01763  loss_mask_6: 0.1935  loss_dice_6: 0.1364  loss_ce_7: 0.01626  loss_mask_7: 0.1876  loss_dice_7: 0.1401  loss_ce_8: 0.01687  loss_mask_8: 0.1877  loss_dice_8: 0.1476  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:18] d2.utils.events INFO:  eta: 0:38:20  iter: 31719  total_loss: 4.16  loss_ce: 0.01333  loss_mask: 0.2204  loss_dice: 0.1384  loss_ce_0: 0.06204  loss_mask_0: 0.2138  loss_dice_0: 0.1399  loss_ce_1: 0.01272  loss_mask_1: 0.2125  loss_dice_1: 0.1444  loss_ce_2: 0.01938  loss_mask_2: 0.2195  loss_dice_2: 0.1335  loss_ce_3: 0.02144  loss_mask_3: 0.2124  loss_dice_3: 0.1411  loss_ce_4: 0.01195  loss_mask_4: 0.2199  loss_dice_4: 0.1413  loss_ce_5: 0.01706  loss_mask_5: 0.2159  loss_dice_5: 0.1415  loss_ce_6: 0.01823  loss_mask_6: 0.2266  loss_dice_6: 0.138  loss_ce_7: 0.01353  loss_mask_7: 0.2152  loss_dice_7: 0.1423  loss_ce_8: 0.01888  loss_mask_8: 0.2184  loss_dice_8: 0.14  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:20] d2.utils.events INFO:  eta: 0:38:16  iter: 31739  total_loss: 4.068  loss_ce: 0.005791  loss_mask: 0.2366  loss_dice: 0.1192  loss_ce_0: 0.06461  loss_mask_0: 0.2353  loss_dice_0: 0.1162  loss_ce_1: 0.005292  loss_mask_1: 0.2402  loss_dice_1: 0.1184  loss_ce_2: 0.007091  loss_mask_2: 0.235  loss_dice_2: 0.1156  loss_ce_3: 0.005413  loss_mask_3: 0.2362  loss_dice_3: 0.1214  loss_ce_4: 0.005719  loss_mask_4: 0.236  loss_dice_4: 0.1182  loss_ce_5: 0.006563  loss_mask_5: 0.2384  loss_dice_5: 0.1161  loss_ce_6: 0.005494  loss_mask_6: 0.2396  loss_dice_6: 0.1213  loss_ce_7: 0.00613  loss_mask_7: 0.2334  loss_dice_7: 0.1155  loss_ce_8: 0.006994  loss_mask_8: 0.2375  loss_dice_8: 0.1164  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:22] d2.utils.events INFO:  eta: 0:38:12  iter: 31759  total_loss: 4.523  loss_ce: 0.00788  loss_mask: 0.2038  loss_dice: 0.145  loss_ce_0: 0.06209  loss_mask_0: 0.2097  loss_dice_0: 0.1487  loss_ce_1: 0.007323  loss_mask_1: 0.2147  loss_dice_1: 0.1556  loss_ce_2: 0.01003  loss_mask_2: 0.214  loss_dice_2: 0.145  loss_ce_3: 0.009847  loss_mask_3: 0.2121  loss_dice_3: 0.1493  loss_ce_4: 0.009448  loss_mask_4: 0.2055  loss_dice_4: 0.1549  loss_ce_5: 0.0117  loss_mask_5: 0.213  loss_dice_5: 0.1497  loss_ce_6: 0.01074  loss_mask_6: 0.2133  loss_dice_6: 0.1542  loss_ce_7: 0.009614  loss_mask_7: 0.206  loss_dice_7: 0.1571  loss_ce_8: 0.01026  loss_mask_8: 0.2142  loss_dice_8: 0.1508  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:25] d2.utils.events INFO:  eta: 0:38:07  iter: 31779  total_loss: 4.098  loss_ce: 0.007387  loss_mask: 0.1837  loss_dice: 0.1501  loss_ce_0: 0.06034  loss_mask_0: 0.1815  loss_dice_0: 0.16  loss_ce_1: 0.0135  loss_mask_1: 0.1761  loss_dice_1: 0.1557  loss_ce_2: 0.006993  loss_mask_2: 0.1779  loss_dice_2: 0.1666  loss_ce_3: 0.01027  loss_mask_3: 0.1841  loss_dice_3: 0.1574  loss_ce_4: 0.009793  loss_mask_4: 0.1726  loss_dice_4: 0.1622  loss_ce_5: 0.007601  loss_mask_5: 0.1837  loss_dice_5: 0.1601  loss_ce_6: 0.009397  loss_mask_6: 0.1928  loss_dice_6: 0.1544  loss_ce_7: 0.01046  loss_mask_7: 0.1768  loss_dice_7: 0.1612  loss_ce_8: 0.008639  loss_mask_8: 0.1885  loss_dice_8: 0.1643  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:27] d2.utils.events INFO:  eta: 0:38:03  iter: 31799  total_loss: 6.135  loss_ce: 0.004357  loss_mask: 0.1866  loss_dice: 0.1845  loss_ce_0: 0.06739  loss_mask_0: 0.1933  loss_dice_0: 0.1889  loss_ce_1: 0.04717  loss_mask_1: 0.1929  loss_dice_1: 0.1902  loss_ce_2: 0.01755  loss_mask_2: 0.189  loss_dice_2: 0.1938  loss_ce_3: 0.003976  loss_mask_3: 0.2017  loss_dice_3: 0.1959  loss_ce_4: 0.004632  loss_mask_4: 0.1931  loss_dice_4: 0.1864  loss_ce_5: 0.00608  loss_mask_5: 0.1876  loss_dice_5: 0.1908  loss_ce_6: 0.003923  loss_mask_6: 0.1942  loss_dice_6: 0.178  loss_ce_7: 0.007716  loss_mask_7: 0.1975  loss_dice_7: 0.1884  loss_ce_8: 0.006798  loss_mask_8: 0.2057  loss_dice_8: 0.1857  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:30] d2.utils.events INFO:  eta: 0:38:00  iter: 31819  total_loss: 4.485  loss_ce: 0.01378  loss_mask: 0.1852  loss_dice: 0.1668  loss_ce_0: 0.0653  loss_mask_0: 0.1747  loss_dice_0: 0.1647  loss_ce_1: 0.00939  loss_mask_1: 0.1683  loss_dice_1: 0.1684  loss_ce_2: 0.01124  loss_mask_2: 0.1796  loss_dice_2: 0.1708  loss_ce_3: 0.009574  loss_mask_3: 0.1816  loss_dice_3: 0.1737  loss_ce_4: 0.009507  loss_mask_4: 0.189  loss_dice_4: 0.1731  loss_ce_5: 0.01111  loss_mask_5: 0.1788  loss_dice_5: 0.167  loss_ce_6: 0.009101  loss_mask_6: 0.178  loss_dice_6: 0.1667  loss_ce_7: 0.01188  loss_mask_7: 0.1875  loss_dice_7: 0.1732  loss_ce_8: 0.01348  loss_mask_8: 0.1783  loss_dice_8: 0.1664  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:32] d2.utils.events INFO:  eta: 0:37:58  iter: 31839  total_loss: 4.359  loss_ce: 0.02031  loss_mask: 0.1903  loss_dice: 0.1472  loss_ce_0: 0.06108  loss_mask_0: 0.1927  loss_dice_0: 0.1536  loss_ce_1: 0.01235  loss_mask_1: 0.1763  loss_dice_1: 0.1459  loss_ce_2: 0.02356  loss_mask_2: 0.1888  loss_dice_2: 0.1555  loss_ce_3: 0.01108  loss_mask_3: 0.1805  loss_dice_3: 0.1441  loss_ce_4: 0.01911  loss_mask_4: 0.1909  loss_dice_4: 0.1521  loss_ce_5: 0.02604  loss_mask_5: 0.189  loss_dice_5: 0.1544  loss_ce_6: 0.01048  loss_mask_6: 0.1843  loss_dice_6: 0.1476  loss_ce_7: 0.01492  loss_mask_7: 0.178  loss_dice_7: 0.1501  loss_ce_8: 0.02097  loss_mask_8: 0.1908  loss_dice_8: 0.1464  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:35] d2.utils.events INFO:  eta: 0:37:57  iter: 31859  total_loss: 4.899  loss_ce: 0.02425  loss_mask: 0.1707  loss_dice: 0.1539  loss_ce_0: 0.06212  loss_mask_0: 0.1776  loss_dice_0: 0.1567  loss_ce_1: 0.1018  loss_mask_1: 0.1706  loss_dice_1: 0.1545  loss_ce_2: 0.05347  loss_mask_2: 0.1765  loss_dice_2: 0.1556  loss_ce_3: 0.02834  loss_mask_3: 0.1837  loss_dice_3: 0.1554  loss_ce_4: 0.05209  loss_mask_4: 0.1687  loss_dice_4: 0.1623  loss_ce_5: 0.0366  loss_mask_5: 0.1677  loss_dice_5: 0.1561  loss_ce_6: 0.02022  loss_mask_6: 0.1726  loss_dice_6: 0.1577  loss_ce_7: 0.05486  loss_mask_7: 0.1757  loss_dice_7: 0.1537  loss_ce_8: 0.04301  loss_mask_8: 0.171  loss_dice_8: 0.1548  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:05:37] d2.utils.events INFO:  eta: 0:37:56  iter: 31879  total_loss: 4.41  loss_ce: 0.009602  loss_mask: 0.2317  loss_dice: 0.1442  loss_ce_0: 0.06796  loss_mask_0: 0.2259  loss_dice_0: 0.1476  loss_ce_1: 0.01122  loss_mask_1: 0.2267  loss_dice_1: 0.1455  loss_ce_2: 0.01068  loss_mask_2: 0.2258  loss_dice_2: 0.1414  loss_ce_3: 0.009203  loss_mask_3: 0.2264  loss_dice_3: 0.1443  loss_ce_4: 0.01301  loss_mask_4: 0.2296  loss_dice_4: 0.1477  loss_ce_5: 0.01145  loss_mask_5: 0.2227  loss_dice_5: 0.1418  loss_ce_6: 0.008917  loss_mask_6: 0.2224  loss_dice_6: 0.1416  loss_ce_7: 0.01822  loss_mask_7: 0.22  loss_dice_7: 0.1449  loss_ce_8: 0.01187  loss_mask_8: 0.2239  loss_dice_8: 0.1438  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:40] d2.utils.events INFO:  eta: 0:37:54  iter: 31899  total_loss: 6.067  loss_ce: 0.04945  loss_mask: 0.2084  loss_dice: 0.1482  loss_ce_0: 0.06202  loss_mask_0: 0.2082  loss_dice_0: 0.153  loss_ce_1: 0.07605  loss_mask_1: 0.196  loss_dice_1: 0.1561  loss_ce_2: 0.04295  loss_mask_2: 0.1891  loss_dice_2: 0.1526  loss_ce_3: 0.03952  loss_mask_3: 0.2025  loss_dice_3: 0.1534  loss_ce_4: 0.07478  loss_mask_4: 0.197  loss_dice_4: 0.1493  loss_ce_5: 0.0411  loss_mask_5: 0.205  loss_dice_5: 0.1564  loss_ce_6: 0.03913  loss_mask_6: 0.2109  loss_dice_6: 0.1542  loss_ce_7: 0.06724  loss_mask_7: 0.203  loss_dice_7: 0.1468  loss_ce_8: 0.03272  loss_mask_8: 0.2017  loss_dice_8: 0.1522  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:05:43] d2.utils.events INFO:  eta: 0:37:53  iter: 31919  total_loss: 4.118  loss_ce: 0.004746  loss_mask: 0.2099  loss_dice: 0.1556  loss_ce_0: 0.05435  loss_mask_0: 0.2169  loss_dice_0: 0.1421  loss_ce_1: 0.004717  loss_mask_1: 0.2062  loss_dice_1: 0.1511  loss_ce_2: 0.005265  loss_mask_2: 0.2035  loss_dice_2: 0.1504  loss_ce_3: 0.004315  loss_mask_3: 0.2073  loss_dice_3: 0.1478  loss_ce_4: 0.005087  loss_mask_4: 0.2135  loss_dice_4: 0.1554  loss_ce_5: 0.005488  loss_mask_5: 0.2152  loss_dice_5: 0.151  loss_ce_6: 0.004669  loss_mask_6: 0.1974  loss_dice_6: 0.1431  loss_ce_7: 0.0056  loss_mask_7: 0.2086  loss_dice_7: 0.1435  loss_ce_8: 0.005259  loss_mask_8: 0.2059  loss_dice_8: 0.1494  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:45] d2.utils.events INFO:  eta: 0:37:50  iter: 31939  total_loss: 4.971  loss_ce: 0.06221  loss_mask: 0.19  loss_dice: 0.1572  loss_ce_0: 0.0469  loss_mask_0: 0.1991  loss_dice_0: 0.1716  loss_ce_1: 0.03395  loss_mask_1: 0.1938  loss_dice_1: 0.1691  loss_ce_2: 0.04845  loss_mask_2: 0.1907  loss_dice_2: 0.1562  loss_ce_3: 0.05496  loss_mask_3: 0.1881  loss_dice_3: 0.1649  loss_ce_4: 0.07015  loss_mask_4: 0.1882  loss_dice_4: 0.1618  loss_ce_5: 0.05239  loss_mask_5: 0.1947  loss_dice_5: 0.1692  loss_ce_6: 0.04903  loss_mask_6: 0.1814  loss_dice_6: 0.154  loss_ce_7: 0.06902  loss_mask_7: 0.1939  loss_dice_7: 0.1629  loss_ce_8: 0.05038  loss_mask_8: 0.1969  loss_dice_8: 0.158  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:48] d2.utils.events INFO:  eta: 0:37:44  iter: 31959  total_loss: 4.108  loss_ce: 0.01519  loss_mask: 0.1959  loss_dice: 0.1594  loss_ce_0: 0.0443  loss_mask_0: 0.1873  loss_dice_0: 0.1509  loss_ce_1: 0.0102  loss_mask_1: 0.1933  loss_dice_1: 0.1558  loss_ce_2: 0.01303  loss_mask_2: 0.1952  loss_dice_2: 0.1615  loss_ce_3: 0.01588  loss_mask_3: 0.1904  loss_dice_3: 0.166  loss_ce_4: 0.01375  loss_mask_4: 0.1961  loss_dice_4: 0.1539  loss_ce_5: 0.01581  loss_mask_5: 0.191  loss_dice_5: 0.1539  loss_ce_6: 0.0154  loss_mask_6: 0.1877  loss_dice_6: 0.1565  loss_ce_7: 0.01402  loss_mask_7: 0.1849  loss_dice_7: 0.1578  loss_ce_8: 0.01442  loss_mask_8: 0.1951  loss_dice_8: 0.1598  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:50] d2.utils.events INFO:  eta: 0:37:42  iter: 31979  total_loss: 4.293  loss_ce: 0.01427  loss_mask: 0.1388  loss_dice: 0.1803  loss_ce_0: 0.08411  loss_mask_0: 0.1377  loss_dice_0: 0.1837  loss_ce_1: 0.04501  loss_mask_1: 0.1422  loss_dice_1: 0.1902  loss_ce_2: 0.01418  loss_mask_2: 0.1368  loss_dice_2: 0.171  loss_ce_3: 0.01621  loss_mask_3: 0.1431  loss_dice_3: 0.1755  loss_ce_4: 0.01458  loss_mask_4: 0.1361  loss_dice_4: 0.1798  loss_ce_5: 0.01482  loss_mask_5: 0.1469  loss_dice_5: 0.1832  loss_ce_6: 0.01571  loss_mask_6: 0.1393  loss_dice_6: 0.1755  loss_ce_7: 0.01765  loss_mask_7: 0.1404  loss_dice_7: 0.1789  loss_ce_8: 0.01391  loss_mask_8: 0.1318  loss_dice_8: 0.185  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:53] d2.utils.events INFO:  eta: 0:37:38  iter: 31999  total_loss: 4.13  loss_ce: 0.005698  loss_mask: 0.205  loss_dice: 0.1266  loss_ce_0: 0.06869  loss_mask_0: 0.1943  loss_dice_0: 0.1275  loss_ce_1: 0.005134  loss_mask_1: 0.2015  loss_dice_1: 0.1285  loss_ce_2: 0.005371  loss_mask_2: 0.2001  loss_dice_2: 0.1245  loss_ce_3: 0.006098  loss_mask_3: 0.2111  loss_dice_3: 0.1319  loss_ce_4: 0.003695  loss_mask_4: 0.1998  loss_dice_4: 0.1211  loss_ce_5: 0.005112  loss_mask_5: 0.202  loss_dice_5: 0.1291  loss_ce_6: 0.005136  loss_mask_6: 0.1893  loss_dice_6: 0.1236  loss_ce_7: 0.003687  loss_mask_7: 0.2014  loss_dice_7: 0.1273  loss_ce_8: 0.005286  loss_mask_8: 0.203  loss_dice_8: 0.1268  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:55] d2.utils.events INFO:  eta: 0:37:36  iter: 32019  total_loss: 4.767  loss_ce: 0.004794  loss_mask: 0.1964  loss_dice: 0.2071  loss_ce_0: 0.06299  loss_mask_0: 0.1948  loss_dice_0: 0.1966  loss_ce_1: 0.01097  loss_mask_1: 0.2008  loss_dice_1: 0.2053  loss_ce_2: 0.006073  loss_mask_2: 0.2049  loss_dice_2: 0.1959  loss_ce_3: 0.004569  loss_mask_3: 0.2134  loss_dice_3: 0.204  loss_ce_4: 0.004554  loss_mask_4: 0.1962  loss_dice_4: 0.2016  loss_ce_5: 0.004999  loss_mask_5: 0.1994  loss_dice_5: 0.2075  loss_ce_6: 0.004307  loss_mask_6: 0.2  loss_dice_6: 0.2057  loss_ce_7: 0.004878  loss_mask_7: 0.2032  loss_dice_7: 0.201  loss_ce_8: 0.005386  loss_mask_8: 0.2127  loss_dice_8: 0.2096  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:05:58] d2.utils.events INFO:  eta: 0:37:31  iter: 32039  total_loss: 3.729  loss_ce: 0.005285  loss_mask: 0.208  loss_dice: 0.1261  loss_ce_0: 0.06478  loss_mask_0: 0.2157  loss_dice_0: 0.1267  loss_ce_1: 0.01106  loss_mask_1: 0.2164  loss_dice_1: 0.1306  loss_ce_2: 0.006297  loss_mask_2: 0.2079  loss_dice_2: 0.1316  loss_ce_3: 0.00542  loss_mask_3: 0.2124  loss_dice_3: 0.1322  loss_ce_4: 0.00521  loss_mask_4: 0.208  loss_dice_4: 0.1297  loss_ce_5: 0.005694  loss_mask_5: 0.2164  loss_dice_5: 0.1305  loss_ce_6: 0.005118  loss_mask_6: 0.2145  loss_dice_6: 0.1336  loss_ce_7: 0.004555  loss_mask_7: 0.2085  loss_dice_7: 0.1332  loss_ce_8: 0.005788  loss_mask_8: 0.2059  loss_dice_8: 0.1294  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:06:00] d2.utils.events INFO:  eta: 0:37:29  iter: 32059  total_loss: 5.098  loss_ce: 0.06047  loss_mask: 0.1531  loss_dice: 0.1205  loss_ce_0: 0.06501  loss_mask_0: 0.1453  loss_dice_0: 0.1179  loss_ce_1: 0.04945  loss_mask_1: 0.1466  loss_dice_1: 0.1167  loss_ce_2: 0.04044  loss_mask_2: 0.1513  loss_dice_2: 0.1174  loss_ce_3: 0.04425  loss_mask_3: 0.1425  loss_dice_3: 0.1147  loss_ce_4: 0.04878  loss_mask_4: 0.1461  loss_dice_4: 0.1181  loss_ce_5: 0.054  loss_mask_5: 0.1541  loss_dice_5: 0.1212  loss_ce_6: 0.04644  loss_mask_6: 0.1546  loss_dice_6: 0.1161  loss_ce_7: 0.06958  loss_mask_7: 0.1556  loss_dice_7: 0.1158  loss_ce_8: 0.07018  loss_mask_8: 0.1612  loss_dice_8: 0.1167  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:03] d2.utils.events INFO:  eta: 0:37:28  iter: 32079  total_loss: 3.972  loss_ce: 0.01623  loss_mask: 0.1991  loss_dice: 0.1439  loss_ce_0: 0.05641  loss_mask_0: 0.196  loss_dice_0: 0.1416  loss_ce_1: 0.01809  loss_mask_1: 0.1963  loss_dice_1: 0.1395  loss_ce_2: 0.0131  loss_mask_2: 0.2006  loss_dice_2: 0.1401  loss_ce_3: 0.01609  loss_mask_3: 0.1957  loss_dice_3: 0.1463  loss_ce_4: 0.01692  loss_mask_4: 0.1898  loss_dice_4: 0.1381  loss_ce_5: 0.01528  loss_mask_5: 0.1872  loss_dice_5: 0.1446  loss_ce_6: 0.01543  loss_mask_6: 0.2011  loss_dice_6: 0.1432  loss_ce_7: 0.01315  loss_mask_7: 0.199  loss_dice_7: 0.1428  loss_ce_8: 0.01304  loss_mask_8: 0.1903  loss_dice_8: 0.1415  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:05] d2.utils.events INFO:  eta: 0:37:25  iter: 32099  total_loss: 4.661  loss_ce: 0.02443  loss_mask: 0.1683  loss_dice: 0.1801  loss_ce_0: 0.0539  loss_mask_0: 0.1708  loss_dice_0: 0.1819  loss_ce_1: 0.02685  loss_mask_1: 0.1752  loss_dice_1: 0.1865  loss_ce_2: 0.01507  loss_mask_2: 0.1637  loss_dice_2: 0.1746  loss_ce_3: 0.02174  loss_mask_3: 0.1729  loss_dice_3: 0.1794  loss_ce_4: 0.01975  loss_mask_4: 0.1703  loss_dice_4: 0.1744  loss_ce_5: 0.01585  loss_mask_5: 0.1654  loss_dice_5: 0.1791  loss_ce_6: 0.01954  loss_mask_6: 0.1738  loss_dice_6: 0.1878  loss_ce_7: 0.01838  loss_mask_7: 0.1757  loss_dice_7: 0.1769  loss_ce_8: 0.01774  loss_mask_8: 0.1703  loss_dice_8: 0.178  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:08] d2.utils.events INFO:  eta: 0:37:23  iter: 32119  total_loss: 4.058  loss_ce: 0.01199  loss_mask: 0.2145  loss_dice: 0.1394  loss_ce_0: 0.06289  loss_mask_0: 0.2075  loss_dice_0: 0.1477  loss_ce_1: 0.01267  loss_mask_1: 0.2072  loss_dice_1: 0.1417  loss_ce_2: 0.009401  loss_mask_2: 0.215  loss_dice_2: 0.1465  loss_ce_3: 0.01123  loss_mask_3: 0.2051  loss_dice_3: 0.1477  loss_ce_4: 0.01064  loss_mask_4: 0.2123  loss_dice_4: 0.1437  loss_ce_5: 0.009699  loss_mask_5: 0.2167  loss_dice_5: 0.1466  loss_ce_6: 0.01218  loss_mask_6: 0.2147  loss_dice_6: 0.1463  loss_ce_7: 0.008454  loss_mask_7: 0.212  loss_dice_7: 0.1463  loss_ce_8: 0.008969  loss_mask_8: 0.2223  loss_dice_8: 0.1434  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:10] d2.utils.events INFO:  eta: 0:37:20  iter: 32139  total_loss: 4.098  loss_ce: 0.0179  loss_mask: 0.2207  loss_dice: 0.1472  loss_ce_0: 0.05244  loss_mask_0: 0.2099  loss_dice_0: 0.1493  loss_ce_1: 0.01654  loss_mask_1: 0.2148  loss_dice_1: 0.1483  loss_ce_2: 0.01517  loss_mask_2: 0.2115  loss_dice_2: 0.1424  loss_ce_3: 0.01594  loss_mask_3: 0.2091  loss_dice_3: 0.1443  loss_ce_4: 0.01774  loss_mask_4: 0.2181  loss_dice_4: 0.1473  loss_ce_5: 0.01584  loss_mask_5: 0.219  loss_dice_5: 0.1512  loss_ce_6: 0.0156  loss_mask_6: 0.2218  loss_dice_6: 0.1412  loss_ce_7: 0.01757  loss_mask_7: 0.2263  loss_dice_7: 0.1482  loss_ce_8: 0.01574  loss_mask_8: 0.219  loss_dice_8: 0.1494  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:13] d2.utils.events INFO:  eta: 0:37:18  iter: 32159  total_loss: 5.415  loss_ce: 0.07635  loss_mask: 0.1805  loss_dice: 0.1345  loss_ce_0: 0.07418  loss_mask_0: 0.1838  loss_dice_0: 0.13  loss_ce_1: 0.07822  loss_mask_1: 0.1708  loss_dice_1: 0.1315  loss_ce_2: 0.06819  loss_mask_2: 0.1799  loss_dice_2: 0.1361  loss_ce_3: 0.07406  loss_mask_3: 0.1766  loss_dice_3: 0.131  loss_ce_4: 0.07369  loss_mask_4: 0.1751  loss_dice_4: 0.1316  loss_ce_5: 0.07363  loss_mask_5: 0.1784  loss_dice_5: 0.1315  loss_ce_6: 0.078  loss_mask_6: 0.1898  loss_dice_6: 0.1301  loss_ce_7: 0.06813  loss_mask_7: 0.1865  loss_dice_7: 0.1356  loss_ce_8: 0.07172  loss_mask_8: 0.1775  loss_dice_8: 0.1313  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:16] d2.utils.events INFO:  eta: 0:37:17  iter: 32179  total_loss: 5.193  loss_ce: 0.08144  loss_mask: 0.2341  loss_dice: 0.1437  loss_ce_0: 0.06855  loss_mask_0: 0.2307  loss_dice_0: 0.1512  loss_ce_1: 0.1217  loss_mask_1: 0.23  loss_dice_1: 0.1454  loss_ce_2: 0.06133  loss_mask_2: 0.234  loss_dice_2: 0.1445  loss_ce_3: 0.07484  loss_mask_3: 0.2305  loss_dice_3: 0.1514  loss_ce_4: 0.1047  loss_mask_4: 0.2248  loss_dice_4: 0.1519  loss_ce_5: 0.08813  loss_mask_5: 0.2277  loss_dice_5: 0.1471  loss_ce_6: 0.07583  loss_mask_6: 0.2355  loss_dice_6: 0.1485  loss_ce_7: 0.1105  loss_mask_7: 0.2357  loss_dice_7: 0.141  loss_ce_8: 0.08271  loss_mask_8: 0.2352  loss_dice_8: 0.1469  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:18] d2.utils.events INFO:  eta: 0:37:15  iter: 32199  total_loss: 5.019  loss_ce: 0.06669  loss_mask: 0.1855  loss_dice: 0.1477  loss_ce_0: 0.06173  loss_mask_0: 0.1994  loss_dice_0: 0.1429  loss_ce_1: 0.1015  loss_mask_1: 0.1889  loss_dice_1: 0.1436  loss_ce_2: 0.03824  loss_mask_2: 0.1832  loss_dice_2: 0.1452  loss_ce_3: 0.05792  loss_mask_3: 0.2012  loss_dice_3: 0.1459  loss_ce_4: 0.04846  loss_mask_4: 0.1895  loss_dice_4: 0.142  loss_ce_5: 0.0379  loss_mask_5: 0.1886  loss_dice_5: 0.1405  loss_ce_6: 0.06507  loss_mask_6: 0.1934  loss_dice_6: 0.143  loss_ce_7: 0.05576  loss_mask_7: 0.1867  loss_dice_7: 0.1435  loss_ce_8: 0.03664  loss_mask_8: 0.1855  loss_dice_8: 0.1438  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:21] d2.utils.events INFO:  eta: 0:37:11  iter: 32219  total_loss: 4.878  loss_ce: 0.05551  loss_mask: 0.1795  loss_dice: 0.161  loss_ce_0: 0.0614  loss_mask_0: 0.1781  loss_dice_0: 0.1575  loss_ce_1: 0.06695  loss_mask_1: 0.1894  loss_dice_1: 0.1667  loss_ce_2: 0.06387  loss_mask_2: 0.1802  loss_dice_2: 0.1592  loss_ce_3: 0.0494  loss_mask_3: 0.1835  loss_dice_3: 0.161  loss_ce_4: 0.07089  loss_mask_4: 0.1826  loss_dice_4: 0.16  loss_ce_5: 0.07565  loss_mask_5: 0.1751  loss_dice_5: 0.1638  loss_ce_6: 0.04294  loss_mask_6: 0.1767  loss_dice_6: 0.1587  loss_ce_7: 0.07086  loss_mask_7: 0.1793  loss_dice_7: 0.1576  loss_ce_8: 0.06794  loss_mask_8: 0.1819  loss_dice_8: 0.1573  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:23] d2.utils.events INFO:  eta: 0:37:08  iter: 32239  total_loss: 4.67  loss_ce: 0.05475  loss_mask: 0.1498  loss_dice: 0.1759  loss_ce_0: 0.06132  loss_mask_0: 0.152  loss_dice_0: 0.173  loss_ce_1: 0.02649  loss_mask_1: 0.1604  loss_dice_1: 0.1756  loss_ce_2: 0.03319  loss_mask_2: 0.1582  loss_dice_2: 0.1689  loss_ce_3: 0.06105  loss_mask_3: 0.1543  loss_dice_3: 0.1735  loss_ce_4: 0.02974  loss_mask_4: 0.1579  loss_dice_4: 0.174  loss_ce_5: 0.04367  loss_mask_5: 0.1545  loss_dice_5: 0.175  loss_ce_6: 0.09072  loss_mask_6: 0.1553  loss_dice_6: 0.1722  loss_ce_7: 0.03734  loss_mask_7: 0.1486  loss_dice_7: 0.1638  loss_ce_8: 0.04272  loss_mask_8: 0.1587  loss_dice_8: 0.174  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:26] d2.utils.events INFO:  eta: 0:37:07  iter: 32259  total_loss: 4.475  loss_ce: 0.00655  loss_mask: 0.2241  loss_dice: 0.1254  loss_ce_0: 0.06265  loss_mask_0: 0.2202  loss_dice_0: 0.125  loss_ce_1: 0.008934  loss_mask_1: 0.2282  loss_dice_1: 0.1277  loss_ce_2: 0.007676  loss_mask_2: 0.2197  loss_dice_2: 0.1246  loss_ce_3: 0.009756  loss_mask_3: 0.219  loss_dice_3: 0.1277  loss_ce_4: 0.005628  loss_mask_4: 0.2252  loss_dice_4: 0.1272  loss_ce_5: 0.00709  loss_mask_5: 0.2232  loss_dice_5: 0.1313  loss_ce_6: 0.009908  loss_mask_6: 0.231  loss_dice_6: 0.1307  loss_ce_7: 0.005187  loss_mask_7: 0.2256  loss_dice_7: 0.1247  loss_ce_8: 0.007577  loss_mask_8: 0.2222  loss_dice_8: 0.1307  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:06:28] d2.utils.events INFO:  eta: 0:37:05  iter: 32279  total_loss: 3.926  loss_ce: 0.01134  loss_mask: 0.1556  loss_dice: 0.1533  loss_ce_0: 0.06595  loss_mask_0: 0.1565  loss_dice_0: 0.1504  loss_ce_1: 0.01093  loss_mask_1: 0.1578  loss_dice_1: 0.1599  loss_ce_2: 0.01078  loss_mask_2: 0.1643  loss_dice_2: 0.1626  loss_ce_3: 0.01163  loss_mask_3: 0.1621  loss_dice_3: 0.1541  loss_ce_4: 0.008049  loss_mask_4: 0.1625  loss_dice_4: 0.1571  loss_ce_5: 0.008595  loss_mask_5: 0.1575  loss_dice_5: 0.1532  loss_ce_6: 0.01217  loss_mask_6: 0.1622  loss_dice_6: 0.1549  loss_ce_7: 0.009598  loss_mask_7: 0.16  loss_dice_7: 0.1582  loss_ce_8: 0.01178  loss_mask_8: 0.1581  loss_dice_8: 0.1552  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:06:31] d2.utils.events INFO:  eta: 0:37:04  iter: 32299  total_loss: 5.322  loss_ce: 0.06221  loss_mask: 0.1946  loss_dice: 0.1939  loss_ce_0: 0.06044  loss_mask_0: 0.1911  loss_dice_0: 0.1906  loss_ce_1: 0.03304  loss_mask_1: 0.1793  loss_dice_1: 0.1836  loss_ce_2: 0.02424  loss_mask_2: 0.1862  loss_dice_2: 0.181  loss_ce_3: 0.03204  loss_mask_3: 0.1876  loss_dice_3: 0.1869  loss_ce_4: 0.03781  loss_mask_4: 0.1902  loss_dice_4: 0.1869  loss_ce_5: 0.02769  loss_mask_5: 0.1899  loss_dice_5: 0.1841  loss_ce_6: 0.0639  loss_mask_6: 0.192  loss_dice_6: 0.192  loss_ce_7: 0.0386  loss_mask_7: 0.1829  loss_dice_7: 0.1864  loss_ce_8: 0.03342  loss_mask_8: 0.1876  loss_dice_8: 0.1834  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:06:33] d2.utils.events INFO:  eta: 0:37:01  iter: 32319  total_loss: 6.598  loss_ce: 0.1322  loss_mask: 0.1723  loss_dice: 0.1628  loss_ce_0: 0.05957  loss_mask_0: 0.1712  loss_dice_0: 0.1639  loss_ce_1: 0.1113  loss_mask_1: 0.173  loss_dice_1: 0.1571  loss_ce_2: 0.1305  loss_mask_2: 0.1783  loss_dice_2: 0.1614  loss_ce_3: 0.1314  loss_mask_3: 0.1785  loss_dice_3: 0.1517  loss_ce_4: 0.1304  loss_mask_4: 0.178  loss_dice_4: 0.1611  loss_ce_5: 0.1308  loss_mask_5: 0.1741  loss_dice_5: 0.1646  loss_ce_6: 0.1321  loss_mask_6: 0.1677  loss_dice_6: 0.1652  loss_ce_7: 0.1316  loss_mask_7: 0.1696  loss_dice_7: 0.1557  loss_ce_8: 0.1307  loss_mask_8: 0.1776  loss_dice_8: 0.1641  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:36] d2.utils.events INFO:  eta: 0:36:58  iter: 32339  total_loss: 4.298  loss_ce: 0.02648  loss_mask: 0.2019  loss_dice: 0.1362  loss_ce_0: 0.05546  loss_mask_0: 0.1956  loss_dice_0: 0.1296  loss_ce_1: 0.06938  loss_mask_1: 0.1979  loss_dice_1: 0.1281  loss_ce_2: 0.03984  loss_mask_2: 0.1955  loss_dice_2: 0.1335  loss_ce_3: 0.02153  loss_mask_3: 0.1931  loss_dice_3: 0.1331  loss_ce_4: 0.03509  loss_mask_4: 0.1982  loss_dice_4: 0.1331  loss_ce_5: 0.03597  loss_mask_5: 0.1997  loss_dice_5: 0.1314  loss_ce_6: 0.02152  loss_mask_6: 0.1965  loss_dice_6: 0.1372  loss_ce_7: 0.04074  loss_mask_7: 0.1846  loss_dice_7: 0.1296  loss_ce_8: 0.0378  loss_mask_8: 0.1965  loss_dice_8: 0.1318  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:38] d2.utils.events INFO:  eta: 0:36:57  iter: 32359  total_loss: 4.82  loss_ce: 0.03151  loss_mask: 0.2199  loss_dice: 0.1506  loss_ce_0: 0.06406  loss_mask_0: 0.2129  loss_dice_0: 0.1498  loss_ce_1: 0.04544  loss_mask_1: 0.2206  loss_dice_1: 0.1565  loss_ce_2: 0.03483  loss_mask_2: 0.2241  loss_dice_2: 0.1554  loss_ce_3: 0.03383  loss_mask_3: 0.2182  loss_dice_3: 0.1518  loss_ce_4: 0.03734  loss_mask_4: 0.2165  loss_dice_4: 0.1482  loss_ce_5: 0.03053  loss_mask_5: 0.2219  loss_dice_5: 0.153  loss_ce_6: 0.03205  loss_mask_6: 0.2083  loss_dice_6: 0.1485  loss_ce_7: 0.04234  loss_mask_7: 0.2289  loss_dice_7: 0.1508  loss_ce_8: 0.0321  loss_mask_8: 0.219  loss_dice_8: 0.1558  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:06:41] d2.utils.events INFO:  eta: 0:36:55  iter: 32379  total_loss: 4.265  loss_ce: 0.0234  loss_mask: 0.2131  loss_dice: 0.1575  loss_ce_0: 0.04651  loss_mask_0: 0.2129  loss_dice_0: 0.1586  loss_ce_1: 0.04846  loss_mask_1: 0.2136  loss_dice_1: 0.1649  loss_ce_2: 0.02265  loss_mask_2: 0.2047  loss_dice_2: 0.1637  loss_ce_3: 0.02214  loss_mask_3: 0.2149  loss_dice_3: 0.1604  loss_ce_4: 0.02665  loss_mask_4: 0.2125  loss_dice_4: 0.1572  loss_ce_5: 0.02344  loss_mask_5: 0.2153  loss_dice_5: 0.1549  loss_ce_6: 0.02334  loss_mask_6: 0.2006  loss_dice_6: 0.1558  loss_ce_7: 0.0339  loss_mask_7: 0.2176  loss_dice_7: 0.1644  loss_ce_8: 0.02388  loss_mask_8: 0.2201  loss_dice_8: 0.1584  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:06:43] d2.utils.events INFO:  eta: 0:36:54  iter: 32399  total_loss: 4.175  loss_ce: 0.01527  loss_mask: 0.1913  loss_dice: 0.1419  loss_ce_0: 0.06497  loss_mask_0: 0.1975  loss_dice_0: 0.1386  loss_ce_1: 0.01422  loss_mask_1: 0.1934  loss_dice_1: 0.1434  loss_ce_2: 0.01562  loss_mask_2: 0.1836  loss_dice_2: 0.1411  loss_ce_3: 0.01725  loss_mask_3: 0.1969  loss_dice_3: 0.136  loss_ce_4: 0.01586  loss_mask_4: 0.1933  loss_dice_4: 0.1434  loss_ce_5: 0.01821  loss_mask_5: 0.1914  loss_dice_5: 0.1395  loss_ce_6: 0.01641  loss_mask_6: 0.1881  loss_dice_6: 0.1409  loss_ce_7: 0.01757  loss_mask_7: 0.1984  loss_dice_7: 0.1358  loss_ce_8: 0.01676  loss_mask_8: 0.186  loss_dice_8: 0.1341  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:46] d2.utils.events INFO:  eta: 0:36:53  iter: 32419  total_loss: 3.689  loss_ce: 0.01278  loss_mask: 0.1873  loss_dice: 0.1317  loss_ce_0: 0.07625  loss_mask_0: 0.1824  loss_dice_0: 0.1259  loss_ce_1: 0.01013  loss_mask_1: 0.1894  loss_dice_1: 0.1316  loss_ce_2: 0.01084  loss_mask_2: 0.1949  loss_dice_2: 0.1333  loss_ce_3: 0.01457  loss_mask_3: 0.185  loss_dice_3: 0.1286  loss_ce_4: 0.01255  loss_mask_4: 0.1842  loss_dice_4: 0.1275  loss_ce_5: 0.01272  loss_mask_5: 0.1851  loss_dice_5: 0.1292  loss_ce_6: 0.01407  loss_mask_6: 0.1855  loss_dice_6: 0.1281  loss_ce_7: 0.01381  loss_mask_7: 0.191  loss_dice_7: 0.1328  loss_ce_8: 0.01124  loss_mask_8: 0.1911  loss_dice_8: 0.1327  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:49] d2.utils.events INFO:  eta: 0:36:52  iter: 32439  total_loss: 5.974  loss_ce: 0.1807  loss_mask: 0.1973  loss_dice: 0.1718  loss_ce_0: 0.05111  loss_mask_0: 0.2014  loss_dice_0: 0.1854  loss_ce_1: 0.1714  loss_mask_1: 0.1945  loss_dice_1: 0.1801  loss_ce_2: 0.1813  loss_mask_2: 0.2016  loss_dice_2: 0.1819  loss_ce_3: 0.2112  loss_mask_3: 0.211  loss_dice_3: 0.1742  loss_ce_4: 0.2112  loss_mask_4: 0.1971  loss_dice_4: 0.1797  loss_ce_5: 0.1691  loss_mask_5: 0.2027  loss_dice_5: 0.1838  loss_ce_6: 0.1985  loss_mask_6: 0.1939  loss_dice_6: 0.1777  loss_ce_7: 0.1924  loss_mask_7: 0.1962  loss_dice_7: 0.1758  loss_ce_8: 0.1683  loss_mask_8: 0.2026  loss_dice_8: 0.1812  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:51] d2.utils.events INFO:  eta: 0:36:51  iter: 32459  total_loss: 5.108  loss_ce: 0.06571  loss_mask: 0.1904  loss_dice: 0.2018  loss_ce_0: 0.06365  loss_mask_0: 0.1903  loss_dice_0: 0.201  loss_ce_1: 0.0678  loss_mask_1: 0.1821  loss_dice_1: 0.1909  loss_ce_2: 0.04541  loss_mask_2: 0.1802  loss_dice_2: 0.2001  loss_ce_3: 0.06422  loss_mask_3: 0.1874  loss_dice_3: 0.1975  loss_ce_4: 0.06816  loss_mask_4: 0.1897  loss_dice_4: 0.2033  loss_ce_5: 0.04021  loss_mask_5: 0.1813  loss_dice_5: 0.2001  loss_ce_6: 0.06874  loss_mask_6: 0.1888  loss_dice_6: 0.1972  loss_ce_7: 0.06916  loss_mask_7: 0.1903  loss_dice_7: 0.1857  loss_ce_8: 0.04322  loss_mask_8: 0.1866  loss_dice_8: 0.1976  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:06:54] d2.utils.events INFO:  eta: 0:36:49  iter: 32479  total_loss: 4.248  loss_ce: 0.0167  loss_mask: 0.2159  loss_dice: 0.1671  loss_ce_0: 0.07912  loss_mask_0: 0.2129  loss_dice_0: 0.166  loss_ce_1: 0.01297  loss_mask_1: 0.2169  loss_dice_1: 0.1679  loss_ce_2: 0.01751  loss_mask_2: 0.214  loss_dice_2: 0.1579  loss_ce_3: 0.02583  loss_mask_3: 0.2141  loss_dice_3: 0.1667  loss_ce_4: 0.02319  loss_mask_4: 0.2076  loss_dice_4: 0.1688  loss_ce_5: 0.01936  loss_mask_5: 0.2208  loss_dice_5: 0.167  loss_ce_6: 0.0219  loss_mask_6: 0.2103  loss_dice_6: 0.164  loss_ce_7: 0.02018  loss_mask_7: 0.214  loss_dice_7: 0.1644  loss_ce_8: 0.0179  loss_mask_8: 0.2097  loss_dice_8: 0.1695  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:06:56] d2.utils.events INFO:  eta: 0:36:45  iter: 32499  total_loss: 4.671  loss_ce: 0.05532  loss_mask: 0.2039  loss_dice: 0.179  loss_ce_0: 0.05307  loss_mask_0: 0.1932  loss_dice_0: 0.1705  loss_ce_1: 0.02398  loss_mask_1: 0.2035  loss_dice_1: 0.1775  loss_ce_2: 0.01825  loss_mask_2: 0.2045  loss_dice_2: 0.1742  loss_ce_3: 0.08291  loss_mask_3: 0.2023  loss_dice_3: 0.1749  loss_ce_4: 0.05535  loss_mask_4: 0.1997  loss_dice_4: 0.1685  loss_ce_5: 0.02152  loss_mask_5: 0.1999  loss_dice_5: 0.1732  loss_ce_6: 0.06666  loss_mask_6: 0.2076  loss_dice_6: 0.1734  loss_ce_7: 0.05715  loss_mask_7: 0.2082  loss_dice_7: 0.1708  loss_ce_8: 0.02195  loss_mask_8: 0.2097  loss_dice_8: 0.1789  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:06:59] d2.utils.events INFO:  eta: 0:36:42  iter: 32519  total_loss: 5.617  loss_ce: 0.09149  loss_mask: 0.147  loss_dice: 0.1975  loss_ce_0: 0.06262  loss_mask_0: 0.1566  loss_dice_0: 0.1966  loss_ce_1: 0.1023  loss_mask_1: 0.1593  loss_dice_1: 0.1886  loss_ce_2: 0.1034  loss_mask_2: 0.1622  loss_dice_2: 0.1876  loss_ce_3: 0.09838  loss_mask_3: 0.1536  loss_dice_3: 0.1818  loss_ce_4: 0.1113  loss_mask_4: 0.1525  loss_dice_4: 0.1904  loss_ce_5: 0.09555  loss_mask_5: 0.1498  loss_dice_5: 0.1914  loss_ce_6: 0.08762  loss_mask_6: 0.1559  loss_dice_6: 0.1897  loss_ce_7: 0.1091  loss_mask_7: 0.1608  loss_dice_7: 0.1953  loss_ce_8: 0.09471  loss_mask_8: 0.1536  loss_dice_8: 0.192  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:07:01] d2.utils.events INFO:  eta: 0:36:38  iter: 32539  total_loss: 4.934  loss_ce: 0.01678  loss_mask: 0.1887  loss_dice: 0.1666  loss_ce_0: 0.05442  loss_mask_0: 0.185  loss_dice_0: 0.1651  loss_ce_1: 0.007306  loss_mask_1: 0.2006  loss_dice_1: 0.1686  loss_ce_2: 0.008873  loss_mask_2: 0.1908  loss_dice_2: 0.1709  loss_ce_3: 0.01907  loss_mask_3: 0.1946  loss_dice_3: 0.1669  loss_ce_4: 0.009665  loss_mask_4: 0.1902  loss_dice_4: 0.1631  loss_ce_5: 0.00719  loss_mask_5: 0.1894  loss_dice_5: 0.1674  loss_ce_6: 0.01262  loss_mask_6: 0.194  loss_dice_6: 0.1728  loss_ce_7: 0.00758  loss_mask_7: 0.1966  loss_dice_7: 0.1627  loss_ce_8: 0.009719  loss_mask_8: 0.1922  loss_dice_8: 0.1641  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:04] d2.utils.events INFO:  eta: 0:36:34  iter: 32559  total_loss: 3.979  loss_ce: 0.006268  loss_mask: 0.1892  loss_dice: 0.1334  loss_ce_0: 0.05371  loss_mask_0: 0.1796  loss_dice_0: 0.1314  loss_ce_1: 0.006216  loss_mask_1: 0.1896  loss_dice_1: 0.1322  loss_ce_2: 0.006572  loss_mask_2: 0.1862  loss_dice_2: 0.1304  loss_ce_3: 0.005908  loss_mask_3: 0.1905  loss_dice_3: 0.1338  loss_ce_4: 0.005482  loss_mask_4: 0.1935  loss_dice_4: 0.1258  loss_ce_5: 0.006379  loss_mask_5: 0.1836  loss_dice_5: 0.1299  loss_ce_6: 0.006414  loss_mask_6: 0.1892  loss_dice_6: 0.137  loss_ce_7: 0.005714  loss_mask_7: 0.1881  loss_dice_7: 0.1304  loss_ce_8: 0.00651  loss_mask_8: 0.1911  loss_dice_8: 0.1271  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:07:06] d2.utils.events INFO:  eta: 0:36:30  iter: 32579  total_loss: 4.557  loss_ce: 0.01687  loss_mask: 0.1408  loss_dice: 0.1674  loss_ce_0: 0.05266  loss_mask_0: 0.1503  loss_dice_0: 0.1743  loss_ce_1: 0.01584  loss_mask_1: 0.1446  loss_dice_1: 0.176  loss_ce_2: 0.01565  loss_mask_2: 0.1415  loss_dice_2: 0.1782  loss_ce_3: 0.02071  loss_mask_3: 0.1427  loss_dice_3: 0.1813  loss_ce_4: 0.0139  loss_mask_4: 0.1421  loss_dice_4: 0.1807  loss_ce_5: 0.01168  loss_mask_5: 0.1484  loss_dice_5: 0.1803  loss_ce_6: 0.03188  loss_mask_6: 0.1333  loss_dice_6: 0.1703  loss_ce_7: 0.01631  loss_mask_7: 0.145  loss_dice_7: 0.1749  loss_ce_8: 0.0125  loss_mask_8: 0.1419  loss_dice_8: 0.1689  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:07:08] d2.utils.events INFO:  eta: 0:36:27  iter: 32599  total_loss: 3.678  loss_ce: 0.01092  loss_mask: 0.1811  loss_dice: 0.1251  loss_ce_0: 0.06304  loss_mask_0: 0.1889  loss_dice_0: 0.1303  loss_ce_1: 0.008037  loss_mask_1: 0.1851  loss_dice_1: 0.1294  loss_ce_2: 0.01171  loss_mask_2: 0.1936  loss_dice_2: 0.1401  loss_ce_3: 0.01222  loss_mask_3: 0.196  loss_dice_3: 0.1318  loss_ce_4: 0.009674  loss_mask_4: 0.1877  loss_dice_4: 0.1285  loss_ce_5: 0.01139  loss_mask_5: 0.1944  loss_dice_5: 0.1314  loss_ce_6: 0.01167  loss_mask_6: 0.1906  loss_dice_6: 0.1331  loss_ce_7: 0.009799  loss_mask_7: 0.1919  loss_dice_7: 0.1254  loss_ce_8: 0.01113  loss_mask_8: 0.1898  loss_dice_8: 0.1344  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:11] d2.utils.events INFO:  eta: 0:36:24  iter: 32619  total_loss: 5.072  loss_ce: 0.007984  loss_mask: 0.1924  loss_dice: 0.2288  loss_ce_0: 0.05552  loss_mask_0: 0.178  loss_dice_0: 0.2302  loss_ce_1: 0.004369  loss_mask_1: 0.1864  loss_dice_1: 0.2355  loss_ce_2: 0.01025  loss_mask_2: 0.1896  loss_dice_2: 0.2329  loss_ce_3: 0.007628  loss_mask_3: 0.1841  loss_dice_3: 0.2351  loss_ce_4: 0.006811  loss_mask_4: 0.1946  loss_dice_4: 0.2393  loss_ce_5: 0.00984  loss_mask_5: 0.1864  loss_dice_5: 0.2321  loss_ce_6: 0.0066  loss_mask_6: 0.1975  loss_dice_6: 0.2305  loss_ce_7: 0.006659  loss_mask_7: 0.1785  loss_dice_7: 0.2406  loss_ce_8: 0.01047  loss_mask_8: 0.1856  loss_dice_8: 0.2361  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:13] d2.utils.events INFO:  eta: 0:36:22  iter: 32639  total_loss: 4.756  loss_ce: 0.02849  loss_mask: 0.1771  loss_dice: 0.1482  loss_ce_0: 0.05346  loss_mask_0: 0.1687  loss_dice_0: 0.153  loss_ce_1: 0.01882  loss_mask_1: 0.1666  loss_dice_1: 0.154  loss_ce_2: 0.02228  loss_mask_2: 0.1697  loss_dice_2: 0.158  loss_ce_3: 0.02643  loss_mask_3: 0.1565  loss_dice_3: 0.1558  loss_ce_4: 0.02176  loss_mask_4: 0.1691  loss_dice_4: 0.1644  loss_ce_5: 0.01968  loss_mask_5: 0.1668  loss_dice_5: 0.1571  loss_ce_6: 0.02707  loss_mask_6: 0.1685  loss_dice_6: 0.1568  loss_ce_7: 0.02458  loss_mask_7: 0.1675  loss_dice_7: 0.1537  loss_ce_8: 0.0249  loss_mask_8: 0.1746  loss_dice_8: 0.16  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:16] d2.utils.events INFO:  eta: 0:36:20  iter: 32659  total_loss: 4.84  loss_ce: 0.01314  loss_mask: 0.2132  loss_dice: 0.2046  loss_ce_0: 0.05025  loss_mask_0: 0.2141  loss_dice_0: 0.1943  loss_ce_1: 0.02082  loss_mask_1: 0.2135  loss_dice_1: 0.1966  loss_ce_2: 0.01399  loss_mask_2: 0.2066  loss_dice_2: 0.2083  loss_ce_3: 0.01371  loss_mask_3: 0.2128  loss_dice_3: 0.1975  loss_ce_4: 0.02229  loss_mask_4: 0.2137  loss_dice_4: 0.2055  loss_ce_5: 0.01435  loss_mask_5: 0.2115  loss_dice_5: 0.2046  loss_ce_6: 0.01333  loss_mask_6: 0.2096  loss_dice_6: 0.1947  loss_ce_7: 0.02408  loss_mask_7: 0.2077  loss_dice_7: 0.1901  loss_ce_8: 0.01108  loss_mask_8: 0.2116  loss_dice_8: 0.2133  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:07:19] d2.utils.events INFO:  eta: 0:36:19  iter: 32679  total_loss: 4.033  loss_ce: 0.008948  loss_mask: 0.2066  loss_dice: 0.1576  loss_ce_0: 0.04993  loss_mask_0: 0.203  loss_dice_0: 0.1598  loss_ce_1: 0.011  loss_mask_1: 0.198  loss_dice_1: 0.1605  loss_ce_2: 0.008787  loss_mask_2: 0.2106  loss_dice_2: 0.1592  loss_ce_3: 0.009104  loss_mask_3: 0.2109  loss_dice_3: 0.1638  loss_ce_4: 0.007855  loss_mask_4: 0.2065  loss_dice_4: 0.1604  loss_ce_5: 0.009313  loss_mask_5: 0.2042  loss_dice_5: 0.1629  loss_ce_6: 0.007974  loss_mask_6: 0.2126  loss_dice_6: 0.1639  loss_ce_7: 0.007517  loss_mask_7: 0.1979  loss_dice_7: 0.1574  loss_ce_8: 0.009977  loss_mask_8: 0.2233  loss_dice_8: 0.1663  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:21] d2.utils.events INFO:  eta: 0:36:18  iter: 32699  total_loss: 4.055  loss_ce: 0.007  loss_mask: 0.1601  loss_dice: 0.1732  loss_ce_0: 0.05084  loss_mask_0: 0.1665  loss_dice_0: 0.1951  loss_ce_1: 0.009119  loss_mask_1: 0.1579  loss_dice_1: 0.1813  loss_ce_2: 0.006817  loss_mask_2: 0.1676  loss_dice_2: 0.1774  loss_ce_3: 0.006245  loss_mask_3: 0.1644  loss_dice_3: 0.1742  loss_ce_4: 0.006018  loss_mask_4: 0.1582  loss_dice_4: 0.1678  loss_ce_5: 0.007352  loss_mask_5: 0.1699  loss_dice_5: 0.1872  loss_ce_6: 0.006529  loss_mask_6: 0.1721  loss_dice_6: 0.1759  loss_ce_7: 0.006048  loss_mask_7: 0.1619  loss_dice_7: 0.1758  loss_ce_8: 0.006953  loss_mask_8: 0.1618  loss_dice_8: 0.1731  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:24] d2.utils.events INFO:  eta: 0:36:18  iter: 32719  total_loss: 3.859  loss_ce: 0.005844  loss_mask: 0.2277  loss_dice: 0.1335  loss_ce_0: 0.0623  loss_mask_0: 0.2277  loss_dice_0: 0.1342  loss_ce_1: 0.007244  loss_mask_1: 0.2323  loss_dice_1: 0.134  loss_ce_2: 0.005134  loss_mask_2: 0.2255  loss_dice_2: 0.1345  loss_ce_3: 0.004986  loss_mask_3: 0.2226  loss_dice_3: 0.1322  loss_ce_4: 0.005351  loss_mask_4: 0.2173  loss_dice_4: 0.1268  loss_ce_5: 0.006337  loss_mask_5: 0.2313  loss_dice_5: 0.1381  loss_ce_6: 0.005042  loss_mask_6: 0.2297  loss_dice_6: 0.1322  loss_ce_7: 0.004914  loss_mask_7: 0.23  loss_dice_7: 0.1354  loss_ce_8: 0.005788  loss_mask_8: 0.2273  loss_dice_8: 0.1357  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:07:26] d2.utils.events INFO:  eta: 0:36:19  iter: 32739  total_loss: 3.718  loss_ce: 0.006665  loss_mask: 0.1671  loss_dice: 0.1362  loss_ce_0: 0.06396  loss_mask_0: 0.1758  loss_dice_0: 0.1365  loss_ce_1: 0.00987  loss_mask_1: 0.1873  loss_dice_1: 0.1401  loss_ce_2: 0.006951  loss_mask_2: 0.1718  loss_dice_2: 0.1412  loss_ce_3: 0.00551  loss_mask_3: 0.1908  loss_dice_3: 0.1401  loss_ce_4: 0.00613  loss_mask_4: 0.1764  loss_dice_4: 0.1398  loss_ce_5: 0.006427  loss_mask_5: 0.1702  loss_dice_5: 0.1368  loss_ce_6: 0.005444  loss_mask_6: 0.183  loss_dice_6: 0.1367  loss_ce_7: 0.006464  loss_mask_7: 0.1744  loss_dice_7: 0.1399  loss_ce_8: 0.006877  loss_mask_8: 0.1715  loss_dice_8: 0.143  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:29] d2.utils.events INFO:  eta: 0:36:19  iter: 32759  total_loss: 3.901  loss_ce: 0.004017  loss_mask: 0.167  loss_dice: 0.1514  loss_ce_0: 0.06152  loss_mask_0: 0.1566  loss_dice_0: 0.1457  loss_ce_1: 0.005204  loss_mask_1: 0.1601  loss_dice_1: 0.1613  loss_ce_2: 0.003028  loss_mask_2: 0.1573  loss_dice_2: 0.1509  loss_ce_3: 0.002936  loss_mask_3: 0.1609  loss_dice_3: 0.1523  loss_ce_4: 0.003243  loss_mask_4: 0.1681  loss_dice_4: 0.1633  loss_ce_5: 0.00342  loss_mask_5: 0.1617  loss_dice_5: 0.1536  loss_ce_6: 0.002891  loss_mask_6: 0.1647  loss_dice_6: 0.1582  loss_ce_7: 0.003264  loss_mask_7: 0.1604  loss_dice_7: 0.154  loss_ce_8: 0.004194  loss_mask_8: 0.1627  loss_dice_8: 0.1432  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:07:31] d2.utils.events INFO:  eta: 0:36:18  iter: 32779  total_loss: 4.301  loss_ce: 0.003597  loss_mask: 0.2064  loss_dice: 0.1424  loss_ce_0: 0.05729  loss_mask_0: 0.2038  loss_dice_0: 0.1403  loss_ce_1: 0.0043  loss_mask_1: 0.2  loss_dice_1: 0.1423  loss_ce_2: 0.004339  loss_mask_2: 0.1955  loss_dice_2: 0.1409  loss_ce_3: 0.003372  loss_mask_3: 0.1987  loss_dice_3: 0.1403  loss_ce_4: 0.003606  loss_mask_4: 0.1999  loss_dice_4: 0.1408  loss_ce_5: 0.003683  loss_mask_5: 0.201  loss_dice_5: 0.1366  loss_ce_6: 0.0033  loss_mask_6: 0.1973  loss_dice_6: 0.1387  loss_ce_7: 0.003086  loss_mask_7: 0.1947  loss_dice_7: 0.1418  loss_ce_8: 0.003453  loss_mask_8: 0.1935  loss_dice_8: 0.1391  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:34] d2.utils.events INFO:  eta: 0:36:17  iter: 32799  total_loss: 5.146  loss_ce: 0.003367  loss_mask: 0.2394  loss_dice: 0.1892  loss_ce_0: 0.07335  loss_mask_0: 0.2405  loss_dice_0: 0.1901  loss_ce_1: 0.00474  loss_mask_1: 0.2508  loss_dice_1: 0.1857  loss_ce_2: 0.003253  loss_mask_2: 0.242  loss_dice_2: 0.188  loss_ce_3: 0.002594  loss_mask_3: 0.2545  loss_dice_3: 0.1884  loss_ce_4: 0.003183  loss_mask_4: 0.2615  loss_dice_4: 0.1874  loss_ce_5: 0.003365  loss_mask_5: 0.2536  loss_dice_5: 0.1958  loss_ce_6: 0.00289  loss_mask_6: 0.2608  loss_dice_6: 0.189  loss_ce_7: 0.003905  loss_mask_7: 0.25  loss_dice_7: 0.1897  loss_ce_8: 0.003178  loss_mask_8: 0.255  loss_dice_8: 0.197  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:37] d2.utils.events INFO:  eta: 0:36:14  iter: 32819  total_loss: 5.505  loss_ce: 0.08071  loss_mask: 0.1993  loss_dice: 0.1832  loss_ce_0: 0.06234  loss_mask_0: 0.189  loss_dice_0: 0.1855  loss_ce_1: 0.04836  loss_mask_1: 0.1906  loss_dice_1: 0.1829  loss_ce_2: 0.07672  loss_mask_2: 0.1983  loss_dice_2: 0.1751  loss_ce_3: 0.08065  loss_mask_3: 0.1842  loss_dice_3: 0.1829  loss_ce_4: 0.06139  loss_mask_4: 0.1917  loss_dice_4: 0.179  loss_ce_5: 0.0711  loss_mask_5: 0.2008  loss_dice_5: 0.1805  loss_ce_6: 0.08033  loss_mask_6: 0.1941  loss_dice_6: 0.1845  loss_ce_7: 0.06762  loss_mask_7: 0.2011  loss_dice_7: 0.1831  loss_ce_8: 0.08122  loss_mask_8: 0.1935  loss_dice_8: 0.1796  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:07:39] d2.utils.events INFO:  eta: 0:36:12  iter: 32839  total_loss: 4.119  loss_ce: 0.01702  loss_mask: 0.2222  loss_dice: 0.1564  loss_ce_0: 0.06217  loss_mask_0: 0.2243  loss_dice_0: 0.1628  loss_ce_1: 0.01258  loss_mask_1: 0.2223  loss_dice_1: 0.1581  loss_ce_2: 0.01727  loss_mask_2: 0.216  loss_dice_2: 0.1553  loss_ce_3: 0.02167  loss_mask_3: 0.2253  loss_dice_3: 0.165  loss_ce_4: 0.01737  loss_mask_4: 0.2199  loss_dice_4: 0.1602  loss_ce_5: 0.01924  loss_mask_5: 0.2296  loss_dice_5: 0.158  loss_ce_6: 0.02147  loss_mask_6: 0.2179  loss_dice_6: 0.1528  loss_ce_7: 0.01836  loss_mask_7: 0.2164  loss_dice_7: 0.1571  loss_ce_8: 0.01581  loss_mask_8: 0.2206  loss_dice_8: 0.1539  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:07:42] d2.utils.events INFO:  eta: 0:36:09  iter: 32859  total_loss: 4.19  loss_ce: 0.01349  loss_mask: 0.2058  loss_dice: 0.1418  loss_ce_0: 0.06219  loss_mask_0: 0.1945  loss_dice_0: 0.1388  loss_ce_1: 0.01187  loss_mask_1: 0.1937  loss_dice_1: 0.1419  loss_ce_2: 0.01285  loss_mask_2: 0.2019  loss_dice_2: 0.1358  loss_ce_3: 0.01552  loss_mask_3: 0.1839  loss_dice_3: 0.1452  loss_ce_4: 0.01266  loss_mask_4: 0.2034  loss_dice_4: 0.1315  loss_ce_5: 0.01169  loss_mask_5: 0.1926  loss_dice_5: 0.1389  loss_ce_6: 0.0141  loss_mask_6: 0.1937  loss_dice_6: 0.136  loss_ce_7: 0.01327  loss_mask_7: 0.1996  loss_dice_7: 0.136  loss_ce_8: 0.01229  loss_mask_8: 0.1988  loss_dice_8: 0.1383  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:44] d2.utils.events INFO:  eta: 0:36:05  iter: 32879  total_loss: 4.988  loss_ce: 0.007054  loss_mask: 0.2462  loss_dice: 0.1632  loss_ce_0: 0.06225  loss_mask_0: 0.2331  loss_dice_0: 0.1733  loss_ce_1: 0.007807  loss_mask_1: 0.2478  loss_dice_1: 0.1727  loss_ce_2: 0.008947  loss_mask_2: 0.2517  loss_dice_2: 0.1685  loss_ce_3: 0.008903  loss_mask_3: 0.2442  loss_dice_3: 0.1631  loss_ce_4: 0.00791  loss_mask_4: 0.2504  loss_dice_4: 0.1746  loss_ce_5: 0.007671  loss_mask_5: 0.255  loss_dice_5: 0.1667  loss_ce_6: 0.009572  loss_mask_6: 0.2524  loss_dice_6: 0.1654  loss_ce_7: 0.006819  loss_mask_7: 0.2425  loss_dice_7: 0.1675  loss_ce_8: 0.007744  loss_mask_8: 0.2451  loss_dice_8: 0.1599  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:47] d2.utils.events INFO:  eta: 0:36:00  iter: 32899  total_loss: 4.262  loss_ce: 0.01679  loss_mask: 0.1985  loss_dice: 0.1905  loss_ce_0: 0.06154  loss_mask_0: 0.1952  loss_dice_0: 0.1892  loss_ce_1: 0.01828  loss_mask_1: 0.2011  loss_dice_1: 0.1814  loss_ce_2: 0.02287  loss_mask_2: 0.2027  loss_dice_2: 0.1867  loss_ce_3: 0.01783  loss_mask_3: 0.1926  loss_dice_3: 0.1888  loss_ce_4: 0.01758  loss_mask_4: 0.2023  loss_dice_4: 0.1903  loss_ce_5: 0.02068  loss_mask_5: 0.1994  loss_dice_5: 0.188  loss_ce_6: 0.0149  loss_mask_6: 0.2009  loss_dice_6: 0.1868  loss_ce_7: 0.01742  loss_mask_7: 0.1973  loss_dice_7: 0.1892  loss_ce_8: 0.02756  loss_mask_8: 0.1856  loss_dice_8: 0.1809  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:49] d2.utils.events INFO:  eta: 0:35:57  iter: 32919  total_loss: 4.335  loss_ce: 0.01235  loss_mask: 0.1279  loss_dice: 0.1996  loss_ce_0: 0.06221  loss_mask_0: 0.1431  loss_dice_0: 0.2072  loss_ce_1: 0.01007  loss_mask_1: 0.1364  loss_dice_1: 0.2028  loss_ce_2: 0.01003  loss_mask_2: 0.1353  loss_dice_2: 0.2079  loss_ce_3: 0.01063  loss_mask_3: 0.1391  loss_dice_3: 0.2037  loss_ce_4: 0.008955  loss_mask_4: 0.1292  loss_dice_4: 0.2056  loss_ce_5: 0.008612  loss_mask_5: 0.1427  loss_dice_5: 0.2129  loss_ce_6: 0.00911  loss_mask_6: 0.1403  loss_dice_6: 0.1992  loss_ce_7: 0.008661  loss_mask_7: 0.1292  loss_dice_7: 0.2099  loss_ce_8: 0.01017  loss_mask_8: 0.1283  loss_dice_8: 0.2049  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:52] d2.utils.events INFO:  eta: 0:35:58  iter: 32939  total_loss: 5.135  loss_ce: 0.01335  loss_mask: 0.2001  loss_dice: 0.1637  loss_ce_0: 0.06259  loss_mask_0: 0.196  loss_dice_0: 0.1609  loss_ce_1: 0.1187  loss_mask_1: 0.2029  loss_dice_1: 0.1602  loss_ce_2: 0.0884  loss_mask_2: 0.206  loss_dice_2: 0.166  loss_ce_3: 0.01658  loss_mask_3: 0.1943  loss_dice_3: 0.1549  loss_ce_4: 0.05935  loss_mask_4: 0.1978  loss_dice_4: 0.1631  loss_ce_5: 0.08242  loss_mask_5: 0.2012  loss_dice_5: 0.1637  loss_ce_6: 0.01588  loss_mask_6: 0.1968  loss_dice_6: 0.1627  loss_ce_7: 0.05945  loss_mask_7: 0.1967  loss_dice_7: 0.1605  loss_ce_8: 0.08452  loss_mask_8: 0.2072  loss_dice_8: 0.1586  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 16:07:54] d2.utils.events INFO:  eta: 0:35:58  iter: 32959  total_loss: 4.587  loss_ce: 0.009868  loss_mask: 0.2258  loss_dice: 0.1451  loss_ce_0: 0.06091  loss_mask_0: 0.2331  loss_dice_0: 0.1386  loss_ce_1: 0.00907  loss_mask_1: 0.2298  loss_dice_1: 0.1398  loss_ce_2: 0.01217  loss_mask_2: 0.2269  loss_dice_2: 0.1413  loss_ce_3: 0.009667  loss_mask_3: 0.2254  loss_dice_3: 0.1412  loss_ce_4: 0.009033  loss_mask_4: 0.2319  loss_dice_4: 0.1445  loss_ce_5: 0.009509  loss_mask_5: 0.2222  loss_dice_5: 0.1442  loss_ce_6: 0.008923  loss_mask_6: 0.2272  loss_dice_6: 0.1405  loss_ce_7: 0.009361  loss_mask_7: 0.2331  loss_dice_7: 0.1418  loss_ce_8: 0.009593  loss_mask_8: 0.2285  loss_dice_8: 0.144  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:57] d2.utils.events INFO:  eta: 0:35:56  iter: 32979  total_loss: 4.232  loss_ce: 0.01378  loss_mask: 0.1921  loss_dice: 0.1523  loss_ce_0: 0.06495  loss_mask_0: 0.1982  loss_dice_0: 0.1583  loss_ce_1: 0.01842  loss_mask_1: 0.2085  loss_dice_1: 0.1539  loss_ce_2: 0.01941  loss_mask_2: 0.2081  loss_dice_2: 0.1582  loss_ce_3: 0.01519  loss_mask_3: 0.1989  loss_dice_3: 0.1581  loss_ce_4: 0.01325  loss_mask_4: 0.2025  loss_dice_4: 0.1492  loss_ce_5: 0.01267  loss_mask_5: 0.1988  loss_dice_5: 0.1591  loss_ce_6: 0.01183  loss_mask_6: 0.1999  loss_dice_6: 0.1535  loss_ce_7: 0.01355  loss_mask_7: 0.1908  loss_dice_7: 0.1553  loss_ce_8: 0.01402  loss_mask_8: 0.2009  loss_dice_8: 0.1536  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:07:59] d2.utils.events INFO:  eta: 0:35:54  iter: 32999  total_loss: 3.79  loss_ce: 0.01068  loss_mask: 0.1756  loss_dice: 0.149  loss_ce_0: 0.06241  loss_mask_0: 0.1728  loss_dice_0: 0.1499  loss_ce_1: 0.0106  loss_mask_1: 0.1774  loss_dice_1: 0.1499  loss_ce_2: 0.01221  loss_mask_2: 0.172  loss_dice_2: 0.1471  loss_ce_3: 0.01023  loss_mask_3: 0.1849  loss_dice_3: 0.145  loss_ce_4: 0.009217  loss_mask_4: 0.1681  loss_dice_4: 0.141  loss_ce_5: 0.01022  loss_mask_5: 0.1694  loss_dice_5: 0.1495  loss_ce_6: 0.01003  loss_mask_6: 0.1814  loss_dice_6: 0.1492  loss_ce_7: 0.009827  loss_mask_7: 0.176  loss_dice_7: 0.1509  loss_ce_8: 0.01194  loss_mask_8: 0.1766  loss_dice_8: 0.1478  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:02] d2.utils.events INFO:  eta: 0:35:52  iter: 33019  total_loss: 3.876  loss_ce: 0.006973  loss_mask: 0.2045  loss_dice: 0.1359  loss_ce_0: 0.06239  loss_mask_0: 0.2084  loss_dice_0: 0.1373  loss_ce_1: 0.007322  loss_mask_1: 0.2087  loss_dice_1: 0.1389  loss_ce_2: 0.008006  loss_mask_2: 0.2125  loss_dice_2: 0.1407  loss_ce_3: 0.006796  loss_mask_3: 0.2131  loss_dice_3: 0.1429  loss_ce_4: 0.007122  loss_mask_4: 0.2125  loss_dice_4: 0.1429  loss_ce_5: 0.007348  loss_mask_5: 0.202  loss_dice_5: 0.1389  loss_ce_6: 0.007057  loss_mask_6: 0.2086  loss_dice_6: 0.1413  loss_ce_7: 0.00704  loss_mask_7: 0.2102  loss_dice_7: 0.1405  loss_ce_8: 0.006664  loss_mask_8: 0.2032  loss_dice_8: 0.1372  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:05] d2.utils.events INFO:  eta: 0:35:50  iter: 33039  total_loss: 4.247  loss_ce: 0.007802  loss_mask: 0.178  loss_dice: 0.1791  loss_ce_0: 0.06322  loss_mask_0: 0.1778  loss_dice_0: 0.185  loss_ce_1: 0.006013  loss_mask_1: 0.1673  loss_dice_1: 0.1823  loss_ce_2: 0.008505  loss_mask_2: 0.1697  loss_dice_2: 0.1807  loss_ce_3: 0.008641  loss_mask_3: 0.1701  loss_dice_3: 0.1805  loss_ce_4: 0.007327  loss_mask_4: 0.1709  loss_dice_4: 0.1857  loss_ce_5: 0.0083  loss_mask_5: 0.1797  loss_dice_5: 0.176  loss_ce_6: 0.008679  loss_mask_6: 0.1639  loss_dice_6: 0.1774  loss_ce_7: 0.006573  loss_mask_7: 0.1724  loss_dice_7: 0.1819  loss_ce_8: 0.008751  loss_mask_8: 0.169  loss_dice_8: 0.1849  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:07] d2.utils.events INFO:  eta: 0:35:47  iter: 33059  total_loss: 3.795  loss_ce: 0.005051  loss_mask: 0.1656  loss_dice: 0.1677  loss_ce_0: 0.06224  loss_mask_0: 0.1726  loss_dice_0: 0.1806  loss_ce_1: 0.005152  loss_mask_1: 0.172  loss_dice_1: 0.178  loss_ce_2: 0.005944  loss_mask_2: 0.1765  loss_dice_2: 0.1713  loss_ce_3: 0.005273  loss_mask_3: 0.1705  loss_dice_3: 0.1808  loss_ce_4: 0.005544  loss_mask_4: 0.175  loss_dice_4: 0.1748  loss_ce_5: 0.005511  loss_mask_5: 0.1745  loss_dice_5: 0.1793  loss_ce_6: 0.005583  loss_mask_6: 0.1694  loss_dice_6: 0.1714  loss_ce_7: 0.005061  loss_mask_7: 0.1705  loss_dice_7: 0.1832  loss_ce_8: 0.004689  loss_mask_8: 0.1679  loss_dice_8: 0.1837  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:08:10] d2.utils.events INFO:  eta: 0:35:44  iter: 33079  total_loss: 3.865  loss_ce: 0.003231  loss_mask: 0.2247  loss_dice: 0.1187  loss_ce_0: 0.06193  loss_mask_0: 0.2258  loss_dice_0: 0.1245  loss_ce_1: 0.004006  loss_mask_1: 0.2239  loss_dice_1: 0.1282  loss_ce_2: 0.003451  loss_mask_2: 0.2103  loss_dice_2: 0.1251  loss_ce_3: 0.003108  loss_mask_3: 0.2253  loss_dice_3: 0.1298  loss_ce_4: 0.003712  loss_mask_4: 0.2232  loss_dice_4: 0.1246  loss_ce_5: 0.003034  loss_mask_5: 0.2142  loss_dice_5: 0.1305  loss_ce_6: 0.002931  loss_mask_6: 0.219  loss_dice_6: 0.131  loss_ce_7: 0.003045  loss_mask_7: 0.2137  loss_dice_7: 0.1272  loss_ce_8: 0.003267  loss_mask_8: 0.2245  loss_dice_8: 0.1286  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:12] d2.utils.events INFO:  eta: 0:35:42  iter: 33099  total_loss: 4.277  loss_ce: 0.004422  loss_mask: 0.1535  loss_dice: 0.1592  loss_ce_0: 0.05937  loss_mask_0: 0.1532  loss_dice_0: 0.1524  loss_ce_1: 0.007136  loss_mask_1: 0.1675  loss_dice_1: 0.1577  loss_ce_2: 0.004603  loss_mask_2: 0.1505  loss_dice_2: 0.1586  loss_ce_3: 0.003945  loss_mask_3: 0.1477  loss_dice_3: 0.1601  loss_ce_4: 0.005538  loss_mask_4: 0.15  loss_dice_4: 0.1541  loss_ce_5: 0.004145  loss_mask_5: 0.1463  loss_dice_5: 0.1675  loss_ce_6: 0.003853  loss_mask_6: 0.1563  loss_dice_6: 0.1499  loss_ce_7: 0.005681  loss_mask_7: 0.1504  loss_dice_7: 0.1642  loss_ce_8: 0.004484  loss_mask_8: 0.1543  loss_dice_8: 0.1636  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:15] d2.utils.events INFO:  eta: 0:35:39  iter: 33119  total_loss: 3.937  loss_ce: 0.007818  loss_mask: 0.2119  loss_dice: 0.1165  loss_ce_0: 0.06103  loss_mask_0: 0.2054  loss_dice_0: 0.116  loss_ce_1: 0.01405  loss_mask_1: 0.2161  loss_dice_1: 0.1237  loss_ce_2: 0.007687  loss_mask_2: 0.2112  loss_dice_2: 0.1178  loss_ce_3: 0.00611  loss_mask_3: 0.2038  loss_dice_3: 0.1202  loss_ce_4: 0.006989  loss_mask_4: 0.2164  loss_dice_4: 0.119  loss_ce_5: 0.006366  loss_mask_5: 0.2101  loss_dice_5: 0.1137  loss_ce_6: 0.005503  loss_mask_6: 0.2067  loss_dice_6: 0.115  loss_ce_7: 0.007735  loss_mask_7: 0.2163  loss_dice_7: 0.1176  loss_ce_8: 0.008598  loss_mask_8: 0.2148  loss_dice_8: 0.1158  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:17] d2.utils.events INFO:  eta: 0:35:36  iter: 33139  total_loss: 5.555  loss_ce: 0.008928  loss_mask: 0.1994  loss_dice: 0.1427  loss_ce_0: 0.0606  loss_mask_0: 0.2041  loss_dice_0: 0.1387  loss_ce_1: 0.009408  loss_mask_1: 0.2002  loss_dice_1: 0.1434  loss_ce_2: 0.008179  loss_mask_2: 0.2  loss_dice_2: 0.1382  loss_ce_3: 0.006676  loss_mask_3: 0.1951  loss_dice_3: 0.1365  loss_ce_4: 0.006498  loss_mask_4: 0.1996  loss_dice_4: 0.1429  loss_ce_5: 0.007396  loss_mask_5: 0.2095  loss_dice_5: 0.1484  loss_ce_6: 0.006331  loss_mask_6: 0.2084  loss_dice_6: 0.1403  loss_ce_7: 0.006064  loss_mask_7: 0.1956  loss_dice_7: 0.1403  loss_ce_8: 0.009453  loss_mask_8: 0.1977  loss_dice_8: 0.1384  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:20] d2.utils.events INFO:  eta: 0:35:33  iter: 33159  total_loss: 4.317  loss_ce: 0.01227  loss_mask: 0.1973  loss_dice: 0.1805  loss_ce_0: 0.06754  loss_mask_0: 0.1995  loss_dice_0: 0.1719  loss_ce_1: 0.01033  loss_mask_1: 0.1912  loss_dice_1: 0.1915  loss_ce_2: 0.01078  loss_mask_2: 0.1972  loss_dice_2: 0.1767  loss_ce_3: 0.008218  loss_mask_3: 0.2023  loss_dice_3: 0.1803  loss_ce_4: 0.0106  loss_mask_4: 0.1948  loss_dice_4: 0.1755  loss_ce_5: 0.01029  loss_mask_5: 0.1962  loss_dice_5: 0.1846  loss_ce_6: 0.008575  loss_mask_6: 0.2028  loss_dice_6: 0.1871  loss_ce_7: 0.01047  loss_mask_7: 0.2035  loss_dice_7: 0.1804  loss_ce_8: 0.01309  loss_mask_8: 0.195  loss_dice_8: 0.1764  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:22] d2.utils.events INFO:  eta: 0:35:29  iter: 33179  total_loss: 4.305  loss_ce: 0.01591  loss_mask: 0.1946  loss_dice: 0.1493  loss_ce_0: 0.05666  loss_mask_0: 0.2058  loss_dice_0: 0.1492  loss_ce_1: 0.009715  loss_mask_1: 0.1983  loss_dice_1: 0.1497  loss_ce_2: 0.01662  loss_mask_2: 0.205  loss_dice_2: 0.1509  loss_ce_3: 0.01298  loss_mask_3: 0.1965  loss_dice_3: 0.149  loss_ce_4: 0.01183  loss_mask_4: 0.2059  loss_dice_4: 0.1532  loss_ce_5: 0.01617  loss_mask_5: 0.1947  loss_dice_5: 0.1485  loss_ce_6: 0.01269  loss_mask_6: 0.2001  loss_dice_6: 0.1448  loss_ce_7: 0.01316  loss_mask_7: 0.2017  loss_dice_7: 0.1497  loss_ce_8: 0.01945  loss_mask_8: 0.2101  loss_dice_8: 0.1486  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:25] d2.utils.events INFO:  eta: 0:35:27  iter: 33199  total_loss: 4.959  loss_ce: 0.003242  loss_mask: 0.2088  loss_dice: 0.2287  loss_ce_0: 0.0526  loss_mask_0: 0.1977  loss_dice_0: 0.2376  loss_ce_1: 0.005734  loss_mask_1: 0.2095  loss_dice_1: 0.2371  loss_ce_2: 0.004281  loss_mask_2: 0.2011  loss_dice_2: 0.2257  loss_ce_3: 0.004841  loss_mask_3: 0.2038  loss_dice_3: 0.236  loss_ce_4: 0.005376  loss_mask_4: 0.201  loss_dice_4: 0.2378  loss_ce_5: 0.004739  loss_mask_5: 0.2121  loss_dice_5: 0.2305  loss_ce_6: 0.005332  loss_mask_6: 0.2071  loss_dice_6: 0.2269  loss_ce_7: 0.004485  loss_mask_7: 0.2091  loss_dice_7: 0.2242  loss_ce_8: 0.003204  loss_mask_8: 0.2021  loss_dice_8: 0.2301  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:27] d2.utils.events INFO:  eta: 0:35:26  iter: 33219  total_loss: 4.781  loss_ce: 0.01184  loss_mask: 0.1752  loss_dice: 0.223  loss_ce_0: 0.07063  loss_mask_0: 0.1856  loss_dice_0: 0.2166  loss_ce_1: 0.07323  loss_mask_1: 0.1827  loss_dice_1: 0.2256  loss_ce_2: 0.0111  loss_mask_2: 0.1764  loss_dice_2: 0.2199  loss_ce_3: 0.008724  loss_mask_3: 0.1818  loss_dice_3: 0.2151  loss_ce_4: 0.01573  loss_mask_4: 0.1807  loss_dice_4: 0.2154  loss_ce_5: 0.008962  loss_mask_5: 0.1789  loss_dice_5: 0.2224  loss_ce_6: 0.00503  loss_mask_6: 0.1759  loss_dice_6: 0.2281  loss_ce_7: 0.01163  loss_mask_7: 0.1803  loss_dice_7: 0.2206  loss_ce_8: 0.01107  loss_mask_8: 0.1739  loss_dice_8: 0.2259  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:30] d2.utils.events INFO:  eta: 0:35:23  iter: 33239  total_loss: 4.401  loss_ce: 0.008126  loss_mask: 0.1982  loss_dice: 0.1677  loss_ce_0: 0.0641  loss_mask_0: 0.1909  loss_dice_0: 0.1589  loss_ce_1: 0.01158  loss_mask_1: 0.1906  loss_dice_1: 0.168  loss_ce_2: 0.007767  loss_mask_2: 0.1821  loss_dice_2: 0.165  loss_ce_3: 0.006638  loss_mask_3: 0.1932  loss_dice_3: 0.1724  loss_ce_4: 0.007176  loss_mask_4: 0.1931  loss_dice_4: 0.1636  loss_ce_5: 0.008051  loss_mask_5: 0.1837  loss_dice_5: 0.1735  loss_ce_6: 0.007451  loss_mask_6: 0.1958  loss_dice_6: 0.1663  loss_ce_7: 0.007668  loss_mask_7: 0.1974  loss_dice_7: 0.1657  loss_ce_8: 0.00879  loss_mask_8: 0.194  loss_dice_8: 0.1656  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:32] d2.utils.events INFO:  eta: 0:35:19  iter: 33259  total_loss: 4.39  loss_ce: 0.01491  loss_mask: 0.1658  loss_dice: 0.177  loss_ce_0: 0.0628  loss_mask_0: 0.1629  loss_dice_0: 0.1721  loss_ce_1: 0.06847  loss_mask_1: 0.1684  loss_dice_1: 0.1803  loss_ce_2: 0.02065  loss_mask_2: 0.1646  loss_dice_2: 0.174  loss_ce_3: 0.01344  loss_mask_3: 0.1652  loss_dice_3: 0.1788  loss_ce_4: 0.01768  loss_mask_4: 0.1633  loss_dice_4: 0.1796  loss_ce_5: 0.01546  loss_mask_5: 0.1579  loss_dice_5: 0.1801  loss_ce_6: 0.01351  loss_mask_6: 0.1658  loss_dice_6: 0.1701  loss_ce_7: 0.01954  loss_mask_7: 0.1694  loss_dice_7: 0.179  loss_ce_8: 0.0162  loss_mask_8: 0.1598  loss_dice_8: 0.1815  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:35] d2.utils.events INFO:  eta: 0:35:16  iter: 33279  total_loss: 4.185  loss_ce: 0.01061  loss_mask: 0.1946  loss_dice: 0.1515  loss_ce_0: 0.05588  loss_mask_0: 0.1905  loss_dice_0: 0.1528  loss_ce_1: 0.01406  loss_mask_1: 0.1972  loss_dice_1: 0.1562  loss_ce_2: 0.01181  loss_mask_2: 0.1948  loss_dice_2: 0.1594  loss_ce_3: 0.0113  loss_mask_3: 0.1871  loss_dice_3: 0.1593  loss_ce_4: 0.01188  loss_mask_4: 0.1926  loss_dice_4: 0.1517  loss_ce_5: 0.01184  loss_mask_5: 0.1962  loss_dice_5: 0.1493  loss_ce_6: 0.01081  loss_mask_6: 0.1956  loss_dice_6: 0.1525  loss_ce_7: 0.01117  loss_mask_7: 0.1835  loss_dice_7: 0.1568  loss_ce_8: 0.01017  loss_mask_8: 0.2052  loss_dice_8: 0.1596  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:37] d2.utils.events INFO:  eta: 0:35:14  iter: 33299  total_loss: 5.878  loss_ce: 0.03357  loss_mask: 0.1544  loss_dice: 0.2654  loss_ce_0: 0.0546  loss_mask_0: 0.1545  loss_dice_0: 0.2692  loss_ce_1: 0.02827  loss_mask_1: 0.1625  loss_dice_1: 0.2792  loss_ce_2: 0.04205  loss_mask_2: 0.1706  loss_dice_2: 0.2839  loss_ce_3: 0.03054  loss_mask_3: 0.1549  loss_dice_3: 0.2631  loss_ce_4: 0.02361  loss_mask_4: 0.1545  loss_dice_4: 0.2825  loss_ce_5: 0.03269  loss_mask_5: 0.1603  loss_dice_5: 0.2853  loss_ce_6: 0.02868  loss_mask_6: 0.1582  loss_dice_6: 0.2688  loss_ce_7: 0.02461  loss_mask_7: 0.1526  loss_dice_7: 0.2753  loss_ce_8: 0.02949  loss_mask_8: 0.1606  loss_dice_8: 0.2849  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:40] d2.utils.events INFO:  eta: 0:35:13  iter: 33319  total_loss: 5.743  loss_ce: 0.01594  loss_mask: 0.1822  loss_dice: 0.2661  loss_ce_0: 0.07278  loss_mask_0: 0.1742  loss_dice_0: 0.2748  loss_ce_1: 0.04424  loss_mask_1: 0.1789  loss_dice_1: 0.2608  loss_ce_2: 0.02276  loss_mask_2: 0.1778  loss_dice_2: 0.2748  loss_ce_3: 0.01874  loss_mask_3: 0.1833  loss_dice_3: 0.2793  loss_ce_4: 0.03702  loss_mask_4: 0.1667  loss_dice_4: 0.2672  loss_ce_5: 0.04865  loss_mask_5: 0.1703  loss_dice_5: 0.2673  loss_ce_6: 0.01886  loss_mask_6: 0.1797  loss_dice_6: 0.2799  loss_ce_7: 0.04097  loss_mask_7: 0.18  loss_dice_7: 0.2614  loss_ce_8: 0.03037  loss_mask_8: 0.1732  loss_dice_8: 0.2709  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:42] d2.utils.events INFO:  eta: 0:35:11  iter: 33339  total_loss: 4.698  loss_ce: 0.01691  loss_mask: 0.2321  loss_dice: 0.1798  loss_ce_0: 0.06839  loss_mask_0: 0.223  loss_dice_0: 0.1631  loss_ce_1: 0.01706  loss_mask_1: 0.2265  loss_dice_1: 0.1786  loss_ce_2: 0.01529  loss_mask_2: 0.2156  loss_dice_2: 0.1716  loss_ce_3: 0.01671  loss_mask_3: 0.2206  loss_dice_3: 0.1746  loss_ce_4: 0.01536  loss_mask_4: 0.2168  loss_dice_4: 0.1632  loss_ce_5: 0.01847  loss_mask_5: 0.2196  loss_dice_5: 0.1742  loss_ce_6: 0.01483  loss_mask_6: 0.2193  loss_dice_6: 0.169  loss_ce_7: 0.01652  loss_mask_7: 0.2211  loss_dice_7: 0.1729  loss_ce_8: 0.01585  loss_mask_8: 0.218  loss_dice_8: 0.1787  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:45] d2.utils.events INFO:  eta: 0:35:09  iter: 33359  total_loss: 4.173  loss_ce: 0.01176  loss_mask: 0.2127  loss_dice: 0.1367  loss_ce_0: 0.0602  loss_mask_0: 0.202  loss_dice_0: 0.1393  loss_ce_1: 0.01802  loss_mask_1: 0.1957  loss_dice_1: 0.1273  loss_ce_2: 0.0114  loss_mask_2: 0.2085  loss_dice_2: 0.1367  loss_ce_3: 0.01107  loss_mask_3: 0.2078  loss_dice_3: 0.1372  loss_ce_4: 0.01282  loss_mask_4: 0.2062  loss_dice_4: 0.138  loss_ce_5: 0.01263  loss_mask_5: 0.2054  loss_dice_5: 0.1352  loss_ce_6: 0.0114  loss_mask_6: 0.2071  loss_dice_6: 0.1412  loss_ce_7: 0.01251  loss_mask_7: 0.2022  loss_dice_7: 0.1365  loss_ce_8: 0.0106  loss_mask_8: 0.2007  loss_dice_8: 0.1365  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:48] d2.utils.events INFO:  eta: 0:35:05  iter: 33379  total_loss: 4.54  loss_ce: 0.01521  loss_mask: 0.2062  loss_dice: 0.2002  loss_ce_0: 0.0639  loss_mask_0: 0.2101  loss_dice_0: 0.1969  loss_ce_1: 0.01425  loss_mask_1: 0.2086  loss_dice_1: 0.1926  loss_ce_2: 0.0155  loss_mask_2: 0.2136  loss_dice_2: 0.2035  loss_ce_3: 0.01413  loss_mask_3: 0.218  loss_dice_3: 0.2003  loss_ce_4: 0.01499  loss_mask_4: 0.2123  loss_dice_4: 0.1928  loss_ce_5: 0.01611  loss_mask_5: 0.2204  loss_dice_5: 0.1915  loss_ce_6: 0.01475  loss_mask_6: 0.2132  loss_dice_6: 0.2055  loss_ce_7: 0.01575  loss_mask_7: 0.2061  loss_dice_7: 0.1992  loss_ce_8: 0.01543  loss_mask_8: 0.2098  loss_dice_8: 0.2044  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:50] d2.utils.events INFO:  eta: 0:35:00  iter: 33399  total_loss: 4.387  loss_ce: 0.01221  loss_mask: 0.1796  loss_dice: 0.1796  loss_ce_0: 0.06205  loss_mask_0: 0.1877  loss_dice_0: 0.1729  loss_ce_1: 0.01291  loss_mask_1: 0.1884  loss_dice_1: 0.1807  loss_ce_2: 0.01191  loss_mask_2: 0.1767  loss_dice_2: 0.1818  loss_ce_3: 0.01088  loss_mask_3: 0.1904  loss_dice_3: 0.1815  loss_ce_4: 0.01146  loss_mask_4: 0.1873  loss_dice_4: 0.1793  loss_ce_5: 0.01253  loss_mask_5: 0.1907  loss_dice_5: 0.1851  loss_ce_6: 0.01262  loss_mask_6: 0.1901  loss_dice_6: 0.174  loss_ce_7: 0.01251  loss_mask_7: 0.1912  loss_dice_7: 0.1787  loss_ce_8: 0.01094  loss_mask_8: 0.1881  loss_dice_8: 0.1753  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:53] d2.utils.events INFO:  eta: 0:34:56  iter: 33419  total_loss: 4.151  loss_ce: 0.01273  loss_mask: 0.1875  loss_dice: 0.1664  loss_ce_0: 0.06389  loss_mask_0: 0.2022  loss_dice_0: 0.1658  loss_ce_1: 0.01045  loss_mask_1: 0.1994  loss_dice_1: 0.1684  loss_ce_2: 0.01118  loss_mask_2: 0.1884  loss_dice_2: 0.1668  loss_ce_3: 0.01125  loss_mask_3: 0.2014  loss_dice_3: 0.1784  loss_ce_4: 0.0113  loss_mask_4: 0.203  loss_dice_4: 0.1653  loss_ce_5: 0.01205  loss_mask_5: 0.1949  loss_dice_5: 0.1702  loss_ce_6: 0.01253  loss_mask_6: 0.1915  loss_dice_6: 0.1695  loss_ce_7: 0.01089  loss_mask_7: 0.2029  loss_dice_7: 0.1654  loss_ce_8: 0.01142  loss_mask_8: 0.1979  loss_dice_8: 0.1728  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:55] d2.utils.events INFO:  eta: 0:34:51  iter: 33439  total_loss: 4.898  loss_ce: 0.01348  loss_mask: 0.1623  loss_dice: 0.1952  loss_ce_0: 0.06201  loss_mask_0: 0.1656  loss_dice_0: 0.21  loss_ce_1: 0.0118  loss_mask_1: 0.1717  loss_dice_1: 0.2091  loss_ce_2: 0.008818  loss_mask_2: 0.1704  loss_dice_2: 0.2119  loss_ce_3: 0.01141  loss_mask_3: 0.1706  loss_dice_3: 0.2036  loss_ce_4: 0.009328  loss_mask_4: 0.1663  loss_dice_4: 0.2079  loss_ce_5: 0.007846  loss_mask_5: 0.1735  loss_dice_5: 0.2066  loss_ce_6: 0.01482  loss_mask_6: 0.1753  loss_dice_6: 0.2111  loss_ce_7: 0.01362  loss_mask_7: 0.1653  loss_dice_7: 0.2123  loss_ce_8: 0.009412  loss_mask_8: 0.1677  loss_dice_8: 0.2107  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:08:58] d2.utils.events INFO:  eta: 0:34:49  iter: 33459  total_loss: 4.033  loss_ce: 0.005555  loss_mask: 0.1765  loss_dice: 0.1608  loss_ce_0: 0.06272  loss_mask_0: 0.1665  loss_dice_0: 0.1721  loss_ce_1: 0.005312  loss_mask_1: 0.1739  loss_dice_1: 0.1552  loss_ce_2: 0.005609  loss_mask_2: 0.1676  loss_dice_2: 0.1574  loss_ce_3: 0.005296  loss_mask_3: 0.1672  loss_dice_3: 0.1574  loss_ce_4: 0.006395  loss_mask_4: 0.17  loss_dice_4: 0.1675  loss_ce_5: 0.005204  loss_mask_5: 0.1654  loss_dice_5: 0.1656  loss_ce_6: 0.006137  loss_mask_6: 0.1695  loss_dice_6: 0.1582  loss_ce_7: 0.005847  loss_mask_7: 0.1711  loss_dice_7: 0.1636  loss_ce_8: 0.004913  loss_mask_8: 0.1686  loss_dice_8: 0.1543  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:00] d2.utils.events INFO:  eta: 0:34:47  iter: 33479  total_loss: 3.896  loss_ce: 0.004987  loss_mask: 0.2026  loss_dice: 0.1555  loss_ce_0: 0.06207  loss_mask_0: 0.1954  loss_dice_0: 0.1517  loss_ce_1: 0.0041  loss_mask_1: 0.2003  loss_dice_1: 0.1506  loss_ce_2: 0.004649  loss_mask_2: 0.2049  loss_dice_2: 0.1497  loss_ce_3: 0.004098  loss_mask_3: 0.2015  loss_dice_3: 0.1436  loss_ce_4: 0.004752  loss_mask_4: 0.2031  loss_dice_4: 0.1486  loss_ce_5: 0.004277  loss_mask_5: 0.1996  loss_dice_5: 0.1497  loss_ce_6: 0.004933  loss_mask_6: 0.2003  loss_dice_6: 0.1504  loss_ce_7: 0.004419  loss_mask_7: 0.2037  loss_dice_7: 0.1469  loss_ce_8: 0.004658  loss_mask_8: 0.197  loss_dice_8: 0.1518  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:03] d2.utils.events INFO:  eta: 0:34:46  iter: 33499  total_loss: 6.891  loss_ce: 0.01525  loss_mask: 0.186  loss_dice: 0.2609  loss_ce_0: 0.06045  loss_mask_0: 0.1882  loss_dice_0: 0.2644  loss_ce_1: 0.0304  loss_mask_1: 0.1785  loss_dice_1: 0.2495  loss_ce_2: 0.01135  loss_mask_2: 0.1735  loss_dice_2: 0.2448  loss_ce_3: 0.0123  loss_mask_3: 0.1818  loss_dice_3: 0.2467  loss_ce_4: 0.01985  loss_mask_4: 0.1901  loss_dice_4: 0.2518  loss_ce_5: 0.01163  loss_mask_5: 0.1869  loss_dice_5: 0.2572  loss_ce_6: 0.01341  loss_mask_6: 0.1868  loss_dice_6: 0.258  loss_ce_7: 0.02365  loss_mask_7: 0.1938  loss_dice_7: 0.2558  loss_ce_8: 0.01233  loss_mask_8: 0.186  loss_dice_8: 0.2504  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:05] d2.utils.events INFO:  eta: 0:34:44  iter: 33519  total_loss: 4.437  loss_ce: 0.009517  loss_mask: 0.2228  loss_dice: 0.1868  loss_ce_0: 0.06595  loss_mask_0: 0.2229  loss_dice_0: 0.1872  loss_ce_1: 0.008279  loss_mask_1: 0.2293  loss_dice_1: 0.1785  loss_ce_2: 0.008449  loss_mask_2: 0.232  loss_dice_2: 0.178  loss_ce_3: 0.008123  loss_mask_3: 0.2281  loss_dice_3: 0.171  loss_ce_4: 0.007344  loss_mask_4: 0.2308  loss_dice_4: 0.1795  loss_ce_5: 0.008705  loss_mask_5: 0.2344  loss_dice_5: 0.1857  loss_ce_6: 0.007875  loss_mask_6: 0.2261  loss_dice_6: 0.1704  loss_ce_7: 0.008482  loss_mask_7: 0.2236  loss_dice_7: 0.1787  loss_ce_8: 0.009365  loss_mask_8: 0.2298  loss_dice_8: 0.1892  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:08] d2.utils.events INFO:  eta: 0:34:43  iter: 33539  total_loss: 4.937  loss_ce: 0.004112  loss_mask: 0.2057  loss_dice: 0.1833  loss_ce_0: 0.06169  loss_mask_0: 0.1979  loss_dice_0: 0.1776  loss_ce_1: 0.004793  loss_mask_1: 0.2181  loss_dice_1: 0.1773  loss_ce_2: 0.004204  loss_mask_2: 0.2097  loss_dice_2: 0.1855  loss_ce_3: 0.003931  loss_mask_3: 0.2039  loss_dice_3: 0.187  loss_ce_4: 0.004692  loss_mask_4: 0.2109  loss_dice_4: 0.1867  loss_ce_5: 0.003702  loss_mask_5: 0.2046  loss_dice_5: 0.1798  loss_ce_6: 0.004073  loss_mask_6: 0.2084  loss_dice_6: 0.1821  loss_ce_7: 0.004301  loss_mask_7: 0.218  loss_dice_7: 0.187  loss_ce_8: 0.004101  loss_mask_8: 0.2164  loss_dice_8: 0.1757  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:10] d2.utils.events INFO:  eta: 0:34:42  iter: 33559  total_loss: 3.94  loss_ce: 0.006134  loss_mask: 0.1544  loss_dice: 0.1648  loss_ce_0: 0.05982  loss_mask_0: 0.1511  loss_dice_0: 0.1772  loss_ce_1: 0.009378  loss_mask_1: 0.1634  loss_dice_1: 0.1616  loss_ce_2: 0.00681  loss_mask_2: 0.1611  loss_dice_2: 0.1672  loss_ce_3: 0.005275  loss_mask_3: 0.1587  loss_dice_3: 0.1733  loss_ce_4: 0.007375  loss_mask_4: 0.163  loss_dice_4: 0.1611  loss_ce_5: 0.0061  loss_mask_5: 0.1612  loss_dice_5: 0.1668  loss_ce_6: 0.004914  loss_mask_6: 0.1589  loss_dice_6: 0.173  loss_ce_7: 0.007422  loss_mask_7: 0.1661  loss_dice_7: 0.1725  loss_ce_8: 0.006191  loss_mask_8: 0.1544  loss_dice_8: 0.1697  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:13] d2.utils.events INFO:  eta: 0:34:40  iter: 33579  total_loss: 4.76  loss_ce: 0.01336  loss_mask: 0.1944  loss_dice: 0.1633  loss_ce_0: 0.06193  loss_mask_0: 0.1949  loss_dice_0: 0.1637  loss_ce_1: 0.01596  loss_mask_1: 0.1983  loss_dice_1: 0.1645  loss_ce_2: 0.01013  loss_mask_2: 0.1884  loss_dice_2: 0.16  loss_ce_3: 0.01664  loss_mask_3: 0.1992  loss_dice_3: 0.1588  loss_ce_4: 0.009518  loss_mask_4: 0.1954  loss_dice_4: 0.1602  loss_ce_5: 0.009679  loss_mask_5: 0.1946  loss_dice_5: 0.1606  loss_ce_6: 0.01098  loss_mask_6: 0.1883  loss_dice_6: 0.1628  loss_ce_7: 0.008756  loss_mask_7: 0.1936  loss_dice_7: 0.1587  loss_ce_8: 0.009606  loss_mask_8: 0.1929  loss_dice_8: 0.1699  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:15] d2.utils.events INFO:  eta: 0:34:38  iter: 33599  total_loss: 4.807  loss_ce: 0.03929  loss_mask: 0.187  loss_dice: 0.1282  loss_ce_0: 0.05421  loss_mask_0: 0.1783  loss_dice_0: 0.1285  loss_ce_1: 0.1188  loss_mask_1: 0.1762  loss_dice_1: 0.1295  loss_ce_2: 0.04318  loss_mask_2: 0.1851  loss_dice_2: 0.1235  loss_ce_3: 0.0342  loss_mask_3: 0.1749  loss_dice_3: 0.129  loss_ce_4: 0.04102  loss_mask_4: 0.1824  loss_dice_4: 0.1283  loss_ce_5: 0.0418  loss_mask_5: 0.1867  loss_dice_5: 0.1296  loss_ce_6: 0.02225  loss_mask_6: 0.1767  loss_dice_6: 0.1276  loss_ce_7: 0.04064  loss_mask_7: 0.1845  loss_dice_7: 0.1221  loss_ce_8: 0.0403  loss_mask_8: 0.1813  loss_dice_8: 0.1253  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:18] d2.utils.events INFO:  eta: 0:34:36  iter: 33619  total_loss: 4.072  loss_ce: 0.01787  loss_mask: 0.1986  loss_dice: 0.1403  loss_ce_0: 0.06311  loss_mask_0: 0.1882  loss_dice_0: 0.1442  loss_ce_1: 0.02823  loss_mask_1: 0.1954  loss_dice_1: 0.1462  loss_ce_2: 0.01356  loss_mask_2: 0.1914  loss_dice_2: 0.1441  loss_ce_3: 0.01505  loss_mask_3: 0.1965  loss_dice_3: 0.1428  loss_ce_4: 0.01451  loss_mask_4: 0.1887  loss_dice_4: 0.1434  loss_ce_5: 0.01573  loss_mask_5: 0.1862  loss_dice_5: 0.14  loss_ce_6: 0.01402  loss_mask_6: 0.1866  loss_dice_6: 0.1432  loss_ce_7: 0.01665  loss_mask_7: 0.1851  loss_dice_7: 0.1401  loss_ce_8: 0.01478  loss_mask_8: 0.1872  loss_dice_8: 0.1429  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:20] d2.utils.events INFO:  eta: 0:34:33  iter: 33639  total_loss: 4.631  loss_ce: 0.1294  loss_mask: 0.1614  loss_dice: 0.1527  loss_ce_0: 0.06306  loss_mask_0: 0.15  loss_dice_0: 0.1523  loss_ce_1: 0.1273  loss_mask_1: 0.1551  loss_dice_1: 0.161  loss_ce_2: 0.129  loss_mask_2: 0.1598  loss_dice_2: 0.158  loss_ce_3: 0.1239  loss_mask_3: 0.1522  loss_dice_3: 0.1579  loss_ce_4: 0.1286  loss_mask_4: 0.1563  loss_dice_4: 0.1532  loss_ce_5: 0.132  loss_mask_5: 0.1487  loss_dice_5: 0.1537  loss_ce_6: 0.1286  loss_mask_6: 0.1635  loss_dice_6: 0.1554  loss_ce_7: 0.1255  loss_mask_7: 0.1485  loss_dice_7: 0.1617  loss_ce_8: 0.1368  loss_mask_8: 0.1635  loss_dice_8: 0.1614  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:23] d2.utils.events INFO:  eta: 0:34:30  iter: 33659  total_loss: 4.903  loss_ce: 0.05977  loss_mask: 0.2253  loss_dice: 0.1495  loss_ce_0: 0.06318  loss_mask_0: 0.2106  loss_dice_0: 0.1462  loss_ce_1: 0.03778  loss_mask_1: 0.2078  loss_dice_1: 0.1527  loss_ce_2: 0.04607  loss_mask_2: 0.2186  loss_dice_2: 0.1474  loss_ce_3: 0.04658  loss_mask_3: 0.2167  loss_dice_3: 0.146  loss_ce_4: 0.0616  loss_mask_4: 0.2083  loss_dice_4: 0.1511  loss_ce_5: 0.05776  loss_mask_5: 0.2196  loss_dice_5: 0.1469  loss_ce_6: 0.04653  loss_mask_6: 0.2108  loss_dice_6: 0.1404  loss_ce_7: 0.05654  loss_mask_7: 0.2202  loss_dice_7: 0.1493  loss_ce_8: 0.06297  loss_mask_8: 0.2157  loss_dice_8: 0.1438  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:09:26] d2.utils.events INFO:  eta: 0:34:28  iter: 33679  total_loss: 3.957  loss_ce: 0.01636  loss_mask: 0.215  loss_dice: 0.1297  loss_ce_0: 0.0634  loss_mask_0: 0.2161  loss_dice_0: 0.1353  loss_ce_1: 0.01876  loss_mask_1: 0.2263  loss_dice_1: 0.1337  loss_ce_2: 0.01652  loss_mask_2: 0.21  loss_dice_2: 0.129  loss_ce_3: 0.01917  loss_mask_3: 0.2077  loss_dice_3: 0.1271  loss_ce_4: 0.01963  loss_mask_4: 0.2065  loss_dice_4: 0.1262  loss_ce_5: 0.01897  loss_mask_5: 0.2091  loss_dice_5: 0.1333  loss_ce_6: 0.01935  loss_mask_6: 0.2053  loss_dice_6: 0.1283  loss_ce_7: 0.02039  loss_mask_7: 0.2155  loss_dice_7: 0.1314  loss_ce_8: 0.01552  loss_mask_8: 0.203  loss_dice_8: 0.1321  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:09:28] d2.utils.events INFO:  eta: 0:34:25  iter: 33699  total_loss: 4.158  loss_ce: 0.00759  loss_mask: 0.2061  loss_dice: 0.1627  loss_ce_0: 0.06252  loss_mask_0: 0.1976  loss_dice_0: 0.1625  loss_ce_1: 0.008486  loss_mask_1: 0.2084  loss_dice_1: 0.1645  loss_ce_2: 0.008588  loss_mask_2: 0.2038  loss_dice_2: 0.1632  loss_ce_3: 0.009374  loss_mask_3: 0.2033  loss_dice_3: 0.155  loss_ce_4: 0.008385  loss_mask_4: 0.1925  loss_dice_4: 0.1585  loss_ce_5: 0.008358  loss_mask_5: 0.2057  loss_dice_5: 0.1578  loss_ce_6: 0.008948  loss_mask_6: 0.2084  loss_dice_6: 0.1613  loss_ce_7: 0.008062  loss_mask_7: 0.2031  loss_dice_7: 0.1526  loss_ce_8: 0.007562  loss_mask_8: 0.2057  loss_dice_8: 0.1526  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:31] d2.utils.events INFO:  eta: 0:34:22  iter: 33719  total_loss: 4.383  loss_ce: 0.008961  loss_mask: 0.2187  loss_dice: 0.1495  loss_ce_0: 0.06993  loss_mask_0: 0.2159  loss_dice_0: 0.1492  loss_ce_1: 0.009904  loss_mask_1: 0.2288  loss_dice_1: 0.1472  loss_ce_2: 0.008623  loss_mask_2: 0.2298  loss_dice_2: 0.1441  loss_ce_3: 0.008592  loss_mask_3: 0.2202  loss_dice_3: 0.1475  loss_ce_4: 0.009608  loss_mask_4: 0.2174  loss_dice_4: 0.1474  loss_ce_5: 0.008468  loss_mask_5: 0.2228  loss_dice_5: 0.1379  loss_ce_6: 0.008433  loss_mask_6: 0.2229  loss_dice_6: 0.1456  loss_ce_7: 0.00974  loss_mask_7: 0.2178  loss_dice_7: 0.1465  loss_ce_8: 0.007841  loss_mask_8: 0.2184  loss_dice_8: 0.1442  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:33] d2.utils.events INFO:  eta: 0:34:20  iter: 33739  total_loss: 3.472  loss_ce: 0.009676  loss_mask: 0.195  loss_dice: 0.1218  loss_ce_0: 0.06662  loss_mask_0: 0.1952  loss_dice_0: 0.1213  loss_ce_1: 0.01388  loss_mask_1: 0.1969  loss_dice_1: 0.1264  loss_ce_2: 0.01129  loss_mask_2: 0.1953  loss_dice_2: 0.1275  loss_ce_3: 0.009046  loss_mask_3: 0.1929  loss_dice_3: 0.13  loss_ce_4: 0.009227  loss_mask_4: 0.1823  loss_dice_4: 0.1254  loss_ce_5: 0.009708  loss_mask_5: 0.1959  loss_dice_5: 0.1259  loss_ce_6: 0.00925  loss_mask_6: 0.1894  loss_dice_6: 0.1257  loss_ce_7: 0.0108  loss_mask_7: 0.1921  loss_dice_7: 0.1269  loss_ce_8: 0.01091  loss_mask_8: 0.1922  loss_dice_8: 0.1269  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:36] d2.utils.events INFO:  eta: 0:34:17  iter: 33759  total_loss: 4.117  loss_ce: 0.0101  loss_mask: 0.2291  loss_dice: 0.1248  loss_ce_0: 0.06189  loss_mask_0: 0.2284  loss_dice_0: 0.1221  loss_ce_1: 0.01103  loss_mask_1: 0.2297  loss_dice_1: 0.1255  loss_ce_2: 0.008294  loss_mask_2: 0.2398  loss_dice_2: 0.1232  loss_ce_3: 0.007369  loss_mask_3: 0.2366  loss_dice_3: 0.1229  loss_ce_4: 0.008967  loss_mask_4: 0.2301  loss_dice_4: 0.1253  loss_ce_5: 0.008062  loss_mask_5: 0.2343  loss_dice_5: 0.1261  loss_ce_6: 0.007068  loss_mask_6: 0.2312  loss_dice_6: 0.1271  loss_ce_7: 0.009421  loss_mask_7: 0.2326  loss_dice_7: 0.1241  loss_ce_8: 0.008804  loss_mask_8: 0.2299  loss_dice_8: 0.1224  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:09:38] d2.utils.events INFO:  eta: 0:34:14  iter: 33779  total_loss: 3.732  loss_ce: 0.006542  loss_mask: 0.1894  loss_dice: 0.1384  loss_ce_0: 0.06332  loss_mask_0: 0.1963  loss_dice_0: 0.1456  loss_ce_1: 0.01077  loss_mask_1: 0.1935  loss_dice_1: 0.1391  loss_ce_2: 0.006813  loss_mask_2: 0.1987  loss_dice_2: 0.1441  loss_ce_3: 0.006542  loss_mask_3: 0.1865  loss_dice_3: 0.1371  loss_ce_4: 0.008893  loss_mask_4: 0.1967  loss_dice_4: 0.14  loss_ce_5: 0.007119  loss_mask_5: 0.1972  loss_dice_5: 0.1427  loss_ce_6: 0.00652  loss_mask_6: 0.1936  loss_dice_6: 0.1344  loss_ce_7: 0.008752  loss_mask_7: 0.1874  loss_dice_7: 0.139  loss_ce_8: 0.006221  loss_mask_8: 0.1934  loss_dice_8: 0.14  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:09:41] d2.utils.events INFO:  eta: 0:34:10  iter: 33799  total_loss: 3.963  loss_ce: 0.004496  loss_mask: 0.2239  loss_dice: 0.1425  loss_ce_0: 0.06525  loss_mask_0: 0.2327  loss_dice_0: 0.1471  loss_ce_1: 0.009251  loss_mask_1: 0.2214  loss_dice_1: 0.1442  loss_ce_2: 0.005022  loss_mask_2: 0.2311  loss_dice_2: 0.1442  loss_ce_3: 0.004173  loss_mask_3: 0.2311  loss_dice_3: 0.1485  loss_ce_4: 0.004436  loss_mask_4: 0.2213  loss_dice_4: 0.1418  loss_ce_5: 0.004533  loss_mask_5: 0.2307  loss_dice_5: 0.1416  loss_ce_6: 0.003994  loss_mask_6: 0.2246  loss_dice_6: 0.1436  loss_ce_7: 0.004196  loss_mask_7: 0.2248  loss_dice_7: 0.1429  loss_ce_8: 0.004257  loss_mask_8: 0.233  loss_dice_8: 0.1403  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:43] d2.utils.events INFO:  eta: 0:34:09  iter: 33819  total_loss: 4.628  loss_ce: 0.003898  loss_mask: 0.2519  loss_dice: 0.1812  loss_ce_0: 0.06143  loss_mask_0: 0.2422  loss_dice_0: 0.1877  loss_ce_1: 0.006458  loss_mask_1: 0.2449  loss_dice_1: 0.1872  loss_ce_2: 0.004351  loss_mask_2: 0.2412  loss_dice_2: 0.1819  loss_ce_3: 0.003447  loss_mask_3: 0.2445  loss_dice_3: 0.1803  loss_ce_4: 0.003763  loss_mask_4: 0.2476  loss_dice_4: 0.1808  loss_ce_5: 0.003685  loss_mask_5: 0.246  loss_dice_5: 0.1916  loss_ce_6: 0.003267  loss_mask_6: 0.2499  loss_dice_6: 0.1789  loss_ce_7: 0.003327  loss_mask_7: 0.239  loss_dice_7: 0.1819  loss_ce_8: 0.003425  loss_mask_8: 0.25  loss_dice_8: 0.1823  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:46] d2.utils.events INFO:  eta: 0:34:04  iter: 33839  total_loss: 4.016  loss_ce: 0.005666  loss_mask: 0.1749  loss_dice: 0.1568  loss_ce_0: 0.06202  loss_mask_0: 0.1841  loss_dice_0: 0.1677  loss_ce_1: 0.01265  loss_mask_1: 0.1856  loss_dice_1: 0.1541  loss_ce_2: 0.006624  loss_mask_2: 0.177  loss_dice_2: 0.1613  loss_ce_3: 0.005422  loss_mask_3: 0.1768  loss_dice_3: 0.1612  loss_ce_4: 0.006267  loss_mask_4: 0.1727  loss_dice_4: 0.1579  loss_ce_5: 0.005836  loss_mask_5: 0.186  loss_dice_5: 0.1608  loss_ce_6: 0.005542  loss_mask_6: 0.1735  loss_dice_6: 0.1588  loss_ce_7: 0.006714  loss_mask_7: 0.1729  loss_dice_7: 0.1548  loss_ce_8: 0.005729  loss_mask_8: 0.1856  loss_dice_8: 0.1592  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:48] d2.utils.events INFO:  eta: 0:34:02  iter: 33859  total_loss: 4.961  loss_ce: 0.009279  loss_mask: 0.2117  loss_dice: 0.2132  loss_ce_0: 0.06216  loss_mask_0: 0.2225  loss_dice_0: 0.2253  loss_ce_1: 0.01531  loss_mask_1: 0.211  loss_dice_1: 0.2269  loss_ce_2: 0.01123  loss_mask_2: 0.2146  loss_dice_2: 0.2231  loss_ce_3: 0.01185  loss_mask_3: 0.2227  loss_dice_3: 0.229  loss_ce_4: 0.01294  loss_mask_4: 0.2241  loss_dice_4: 0.2361  loss_ce_5: 0.009655  loss_mask_5: 0.218  loss_dice_5: 0.2308  loss_ce_6: 0.01021  loss_mask_6: 0.2192  loss_dice_6: 0.2258  loss_ce_7: 0.01046  loss_mask_7: 0.2127  loss_dice_7: 0.232  loss_ce_8: 0.00921  loss_mask_8: 0.2181  loss_dice_8: 0.2233  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:51] d2.utils.events INFO:  eta: 0:33:59  iter: 33879  total_loss: 4.077  loss_ce: 0.007657  loss_mask: 0.1843  loss_dice: 0.1483  loss_ce_0: 0.06329  loss_mask_0: 0.1861  loss_dice_0: 0.1425  loss_ce_1: 0.05351  loss_mask_1: 0.1804  loss_dice_1: 0.1487  loss_ce_2: 0.01563  loss_mask_2: 0.1809  loss_dice_2: 0.1539  loss_ce_3: 0.01094  loss_mask_3: 0.178  loss_dice_3: 0.1436  loss_ce_4: 0.0119  loss_mask_4: 0.1858  loss_dice_4: 0.1435  loss_ce_5: 0.007072  loss_mask_5: 0.1712  loss_dice_5: 0.1471  loss_ce_6: 0.00753  loss_mask_6: 0.1773  loss_dice_6: 0.1476  loss_ce_7: 0.007184  loss_mask_7: 0.1828  loss_dice_7: 0.1421  loss_ce_8: 0.007086  loss_mask_8: 0.1798  loss_dice_8: 0.1451  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:53] d2.utils.events INFO:  eta: 0:33:57  iter: 33899  total_loss: 4.361  loss_ce: 0.008667  loss_mask: 0.211  loss_dice: 0.1649  loss_ce_0: 0.06118  loss_mask_0: 0.2061  loss_dice_0: 0.164  loss_ce_1: 0.008873  loss_mask_1: 0.2055  loss_dice_1: 0.1702  loss_ce_2: 0.008143  loss_mask_2: 0.2019  loss_dice_2: 0.1697  loss_ce_3: 0.008024  loss_mask_3: 0.2115  loss_dice_3: 0.17  loss_ce_4: 0.009055  loss_mask_4: 0.2042  loss_dice_4: 0.1674  loss_ce_5: 0.00804  loss_mask_5: 0.2064  loss_dice_5: 0.1692  loss_ce_6: 0.008281  loss_mask_6: 0.2106  loss_dice_6: 0.1661  loss_ce_7: 0.006929  loss_mask_7: 0.2007  loss_dice_7: 0.1675  loss_ce_8: 0.00847  loss_mask_8: 0.2025  loss_dice_8: 0.1707  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:56] d2.utils.events INFO:  eta: 0:33:54  iter: 33919  total_loss: 3.817  loss_ce: 0.009653  loss_mask: 0.1849  loss_dice: 0.1365  loss_ce_0: 0.06236  loss_mask_0: 0.1826  loss_dice_0: 0.1425  loss_ce_1: 0.008822  loss_mask_1: 0.193  loss_dice_1: 0.1392  loss_ce_2: 0.006979  loss_mask_2: 0.1891  loss_dice_2: 0.1496  loss_ce_3: 0.009997  loss_mask_3: 0.1925  loss_dice_3: 0.1505  loss_ce_4: 0.01185  loss_mask_4: 0.1834  loss_dice_4: 0.1429  loss_ce_5: 0.007922  loss_mask_5: 0.1805  loss_dice_5: 0.1434  loss_ce_6: 0.00979  loss_mask_6: 0.1903  loss_dice_6: 0.1408  loss_ce_7: 0.009563  loss_mask_7: 0.192  loss_dice_7: 0.1457  loss_ce_8: 0.008474  loss_mask_8: 0.1877  loss_dice_8: 0.1377  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:09:58] d2.utils.events INFO:  eta: 0:33:51  iter: 33939  total_loss: 6.182  loss_ce: 0.02252  loss_mask: 0.2112  loss_dice: 0.2188  loss_ce_0: 0.05711  loss_mask_0: 0.2122  loss_dice_0: 0.2082  loss_ce_1: 0.0392  loss_mask_1: 0.2135  loss_dice_1: 0.2204  loss_ce_2: 0.03388  loss_mask_2: 0.2248  loss_dice_2: 0.2311  loss_ce_3: 0.02523  loss_mask_3: 0.2089  loss_dice_3: 0.2152  loss_ce_4: 0.03317  loss_mask_4: 0.2125  loss_dice_4: 0.2201  loss_ce_5: 0.03128  loss_mask_5: 0.1995  loss_dice_5: 0.212  loss_ce_6: 0.01638  loss_mask_6: 0.2058  loss_dice_6: 0.2148  loss_ce_7: 0.0207  loss_mask_7: 0.2093  loss_dice_7: 0.2178  loss_ce_8: 0.0297  loss_mask_8: 0.2065  loss_dice_8: 0.2194  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:01] d2.utils.events INFO:  eta: 0:33:46  iter: 33959  total_loss: 4.312  loss_ce: 0.005721  loss_mask: 0.2015  loss_dice: 0.1937  loss_ce_0: 0.05623  loss_mask_0: 0.2029  loss_dice_0: 0.1871  loss_ce_1: 0.003502  loss_mask_1: 0.2023  loss_dice_1: 0.1903  loss_ce_2: 0.006414  loss_mask_2: 0.2024  loss_dice_2: 0.1996  loss_ce_3: 0.006558  loss_mask_3: 0.1911  loss_dice_3: 0.1961  loss_ce_4: 0.006769  loss_mask_4: 0.1937  loss_dice_4: 0.1834  loss_ce_5: 0.005976  loss_mask_5: 0.2028  loss_dice_5: 0.1848  loss_ce_6: 0.005282  loss_mask_6: 0.1975  loss_dice_6: 0.1949  loss_ce_7: 0.005569  loss_mask_7: 0.2029  loss_dice_7: 0.1956  loss_ce_8: 0.004893  loss_mask_8: 0.2068  loss_dice_8: 0.1934  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:10:04] d2.utils.events INFO:  eta: 0:33:42  iter: 33979  total_loss: 4.102  loss_ce: 0.01761  loss_mask: 0.2387  loss_dice: 0.1517  loss_ce_0: 0.06355  loss_mask_0: 0.2333  loss_dice_0: 0.1505  loss_ce_1: 0.01206  loss_mask_1: 0.2351  loss_dice_1: 0.1509  loss_ce_2: 0.01568  loss_mask_2: 0.2336  loss_dice_2: 0.1481  loss_ce_3: 0.01651  loss_mask_3: 0.2403  loss_dice_3: 0.1456  loss_ce_4: 0.0174  loss_mask_4: 0.2319  loss_dice_4: 0.1573  loss_ce_5: 0.01715  loss_mask_5: 0.2422  loss_dice_5: 0.1544  loss_ce_6: 0.01767  loss_mask_6: 0.2354  loss_dice_6: 0.1445  loss_ce_7: 0.01424  loss_mask_7: 0.244  loss_dice_7: 0.1512  loss_ce_8: 0.01717  loss_mask_8: 0.2315  loss_dice_8: 0.1409  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:10:06] d2.utils.events INFO:  eta: 0:33:39  iter: 33999  total_loss: 5.103  loss_ce: 0.003482  loss_mask: 0.2115  loss_dice: 0.2086  loss_ce_0: 0.04942  loss_mask_0: 0.217  loss_dice_0: 0.2129  loss_ce_1: 0.002663  loss_mask_1: 0.2074  loss_dice_1: 0.2035  loss_ce_2: 0.003693  loss_mask_2: 0.2129  loss_dice_2: 0.2018  loss_ce_3: 0.003695  loss_mask_3: 0.2159  loss_dice_3: 0.2113  loss_ce_4: 0.004284  loss_mask_4: 0.2057  loss_dice_4: 0.2084  loss_ce_5: 0.003888  loss_mask_5: 0.2047  loss_dice_5: 0.2056  loss_ce_6: 0.003982  loss_mask_6: 0.2163  loss_dice_6: 0.2115  loss_ce_7: 0.005035  loss_mask_7: 0.2093  loss_dice_7: 0.2041  loss_ce_8: 0.003345  loss_mask_8: 0.207  loss_dice_8: 0.2108  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:09] d2.utils.events INFO:  eta: 0:33:37  iter: 34019  total_loss: 4.182  loss_ce: 0.01034  loss_mask: 0.1751  loss_dice: 0.1562  loss_ce_0: 0.0488  loss_mask_0: 0.1719  loss_dice_0: 0.1495  loss_ce_1: 0.009769  loss_mask_1: 0.1726  loss_dice_1: 0.1544  loss_ce_2: 0.009264  loss_mask_2: 0.1823  loss_dice_2: 0.1554  loss_ce_3: 0.009648  loss_mask_3: 0.1697  loss_dice_3: 0.1563  loss_ce_4: 0.01019  loss_mask_4: 0.1803  loss_dice_4: 0.1539  loss_ce_5: 0.01164  loss_mask_5: 0.1717  loss_dice_5: 0.1524  loss_ce_6: 0.01125  loss_mask_6: 0.1652  loss_dice_6: 0.1539  loss_ce_7: 0.008389  loss_mask_7: 0.1768  loss_dice_7: 0.1567  loss_ce_8: 0.0102  loss_mask_8: 0.1795  loss_dice_8: 0.1488  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:11] d2.utils.events INFO:  eta: 0:33:33  iter: 34039  total_loss: 4.041  loss_ce: 0.005634  loss_mask: 0.2033  loss_dice: 0.1431  loss_ce_0: 0.0698  loss_mask_0: 0.2039  loss_dice_0: 0.1423  loss_ce_1: 0.01166  loss_mask_1: 0.2034  loss_dice_1: 0.1484  loss_ce_2: 0.01395  loss_mask_2: 0.2002  loss_dice_2: 0.1372  loss_ce_3: 0.01082  loss_mask_3: 0.2025  loss_dice_3: 0.1374  loss_ce_4: 0.01033  loss_mask_4: 0.2054  loss_dice_4: 0.1346  loss_ce_5: 0.008368  loss_mask_5: 0.1933  loss_dice_5: 0.1393  loss_ce_6: 0.006068  loss_mask_6: 0.205  loss_dice_6: 0.1441  loss_ce_7: 0.00437  loss_mask_7: 0.2056  loss_dice_7: 0.1385  loss_ce_8: 0.006587  loss_mask_8: 0.2073  loss_dice_8: 0.1433  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:14] d2.utils.events INFO:  eta: 0:33:32  iter: 34059  total_loss: 4.45  loss_ce: 0.003796  loss_mask: 0.2106  loss_dice: 0.1372  loss_ce_0: 0.06198  loss_mask_0: 0.2139  loss_dice_0: 0.1402  loss_ce_1: 0.011  loss_mask_1: 0.2187  loss_dice_1: 0.1477  loss_ce_2: 0.01292  loss_mask_2: 0.2145  loss_dice_2: 0.1414  loss_ce_3: 0.009467  loss_mask_3: 0.2023  loss_dice_3: 0.1424  loss_ce_4: 0.009051  loss_mask_4: 0.2138  loss_dice_4: 0.1433  loss_ce_5: 0.006813  loss_mask_5: 0.2151  loss_dice_5: 0.1507  loss_ce_6: 0.00458  loss_mask_6: 0.2073  loss_dice_6: 0.1421  loss_ce_7: 0.003443  loss_mask_7: 0.218  loss_dice_7: 0.1408  loss_ce_8: 0.004771  loss_mask_8: 0.2158  loss_dice_8: 0.1442  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:16] d2.utils.events INFO:  eta: 0:33:29  iter: 34079  total_loss: 4.229  loss_ce: 0.003258  loss_mask: 0.2146  loss_dice: 0.149  loss_ce_0: 0.06249  loss_mask_0: 0.2174  loss_dice_0: 0.146  loss_ce_1: 0.008301  loss_mask_1: 0.2093  loss_dice_1: 0.1474  loss_ce_2: 0.008989  loss_mask_2: 0.2037  loss_dice_2: 0.1543  loss_ce_3: 0.007569  loss_mask_3: 0.2135  loss_dice_3: 0.1484  loss_ce_4: 0.006598  loss_mask_4: 0.2167  loss_dice_4: 0.1509  loss_ce_5: 0.005625  loss_mask_5: 0.2164  loss_dice_5: 0.151  loss_ce_6: 0.004421  loss_mask_6: 0.2114  loss_dice_6: 0.1474  loss_ce_7: 0.002709  loss_mask_7: 0.212  loss_dice_7: 0.1512  loss_ce_8: 0.004107  loss_mask_8: 0.2098  loss_dice_8: 0.1534  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:19] d2.utils.events INFO:  eta: 0:33:27  iter: 34099  total_loss: 4.156  loss_ce: 0.006488  loss_mask: 0.2342  loss_dice: 0.1379  loss_ce_0: 0.06743  loss_mask_0: 0.245  loss_dice_0: 0.1349  loss_ce_1: 0.007281  loss_mask_1: 0.2224  loss_dice_1: 0.1341  loss_ce_2: 0.007733  loss_mask_2: 0.2243  loss_dice_2: 0.1354  loss_ce_3: 0.006766  loss_mask_3: 0.2365  loss_dice_3: 0.1402  loss_ce_4: 0.006303  loss_mask_4: 0.2365  loss_dice_4: 0.1384  loss_ce_5: 0.005568  loss_mask_5: 0.2334  loss_dice_5: 0.138  loss_ce_6: 0.006626  loss_mask_6: 0.2423  loss_dice_6: 0.1337  loss_ce_7: 0.006675  loss_mask_7: 0.2356  loss_dice_7: 0.1415  loss_ce_8: 0.006684  loss_mask_8: 0.2286  loss_dice_8: 0.1347  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:21] d2.utils.events INFO:  eta: 0:33:25  iter: 34119  total_loss: 4.477  loss_ce: 0.0113  loss_mask: 0.195  loss_dice: 0.1659  loss_ce_0: 0.05453  loss_mask_0: 0.2  loss_dice_0: 0.1606  loss_ce_1: 0.01085  loss_mask_1: 0.1947  loss_dice_1: 0.1615  loss_ce_2: 0.01397  loss_mask_2: 0.1864  loss_dice_2: 0.1621  loss_ce_3: 0.01143  loss_mask_3: 0.1854  loss_dice_3: 0.1689  loss_ce_4: 0.01087  loss_mask_4: 0.1902  loss_dice_4: 0.1589  loss_ce_5: 0.01331  loss_mask_5: 0.1963  loss_dice_5: 0.1663  loss_ce_6: 0.01335  loss_mask_6: 0.1836  loss_dice_6: 0.1676  loss_ce_7: 0.01481  loss_mask_7: 0.1923  loss_dice_7: 0.1623  loss_ce_8: 0.01261  loss_mask_8: 0.1884  loss_dice_8: 0.1568  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:24] d2.utils.events INFO:  eta: 0:33:23  iter: 34139  total_loss: 4.291  loss_ce: 0.007975  loss_mask: 0.1973  loss_dice: 0.1542  loss_ce_0: 0.05212  loss_mask_0: 0.196  loss_dice_0: 0.1549  loss_ce_1: 0.01086  loss_mask_1: 0.1864  loss_dice_1: 0.1511  loss_ce_2: 0.01124  loss_mask_2: 0.197  loss_dice_2: 0.1552  loss_ce_3: 0.01086  loss_mask_3: 0.1931  loss_dice_3: 0.1494  loss_ce_4: 0.009587  loss_mask_4: 0.1874  loss_dice_4: 0.1489  loss_ce_5: 0.009942  loss_mask_5: 0.197  loss_dice_5: 0.151  loss_ce_6: 0.009499  loss_mask_6: 0.1979  loss_dice_6: 0.1493  loss_ce_7: 0.009961  loss_mask_7: 0.192  loss_dice_7: 0.1519  loss_ce_8: 0.009424  loss_mask_8: 0.1864  loss_dice_8: 0.149  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:26] d2.utils.events INFO:  eta: 0:33:20  iter: 34159  total_loss: 4.549  loss_ce: 0.004561  loss_mask: 0.2394  loss_dice: 0.176  loss_ce_0: 0.07588  loss_mask_0: 0.2294  loss_dice_0: 0.1781  loss_ce_1: 0.005848  loss_mask_1: 0.2257  loss_dice_1: 0.166  loss_ce_2: 0.007454  loss_mask_2: 0.2411  loss_dice_2: 0.1747  loss_ce_3: 0.007178  loss_mask_3: 0.2252  loss_dice_3: 0.1753  loss_ce_4: 0.006507  loss_mask_4: 0.2439  loss_dice_4: 0.1722  loss_ce_5: 0.0051  loss_mask_5: 0.233  loss_dice_5: 0.1775  loss_ce_6: 0.004733  loss_mask_6: 0.2365  loss_dice_6: 0.1807  loss_ce_7: 0.004625  loss_mask_7: 0.2406  loss_dice_7: 0.1796  loss_ce_8: 0.006296  loss_mask_8: 0.2339  loss_dice_8: 0.1681  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:29] d2.utils.events INFO:  eta: 0:33:18  iter: 34179  total_loss: 4.372  loss_ce: 0.007474  loss_mask: 0.1702  loss_dice: 0.1521  loss_ce_0: 0.05184  loss_mask_0: 0.1752  loss_dice_0: 0.1469  loss_ce_1: 0.01186  loss_mask_1: 0.1795  loss_dice_1: 0.1381  loss_ce_2: 0.009856  loss_mask_2: 0.1875  loss_dice_2: 0.1471  loss_ce_3: 0.008379  loss_mask_3: 0.1767  loss_dice_3: 0.1514  loss_ce_4: 0.009269  loss_mask_4: 0.1869  loss_dice_4: 0.1482  loss_ce_5: 0.007938  loss_mask_5: 0.1905  loss_dice_5: 0.1433  loss_ce_6: 0.006629  loss_mask_6: 0.1796  loss_dice_6: 0.1451  loss_ce_7: 0.01299  loss_mask_7: 0.1804  loss_dice_7: 0.144  loss_ce_8: 0.00827  loss_mask_8: 0.1826  loss_dice_8: 0.1497  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:31] d2.utils.events INFO:  eta: 0:33:16  iter: 34199  total_loss: 4.025  loss_ce: 0.004492  loss_mask: 0.1792  loss_dice: 0.152  loss_ce_0: 0.06299  loss_mask_0: 0.1905  loss_dice_0: 0.153  loss_ce_1: 0.004731  loss_mask_1: 0.1855  loss_dice_1: 0.1543  loss_ce_2: 0.005328  loss_mask_2: 0.1744  loss_dice_2: 0.1434  loss_ce_3: 0.005769  loss_mask_3: 0.183  loss_dice_3: 0.1539  loss_ce_4: 0.004701  loss_mask_4: 0.1794  loss_dice_4: 0.1567  loss_ce_5: 0.004206  loss_mask_5: 0.1843  loss_dice_5: 0.1471  loss_ce_6: 0.004214  loss_mask_6: 0.1811  loss_dice_6: 0.1526  loss_ce_7: 0.004354  loss_mask_7: 0.1824  loss_dice_7: 0.1516  loss_ce_8: 0.005539  loss_mask_8: 0.174  loss_dice_8: 0.1515  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:10:34] d2.utils.events INFO:  eta: 0:33:13  iter: 34219  total_loss: 5.174  loss_ce: 0.005529  loss_mask: 0.2306  loss_dice: 0.1709  loss_ce_0: 0.06233  loss_mask_0: 0.2379  loss_dice_0: 0.1791  loss_ce_1: 0.00587  loss_mask_1: 0.2297  loss_dice_1: 0.1707  loss_ce_2: 0.006083  loss_mask_2: 0.2251  loss_dice_2: 0.1704  loss_ce_3: 0.005699  loss_mask_3: 0.2314  loss_dice_3: 0.1758  loss_ce_4: 0.005652  loss_mask_4: 0.2303  loss_dice_4: 0.1742  loss_ce_5: 0.006115  loss_mask_5: 0.2426  loss_dice_5: 0.1776  loss_ce_6: 0.005137  loss_mask_6: 0.2362  loss_dice_6: 0.1808  loss_ce_7: 0.0068  loss_mask_7: 0.2371  loss_dice_7: 0.1752  loss_ce_8: 0.006099  loss_mask_8: 0.2329  loss_dice_8: 0.1805  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:10:36] d2.utils.events INFO:  eta: 0:33:11  iter: 34239  total_loss: 3.948  loss_ce: 0.005941  loss_mask: 0.2156  loss_dice: 0.1195  loss_ce_0: 0.07001  loss_mask_0: 0.202  loss_dice_0: 0.1233  loss_ce_1: 0.00692  loss_mask_1: 0.2083  loss_dice_1: 0.1224  loss_ce_2: 0.006218  loss_mask_2: 0.2119  loss_dice_2: 0.1223  loss_ce_3: 0.00657  loss_mask_3: 0.2149  loss_dice_3: 0.1224  loss_ce_4: 0.006319  loss_mask_4: 0.2122  loss_dice_4: 0.1186  loss_ce_5: 0.00585  loss_mask_5: 0.2139  loss_dice_5: 0.1231  loss_ce_6: 0.005317  loss_mask_6: 0.2151  loss_dice_6: 0.1228  loss_ce_7: 0.006522  loss_mask_7: 0.2033  loss_dice_7: 0.1215  loss_ce_8: 0.006836  loss_mask_8: 0.2175  loss_dice_8: 0.1208  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:10:39] d2.utils.events INFO:  eta: 0:33:09  iter: 34259  total_loss: 4.437  loss_ce: 0.01277  loss_mask: 0.1786  loss_dice: 0.1442  loss_ce_0: 0.05226  loss_mask_0: 0.1837  loss_dice_0: 0.1475  loss_ce_1: 0.006854  loss_mask_1: 0.1833  loss_dice_1: 0.1385  loss_ce_2: 0.01387  loss_mask_2: 0.1793  loss_dice_2: 0.1453  loss_ce_3: 0.01247  loss_mask_3: 0.1814  loss_dice_3: 0.148  loss_ce_4: 0.009146  loss_mask_4: 0.1776  loss_dice_4: 0.1469  loss_ce_5: 0.01357  loss_mask_5: 0.179  loss_dice_5: 0.152  loss_ce_6: 0.01553  loss_mask_6: 0.1755  loss_dice_6: 0.1436  loss_ce_7: 0.0101  loss_mask_7: 0.1811  loss_dice_7: 0.146  loss_ce_8: 0.0111  loss_mask_8: 0.1787  loss_dice_8: 0.1439  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:42] d2.utils.events INFO:  eta: 0:33:08  iter: 34279  total_loss: 4.661  loss_ce: 0.03732  loss_mask: 0.1787  loss_dice: 0.1729  loss_ce_0: 0.05038  loss_mask_0: 0.1627  loss_dice_0: 0.1744  loss_ce_1: 0.0207  loss_mask_1: 0.169  loss_dice_1: 0.1775  loss_ce_2: 0.01818  loss_mask_2: 0.178  loss_dice_2: 0.1681  loss_ce_3: 0.02418  loss_mask_3: 0.1728  loss_dice_3: 0.1745  loss_ce_4: 0.02588  loss_mask_4: 0.1853  loss_dice_4: 0.1775  loss_ce_5: 0.02591  loss_mask_5: 0.1655  loss_dice_5: 0.1823  loss_ce_6: 0.02685  loss_mask_6: 0.1763  loss_dice_6: 0.1743  loss_ce_7: 0.02839  loss_mask_7: 0.1694  loss_dice_7: 0.1782  loss_ce_8: 0.02081  loss_mask_8: 0.1738  loss_dice_8: 0.1666  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:44] d2.utils.events INFO:  eta: 0:33:06  iter: 34299  total_loss: 4.425  loss_ce: 0.01254  loss_mask: 0.217  loss_dice: 0.1616  loss_ce_0: 0.08  loss_mask_0: 0.2084  loss_dice_0: 0.1629  loss_ce_1: 0.01811  loss_mask_1: 0.2169  loss_dice_1: 0.1653  loss_ce_2: 0.0124  loss_mask_2: 0.2146  loss_dice_2: 0.163  loss_ce_3: 0.009654  loss_mask_3: 0.2105  loss_dice_3: 0.1637  loss_ce_4: 0.01064  loss_mask_4: 0.2083  loss_dice_4: 0.1628  loss_ce_5: 0.01275  loss_mask_5: 0.2213  loss_dice_5: 0.1697  loss_ce_6: 0.01191  loss_mask_6: 0.2092  loss_dice_6: 0.1622  loss_ce_7: 0.01461  loss_mask_7: 0.213  loss_dice_7: 0.1624  loss_ce_8: 0.01307  loss_mask_8: 0.2142  loss_dice_8: 0.1607  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:47] d2.utils.events INFO:  eta: 0:33:03  iter: 34319  total_loss: 4.554  loss_ce: 0.009527  loss_mask: 0.2072  loss_dice: 0.1567  loss_ce_0: 0.07365  loss_mask_0: 0.204  loss_dice_0: 0.1619  loss_ce_1: 0.01787  loss_mask_1: 0.2072  loss_dice_1: 0.1607  loss_ce_2: 0.0128  loss_mask_2: 0.2026  loss_dice_2: 0.1598  loss_ce_3: 0.007147  loss_mask_3: 0.2056  loss_dice_3: 0.1594  loss_ce_4: 0.008172  loss_mask_4: 0.2052  loss_dice_4: 0.1598  loss_ce_5: 0.009265  loss_mask_5: 0.195  loss_dice_5: 0.1576  loss_ce_6: 0.009043  loss_mask_6: 0.1972  loss_dice_6: 0.1628  loss_ce_7: 0.01162  loss_mask_7: 0.2073  loss_dice_7: 0.1532  loss_ce_8: 0.01045  loss_mask_8: 0.2145  loss_dice_8: 0.1593  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:49] d2.utils.events INFO:  eta: 0:33:01  iter: 34339  total_loss: 4.471  loss_ce: 0.01011  loss_mask: 0.1927  loss_dice: 0.1319  loss_ce_0: 0.06998  loss_mask_0: 0.1983  loss_dice_0: 0.1348  loss_ce_1: 0.01438  loss_mask_1: 0.1916  loss_dice_1: 0.1315  loss_ce_2: 0.01196  loss_mask_2: 0.1975  loss_dice_2: 0.133  loss_ce_3: 0.007603  loss_mask_3: 0.2031  loss_dice_3: 0.1344  loss_ce_4: 0.01051  loss_mask_4: 0.2021  loss_dice_4: 0.1322  loss_ce_5: 0.009874  loss_mask_5: 0.19  loss_dice_5: 0.132  loss_ce_6: 0.009248  loss_mask_6: 0.1942  loss_dice_6: 0.1333  loss_ce_7: 0.01269  loss_mask_7: 0.1993  loss_dice_7: 0.1275  loss_ce_8: 0.01127  loss_mask_8: 0.197  loss_dice_8: 0.1294  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:52] d2.utils.events INFO:  eta: 0:32:58  iter: 34359  total_loss: 4.249  loss_ce: 0.009459  loss_mask: 0.2134  loss_dice: 0.1559  loss_ce_0: 0.06342  loss_mask_0: 0.227  loss_dice_0: 0.1569  loss_ce_1: 0.006893  loss_mask_1: 0.2252  loss_dice_1: 0.1525  loss_ce_2: 0.008108  loss_mask_2: 0.206  loss_dice_2: 0.1548  loss_ce_3: 0.007669  loss_mask_3: 0.2172  loss_dice_3: 0.1551  loss_ce_4: 0.00837  loss_mask_4: 0.2106  loss_dice_4: 0.1537  loss_ce_5: 0.009001  loss_mask_5: 0.2216  loss_dice_5: 0.1521  loss_ce_6: 0.008859  loss_mask_6: 0.218  loss_dice_6: 0.1501  loss_ce_7: 0.008903  loss_mask_7: 0.2178  loss_dice_7: 0.1539  loss_ce_8: 0.008841  loss_mask_8: 0.2261  loss_dice_8: 0.1533  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:54] d2.utils.events INFO:  eta: 0:32:56  iter: 34379  total_loss: 4.335  loss_ce: 0.006912  loss_mask: 0.1789  loss_dice: 0.2082  loss_ce_0: 0.06167  loss_mask_0: 0.164  loss_dice_0: 0.1962  loss_ce_1: 0.008961  loss_mask_1: 0.1641  loss_dice_1: 0.1993  loss_ce_2: 0.006685  loss_mask_2: 0.1652  loss_dice_2: 0.2049  loss_ce_3: 0.006316  loss_mask_3: 0.1639  loss_dice_3: 0.2069  loss_ce_4: 0.007122  loss_mask_4: 0.1694  loss_dice_4: 0.2021  loss_ce_5: 0.00695  loss_mask_5: 0.1704  loss_dice_5: 0.2018  loss_ce_6: 0.006877  loss_mask_6: 0.1647  loss_dice_6: 0.1991  loss_ce_7: 0.007776  loss_mask_7: 0.1811  loss_dice_7: 0.196  loss_ce_8: 0.007118  loss_mask_8: 0.1775  loss_dice_8: 0.2026  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:10:57] d2.utils.events INFO:  eta: 0:32:54  iter: 34399  total_loss: 4.702  loss_ce: 0.007828  loss_mask: 0.2024  loss_dice: 0.1466  loss_ce_0: 0.05942  loss_mask_0: 0.2091  loss_dice_0: 0.1458  loss_ce_1: 0.01859  loss_mask_1: 0.2055  loss_dice_1: 0.1393  loss_ce_2: 0.006993  loss_mask_2: 0.2004  loss_dice_2: 0.1418  loss_ce_3: 0.006352  loss_mask_3: 0.2018  loss_dice_3: 0.1412  loss_ce_4: 0.0065  loss_mask_4: 0.2011  loss_dice_4: 0.1507  loss_ce_5: 0.007727  loss_mask_5: 0.1925  loss_dice_5: 0.1477  loss_ce_6: 0.007809  loss_mask_6: 0.1969  loss_dice_6: 0.1487  loss_ce_7: 0.01107  loss_mask_7: 0.212  loss_dice_7: 0.1454  loss_ce_8: 0.006613  loss_mask_8: 0.1997  loss_dice_8: 0.1454  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:10:59] d2.utils.events INFO:  eta: 0:32:53  iter: 34419  total_loss: 4.497  loss_ce: 0.00789  loss_mask: 0.1853  loss_dice: 0.2006  loss_ce_0: 0.05685  loss_mask_0: 0.187  loss_dice_0: 0.1916  loss_ce_1: 0.01394  loss_mask_1: 0.1922  loss_dice_1: 0.2003  loss_ce_2: 0.008911  loss_mask_2: 0.1893  loss_dice_2: 0.1885  loss_ce_3: 0.008495  loss_mask_3: 0.1869  loss_dice_3: 0.1984  loss_ce_4: 0.008888  loss_mask_4: 0.1833  loss_dice_4: 0.1952  loss_ce_5: 0.01043  loss_mask_5: 0.1906  loss_dice_5: 0.1877  loss_ce_6: 0.01267  loss_mask_6: 0.1856  loss_dice_6: 0.1902  loss_ce_7: 0.01456  loss_mask_7: 0.1962  loss_dice_7: 0.1972  loss_ce_8: 0.00876  loss_mask_8: 0.1932  loss_dice_8: 0.2048  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:02] d2.utils.events INFO:  eta: 0:32:51  iter: 34439  total_loss: 4.79  loss_ce: 0.008771  loss_mask: 0.2373  loss_dice: 0.1742  loss_ce_0: 0.05347  loss_mask_0: 0.2283  loss_dice_0: 0.1621  loss_ce_1: 0.02852  loss_mask_1: 0.2318  loss_dice_1: 0.1661  loss_ce_2: 0.04691  loss_mask_2: 0.2303  loss_dice_2: 0.1585  loss_ce_3: 0.01004  loss_mask_3: 0.2279  loss_dice_3: 0.1577  loss_ce_4: 0.008779  loss_mask_4: 0.2362  loss_dice_4: 0.1618  loss_ce_5: 0.01986  loss_mask_5: 0.246  loss_dice_5: 0.1623  loss_ce_6: 0.01188  loss_mask_6: 0.2425  loss_dice_6: 0.1571  loss_ce_7: 0.01319  loss_mask_7: 0.2458  loss_dice_7: 0.1599  loss_ce_8: 0.02334  loss_mask_8: 0.2328  loss_dice_8: 0.1532  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:04] d2.utils.events INFO:  eta: 0:32:48  iter: 34459  total_loss: 3.77  loss_ce: 0.006145  loss_mask: 0.1705  loss_dice: 0.1599  loss_ce_0: 0.05145  loss_mask_0: 0.1613  loss_dice_0: 0.1496  loss_ce_1: 0.009943  loss_mask_1: 0.1732  loss_dice_1: 0.153  loss_ce_2: 0.006804  loss_mask_2: 0.1703  loss_dice_2: 0.1538  loss_ce_3: 0.006562  loss_mask_3: 0.1684  loss_dice_3: 0.1509  loss_ce_4: 0.006816  loss_mask_4: 0.1634  loss_dice_4: 0.1527  loss_ce_5: 0.009445  loss_mask_5: 0.1689  loss_dice_5: 0.1489  loss_ce_6: 0.008083  loss_mask_6: 0.1568  loss_dice_6: 0.1484  loss_ce_7: 0.00901  loss_mask_7: 0.1638  loss_dice_7: 0.1498  loss_ce_8: 0.006436  loss_mask_8: 0.1653  loss_dice_8: 0.1547  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:07] d2.utils.events INFO:  eta: 0:32:46  iter: 34479  total_loss: 4.239  loss_ce: 0.005421  loss_mask: 0.2008  loss_dice: 0.1681  loss_ce_0: 0.06301  loss_mask_0: 0.197  loss_dice_0: 0.1614  loss_ce_1: 0.007875  loss_mask_1: 0.1943  loss_dice_1: 0.1625  loss_ce_2: 0.005404  loss_mask_2: 0.206  loss_dice_2: 0.1696  loss_ce_3: 0.004884  loss_mask_3: 0.1977  loss_dice_3: 0.1591  loss_ce_4: 0.004994  loss_mask_4: 0.1945  loss_dice_4: 0.159  loss_ce_5: 0.006673  loss_mask_5: 0.1931  loss_dice_5: 0.1666  loss_ce_6: 0.005295  loss_mask_6: 0.1919  loss_dice_6: 0.1669  loss_ce_7: 0.005308  loss_mask_7: 0.2063  loss_dice_7: 0.1631  loss_ce_8: 0.004793  loss_mask_8: 0.2061  loss_dice_8: 0.1644  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:11:10] d2.utils.events INFO:  eta: 0:32:43  iter: 34499  total_loss: 5.847  loss_ce: 0.005902  loss_mask: 0.2321  loss_dice: 0.2153  loss_ce_0: 0.05178  loss_mask_0: 0.2302  loss_dice_0: 0.2224  loss_ce_1: 0.02738  loss_mask_1: 0.2288  loss_dice_1: 0.2188  loss_ce_2: 0.01138  loss_mask_2: 0.2249  loss_dice_2: 0.2154  loss_ce_3: 0.005162  loss_mask_3: 0.225  loss_dice_3: 0.2165  loss_ce_4: 0.005687  loss_mask_4: 0.2233  loss_dice_4: 0.2089  loss_ce_5: 0.01696  loss_mask_5: 0.227  loss_dice_5: 0.2188  loss_ce_6: 0.005492  loss_mask_6: 0.2261  loss_dice_6: 0.2167  loss_ce_7: 0.007287  loss_mask_7: 0.2272  loss_dice_7: 0.2197  loss_ce_8: 0.009568  loss_mask_8: 0.2391  loss_dice_8: 0.2232  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:12] d2.utils.events INFO:  eta: 0:32:40  iter: 34519  total_loss: 4.86  loss_ce: 0.007505  loss_mask: 0.197  loss_dice: 0.1769  loss_ce_0: 0.05143  loss_mask_0: 0.2  loss_dice_0: 0.1718  loss_ce_1: 0.03713  loss_mask_1: 0.1924  loss_dice_1: 0.1762  loss_ce_2: 0.009173  loss_mask_2: 0.2001  loss_dice_2: 0.1695  loss_ce_3: 0.007164  loss_mask_3: 0.1905  loss_dice_3: 0.1727  loss_ce_4: 0.008215  loss_mask_4: 0.1934  loss_dice_4: 0.1762  loss_ce_5: 0.009413  loss_mask_5: 0.1947  loss_dice_5: 0.1757  loss_ce_6: 0.00824  loss_mask_6: 0.1922  loss_dice_6: 0.1776  loss_ce_7: 0.0114  loss_mask_7: 0.1943  loss_dice_7: 0.1738  loss_ce_8: 0.007561  loss_mask_8: 0.188  loss_dice_8: 0.172  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:15] d2.utils.events INFO:  eta: 0:32:36  iter: 34539  total_loss: 4.598  loss_ce: 0.01859  loss_mask: 0.2267  loss_dice: 0.1513  loss_ce_0: 0.05133  loss_mask_0: 0.2201  loss_dice_0: 0.1499  loss_ce_1: 0.03075  loss_mask_1: 0.2234  loss_dice_1: 0.1518  loss_ce_2: 0.01888  loss_mask_2: 0.2175  loss_dice_2: 0.1576  loss_ce_3: 0.01581  loss_mask_3: 0.226  loss_dice_3: 0.156  loss_ce_4: 0.02205  loss_mask_4: 0.2173  loss_dice_4: 0.1548  loss_ce_5: 0.02103  loss_mask_5: 0.2183  loss_dice_5: 0.1566  loss_ce_6: 0.01396  loss_mask_6: 0.2182  loss_dice_6: 0.1551  loss_ce_7: 0.02115  loss_mask_7: 0.2265  loss_dice_7: 0.155  loss_ce_8: 0.01811  loss_mask_8: 0.2198  loss_dice_8: 0.147  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:17] d2.utils.events INFO:  eta: 0:32:33  iter: 34559  total_loss: 4.835  loss_ce: 0.01176  loss_mask: 0.2121  loss_dice: 0.1946  loss_ce_0: 0.05558  loss_mask_0: 0.2094  loss_dice_0: 0.1904  loss_ce_1: 0.01259  loss_mask_1: 0.2047  loss_dice_1: 0.1945  loss_ce_2: 0.01002  loss_mask_2: 0.2033  loss_dice_2: 0.1993  loss_ce_3: 0.01124  loss_mask_3: 0.202  loss_dice_3: 0.2018  loss_ce_4: 0.01333  loss_mask_4: 0.208  loss_dice_4: 0.2015  loss_ce_5: 0.01248  loss_mask_5: 0.2034  loss_dice_5: 0.1971  loss_ce_6: 0.01144  loss_mask_6: 0.2056  loss_dice_6: 0.2038  loss_ce_7: 0.01243  loss_mask_7: 0.2105  loss_dice_7: 0.1989  loss_ce_8: 0.01119  loss_mask_8: 0.2065  loss_dice_8: 0.1942  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:20] d2.utils.events INFO:  eta: 0:32:30  iter: 34579  total_loss: 3.956  loss_ce: 0.00747  loss_mask: 0.2124  loss_dice: 0.1329  loss_ce_0: 0.05723  loss_mask_0: 0.2183  loss_dice_0: 0.1296  loss_ce_1: 0.008001  loss_mask_1: 0.2152  loss_dice_1: 0.1328  loss_ce_2: 0.01019  loss_mask_2: 0.2237  loss_dice_2: 0.1361  loss_ce_3: 0.008754  loss_mask_3: 0.2196  loss_dice_3: 0.1321  loss_ce_4: 0.009294  loss_mask_4: 0.2217  loss_dice_4: 0.1355  loss_ce_5: 0.01223  loss_mask_5: 0.2209  loss_dice_5: 0.1269  loss_ce_6: 0.007863  loss_mask_6: 0.2156  loss_dice_6: 0.1317  loss_ce_7: 0.009396  loss_mask_7: 0.2138  loss_dice_7: 0.1309  loss_ce_8: 0.00933  loss_mask_8: 0.2145  loss_dice_8: 0.1344  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:22] d2.utils.events INFO:  eta: 0:32:29  iter: 34599  total_loss: 3.945  loss_ce: 0.009847  loss_mask: 0.1949  loss_dice: 0.1868  loss_ce_0: 0.05611  loss_mask_0: 0.1913  loss_dice_0: 0.1825  loss_ce_1: 0.01013  loss_mask_1: 0.1832  loss_dice_1: 0.1964  loss_ce_2: 0.01114  loss_mask_2: 0.1856  loss_dice_2: 0.1912  loss_ce_3: 0.01015  loss_mask_3: 0.1894  loss_dice_3: 0.1902  loss_ce_4: 0.01071  loss_mask_4: 0.1795  loss_dice_4: 0.1874  loss_ce_5: 0.01251  loss_mask_5: 0.1775  loss_dice_5: 0.1852  loss_ce_6: 0.01048  loss_mask_6: 0.188  loss_dice_6: 0.1922  loss_ce_7: 0.0109  loss_mask_7: 0.1853  loss_dice_7: 0.1888  loss_ce_8: 0.0153  loss_mask_8: 0.1841  loss_dice_8: 0.1866  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:11:25] d2.utils.events INFO:  eta: 0:32:26  iter: 34619  total_loss: 3.986  loss_ce: 0.006807  loss_mask: 0.2115  loss_dice: 0.1483  loss_ce_0: 0.05793  loss_mask_0: 0.207  loss_dice_0: 0.1434  loss_ce_1: 0.006801  loss_mask_1: 0.2093  loss_dice_1: 0.1504  loss_ce_2: 0.007783  loss_mask_2: 0.2195  loss_dice_2: 0.1527  loss_ce_3: 0.007141  loss_mask_3: 0.1949  loss_dice_3: 0.1454  loss_ce_4: 0.006138  loss_mask_4: 0.2111  loss_dice_4: 0.1454  loss_ce_5: 0.006576  loss_mask_5: 0.216  loss_dice_5: 0.1495  loss_ce_6: 0.009788  loss_mask_6: 0.2135  loss_dice_6: 0.1478  loss_ce_7: 0.009645  loss_mask_7: 0.2088  loss_dice_7: 0.146  loss_ce_8: 0.009429  loss_mask_8: 0.218  loss_dice_8: 0.15  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:11:27] d2.utils.events INFO:  eta: 0:32:24  iter: 34639  total_loss: 4.189  loss_ce: 0.008264  loss_mask: 0.2062  loss_dice: 0.1467  loss_ce_0: 0.05546  loss_mask_0: 0.1979  loss_dice_0: 0.1455  loss_ce_1: 0.01823  loss_mask_1: 0.1998  loss_dice_1: 0.1527  loss_ce_2: 0.007939  loss_mask_2: 0.1985  loss_dice_2: 0.1478  loss_ce_3: 0.00863  loss_mask_3: 0.2043  loss_dice_3: 0.1554  loss_ce_4: 0.008978  loss_mask_4: 0.2117  loss_dice_4: 0.1554  loss_ce_5: 0.008795  loss_mask_5: 0.2034  loss_dice_5: 0.1525  loss_ce_6: 0.009747  loss_mask_6: 0.1973  loss_dice_6: 0.147  loss_ce_7: 0.009913  loss_mask_7: 0.1999  loss_dice_7: 0.1515  loss_ce_8: 0.007638  loss_mask_8: 0.1988  loss_dice_8: 0.1504  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:30] d2.utils.events INFO:  eta: 0:32:20  iter: 34659  total_loss: 5.372  loss_ce: 0.007295  loss_mask: 0.1843  loss_dice: 0.1565  loss_ce_0: 0.05148  loss_mask_0: 0.1924  loss_dice_0: 0.1647  loss_ce_1: 0.005042  loss_mask_1: 0.1847  loss_dice_1: 0.1641  loss_ce_2: 0.006534  loss_mask_2: 0.1912  loss_dice_2: 0.1677  loss_ce_3: 0.0068  loss_mask_3: 0.1814  loss_dice_3: 0.159  loss_ce_4: 0.005094  loss_mask_4: 0.1826  loss_dice_4: 0.1707  loss_ce_5: 0.007967  loss_mask_5: 0.1877  loss_dice_5: 0.1624  loss_ce_6: 0.009031  loss_mask_6: 0.1869  loss_dice_6: 0.1644  loss_ce_7: 0.007654  loss_mask_7: 0.1946  loss_dice_7: 0.1785  loss_ce_8: 0.006055  loss_mask_8: 0.1884  loss_dice_8: 0.1675  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:32] d2.utils.events INFO:  eta: 0:32:15  iter: 34679  total_loss: 3.677  loss_ce: 0.006704  loss_mask: 0.1758  loss_dice: 0.1343  loss_ce_0: 0.07849  loss_mask_0: 0.1681  loss_dice_0: 0.1368  loss_ce_1: 0.006116  loss_mask_1: 0.178  loss_dice_1: 0.1321  loss_ce_2: 0.00737  loss_mask_2: 0.1632  loss_dice_2: 0.1349  loss_ce_3: 0.007682  loss_mask_3: 0.1664  loss_dice_3: 0.1294  loss_ce_4: 0.008267  loss_mask_4: 0.1696  loss_dice_4: 0.1331  loss_ce_5: 0.007411  loss_mask_5: 0.1708  loss_dice_5: 0.1312  loss_ce_6: 0.008454  loss_mask_6: 0.1673  loss_dice_6: 0.1285  loss_ce_7: 0.007595  loss_mask_7: 0.1743  loss_dice_7: 0.133  loss_ce_8: 0.006634  loss_mask_8: 0.1667  loss_dice_8: 0.1319  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:35] d2.utils.events INFO:  eta: 0:32:10  iter: 34699  total_loss: 4.241  loss_ce: 0.0271  loss_mask: 0.1822  loss_dice: 0.1726  loss_ce_0: 0.07214  loss_mask_0: 0.1857  loss_dice_0: 0.1761  loss_ce_1: 0.0337  loss_mask_1: 0.1844  loss_dice_1: 0.1769  loss_ce_2: 0.02365  loss_mask_2: 0.1922  loss_dice_2: 0.1707  loss_ce_3: 0.0293  loss_mask_3: 0.1885  loss_dice_3: 0.1776  loss_ce_4: 0.02585  loss_mask_4: 0.1829  loss_dice_4: 0.1634  loss_ce_5: 0.02678  loss_mask_5: 0.1912  loss_dice_5: 0.173  loss_ce_6: 0.02756  loss_mask_6: 0.1894  loss_dice_6: 0.1753  loss_ce_7: 0.03929  loss_mask_7: 0.1914  loss_dice_7: 0.1723  loss_ce_8: 0.03818  loss_mask_8: 0.188  loss_dice_8: 0.1797  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:37] d2.utils.events INFO:  eta: 0:32:06  iter: 34719  total_loss: 4.628  loss_ce: 0.01057  loss_mask: 0.2257  loss_dice: 0.1503  loss_ce_0: 0.0544  loss_mask_0: 0.2355  loss_dice_0: 0.1474  loss_ce_1: 0.009767  loss_mask_1: 0.2279  loss_dice_1: 0.1509  loss_ce_2: 0.01126  loss_mask_2: 0.235  loss_dice_2: 0.151  loss_ce_3: 0.01159  loss_mask_3: 0.2327  loss_dice_3: 0.1463  loss_ce_4: 0.007572  loss_mask_4: 0.2341  loss_dice_4: 0.1504  loss_ce_5: 0.009004  loss_mask_5: 0.2248  loss_dice_5: 0.1504  loss_ce_6: 0.01235  loss_mask_6: 0.2306  loss_dice_6: 0.1493  loss_ce_7: 0.01268  loss_mask_7: 0.2256  loss_dice_7: 0.1497  loss_ce_8: 0.01185  loss_mask_8: 0.2202  loss_dice_8: 0.1501  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:40] d2.utils.events INFO:  eta: 0:32:02  iter: 34739  total_loss: 4.991  loss_ce: 0.01554  loss_mask: 0.2208  loss_dice: 0.1565  loss_ce_0: 0.05142  loss_mask_0: 0.2202  loss_dice_0: 0.1564  loss_ce_1: 0.01164  loss_mask_1: 0.2246  loss_dice_1: 0.1564  loss_ce_2: 0.01175  loss_mask_2: 0.2224  loss_dice_2: 0.147  loss_ce_3: 0.01157  loss_mask_3: 0.2283  loss_dice_3: 0.1532  loss_ce_4: 0.009213  loss_mask_4: 0.2297  loss_dice_4: 0.1531  loss_ce_5: 0.0111  loss_mask_5: 0.2256  loss_dice_5: 0.1496  loss_ce_6: 0.01222  loss_mask_6: 0.2222  loss_dice_6: 0.1528  loss_ce_7: 0.01253  loss_mask_7: 0.2197  loss_dice_7: 0.1474  loss_ce_8: 0.008631  loss_mask_8: 0.228  loss_dice_8: 0.1493  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:42] d2.utils.events INFO:  eta: 0:31:58  iter: 34759  total_loss: 4.372  loss_ce: 0.007274  loss_mask: 0.1832  loss_dice: 0.18  loss_ce_0: 0.04774  loss_mask_0: 0.1779  loss_dice_0: 0.1796  loss_ce_1: 0.007112  loss_mask_1: 0.1834  loss_dice_1: 0.1872  loss_ce_2: 0.008106  loss_mask_2: 0.1816  loss_dice_2: 0.1844  loss_ce_3: 0.00779  loss_mask_3: 0.1921  loss_dice_3: 0.1798  loss_ce_4: 0.008875  loss_mask_4: 0.1906  loss_dice_4: 0.1819  loss_ce_5: 0.008565  loss_mask_5: 0.1895  loss_dice_5: 0.1802  loss_ce_6: 0.00878  loss_mask_6: 0.1781  loss_dice_6: 0.1841  loss_ce_7: 0.00885  loss_mask_7: 0.1904  loss_dice_7: 0.178  loss_ce_8: 0.006954  loss_mask_8: 0.1772  loss_dice_8: 0.177  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:45] d2.utils.events INFO:  eta: 0:31:58  iter: 34779  total_loss: 3.673  loss_ce: 0.006094  loss_mask: 0.2134  loss_dice: 0.1341  loss_ce_0: 0.07707  loss_mask_0: 0.2058  loss_dice_0: 0.1282  loss_ce_1: 0.006157  loss_mask_1: 0.1986  loss_dice_1: 0.136  loss_ce_2: 0.006991  loss_mask_2: 0.2038  loss_dice_2: 0.1341  loss_ce_3: 0.006965  loss_mask_3: 0.2006  loss_dice_3: 0.1332  loss_ce_4: 0.00714  loss_mask_4: 0.2199  loss_dice_4: 0.1355  loss_ce_5: 0.006396  loss_mask_5: 0.218  loss_dice_5: 0.1366  loss_ce_6: 0.007734  loss_mask_6: 0.2079  loss_dice_6: 0.1356  loss_ce_7: 0.007119  loss_mask_7: 0.2128  loss_dice_7: 0.1384  loss_ce_8: 0.006774  loss_mask_8: 0.2083  loss_dice_8: 0.1348  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:47] d2.utils.events INFO:  eta: 0:31:55  iter: 34799  total_loss: 4.015  loss_ce: 0.005006  loss_mask: 0.2207  loss_dice: 0.1706  loss_ce_0: 0.0691  loss_mask_0: 0.2203  loss_dice_0: 0.168  loss_ce_1: 0.004909  loss_mask_1: 0.229  loss_dice_1: 0.1664  loss_ce_2: 0.00602  loss_mask_2: 0.2188  loss_dice_2: 0.1652  loss_ce_3: 0.006336  loss_mask_3: 0.2155  loss_dice_3: 0.1694  loss_ce_4: 0.00573  loss_mask_4: 0.2179  loss_dice_4: 0.1633  loss_ce_5: 0.005559  loss_mask_5: 0.2188  loss_dice_5: 0.1681  loss_ce_6: 0.006491  loss_mask_6: 0.2212  loss_dice_6: 0.1733  loss_ce_7: 0.005095  loss_mask_7: 0.2177  loss_dice_7: 0.1672  loss_ce_8: 0.005642  loss_mask_8: 0.2129  loss_dice_8: 0.1677  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:50] d2.utils.events INFO:  eta: 0:31:55  iter: 34819  total_loss: 4.932  loss_ce: 0.00843  loss_mask: 0.1978  loss_dice: 0.2104  loss_ce_0: 0.0591  loss_mask_0: 0.1983  loss_dice_0: 0.217  loss_ce_1: 0.006738  loss_mask_1: 0.2126  loss_dice_1: 0.2207  loss_ce_2: 0.006945  loss_mask_2: 0.2051  loss_dice_2: 0.2207  loss_ce_3: 0.007715  loss_mask_3: 0.1946  loss_dice_3: 0.2032  loss_ce_4: 0.007745  loss_mask_4: 0.1932  loss_dice_4: 0.2148  loss_ce_5: 0.007226  loss_mask_5: 0.1962  loss_dice_5: 0.2117  loss_ce_6: 0.009586  loss_mask_6: 0.1984  loss_dice_6: 0.2133  loss_ce_7: 0.008417  loss_mask_7: 0.2017  loss_dice_7: 0.2168  loss_ce_8: 0.007582  loss_mask_8: 0.1984  loss_dice_8: 0.2121  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:52] d2.utils.events INFO:  eta: 0:31:54  iter: 34839  total_loss: 4.302  loss_ce: 0.006538  loss_mask: 0.188  loss_dice: 0.168  loss_ce_0: 0.05801  loss_mask_0: 0.1928  loss_dice_0: 0.173  loss_ce_1: 0.008526  loss_mask_1: 0.1901  loss_dice_1: 0.1708  loss_ce_2: 0.005821  loss_mask_2: 0.1787  loss_dice_2: 0.1675  loss_ce_3: 0.006988  loss_mask_3: 0.1849  loss_dice_3: 0.1699  loss_ce_4: 0.004865  loss_mask_4: 0.1799  loss_dice_4: 0.1667  loss_ce_5: 0.004486  loss_mask_5: 0.1955  loss_dice_5: 0.1799  loss_ce_6: 0.005682  loss_mask_6: 0.1849  loss_dice_6: 0.1736  loss_ce_7: 0.00653  loss_mask_7: 0.1907  loss_dice_7: 0.1764  loss_ce_8: 0.005664  loss_mask_8: 0.1868  loss_dice_8: 0.1675  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:11:55] d2.utils.events INFO:  eta: 0:31:52  iter: 34859  total_loss: 3.817  loss_ce: 0.008647  loss_mask: 0.1914  loss_dice: 0.1488  loss_ce_0: 0.07203  loss_mask_0: 0.1928  loss_dice_0: 0.1436  loss_ce_1: 0.01011  loss_mask_1: 0.1912  loss_dice_1: 0.145  loss_ce_2: 0.007605  loss_mask_2: 0.1973  loss_dice_2: 0.1431  loss_ce_3: 0.007927  loss_mask_3: 0.1845  loss_dice_3: 0.1397  loss_ce_4: 0.008617  loss_mask_4: 0.2  loss_dice_4: 0.141  loss_ce_5: 0.007866  loss_mask_5: 0.1945  loss_dice_5: 0.1473  loss_ce_6: 0.008484  loss_mask_6: 0.1819  loss_dice_6: 0.1386  loss_ce_7: 0.01049  loss_mask_7: 0.194  loss_dice_7: 0.1404  loss_ce_8: 0.008494  loss_mask_8: 0.1872  loss_dice_8: 0.1415  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:11:57] d2.utils.events INFO:  eta: 0:31:50  iter: 34879  total_loss: 4.059  loss_ce: 0.005919  loss_mask: 0.2098  loss_dice: 0.1533  loss_ce_0: 0.05549  loss_mask_0: 0.2061  loss_dice_0: 0.1562  loss_ce_1: 0.002977  loss_mask_1: 0.2038  loss_dice_1: 0.1525  loss_ce_2: 0.003563  loss_mask_2: 0.2089  loss_dice_2: 0.1495  loss_ce_3: 0.005296  loss_mask_3: 0.2094  loss_dice_3: 0.1524  loss_ce_4: 0.004312  loss_mask_4: 0.2159  loss_dice_4: 0.1469  loss_ce_5: 0.004029  loss_mask_5: 0.2032  loss_dice_5: 0.1524  loss_ce_6: 0.005592  loss_mask_6: 0.212  loss_dice_6: 0.1465  loss_ce_7: 0.004822  loss_mask_7: 0.2135  loss_dice_7: 0.1525  loss_ce_8: 0.005587  loss_mask_8: 0.2052  loss_dice_8: 0.151  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:12:00] d2.utils.events INFO:  eta: 0:31:49  iter: 34899  total_loss: 4.191  loss_ce: 0.005683  loss_mask: 0.1902  loss_dice: 0.1705  loss_ce_0: 0.06279  loss_mask_0: 0.2014  loss_dice_0: 0.1683  loss_ce_1: 0.005237  loss_mask_1: 0.1963  loss_dice_1: 0.1754  loss_ce_2: 0.005608  loss_mask_2: 0.2101  loss_dice_2: 0.171  loss_ce_3: 0.005697  loss_mask_3: 0.2205  loss_dice_3: 0.1762  loss_ce_4: 0.006033  loss_mask_4: 0.1985  loss_dice_4: 0.1576  loss_ce_5: 0.005737  loss_mask_5: 0.2072  loss_dice_5: 0.1725  loss_ce_6: 0.005811  loss_mask_6: 0.2003  loss_dice_6: 0.1714  loss_ce_7: 0.005432  loss_mask_7: 0.1996  loss_dice_7: 0.1708  loss_ce_8: 0.006249  loss_mask_8: 0.223  loss_dice_8: 0.1792  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:12:03] d2.utils.events INFO:  eta: 0:31:48  iter: 34919  total_loss: 4.265  loss_ce: 0.009534  loss_mask: 0.1958  loss_dice: 0.1469  loss_ce_0: 0.06216  loss_mask_0: 0.1959  loss_dice_0: 0.1477  loss_ce_1: 0.03522  loss_mask_1: 0.1888  loss_dice_1: 0.1495  loss_ce_2: 0.01229  loss_mask_2: 0.1876  loss_dice_2: 0.148  loss_ce_3: 0.00895  loss_mask_3: 0.1989  loss_dice_3: 0.1476  loss_ce_4: 0.01224  loss_mask_4: 0.1995  loss_dice_4: 0.1426  loss_ce_5: 0.008645  loss_mask_5: 0.1946  loss_dice_5: 0.1494  loss_ce_6: 0.008402  loss_mask_6: 0.1966  loss_dice_6: 0.1525  loss_ce_7: 0.01547  loss_mask_7: 0.1914  loss_dice_7: 0.1453  loss_ce_8: 0.009258  loss_mask_8: 0.1951  loss_dice_8: 0.1449  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:12:05] d2.utils.events INFO:  eta: 0:31:46  iter: 34939  total_loss: 4.178  loss_ce: 0.008303  loss_mask: 0.2127  loss_dice: 0.1667  loss_ce_0: 0.062  loss_mask_0: 0.2077  loss_dice_0: 0.154  loss_ce_1: 0.007487  loss_mask_1: 0.2068  loss_dice_1: 0.1588  loss_ce_2: 0.006587  loss_mask_2: 0.2195  loss_dice_2: 0.1628  loss_ce_3: 0.006895  loss_mask_3: 0.2105  loss_dice_3: 0.1615  loss_ce_4: 0.007432  loss_mask_4: 0.2039  loss_dice_4: 0.1572  loss_ce_5: 0.007843  loss_mask_5: 0.2185  loss_dice_5: 0.1675  loss_ce_6: 0.007275  loss_mask_6: 0.2228  loss_dice_6: 0.1649  loss_ce_7: 0.007566  loss_mask_7: 0.2093  loss_dice_7: 0.1614  loss_ce_8: 0.00765  loss_mask_8: 0.2146  loss_dice_8: 0.1693  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:12:08] d2.utils.events INFO:  eta: 0:31:44  iter: 34959  total_loss: 4.085  loss_ce: 0.002904  loss_mask: 0.2403  loss_dice: 0.1336  loss_ce_0: 0.06  loss_mask_0: 0.2464  loss_dice_0: 0.137  loss_ce_1: 0.003771  loss_mask_1: 0.243  loss_dice_1: 0.1394  loss_ce_2: 0.002591  loss_mask_2: 0.2389  loss_dice_2: 0.139  loss_ce_3: 0.002972  loss_mask_3: 0.2414  loss_dice_3: 0.1427  loss_ce_4: 0.003239  loss_mask_4: 0.245  loss_dice_4: 0.144  loss_ce_5: 0.002394  loss_mask_5: 0.24  loss_dice_5: 0.1395  loss_ce_6: 0.003149  loss_mask_6: 0.2451  loss_dice_6: 0.1421  loss_ce_7: 0.003161  loss_mask_7: 0.2403  loss_dice_7: 0.1381  loss_ce_8: 0.002943  loss_mask_8: 0.2462  loss_dice_8: 0.1418  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:12:10] d2.utils.events INFO:  eta: 0:31:43  iter: 34979  total_loss: 3.962  loss_ce: 0.00519  loss_mask: 0.2021  loss_dice: 0.1345  loss_ce_0: 0.05372  loss_mask_0: 0.2148  loss_dice_0: 0.1363  loss_ce_1: 0.007638  loss_mask_1: 0.2169  loss_dice_1: 0.1397  loss_ce_2: 0.004754  loss_mask_2: 0.2187  loss_dice_2: 0.1379  loss_ce_3: 0.005067  loss_mask_3: 0.2098  loss_dice_3: 0.1414  loss_ce_4: 0.004937  loss_mask_4: 0.215  loss_dice_4: 0.1348  loss_ce_5: 0.004432  loss_mask_5: 0.204  loss_dice_5: 0.1318  loss_ce_6: 0.004511  loss_mask_6: 0.2093  loss_dice_6: 0.1384  loss_ce_7: 0.004781  loss_mask_7: 0.2042  loss_dice_7: 0.1366  loss_ce_8: 0.004578  loss_mask_8: 0.2119  loss_dice_8: 0.1375  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:12:13] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0034999.pth
[04/13 16:12:13] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 16:12:13] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 16:12:13] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 16:12:13] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 16:12:13] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 16:12:16] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0005 s/iter. Inference: 0.0527 s/iter. Eval: 0.2302 s/iter. Total: 0.2834 s/iter. ETA=0:03:59
[04/13 16:12:21] d2.evaluation.evaluator INFO: Inference done 29/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2291 s/iter. Total: 0.2828 s/iter. ETA=0:03:53
[04/13 16:12:27] d2.evaluation.evaluator INFO: Inference done 47/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2293 s/iter. Total: 0.2832 s/iter. ETA=0:03:49
[04/13 16:12:32] d2.evaluation.evaluator INFO: Inference done 65/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2307 s/iter. Total: 0.2846 s/iter. ETA=0:03:45
[04/13 16:12:37] d2.evaluation.evaluator INFO: Inference done 83/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2305 s/iter. Total: 0.2844 s/iter. ETA=0:03:39
[04/13 16:12:42] d2.evaluation.evaluator INFO: Inference done 101/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2305 s/iter. Total: 0.2845 s/iter. ETA=0:03:34
[04/13 16:12:47] d2.evaluation.evaluator INFO: Inference done 119/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2309 s/iter. Total: 0.2848 s/iter. ETA=0:03:29
[04/13 16:12:52] d2.evaluation.evaluator INFO: Inference done 137/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2310 s/iter. Total: 0.2849 s/iter. ETA=0:03:24
[04/13 16:12:57] d2.evaluation.evaluator INFO: Inference done 155/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2309 s/iter. Total: 0.2848 s/iter. ETA=0:03:19
[04/13 16:13:03] d2.evaluation.evaluator INFO: Inference done 173/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2308 s/iter. Total: 0.2846 s/iter. ETA=0:03:14
[04/13 16:13:08] d2.evaluation.evaluator INFO: Inference done 191/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2310 s/iter. Total: 0.2848 s/iter. ETA=0:03:09
[04/13 16:13:13] d2.evaluation.evaluator INFO: Inference done 209/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2312 s/iter. Total: 0.2850 s/iter. ETA=0:03:04
[04/13 16:13:18] d2.evaluation.evaluator INFO: Inference done 227/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2313 s/iter. Total: 0.2851 s/iter. ETA=0:02:59
[04/13 16:13:23] d2.evaluation.evaluator INFO: Inference done 245/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2316 s/iter. Total: 0.2854 s/iter. ETA=0:02:54
[04/13 16:13:28] d2.evaluation.evaluator INFO: Inference done 263/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2316 s/iter. Total: 0.2855 s/iter. ETA=0:02:49
[04/13 16:13:34] d2.evaluation.evaluator INFO: Inference done 281/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2321 s/iter. Total: 0.2859 s/iter. ETA=0:02:44
[04/13 16:13:39] d2.evaluation.evaluator INFO: Inference done 299/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2322 s/iter. Total: 0.2861 s/iter. ETA=0:02:39
[04/13 16:13:44] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2323 s/iter. Total: 0.2861 s/iter. ETA=0:02:34
[04/13 16:13:49] d2.evaluation.evaluator INFO: Inference done 335/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2323 s/iter. Total: 0.2861 s/iter. ETA=0:02:29
[04/13 16:13:54] d2.evaluation.evaluator INFO: Inference done 353/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2325 s/iter. Total: 0.2863 s/iter. ETA=0:02:24
[04/13 16:14:00] d2.evaluation.evaluator INFO: Inference done 371/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2325 s/iter. Total: 0.2864 s/iter. ETA=0:02:18
[04/13 16:14:05] d2.evaluation.evaluator INFO: Inference done 389/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2326 s/iter. Total: 0.2864 s/iter. ETA=0:02:13
[04/13 16:14:10] d2.evaluation.evaluator INFO: Inference done 407/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2327 s/iter. Total: 0.2865 s/iter. ETA=0:02:08
[04/13 16:14:15] d2.evaluation.evaluator INFO: Inference done 425/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2327 s/iter. Total: 0.2865 s/iter. ETA=0:02:03
[04/13 16:14:20] d2.evaluation.evaluator INFO: Inference done 443/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2328 s/iter. Total: 0.2866 s/iter. ETA=0:01:58
[04/13 16:14:25] d2.evaluation.evaluator INFO: Inference done 461/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2328 s/iter. Total: 0.2866 s/iter. ETA=0:01:53
[04/13 16:14:31] d2.evaluation.evaluator INFO: Inference done 479/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2328 s/iter. Total: 0.2866 s/iter. ETA=0:01:48
[04/13 16:14:36] d2.evaluation.evaluator INFO: Inference done 496/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2331 s/iter. Total: 0.2869 s/iter. ETA=0:01:43
[04/13 16:14:41] d2.evaluation.evaluator INFO: Inference done 514/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2331 s/iter. Total: 0.2870 s/iter. ETA=0:01:38
[04/13 16:14:46] d2.evaluation.evaluator INFO: Inference done 532/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2332 s/iter. Total: 0.2870 s/iter. ETA=0:01:32
[04/13 16:14:51] d2.evaluation.evaluator INFO: Inference done 550/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2332 s/iter. Total: 0.2870 s/iter. ETA=0:01:27
[04/13 16:14:56] d2.evaluation.evaluator INFO: Inference done 568/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2332 s/iter. Total: 0.2871 s/iter. ETA=0:01:22
[04/13 16:15:02] d2.evaluation.evaluator INFO: Inference done 586/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2334 s/iter. Total: 0.2872 s/iter. ETA=0:01:17
[04/13 16:15:07] d2.evaluation.evaluator INFO: Inference done 604/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2334 s/iter. Total: 0.2872 s/iter. ETA=0:01:12
[04/13 16:15:12] d2.evaluation.evaluator INFO: Inference done 622/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2334 s/iter. Total: 0.2872 s/iter. ETA=0:01:07
[04/13 16:15:17] d2.evaluation.evaluator INFO: Inference done 639/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2337 s/iter. Total: 0.2875 s/iter. ETA=0:01:02
[04/13 16:15:22] d2.evaluation.evaluator INFO: Inference done 656/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2339 s/iter. Total: 0.2877 s/iter. ETA=0:00:57
[04/13 16:15:27] d2.evaluation.evaluator INFO: Inference done 673/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2341 s/iter. Total: 0.2880 s/iter. ETA=0:00:52
[04/13 16:15:32] d2.evaluation.evaluator INFO: Inference done 690/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2344 s/iter. Total: 0.2882 s/iter. ETA=0:00:47
[04/13 16:15:37] d2.evaluation.evaluator INFO: Inference done 707/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2346 s/iter. Total: 0.2884 s/iter. ETA=0:00:42
[04/13 16:15:42] d2.evaluation.evaluator INFO: Inference done 724/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2347 s/iter. Total: 0.2885 s/iter. ETA=0:00:38
[04/13 16:15:47] d2.evaluation.evaluator INFO: Inference done 741/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2350 s/iter. Total: 0.2888 s/iter. ETA=0:00:33
[04/13 16:15:52] d2.evaluation.evaluator INFO: Inference done 758/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2352 s/iter. Total: 0.2890 s/iter. ETA=0:00:28
[04/13 16:15:57] d2.evaluation.evaluator INFO: Inference done 775/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2353 s/iter. Total: 0.2892 s/iter. ETA=0:00:23
[04/13 16:16:02] d2.evaluation.evaluator INFO: Inference done 792/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2355 s/iter. Total: 0.2893 s/iter. ETA=0:00:18
[04/13 16:16:08] d2.evaluation.evaluator INFO: Inference done 810/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2356 s/iter. Total: 0.2894 s/iter. ETA=0:00:13
[04/13 16:16:13] d2.evaluation.evaluator INFO: Inference done 827/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2357 s/iter. Total: 0.2896 s/iter. ETA=0:00:08
[04/13 16:16:18] d2.evaluation.evaluator INFO: Inference done 845/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2358 s/iter. Total: 0.2896 s/iter. ETA=0:00:03
[04/13 16:16:21] d2.evaluation.evaluator INFO: Total inference time: 0:04:06.617671 (0.289797 s / iter per device, on 1 devices)
[04/13 16:16:21] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:45 (0.052924 s / iter per device, on 1 devices)
[04/13 16:16:22] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 16:16:22] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 16:16:24] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 16:16:24] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 16:16:24] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.27 seconds.
[04/13 16:16:24] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 16:16:24] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 16:16:24] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 16:16:24] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 16:16:24] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 16:16:28] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 16:16:30] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 2.02 seconds.
[04/13 16:16:30] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 16:16:30] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 16:16:30] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 76.748 | 76.748 | 76.748 |  nan  |  nan  | 76.748 |
[04/13 16:16:30] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 16:16:30] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 75.709 | defect     | 77.787 |
[04/13 16:16:30] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 16:16:30] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 16:16:30] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 16:16:30] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 16:16:30] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 16:16:30] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 16:16:30] d2.evaluation.testing INFO: copypaste: 76.7480,76.7480,76.7480,nan,nan,76.7480
[04/13 16:16:30] d2.utils.events INFO:  eta: 0:31:41  iter: 34999  total_loss: 3.705  loss_ce: 0.008224  loss_mask: 0.1758  loss_dice: 0.1777  loss_ce_0: 0.06284  loss_mask_0: 0.1847  loss_dice_0: 0.1799  loss_ce_1: 0.00986  loss_mask_1: 0.1824  loss_dice_1: 0.1888  loss_ce_2: 0.007893  loss_mask_2: 0.1825  loss_dice_2: 0.1839  loss_ce_3: 0.008113  loss_mask_3: 0.1779  loss_dice_3: 0.1896  loss_ce_4: 0.007951  loss_mask_4: 0.1802  loss_dice_4: 0.1812  loss_ce_5: 0.008192  loss_mask_5: 0.177  loss_dice_5: 0.1752  loss_ce_6: 0.007722  loss_mask_6: 0.1752  loss_dice_6: 0.1854  loss_ce_7: 0.007423  loss_mask_7: 0.1784  loss_dice_7: 0.1789  loss_ce_8: 0.007936  loss_mask_8: 0.1855  loss_dice_8: 0.183  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:16:33] d2.utils.events INFO:  eta: 0:31:39  iter: 35019  total_loss: 4.521  loss_ce: 0.007512  loss_mask: 0.2095  loss_dice: 0.1656  loss_ce_0: 0.05102  loss_mask_0: 0.2123  loss_dice_0: 0.1642  loss_ce_1: 0.009661  loss_mask_1: 0.2092  loss_dice_1: 0.1674  loss_ce_2: 0.006475  loss_mask_2: 0.2126  loss_dice_2: 0.1722  loss_ce_3: 0.008559  loss_mask_3: 0.2118  loss_dice_3: 0.1702  loss_ce_4: 0.006589  loss_mask_4: 0.2138  loss_dice_4: 0.1653  loss_ce_5: 0.005995  loss_mask_5: 0.2142  loss_dice_5: 0.163  loss_ce_6: 0.006359  loss_mask_6: 0.2141  loss_dice_6: 0.166  loss_ce_7: 0.007519  loss_mask_7: 0.2103  loss_dice_7: 0.162  loss_ce_8: 0.006627  loss_mask_8: 0.2061  loss_dice_8: 0.1692  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:16:35] d2.utils.events INFO:  eta: 0:31:36  iter: 35039  total_loss: 5.146  loss_ce: 0.04232  loss_mask: 0.1293  loss_dice: 0.2636  loss_ce_0: 0.04916  loss_mask_0: 0.1418  loss_dice_0: 0.2662  loss_ce_1: 0.02394  loss_mask_1: 0.1432  loss_dice_1: 0.2734  loss_ce_2: 0.03563  loss_mask_2: 0.1339  loss_dice_2: 0.2707  loss_ce_3: 0.03853  loss_mask_3: 0.1421  loss_dice_3: 0.2529  loss_ce_4: 0.03716  loss_mask_4: 0.1389  loss_dice_4: 0.2576  loss_ce_5: 0.04242  loss_mask_5: 0.1444  loss_dice_5: 0.242  loss_ce_6: 0.03795  loss_mask_6: 0.1464  loss_dice_6: 0.2653  loss_ce_7: 0.03739  loss_mask_7: 0.1383  loss_dice_7: 0.2623  loss_ce_8: 0.04063  loss_mask_8: 0.1364  loss_dice_8: 0.2701  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:16:38] d2.utils.events INFO:  eta: 0:31:33  iter: 35059  total_loss: 4.063  loss_ce: 0.008793  loss_mask: 0.2135  loss_dice: 0.1551  loss_ce_0: 0.04457  loss_mask_0: 0.2196  loss_dice_0: 0.1495  loss_ce_1: 0.004434  loss_mask_1: 0.2203  loss_dice_1: 0.1558  loss_ce_2: 0.004558  loss_mask_2: 0.2082  loss_dice_2: 0.1563  loss_ce_3: 0.007098  loss_mask_3: 0.2102  loss_dice_3: 0.1527  loss_ce_4: 0.005209  loss_mask_4: 0.2157  loss_dice_4: 0.1635  loss_ce_5: 0.003771  loss_mask_5: 0.2129  loss_dice_5: 0.1566  loss_ce_6: 0.00678  loss_mask_6: 0.2101  loss_dice_6: 0.1599  loss_ce_7: 0.004941  loss_mask_7: 0.2126  loss_dice_7: 0.1525  loss_ce_8: 0.003967  loss_mask_8: 0.2084  loss_dice_8: 0.1579  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:16:41] d2.utils.events INFO:  eta: 0:31:30  iter: 35079  total_loss: 4.216  loss_ce: 0.002595  loss_mask: 0.2038  loss_dice: 0.1704  loss_ce_0: 0.0438  loss_mask_0: 0.2094  loss_dice_0: 0.1772  loss_ce_1: 0.002865  loss_mask_1: 0.1978  loss_dice_1: 0.178  loss_ce_2: 0.002695  loss_mask_2: 0.1999  loss_dice_2: 0.1769  loss_ce_3: 0.003003  loss_mask_3: 0.1932  loss_dice_3: 0.1748  loss_ce_4: 0.00277  loss_mask_4: 0.2125  loss_dice_4: 0.1709  loss_ce_5: 0.002625  loss_mask_5: 0.1988  loss_dice_5: 0.1785  loss_ce_6: 0.002994  loss_mask_6: 0.195  loss_dice_6: 0.1719  loss_ce_7: 0.002277  loss_mask_7: 0.1942  loss_dice_7: 0.1821  loss_ce_8: 0.002741  loss_mask_8: 0.1983  loss_dice_8: 0.1736  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:16:43] d2.utils.events INFO:  eta: 0:31:28  iter: 35099  total_loss: 3.6  loss_ce: 0.008074  loss_mask: 0.1914  loss_dice: 0.1302  loss_ce_0: 0.07976  loss_mask_0: 0.1884  loss_dice_0: 0.1247  loss_ce_1: 0.01007  loss_mask_1: 0.1954  loss_dice_1: 0.1299  loss_ce_2: 0.008073  loss_mask_2: 0.1902  loss_dice_2: 0.1243  loss_ce_3: 0.008296  loss_mask_3: 0.1908  loss_dice_3: 0.1298  loss_ce_4: 0.008341  loss_mask_4: 0.1872  loss_dice_4: 0.1277  loss_ce_5: 0.008783  loss_mask_5: 0.2033  loss_dice_5: 0.1207  loss_ce_6: 0.008236  loss_mask_6: 0.198  loss_dice_6: 0.1277  loss_ce_7: 0.008655  loss_mask_7: 0.1862  loss_dice_7: 0.1302  loss_ce_8: 0.009044  loss_mask_8: 0.1994  loss_dice_8: 0.1272  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:16:46] d2.utils.events INFO:  eta: 0:31:26  iter: 35119  total_loss: 4.573  loss_ce: 0.006027  loss_mask: 0.2126  loss_dice: 0.1314  loss_ce_0: 0.05686  loss_mask_0: 0.2186  loss_dice_0: 0.1327  loss_ce_1: 0.007415  loss_mask_1: 0.2167  loss_dice_1: 0.1305  loss_ce_2: 0.007122  loss_mask_2: 0.2167  loss_dice_2: 0.1301  loss_ce_3: 0.008696  loss_mask_3: 0.2141  loss_dice_3: 0.1307  loss_ce_4: 0.006194  loss_mask_4: 0.2222  loss_dice_4: 0.1358  loss_ce_5: 0.006404  loss_mask_5: 0.2283  loss_dice_5: 0.1336  loss_ce_6: 0.006701  loss_mask_6: 0.2216  loss_dice_6: 0.1381  loss_ce_7: 0.008355  loss_mask_7: 0.2178  loss_dice_7: 0.1305  loss_ce_8: 0.006387  loss_mask_8: 0.2111  loss_dice_8: 0.1336  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:16:48] d2.utils.events INFO:  eta: 0:31:23  iter: 35139  total_loss: 3.956  loss_ce: 0.006305  loss_mask: 0.1773  loss_dice: 0.1331  loss_ce_0: 0.06565  loss_mask_0: 0.1674  loss_dice_0: 0.1263  loss_ce_1: 0.01458  loss_mask_1: 0.1766  loss_dice_1: 0.126  loss_ce_2: 0.01127  loss_mask_2: 0.1646  loss_dice_2: 0.1326  loss_ce_3: 0.01012  loss_mask_3: 0.1689  loss_dice_3: 0.1319  loss_ce_4: 0.009364  loss_mask_4: 0.1722  loss_dice_4: 0.1304  loss_ce_5: 0.008939  loss_mask_5: 0.1603  loss_dice_5: 0.1265  loss_ce_6: 0.008493  loss_mask_6: 0.1607  loss_dice_6: 0.1313  loss_ce_7: 0.01158  loss_mask_7: 0.1657  loss_dice_7: 0.135  loss_ce_8: 0.008997  loss_mask_8: 0.1625  loss_dice_8: 0.1311  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:16:51] d2.utils.events INFO:  eta: 0:31:22  iter: 35159  total_loss: 5.621  loss_ce: 0.01958  loss_mask: 0.2029  loss_dice: 0.1389  loss_ce_0: 0.06259  loss_mask_0: 0.1928  loss_dice_0: 0.1364  loss_ce_1: 0.02685  loss_mask_1: 0.199  loss_dice_1: 0.1431  loss_ce_2: 0.02275  loss_mask_2: 0.2042  loss_dice_2: 0.1406  loss_ce_3: 0.01184  loss_mask_3: 0.2066  loss_dice_3: 0.1397  loss_ce_4: 0.01037  loss_mask_4: 0.1975  loss_dice_4: 0.1429  loss_ce_5: 0.01637  loss_mask_5: 0.1993  loss_dice_5: 0.1364  loss_ce_6: 0.01272  loss_mask_6: 0.1944  loss_dice_6: 0.1405  loss_ce_7: 0.01445  loss_mask_7: 0.1979  loss_dice_7: 0.1424  loss_ce_8: 0.024  loss_mask_8: 0.2107  loss_dice_8: 0.1438  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:16:53] d2.utils.events INFO:  eta: 0:31:20  iter: 35179  total_loss: 4.618  loss_ce: 0.00997  loss_mask: 0.2234  loss_dice: 0.1579  loss_ce_0: 0.05687  loss_mask_0: 0.2224  loss_dice_0: 0.1489  loss_ce_1: 0.008462  loss_mask_1: 0.227  loss_dice_1: 0.1532  loss_ce_2: 0.008166  loss_mask_2: 0.2207  loss_dice_2: 0.1484  loss_ce_3: 0.0104  loss_mask_3: 0.2245  loss_dice_3: 0.1548  loss_ce_4: 0.008501  loss_mask_4: 0.2189  loss_dice_4: 0.1506  loss_ce_5: 0.008318  loss_mask_5: 0.2202  loss_dice_5: 0.1524  loss_ce_6: 0.008492  loss_mask_6: 0.2161  loss_dice_6: 0.1542  loss_ce_7: 0.009248  loss_mask_7: 0.2208  loss_dice_7: 0.1518  loss_ce_8: 0.009549  loss_mask_8: 0.2201  loss_dice_8: 0.1476  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:16:56] d2.utils.events INFO:  eta: 0:31:17  iter: 35199  total_loss: 4.725  loss_ce: 0.02157  loss_mask: 0.2081  loss_dice: 0.1904  loss_ce_0: 0.07156  loss_mask_0: 0.2091  loss_dice_0: 0.1985  loss_ce_1: 0.02035  loss_mask_1: 0.2127  loss_dice_1: 0.2073  loss_ce_2: 0.01675  loss_mask_2: 0.2224  loss_dice_2: 0.1851  loss_ce_3: 0.018  loss_mask_3: 0.2087  loss_dice_3: 0.1985  loss_ce_4: 0.01523  loss_mask_4: 0.2028  loss_dice_4: 0.184  loss_ce_5: 0.01485  loss_mask_5: 0.2089  loss_dice_5: 0.1914  loss_ce_6: 0.01576  loss_mask_6: 0.208  loss_dice_6: 0.1858  loss_ce_7: 0.01775  loss_mask_7: 0.2087  loss_dice_7: 0.2025  loss_ce_8: 0.01653  loss_mask_8: 0.2077  loss_dice_8: 0.1989  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:16:58] d2.utils.events INFO:  eta: 0:31:14  iter: 35219  total_loss: 3.926  loss_ce: 0.009624  loss_mask: 0.2027  loss_dice: 0.1576  loss_ce_0: 0.05444  loss_mask_0: 0.2071  loss_dice_0: 0.1584  loss_ce_1: 0.00741  loss_mask_1: 0.2053  loss_dice_1: 0.1552  loss_ce_2: 0.007308  loss_mask_2: 0.1982  loss_dice_2: 0.1562  loss_ce_3: 0.009595  loss_mask_3: 0.2034  loss_dice_3: 0.1521  loss_ce_4: 0.008656  loss_mask_4: 0.1974  loss_dice_4: 0.153  loss_ce_5: 0.00893  loss_mask_5: 0.2037  loss_dice_5: 0.1538  loss_ce_6: 0.009707  loss_mask_6: 0.204  loss_dice_6: 0.1547  loss_ce_7: 0.008264  loss_mask_7: 0.2011  loss_dice_7: 0.1559  loss_ce_8: 0.008346  loss_mask_8: 0.2109  loss_dice_8: 0.1597  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:01] d2.utils.events INFO:  eta: 0:31:10  iter: 35239  total_loss: 3.825  loss_ce: 0.006909  loss_mask: 0.2087  loss_dice: 0.1519  loss_ce_0: 0.06247  loss_mask_0: 0.2111  loss_dice_0: 0.1525  loss_ce_1: 0.006927  loss_mask_1: 0.2128  loss_dice_1: 0.1539  loss_ce_2: 0.006625  loss_mask_2: 0.2041  loss_dice_2: 0.1447  loss_ce_3: 0.007209  loss_mask_3: 0.2169  loss_dice_3: 0.1499  loss_ce_4: 0.007644  loss_mask_4: 0.2055  loss_dice_4: 0.1424  loss_ce_5: 0.007265  loss_mask_5: 0.2088  loss_dice_5: 0.1479  loss_ce_6: 0.007667  loss_mask_6: 0.2098  loss_dice_6: 0.15  loss_ce_7: 0.0073  loss_mask_7: 0.2077  loss_dice_7: 0.1519  loss_ce_8: 0.006994  loss_mask_8: 0.2123  loss_dice_8: 0.1535  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:03] d2.utils.events INFO:  eta: 0:31:06  iter: 35259  total_loss: 4.116  loss_ce: 0.005625  loss_mask: 0.2012  loss_dice: 0.1728  loss_ce_0: 0.07629  loss_mask_0: 0.1955  loss_dice_0: 0.1732  loss_ce_1: 0.005335  loss_mask_1: 0.1956  loss_dice_1: 0.1705  loss_ce_2: 0.005385  loss_mask_2: 0.2046  loss_dice_2: 0.1693  loss_ce_3: 0.005418  loss_mask_3: 0.1972  loss_dice_3: 0.1616  loss_ce_4: 0.005916  loss_mask_4: 0.2002  loss_dice_4: 0.1646  loss_ce_5: 0.005273  loss_mask_5: 0.1972  loss_dice_5: 0.1689  loss_ce_6: 0.005558  loss_mask_6: 0.1983  loss_dice_6: 0.1723  loss_ce_7: 0.00551  loss_mask_7: 0.1919  loss_dice_7: 0.1643  loss_ce_8: 0.004915  loss_mask_8: 0.1924  loss_dice_8: 0.1645  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:06] d2.utils.events INFO:  eta: 0:31:03  iter: 35279  total_loss: 5.463  loss_ce: 0.08537  loss_mask: 0.2027  loss_dice: 0.12  loss_ce_0: 0.05656  loss_mask_0: 0.2081  loss_dice_0: 0.1161  loss_ce_1: 0.0261  loss_mask_1: 0.2036  loss_dice_1: 0.1184  loss_ce_2: 0.04123  loss_mask_2: 0.205  loss_dice_2: 0.122  loss_ce_3: 0.06154  loss_mask_3: 0.2005  loss_dice_3: 0.1191  loss_ce_4: 0.05522  loss_mask_4: 0.2033  loss_dice_4: 0.1196  loss_ce_5: 0.05776  loss_mask_5: 0.2022  loss_dice_5: 0.1214  loss_ce_6: 0.1205  loss_mask_6: 0.2079  loss_dice_6: 0.1206  loss_ce_7: 0.06174  loss_mask_7: 0.2041  loss_dice_7: 0.1223  loss_ce_8: 0.03808  loss_mask_8: 0.2008  loss_dice_8: 0.1209  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:17:08] d2.utils.events INFO:  eta: 0:30:59  iter: 35299  total_loss: 4.42  loss_ce: 0.02009  loss_mask: 0.1978  loss_dice: 0.1571  loss_ce_0: 0.0622  loss_mask_0: 0.1956  loss_dice_0: 0.153  loss_ce_1: 0.01875  loss_mask_1: 0.203  loss_dice_1: 0.1551  loss_ce_2: 0.01762  loss_mask_2: 0.1956  loss_dice_2: 0.1567  loss_ce_3: 0.01879  loss_mask_3: 0.1881  loss_dice_3: 0.1544  loss_ce_4: 0.01656  loss_mask_4: 0.1862  loss_dice_4: 0.1466  loss_ce_5: 0.01507  loss_mask_5: 0.1896  loss_dice_5: 0.1531  loss_ce_6: 0.0174  loss_mask_6: 0.2039  loss_dice_6: 0.1567  loss_ce_7: 0.01838  loss_mask_7: 0.2047  loss_dice_7: 0.1596  loss_ce_8: 0.01877  loss_mask_8: 0.1978  loss_dice_8: 0.147  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:17:11] d2.utils.events INFO:  eta: 0:30:57  iter: 35319  total_loss: 4.644  loss_ce: 0.0148  loss_mask: 0.2283  loss_dice: 0.1587  loss_ce_0: 0.06935  loss_mask_0: 0.2384  loss_dice_0: 0.1548  loss_ce_1: 0.01557  loss_mask_1: 0.2331  loss_dice_1: 0.1574  loss_ce_2: 0.01245  loss_mask_2: 0.2363  loss_dice_2: 0.1553  loss_ce_3: 0.01209  loss_mask_3: 0.2355  loss_dice_3: 0.1596  loss_ce_4: 0.01104  loss_mask_4: 0.2369  loss_dice_4: 0.1538  loss_ce_5: 0.01225  loss_mask_5: 0.2329  loss_dice_5: 0.1586  loss_ce_6: 0.01243  loss_mask_6: 0.236  loss_dice_6: 0.156  loss_ce_7: 0.01174  loss_mask_7: 0.2315  loss_dice_7: 0.1596  loss_ce_8: 0.01469  loss_mask_8: 0.2342  loss_dice_8: 0.1578  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:17:13] d2.utils.events INFO:  eta: 0:30:53  iter: 35339  total_loss: 3.698  loss_ce: 0.01359  loss_mask: 0.1996  loss_dice: 0.144  loss_ce_0: 0.06201  loss_mask_0: 0.2059  loss_dice_0: 0.1387  loss_ce_1: 0.0154  loss_mask_1: 0.2  loss_dice_1: 0.1379  loss_ce_2: 0.01733  loss_mask_2: 0.2025  loss_dice_2: 0.1379  loss_ce_3: 0.01351  loss_mask_3: 0.2062  loss_dice_3: 0.139  loss_ce_4: 0.01178  loss_mask_4: 0.2035  loss_dice_4: 0.1404  loss_ce_5: 0.01676  loss_mask_5: 0.2087  loss_dice_5: 0.1359  loss_ce_6: 0.01287  loss_mask_6: 0.2041  loss_dice_6: 0.1416  loss_ce_7: 0.01356  loss_mask_7: 0.2054  loss_dice_7: 0.1404  loss_ce_8: 0.01427  loss_mask_8: 0.2044  loss_dice_8: 0.1409  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:17:16] d2.utils.events INFO:  eta: 0:30:49  iter: 35359  total_loss: 4.507  loss_ce: 0.0106  loss_mask: 0.168  loss_dice: 0.1666  loss_ce_0: 0.06095  loss_mask_0: 0.1656  loss_dice_0: 0.1691  loss_ce_1: 0.01074  loss_mask_1: 0.1752  loss_dice_1: 0.1712  loss_ce_2: 0.0101  loss_mask_2: 0.1733  loss_dice_2: 0.1636  loss_ce_3: 0.01007  loss_mask_3: 0.1706  loss_dice_3: 0.1627  loss_ce_4: 0.01062  loss_mask_4: 0.1684  loss_dice_4: 0.1711  loss_ce_5: 0.01064  loss_mask_5: 0.1743  loss_dice_5: 0.1656  loss_ce_6: 0.009724  loss_mask_6: 0.1665  loss_dice_6: 0.1623  loss_ce_7: 0.01143  loss_mask_7: 0.1604  loss_dice_7: 0.1663  loss_ce_8: 0.00979  loss_mask_8: 0.1611  loss_dice_8: 0.1717  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:18] d2.utils.events INFO:  eta: 0:30:47  iter: 35379  total_loss: 4.391  loss_ce: 0.01272  loss_mask: 0.1918  loss_dice: 0.1475  loss_ce_0: 0.05641  loss_mask_0: 0.1836  loss_dice_0: 0.1484  loss_ce_1: 0.01313  loss_mask_1: 0.1905  loss_dice_1: 0.1423  loss_ce_2: 0.0157  loss_mask_2: 0.1907  loss_dice_2: 0.1457  loss_ce_3: 0.01376  loss_mask_3: 0.1853  loss_dice_3: 0.154  loss_ce_4: 0.01132  loss_mask_4: 0.1851  loss_dice_4: 0.1477  loss_ce_5: 0.01129  loss_mask_5: 0.1869  loss_dice_5: 0.1509  loss_ce_6: 0.01461  loss_mask_6: 0.1836  loss_dice_6: 0.1451  loss_ce_7: 0.01244  loss_mask_7: 0.1865  loss_dice_7: 0.1469  loss_ce_8: 0.01244  loss_mask_8: 0.1877  loss_dice_8: 0.1506  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:17:21] d2.utils.events INFO:  eta: 0:30:44  iter: 35399  total_loss: 4.452  loss_ce: 0.02457  loss_mask: 0.1497  loss_dice: 0.1719  loss_ce_0: 0.05203  loss_mask_0: 0.1521  loss_dice_0: 0.179  loss_ce_1: 0.02139  loss_mask_1: 0.1579  loss_dice_1: 0.1749  loss_ce_2: 0.03067  loss_mask_2: 0.1524  loss_dice_2: 0.1767  loss_ce_3: 0.02526  loss_mask_3: 0.154  loss_dice_3: 0.172  loss_ce_4: 0.01948  loss_mask_4: 0.1591  loss_dice_4: 0.1693  loss_ce_5: 0.01964  loss_mask_5: 0.1566  loss_dice_5: 0.1797  loss_ce_6: 0.024  loss_mask_6: 0.158  loss_dice_6: 0.1746  loss_ce_7: 0.02074  loss_mask_7: 0.1469  loss_dice_7: 0.1822  loss_ce_8: 0.02518  loss_mask_8: 0.1584  loss_dice_8: 0.1717  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:17:23] d2.utils.events INFO:  eta: 0:30:42  iter: 35419  total_loss: 4.075  loss_ce: 0.007775  loss_mask: 0.2306  loss_dice: 0.1323  loss_ce_0: 0.07611  loss_mask_0: 0.2291  loss_dice_0: 0.13  loss_ce_1: 0.009151  loss_mask_1: 0.2286  loss_dice_1: 0.1309  loss_ce_2: 0.01029  loss_mask_2: 0.2285  loss_dice_2: 0.1351  loss_ce_3: 0.009695  loss_mask_3: 0.2324  loss_dice_3: 0.1326  loss_ce_4: 0.009013  loss_mask_4: 0.2336  loss_dice_4: 0.136  loss_ce_5: 0.01093  loss_mask_5: 0.236  loss_dice_5: 0.1326  loss_ce_6: 0.009302  loss_mask_6: 0.2294  loss_dice_6: 0.1305  loss_ce_7: 0.008508  loss_mask_7: 0.2259  loss_dice_7: 0.1322  loss_ce_8: 0.01216  loss_mask_8: 0.2329  loss_dice_8: 0.1314  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:26] d2.utils.events INFO:  eta: 0:30:39  iter: 35439  total_loss: 4.502  loss_ce: 0.0066  loss_mask: 0.2226  loss_dice: 0.1608  loss_ce_0: 0.06301  loss_mask_0: 0.2284  loss_dice_0: 0.1664  loss_ce_1: 0.007231  loss_mask_1: 0.2236  loss_dice_1: 0.1669  loss_ce_2: 0.007503  loss_mask_2: 0.2206  loss_dice_2: 0.166  loss_ce_3: 0.006439  loss_mask_3: 0.2193  loss_dice_3: 0.1584  loss_ce_4: 0.007168  loss_mask_4: 0.2303  loss_dice_4: 0.1611  loss_ce_5: 0.006531  loss_mask_5: 0.2284  loss_dice_5: 0.1624  loss_ce_6: 0.006466  loss_mask_6: 0.2238  loss_dice_6: 0.1685  loss_ce_7: 0.006864  loss_mask_7: 0.2347  loss_dice_7: 0.1658  loss_ce_8: 0.007404  loss_mask_8: 0.2276  loss_dice_8: 0.1609  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:28] d2.utils.events INFO:  eta: 0:30:36  iter: 35459  total_loss: 5.088  loss_ce: 0.007599  loss_mask: 0.2546  loss_dice: 0.2036  loss_ce_0: 0.07226  loss_mask_0: 0.2548  loss_dice_0: 0.2028  loss_ce_1: 0.007273  loss_mask_1: 0.2444  loss_dice_1: 0.2011  loss_ce_2: 0.007791  loss_mask_2: 0.2448  loss_dice_2: 0.1991  loss_ce_3: 0.006328  loss_mask_3: 0.256  loss_dice_3: 0.2018  loss_ce_4: 0.007128  loss_mask_4: 0.2408  loss_dice_4: 0.2011  loss_ce_5: 0.007432  loss_mask_5: 0.2496  loss_dice_5: 0.1982  loss_ce_6: 0.007563  loss_mask_6: 0.2394  loss_dice_6: 0.1979  loss_ce_7: 0.007196  loss_mask_7: 0.2368  loss_dice_7: 0.2048  loss_ce_8: 0.007506  loss_mask_8: 0.2424  loss_dice_8: 0.1938  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:31] d2.utils.events INFO:  eta: 0:30:33  iter: 35479  total_loss: 4.817  loss_ce: 0.008972  loss_mask: 0.2242  loss_dice: 0.1628  loss_ce_0: 0.06395  loss_mask_0: 0.2294  loss_dice_0: 0.1512  loss_ce_1: 0.007886  loss_mask_1: 0.2283  loss_dice_1: 0.1548  loss_ce_2: 0.009614  loss_mask_2: 0.2235  loss_dice_2: 0.1539  loss_ce_3: 0.007819  loss_mask_3: 0.2266  loss_dice_3: 0.158  loss_ce_4: 0.008616  loss_mask_4: 0.2239  loss_dice_4: 0.1535  loss_ce_5: 0.008076  loss_mask_5: 0.2252  loss_dice_5: 0.15  loss_ce_6: 0.01116  loss_mask_6: 0.2196  loss_dice_6: 0.1537  loss_ce_7: 0.009694  loss_mask_7: 0.2236  loss_dice_7: 0.1573  loss_ce_8: 0.008583  loss_mask_8: 0.227  loss_dice_8: 0.1539  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:34] d2.utils.events INFO:  eta: 0:30:31  iter: 35499  total_loss: 3.844  loss_ce: 0.01786  loss_mask: 0.2154  loss_dice: 0.1331  loss_ce_0: 0.06526  loss_mask_0: 0.2081  loss_dice_0: 0.1275  loss_ce_1: 0.01098  loss_mask_1: 0.1994  loss_dice_1: 0.1316  loss_ce_2: 0.02003  loss_mask_2: 0.2047  loss_dice_2: 0.1312  loss_ce_3: 0.02367  loss_mask_3: 0.2047  loss_dice_3: 0.1255  loss_ce_4: 0.01255  loss_mask_4: 0.2063  loss_dice_4: 0.1285  loss_ce_5: 0.0154  loss_mask_5: 0.2031  loss_dice_5: 0.1257  loss_ce_6: 0.02031  loss_mask_6: 0.2143  loss_dice_6: 0.1284  loss_ce_7: 0.01309  loss_mask_7: 0.2067  loss_dice_7: 0.1267  loss_ce_8: 0.01488  loss_mask_8: 0.2099  loss_dice_8: 0.1293  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:36] d2.utils.events INFO:  eta: 0:30:29  iter: 35519  total_loss: 4.676  loss_ce: 0.006762  loss_mask: 0.2225  loss_dice: 0.1616  loss_ce_0: 0.06569  loss_mask_0: 0.224  loss_dice_0: 0.1609  loss_ce_1: 0.007617  loss_mask_1: 0.2249  loss_dice_1: 0.1585  loss_ce_2: 0.007078  loss_mask_2: 0.2243  loss_dice_2: 0.1533  loss_ce_3: 0.006702  loss_mask_3: 0.2253  loss_dice_3: 0.1636  loss_ce_4: 0.007792  loss_mask_4: 0.2308  loss_dice_4: 0.1604  loss_ce_5: 0.008256  loss_mask_5: 0.2213  loss_dice_5: 0.1621  loss_ce_6: 0.006201  loss_mask_6: 0.2299  loss_dice_6: 0.1583  loss_ce_7: 0.006163  loss_mask_7: 0.2248  loss_dice_7: 0.1635  loss_ce_8: 0.008159  loss_mask_8: 0.22  loss_dice_8: 0.1568  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:39] d2.utils.events INFO:  eta: 0:30:26  iter: 35539  total_loss: 4.028  loss_ce: 0.003605  loss_mask: 0.2356  loss_dice: 0.1329  loss_ce_0: 0.06264  loss_mask_0: 0.2383  loss_dice_0: 0.1394  loss_ce_1: 0.004998  loss_mask_1: 0.2425  loss_dice_1: 0.14  loss_ce_2: 0.00469  loss_mask_2: 0.2435  loss_dice_2: 0.1392  loss_ce_3: 0.004155  loss_mask_3: 0.2352  loss_dice_3: 0.1363  loss_ce_4: 0.00451  loss_mask_4: 0.2447  loss_dice_4: 0.1415  loss_ce_5: 0.004881  loss_mask_5: 0.2316  loss_dice_5: 0.1361  loss_ce_6: 0.00415  loss_mask_6: 0.2391  loss_dice_6: 0.1407  loss_ce_7: 0.0041  loss_mask_7: 0.2307  loss_dice_7: 0.1399  loss_ce_8: 0.005003  loss_mask_8: 0.2439  loss_dice_8: 0.1392  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:41] d2.utils.events INFO:  eta: 0:30:25  iter: 35559  total_loss: 3.848  loss_ce: 0.005783  loss_mask: 0.1547  loss_dice: 0.1735  loss_ce_0: 0.05807  loss_mask_0: 0.1604  loss_dice_0: 0.1696  loss_ce_1: 0.00702  loss_mask_1: 0.1512  loss_dice_1: 0.1694  loss_ce_2: 0.00594  loss_mask_2: 0.1513  loss_dice_2: 0.1638  loss_ce_3: 0.007741  loss_mask_3: 0.1556  loss_dice_3: 0.1627  loss_ce_4: 0.006524  loss_mask_4: 0.1548  loss_dice_4: 0.1741  loss_ce_5: 0.006894  loss_mask_5: 0.1528  loss_dice_5: 0.1649  loss_ce_6: 0.006204  loss_mask_6: 0.1577  loss_dice_6: 0.1682  loss_ce_7: 0.005438  loss_mask_7: 0.1503  loss_dice_7: 0.1628  loss_ce_8: 0.006998  loss_mask_8: 0.1556  loss_dice_8: 0.1649  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:17:44] d2.utils.events INFO:  eta: 0:30:22  iter: 35579  total_loss: 4.149  loss_ce: 0.00491  loss_mask: 0.1952  loss_dice: 0.168  loss_ce_0: 0.07104  loss_mask_0: 0.1988  loss_dice_0: 0.1665  loss_ce_1: 0.005968  loss_mask_1: 0.1986  loss_dice_1: 0.1635  loss_ce_2: 0.005029  loss_mask_2: 0.2031  loss_dice_2: 0.1681  loss_ce_3: 0.004548  loss_mask_3: 0.1973  loss_dice_3: 0.17  loss_ce_4: 0.006106  loss_mask_4: 0.1948  loss_dice_4: 0.1573  loss_ce_5: 0.006265  loss_mask_5: 0.1927  loss_dice_5: 0.1606  loss_ce_6: 0.00531  loss_mask_6: 0.2044  loss_dice_6: 0.1689  loss_ce_7: 0.006538  loss_mask_7: 0.1984  loss_dice_7: 0.1566  loss_ce_8: 0.006537  loss_mask_8: 0.2016  loss_dice_8: 0.1704  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:46] d2.utils.events INFO:  eta: 0:30:19  iter: 35599  total_loss: 4.413  loss_ce: 0.004254  loss_mask: 0.2184  loss_dice: 0.199  loss_ce_0: 0.06992  loss_mask_0: 0.2099  loss_dice_0: 0.2025  loss_ce_1: 0.004744  loss_mask_1: 0.2186  loss_dice_1: 0.2095  loss_ce_2: 0.005917  loss_mask_2: 0.2193  loss_dice_2: 0.2139  loss_ce_3: 0.004792  loss_mask_3: 0.2176  loss_dice_3: 0.2001  loss_ce_4: 0.004984  loss_mask_4: 0.2179  loss_dice_4: 0.198  loss_ce_5: 0.005479  loss_mask_5: 0.2142  loss_dice_5: 0.1981  loss_ce_6: 0.005013  loss_mask_6: 0.2202  loss_dice_6: 0.2009  loss_ce_7: 0.005661  loss_mask_7: 0.2122  loss_dice_7: 0.2014  loss_ce_8: 0.006114  loss_mask_8: 0.2285  loss_dice_8: 0.2081  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:49] d2.utils.events INFO:  eta: 0:30:16  iter: 35619  total_loss: 4.507  loss_ce: 0.004598  loss_mask: 0.1895  loss_dice: 0.1701  loss_ce_0: 0.05665  loss_mask_0: 0.1804  loss_dice_0: 0.1707  loss_ce_1: 0.005232  loss_mask_1: 0.1913  loss_dice_1: 0.1667  loss_ce_2: 0.004465  loss_mask_2: 0.191  loss_dice_2: 0.1709  loss_ce_3: 0.004118  loss_mask_3: 0.1765  loss_dice_3: 0.1702  loss_ce_4: 0.004964  loss_mask_4: 0.1875  loss_dice_4: 0.167  loss_ce_5: 0.004334  loss_mask_5: 0.1942  loss_dice_5: 0.1728  loss_ce_6: 0.00487  loss_mask_6: 0.1937  loss_dice_6: 0.1642  loss_ce_7: 0.006165  loss_mask_7: 0.2012  loss_dice_7: 0.1692  loss_ce_8: 0.005415  loss_mask_8: 0.1797  loss_dice_8: 0.1646  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:51] d2.utils.events INFO:  eta: 0:30:13  iter: 35639  total_loss: 4.521  loss_ce: 0.007862  loss_mask: 0.2039  loss_dice: 0.1587  loss_ce_0: 0.06824  loss_mask_0: 0.206  loss_dice_0: 0.1647  loss_ce_1: 0.007644  loss_mask_1: 0.2102  loss_dice_1: 0.1526  loss_ce_2: 0.006906  loss_mask_2: 0.2092  loss_dice_2: 0.1652  loss_ce_3: 0.00807  loss_mask_3: 0.2028  loss_dice_3: 0.158  loss_ce_4: 0.006745  loss_mask_4: 0.2131  loss_dice_4: 0.1568  loss_ce_5: 0.006444  loss_mask_5: 0.2031  loss_dice_5: 0.1595  loss_ce_6: 0.007355  loss_mask_6: 0.2172  loss_dice_6: 0.1662  loss_ce_7: 0.009713  loss_mask_7: 0.2071  loss_dice_7: 0.1563  loss_ce_8: 0.007364  loss_mask_8: 0.2103  loss_dice_8: 0.1585  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:54] d2.utils.events INFO:  eta: 0:30:11  iter: 35659  total_loss: 4.249  loss_ce: 0.0157  loss_mask: 0.2037  loss_dice: 0.1561  loss_ce_0: 0.05537  loss_mask_0: 0.2063  loss_dice_0: 0.1624  loss_ce_1: 0.02094  loss_mask_1: 0.2005  loss_dice_1: 0.1627  loss_ce_2: 0.01543  loss_mask_2: 0.1985  loss_dice_2: 0.1543  loss_ce_3: 0.01672  loss_mask_3: 0.2021  loss_dice_3: 0.1563  loss_ce_4: 0.01899  loss_mask_4: 0.1987  loss_dice_4: 0.1588  loss_ce_5: 0.01442  loss_mask_5: 0.2048  loss_dice_5: 0.1589  loss_ce_6: 0.01405  loss_mask_6: 0.1992  loss_dice_6: 0.1653  loss_ce_7: 0.02035  loss_mask_7: 0.2004  loss_dice_7: 0.1536  loss_ce_8: 0.01689  loss_mask_8: 0.2076  loss_dice_8: 0.1538  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:56] d2.utils.events INFO:  eta: 0:30:10  iter: 35679  total_loss: 4.259  loss_ce: 0.01167  loss_mask: 0.2167  loss_dice: 0.1559  loss_ce_0: 0.06913  loss_mask_0: 0.2267  loss_dice_0: 0.1539  loss_ce_1: 0.01047  loss_mask_1: 0.2245  loss_dice_1: 0.1581  loss_ce_2: 0.01016  loss_mask_2: 0.2323  loss_dice_2: 0.1514  loss_ce_3: 0.01076  loss_mask_3: 0.2247  loss_dice_3: 0.1532  loss_ce_4: 0.01123  loss_mask_4: 0.2291  loss_dice_4: 0.1556  loss_ce_5: 0.01233  loss_mask_5: 0.227  loss_dice_5: 0.1556  loss_ce_6: 0.01143  loss_mask_6: 0.2252  loss_dice_6: 0.1513  loss_ce_7: 0.01157  loss_mask_7: 0.2242  loss_dice_7: 0.1555  loss_ce_8: 0.01103  loss_mask_8: 0.2223  loss_dice_8: 0.1561  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:17:59] d2.utils.events INFO:  eta: 0:30:07  iter: 35699  total_loss: 3.975  loss_ce: 0.008515  loss_mask: 0.1866  loss_dice: 0.1479  loss_ce_0: 0.06754  loss_mask_0: 0.1858  loss_dice_0: 0.1506  loss_ce_1: 0.007821  loss_mask_1: 0.1733  loss_dice_1: 0.1543  loss_ce_2: 0.008128  loss_mask_2: 0.1827  loss_dice_2: 0.1481  loss_ce_3: 0.008102  loss_mask_3: 0.1738  loss_dice_3: 0.1422  loss_ce_4: 0.008291  loss_mask_4: 0.1886  loss_dice_4: 0.1536  loss_ce_5: 0.008226  loss_mask_5: 0.1768  loss_dice_5: 0.1522  loss_ce_6: 0.008683  loss_mask_6: 0.1886  loss_dice_6: 0.1492  loss_ce_7: 0.007874  loss_mask_7: 0.1839  loss_dice_7: 0.1568  loss_ce_8: 0.008326  loss_mask_8: 0.1774  loss_dice_8: 0.1545  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:18:01] d2.utils.events INFO:  eta: 0:30:04  iter: 35719  total_loss: 4.331  loss_ce: 0.009832  loss_mask: 0.1812  loss_dice: 0.1885  loss_ce_0: 0.06232  loss_mask_0: 0.1835  loss_dice_0: 0.1945  loss_ce_1: 0.008182  loss_mask_1: 0.193  loss_dice_1: 0.1852  loss_ce_2: 0.01255  loss_mask_2: 0.1932  loss_dice_2: 0.1881  loss_ce_3: 0.009005  loss_mask_3: 0.1831  loss_dice_3: 0.1879  loss_ce_4: 0.008169  loss_mask_4: 0.1796  loss_dice_4: 0.2016  loss_ce_5: 0.008613  loss_mask_5: 0.1904  loss_dice_5: 0.1803  loss_ce_6: 0.01164  loss_mask_6: 0.1853  loss_dice_6: 0.1985  loss_ce_7: 0.009866  loss_mask_7: 0.1943  loss_dice_7: 0.1909  loss_ce_8: 0.01176  loss_mask_8: 0.1829  loss_dice_8: 0.1933  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:18:04] d2.utils.events INFO:  eta: 0:30:00  iter: 35739  total_loss: 4.472  loss_ce: 0.00827  loss_mask: 0.1875  loss_dice: 0.2092  loss_ce_0: 0.065  loss_mask_0: 0.1856  loss_dice_0: 0.2138  loss_ce_1: 0.009181  loss_mask_1: 0.1915  loss_dice_1: 0.2064  loss_ce_2: 0.01082  loss_mask_2: 0.1904  loss_dice_2: 0.2065  loss_ce_3: 0.01159  loss_mask_3: 0.1966  loss_dice_3: 0.2086  loss_ce_4: 0.009452  loss_mask_4: 0.1885  loss_dice_4: 0.2091  loss_ce_5: 0.009183  loss_mask_5: 0.1878  loss_dice_5: 0.218  loss_ce_6: 0.01117  loss_mask_6: 0.1936  loss_dice_6: 0.211  loss_ce_7: 0.008169  loss_mask_7: 0.1964  loss_dice_7: 0.2066  loss_ce_8: 0.01132  loss_mask_8: 0.193  loss_dice_8: 0.2095  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:18:06] d2.utils.events INFO:  eta: 0:29:59  iter: 35759  total_loss: 3.699  loss_ce: 0.007601  loss_mask: 0.1923  loss_dice: 0.1171  loss_ce_0: 0.06218  loss_mask_0: 0.1897  loss_dice_0: 0.1138  loss_ce_1: 0.00622  loss_mask_1: 0.182  loss_dice_1: 0.1193  loss_ce_2: 0.007378  loss_mask_2: 0.1874  loss_dice_2: 0.1172  loss_ce_3: 0.007803  loss_mask_3: 0.1768  loss_dice_3: 0.1135  loss_ce_4: 0.006778  loss_mask_4: 0.1869  loss_dice_4: 0.1154  loss_ce_5: 0.00729  loss_mask_5: 0.1872  loss_dice_5: 0.1159  loss_ce_6: 0.009191  loss_mask_6: 0.1896  loss_dice_6: 0.1136  loss_ce_7: 0.006734  loss_mask_7: 0.1835  loss_dice_7: 0.1145  loss_ce_8: 0.006824  loss_mask_8: 0.1872  loss_dice_8: 0.1167  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:18:09] d2.utils.events INFO:  eta: 0:29:57  iter: 35779  total_loss: 4.34  loss_ce: 0.007076  loss_mask: 0.2115  loss_dice: 0.1624  loss_ce_0: 0.06214  loss_mask_0: 0.2108  loss_dice_0: 0.1635  loss_ce_1: 0.007014  loss_mask_1: 0.2072  loss_dice_1: 0.16  loss_ce_2: 0.006873  loss_mask_2: 0.214  loss_dice_2: 0.158  loss_ce_3: 0.006677  loss_mask_3: 0.2076  loss_dice_3: 0.1599  loss_ce_4: 0.007084  loss_mask_4: 0.2138  loss_dice_4: 0.1581  loss_ce_5: 0.007334  loss_mask_5: 0.2095  loss_dice_5: 0.156  loss_ce_6: 0.007483  loss_mask_6: 0.2099  loss_dice_6: 0.155  loss_ce_7: 0.007431  loss_mask_7: 0.2184  loss_dice_7: 0.1602  loss_ce_8: 0.007319  loss_mask_8: 0.2237  loss_dice_8: 0.1601  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:11] d2.utils.events INFO:  eta: 0:29:55  iter: 35799  total_loss: 4.624  loss_ce: 0.009481  loss_mask: 0.2091  loss_dice: 0.1732  loss_ce_0: 0.06289  loss_mask_0: 0.1965  loss_dice_0: 0.1774  loss_ce_1: 0.01072  loss_mask_1: 0.1978  loss_dice_1: 0.1699  loss_ce_2: 0.008523  loss_mask_2: 0.2078  loss_dice_2: 0.1842  loss_ce_3: 0.009028  loss_mask_3: 0.2173  loss_dice_3: 0.1783  loss_ce_4: 0.008725  loss_mask_4: 0.2101  loss_dice_4: 0.1805  loss_ce_5: 0.009377  loss_mask_5: 0.2041  loss_dice_5: 0.1755  loss_ce_6: 0.0096  loss_mask_6: 0.2051  loss_dice_6: 0.1786  loss_ce_7: 0.009512  loss_mask_7: 0.1997  loss_dice_7: 0.1803  loss_ce_8: 0.008969  loss_mask_8: 0.2046  loss_dice_8: 0.1781  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:14] d2.utils.events INFO:  eta: 0:29:52  iter: 35819  total_loss: 4.611  loss_ce: 0.009761  loss_mask: 0.2019  loss_dice: 0.2534  loss_ce_0: 0.0613  loss_mask_0: 0.1919  loss_dice_0: 0.2424  loss_ce_1: 0.009213  loss_mask_1: 0.1967  loss_dice_1: 0.2473  loss_ce_2: 0.01075  loss_mask_2: 0.1983  loss_dice_2: 0.2514  loss_ce_3: 0.008817  loss_mask_3: 0.2058  loss_dice_3: 0.2506  loss_ce_4: 0.00788  loss_mask_4: 0.2001  loss_dice_4: 0.2477  loss_ce_5: 0.01048  loss_mask_5: 0.2054  loss_dice_5: 0.2344  loss_ce_6: 0.008921  loss_mask_6: 0.1998  loss_dice_6: 0.2535  loss_ce_7: 0.01009  loss_mask_7: 0.2092  loss_dice_7: 0.2532  loss_ce_8: 0.009747  loss_mask_8: 0.2037  loss_dice_8: 0.2533  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:16] d2.utils.events INFO:  eta: 0:29:49  iter: 35839  total_loss: 5.676  loss_ce: 0.00907  loss_mask: 0.2015  loss_dice: 0.2848  loss_ce_0: 0.06607  loss_mask_0: 0.1917  loss_dice_0: 0.266  loss_ce_1: 0.01031  loss_mask_1: 0.1998  loss_dice_1: 0.2599  loss_ce_2: 0.008289  loss_mask_2: 0.1909  loss_dice_2: 0.2544  loss_ce_3: 0.008282  loss_mask_3: 0.1966  loss_dice_3: 0.2662  loss_ce_4: 0.009199  loss_mask_4: 0.1981  loss_dice_4: 0.2718  loss_ce_5: 0.008917  loss_mask_5: 0.1948  loss_dice_5: 0.2581  loss_ce_6: 0.008575  loss_mask_6: 0.1884  loss_dice_6: 0.2638  loss_ce_7: 0.01047  loss_mask_7: 0.192  loss_dice_7: 0.2605  loss_ce_8: 0.008991  loss_mask_8: 0.187  loss_dice_8: 0.257  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:19] d2.utils.events INFO:  eta: 0:29:48  iter: 35859  total_loss: 3.699  loss_ce: 0.007552  loss_mask: 0.1996  loss_dice: 0.1501  loss_ce_0: 0.06411  loss_mask_0: 0.2039  loss_dice_0: 0.1569  loss_ce_1: 0.008527  loss_mask_1: 0.203  loss_dice_1: 0.163  loss_ce_2: 0.007144  loss_mask_2: 0.1957  loss_dice_2: 0.1609  loss_ce_3: 0.00718  loss_mask_3: 0.2008  loss_dice_3: 0.1609  loss_ce_4: 0.007689  loss_mask_4: 0.1902  loss_dice_4: 0.1628  loss_ce_5: 0.007602  loss_mask_5: 0.2012  loss_dice_5: 0.1582  loss_ce_6: 0.007482  loss_mask_6: 0.2059  loss_dice_6: 0.1465  loss_ce_7: 0.007613  loss_mask_7: 0.1974  loss_dice_7: 0.159  loss_ce_8: 0.007488  loss_mask_8: 0.1966  loss_dice_8: 0.1636  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:18:22] d2.utils.events INFO:  eta: 0:29:45  iter: 35879  total_loss: 4.193  loss_ce: 0.01027  loss_mask: 0.2045  loss_dice: 0.1745  loss_ce_0: 0.06169  loss_mask_0: 0.2088  loss_dice_0: 0.1764  loss_ce_1: 0.01019  loss_mask_1: 0.1968  loss_dice_1: 0.1666  loss_ce_2: 0.009764  loss_mask_2: 0.2124  loss_dice_2: 0.1755  loss_ce_3: 0.009208  loss_mask_3: 0.2094  loss_dice_3: 0.1754  loss_ce_4: 0.01009  loss_mask_4: 0.2098  loss_dice_4: 0.1724  loss_ce_5: 0.009893  loss_mask_5: 0.2052  loss_dice_5: 0.1719  loss_ce_6: 0.009462  loss_mask_6: 0.2  loss_dice_6: 0.1758  loss_ce_7: 0.01064  loss_mask_7: 0.2121  loss_dice_7: 0.1696  loss_ce_8: 0.01003  loss_mask_8: 0.2012  loss_dice_8: 0.1737  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:18:24] d2.utils.events INFO:  eta: 0:29:42  iter: 35899  total_loss: 5.301  loss_ce: 0.02966  loss_mask: 0.1356  loss_dice: 0.2003  loss_ce_0: 0.06133  loss_mask_0: 0.1359  loss_dice_0: 0.2013  loss_ce_1: 0.03781  loss_mask_1: 0.1272  loss_dice_1: 0.1871  loss_ce_2: 0.03732  loss_mask_2: 0.1206  loss_dice_2: 0.1889  loss_ce_3: 0.03528  loss_mask_3: 0.1268  loss_dice_3: 0.1898  loss_ce_4: 0.04709  loss_mask_4: 0.1316  loss_dice_4: 0.2003  loss_ce_5: 0.03734  loss_mask_5: 0.1227  loss_dice_5: 0.1979  loss_ce_6: 0.03138  loss_mask_6: 0.1335  loss_dice_6: 0.1916  loss_ce_7: 0.04382  loss_mask_7: 0.1202  loss_dice_7: 0.1917  loss_ce_8: 0.03209  loss_mask_8: 0.1394  loss_dice_8: 0.1905  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:27] d2.utils.events INFO:  eta: 0:29:39  iter: 35919  total_loss: 4.427  loss_ce: 0.008247  loss_mask: 0.1989  loss_dice: 0.1909  loss_ce_0: 0.05827  loss_mask_0: 0.1988  loss_dice_0: 0.1855  loss_ce_1: 0.00814  loss_mask_1: 0.1993  loss_dice_1: 0.1994  loss_ce_2: 0.0107  loss_mask_2: 0.2054  loss_dice_2: 0.1935  loss_ce_3: 0.00928  loss_mask_3: 0.2026  loss_dice_3: 0.1938  loss_ce_4: 0.008407  loss_mask_4: 0.209  loss_dice_4: 0.1947  loss_ce_5: 0.01062  loss_mask_5: 0.2002  loss_dice_5: 0.1961  loss_ce_6: 0.008714  loss_mask_6: 0.2005  loss_dice_6: 0.1931  loss_ce_7: 0.007888  loss_mask_7: 0.1944  loss_dice_7: 0.1848  loss_ce_8: 0.007242  loss_mask_8: 0.1959  loss_dice_8: 0.1939  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:29] d2.utils.events INFO:  eta: 0:29:35  iter: 35939  total_loss: 3.775  loss_ce: 0.006085  loss_mask: 0.1416  loss_dice: 0.1653  loss_ce_0: 0.05151  loss_mask_0: 0.1396  loss_dice_0: 0.1632  loss_ce_1: 0.00251  loss_mask_1: 0.1405  loss_dice_1: 0.1624  loss_ce_2: 0.006185  loss_mask_2: 0.1426  loss_dice_2: 0.1777  loss_ce_3: 0.005596  loss_mask_3: 0.1455  loss_dice_3: 0.1708  loss_ce_4: 0.005651  loss_mask_4: 0.1409  loss_dice_4: 0.1657  loss_ce_5: 0.006116  loss_mask_5: 0.139  loss_dice_5: 0.172  loss_ce_6: 0.005518  loss_mask_6: 0.1416  loss_dice_6: 0.1693  loss_ce_7: 0.004324  loss_mask_7: 0.1452  loss_dice_7: 0.1704  loss_ce_8: 0.006471  loss_mask_8: 0.1435  loss_dice_8: 0.1759  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:32] d2.utils.events INFO:  eta: 0:29:31  iter: 35959  total_loss: 4.292  loss_ce: 0.006868  loss_mask: 0.2075  loss_dice: 0.1753  loss_ce_0: 0.07604  loss_mask_0: 0.2071  loss_dice_0: 0.1714  loss_ce_1: 0.0071  loss_mask_1: 0.2012  loss_dice_1: 0.1739  loss_ce_2: 0.007063  loss_mask_2: 0.2046  loss_dice_2: 0.1718  loss_ce_3: 0.006985  loss_mask_3: 0.2035  loss_dice_3: 0.1756  loss_ce_4: 0.007811  loss_mask_4: 0.1997  loss_dice_4: 0.1699  loss_ce_5: 0.006927  loss_mask_5: 0.2092  loss_dice_5: 0.1778  loss_ce_6: 0.007068  loss_mask_6: 0.2058  loss_dice_6: 0.1731  loss_ce_7: 0.007417  loss_mask_7: 0.1993  loss_dice_7: 0.176  loss_ce_8: 0.006792  loss_mask_8: 0.1983  loss_dice_8: 0.1735  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:34] d2.utils.events INFO:  eta: 0:29:28  iter: 35979  total_loss: 4.267  loss_ce: 0.0098  loss_mask: 0.1971  loss_dice: 0.1434  loss_ce_0: 0.06617  loss_mask_0: 0.2015  loss_dice_0: 0.1391  loss_ce_1: 0.009572  loss_mask_1: 0.203  loss_dice_1: 0.1418  loss_ce_2: 0.01074  loss_mask_2: 0.2035  loss_dice_2: 0.1404  loss_ce_3: 0.01032  loss_mask_3: 0.1986  loss_dice_3: 0.1385  loss_ce_4: 0.0106  loss_mask_4: 0.2066  loss_dice_4: 0.1389  loss_ce_5: 0.009312  loss_mask_5: 0.2003  loss_dice_5: 0.1432  loss_ce_6: 0.009676  loss_mask_6: 0.192  loss_dice_6: 0.1394  loss_ce_7: 0.01082  loss_mask_7: 0.2023  loss_dice_7: 0.141  loss_ce_8: 0.009691  loss_mask_8: 0.2066  loss_dice_8: 0.1413  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:37] d2.utils.events INFO:  eta: 0:29:25  iter: 35999  total_loss: 4.112  loss_ce: 0.009898  loss_mask: 0.2051  loss_dice: 0.1526  loss_ce_0: 0.06237  loss_mask_0: 0.2082  loss_dice_0: 0.1576  loss_ce_1: 0.009577  loss_mask_1: 0.1984  loss_dice_1: 0.1586  loss_ce_2: 0.009293  loss_mask_2: 0.2015  loss_dice_2: 0.1411  loss_ce_3: 0.009483  loss_mask_3: 0.1951  loss_dice_3: 0.1536  loss_ce_4: 0.01008  loss_mask_4: 0.2002  loss_dice_4: 0.15  loss_ce_5: 0.01009  loss_mask_5: 0.2022  loss_dice_5: 0.1525  loss_ce_6: 0.009861  loss_mask_6: 0.204  loss_dice_6: 0.1521  loss_ce_7: 0.01064  loss_mask_7: 0.2007  loss_dice_7: 0.1589  loss_ce_8: 0.01005  loss_mask_8: 0.1949  loss_dice_8: 0.1517  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:39] d2.utils.events INFO:  eta: 0:29:21  iter: 36019  total_loss: 4.29  loss_ce: 0.009323  loss_mask: 0.1829  loss_dice: 0.1463  loss_ce_0: 0.05934  loss_mask_0: 0.1805  loss_dice_0: 0.147  loss_ce_1: 0.01365  loss_mask_1: 0.1791  loss_dice_1: 0.1486  loss_ce_2: 0.0091  loss_mask_2: 0.1775  loss_dice_2: 0.1487  loss_ce_3: 0.009335  loss_mask_3: 0.1794  loss_dice_3: 0.1482  loss_ce_4: 0.01034  loss_mask_4: 0.1813  loss_dice_4: 0.1441  loss_ce_5: 0.009918  loss_mask_5: 0.1844  loss_dice_5: 0.1467  loss_ce_6: 0.01022  loss_mask_6: 0.1812  loss_dice_6: 0.1448  loss_ce_7: 0.01106  loss_mask_7: 0.1842  loss_dice_7: 0.1457  loss_ce_8: 0.009999  loss_mask_8: 0.1796  loss_dice_8: 0.1459  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:42] d2.utils.events INFO:  eta: 0:29:20  iter: 36039  total_loss: 3.915  loss_ce: 0.007242  loss_mask: 0.1741  loss_dice: 0.1645  loss_ce_0: 0.05584  loss_mask_0: 0.1688  loss_dice_0: 0.1651  loss_ce_1: 0.005772  loss_mask_1: 0.1785  loss_dice_1: 0.1619  loss_ce_2: 0.006624  loss_mask_2: 0.1798  loss_dice_2: 0.1628  loss_ce_3: 0.006632  loss_mask_3: 0.1753  loss_dice_3: 0.1617  loss_ce_4: 0.006615  loss_mask_4: 0.18  loss_dice_4: 0.1623  loss_ce_5: 0.007123  loss_mask_5: 0.1696  loss_dice_5: 0.1634  loss_ce_6: 0.006931  loss_mask_6: 0.1806  loss_dice_6: 0.1704  loss_ce_7: 0.006042  loss_mask_7: 0.1686  loss_dice_7: 0.1662  loss_ce_8: 0.006938  loss_mask_8: 0.17  loss_dice_8: 0.1591  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:44] d2.utils.events INFO:  eta: 0:29:17  iter: 36059  total_loss: 4.731  loss_ce: 0.00721  loss_mask: 0.1536  loss_dice: 0.1935  loss_ce_0: 0.05525  loss_mask_0: 0.1532  loss_dice_0: 0.1964  loss_ce_1: 0.006861  loss_mask_1: 0.1664  loss_dice_1: 0.2003  loss_ce_2: 0.006724  loss_mask_2: 0.1653  loss_dice_2: 0.1912  loss_ce_3: 0.006751  loss_mask_3: 0.1631  loss_dice_3: 0.1973  loss_ce_4: 0.007349  loss_mask_4: 0.1585  loss_dice_4: 0.1911  loss_ce_5: 0.007292  loss_mask_5: 0.1648  loss_dice_5: 0.2043  loss_ce_6: 0.007006  loss_mask_6: 0.1597  loss_dice_6: 0.2087  loss_ce_7: 0.007282  loss_mask_7: 0.1734  loss_dice_7: 0.198  loss_ce_8: 0.007462  loss_mask_8: 0.1615  loss_dice_8: 0.1987  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:18:47] d2.utils.events INFO:  eta: 0:29:15  iter: 36079  total_loss: 4.713  loss_ce: 0.006333  loss_mask: 0.2043  loss_dice: 0.1854  loss_ce_0: 0.07488  loss_mask_0: 0.2051  loss_dice_0: 0.1806  loss_ce_1: 0.007769  loss_mask_1: 0.2107  loss_dice_1: 0.1902  loss_ce_2: 0.006375  loss_mask_2: 0.2095  loss_dice_2: 0.1821  loss_ce_3: 0.006038  loss_mask_3: 0.2103  loss_dice_3: 0.1891  loss_ce_4: 0.006791  loss_mask_4: 0.22  loss_dice_4: 0.1881  loss_ce_5: 0.006609  loss_mask_5: 0.2152  loss_dice_5: 0.1798  loss_ce_6: 0.006102  loss_mask_6: 0.2205  loss_dice_6: 0.1863  loss_ce_7: 0.006421  loss_mask_7: 0.2203  loss_dice_7: 0.179  loss_ce_8: 0.006583  loss_mask_8: 0.2181  loss_dice_8: 0.1861  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:50] d2.utils.events INFO:  eta: 0:29:13  iter: 36099  total_loss: 4.865  loss_ce: 0.02023  loss_mask: 0.2029  loss_dice: 0.1853  loss_ce_0: 0.05169  loss_mask_0: 0.1973  loss_dice_0: 0.1817  loss_ce_1: 0.02956  loss_mask_1: 0.2053  loss_dice_1: 0.1847  loss_ce_2: 0.02268  loss_mask_2: 0.1989  loss_dice_2: 0.1856  loss_ce_3: 0.01638  loss_mask_3: 0.2023  loss_dice_3: 0.1891  loss_ce_4: 0.02113  loss_mask_4: 0.2041  loss_dice_4: 0.1855  loss_ce_5: 0.0196  loss_mask_5: 0.2071  loss_dice_5: 0.1867  loss_ce_6: 0.01513  loss_mask_6: 0.2032  loss_dice_6: 0.1856  loss_ce_7: 0.02502  loss_mask_7: 0.1997  loss_dice_7: 0.1795  loss_ce_8: 0.0138  loss_mask_8: 0.2063  loss_dice_8: 0.1802  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:52] d2.utils.events INFO:  eta: 0:29:09  iter: 36119  total_loss: 4.722  loss_ce: 0.01998  loss_mask: 0.1941  loss_dice: 0.1495  loss_ce_0: 0.06245  loss_mask_0: 0.1999  loss_dice_0: 0.1498  loss_ce_1: 0.0129  loss_mask_1: 0.1966  loss_dice_1: 0.1458  loss_ce_2: 0.01962  loss_mask_2: 0.1989  loss_dice_2: 0.155  loss_ce_3: 0.02196  loss_mask_3: 0.2014  loss_dice_3: 0.1459  loss_ce_4: 0.01463  loss_mask_4: 0.1902  loss_dice_4: 0.1521  loss_ce_5: 0.01708  loss_mask_5: 0.2011  loss_dice_5: 0.1531  loss_ce_6: 0.01917  loss_mask_6: 0.1874  loss_dice_6: 0.1527  loss_ce_7: 0.0157  loss_mask_7: 0.2055  loss_dice_7: 0.1514  loss_ce_8: 0.01953  loss_mask_8: 0.1974  loss_dice_8: 0.148  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:55] d2.utils.events INFO:  eta: 0:29:06  iter: 36139  total_loss: 4.711  loss_ce: 0.03623  loss_mask: 0.2169  loss_dice: 0.1312  loss_ce_0: 0.05422  loss_mask_0: 0.2097  loss_dice_0: 0.1279  loss_ce_1: 0.04931  loss_mask_1: 0.2134  loss_dice_1: 0.128  loss_ce_2: 0.04545  loss_mask_2: 0.211  loss_dice_2: 0.13  loss_ce_3: 0.02768  loss_mask_3: 0.2107  loss_dice_3: 0.1274  loss_ce_4: 0.03141  loss_mask_4: 0.2089  loss_dice_4: 0.1314  loss_ce_5: 0.04426  loss_mask_5: 0.2037  loss_dice_5: 0.1276  loss_ce_6: 0.0247  loss_mask_6: 0.2113  loss_dice_6: 0.1295  loss_ce_7: 0.02453  loss_mask_7: 0.216  loss_dice_7: 0.1302  loss_ce_8: 0.0461  loss_mask_8: 0.1983  loss_dice_8: 0.1294  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:18:57] d2.utils.events INFO:  eta: 0:29:03  iter: 36159  total_loss: 4.442  loss_ce: 0.007488  loss_mask: 0.2238  loss_dice: 0.171  loss_ce_0: 0.07114  loss_mask_0: 0.2118  loss_dice_0: 0.1782  loss_ce_1: 0.006922  loss_mask_1: 0.2212  loss_dice_1: 0.1833  loss_ce_2: 0.00698  loss_mask_2: 0.2238  loss_dice_2: 0.1859  loss_ce_3: 0.007899  loss_mask_3: 0.2204  loss_dice_3: 0.1665  loss_ce_4: 0.007361  loss_mask_4: 0.2179  loss_dice_4: 0.1761  loss_ce_5: 0.007412  loss_mask_5: 0.2217  loss_dice_5: 0.1829  loss_ce_6: 0.008477  loss_mask_6: 0.2175  loss_dice_6: 0.1787  loss_ce_7: 0.007476  loss_mask_7: 0.2226  loss_dice_7: 0.1768  loss_ce_8: 0.007773  loss_mask_8: 0.2153  loss_dice_8: 0.182  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:00] d2.utils.events INFO:  eta: 0:29:00  iter: 36179  total_loss: 5.366  loss_ce: 0.01624  loss_mask: 0.1857  loss_dice: 0.2077  loss_ce_0: 0.05549  loss_mask_0: 0.1939  loss_dice_0: 0.2141  loss_ce_1: 0.06602  loss_mask_1: 0.1912  loss_dice_1: 0.2099  loss_ce_2: 0.03611  loss_mask_2: 0.1942  loss_dice_2: 0.2119  loss_ce_3: 0.01924  loss_mask_3: 0.1938  loss_dice_3: 0.2125  loss_ce_4: 0.03962  loss_mask_4: 0.1875  loss_dice_4: 0.2118  loss_ce_5: 0.01507  loss_mask_5: 0.1859  loss_dice_5: 0.2102  loss_ce_6: 0.01966  loss_mask_6: 0.1901  loss_dice_6: 0.2128  loss_ce_7: 0.03058  loss_mask_7: 0.1957  loss_dice_7: 0.206  loss_ce_8: 0.01638  loss_mask_8: 0.2003  loss_dice_8: 0.2099  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:02] d2.utils.events INFO:  eta: 0:28:57  iter: 36199  total_loss: 4.572  loss_ce: 0.04702  loss_mask: 0.18  loss_dice: 0.2121  loss_ce_0: 0.06788  loss_mask_0: 0.1863  loss_dice_0: 0.2099  loss_ce_1: 0.01873  loss_mask_1: 0.1821  loss_dice_1: 0.2156  loss_ce_2: 0.02512  loss_mask_2: 0.1759  loss_dice_2: 0.1999  loss_ce_3: 0.04873  loss_mask_3: 0.1844  loss_dice_3: 0.2144  loss_ce_4: 0.04167  loss_mask_4: 0.1768  loss_dice_4: 0.205  loss_ce_5: 0.02402  loss_mask_5: 0.1885  loss_dice_5: 0.2191  loss_ce_6: 0.04933  loss_mask_6: 0.1711  loss_dice_6: 0.2019  loss_ce_7: 0.03277  loss_mask_7: 0.179  loss_dice_7: 0.2079  loss_ce_8: 0.02132  loss_mask_8: 0.1856  loss_dice_8: 0.2202  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:05] d2.utils.events INFO:  eta: 0:28:55  iter: 36219  total_loss: 4.819  loss_ce: 0.008066  loss_mask: 0.1834  loss_dice: 0.1875  loss_ce_0: 0.06668  loss_mask_0: 0.1958  loss_dice_0: 0.1852  loss_ce_1: 0.006704  loss_mask_1: 0.1896  loss_dice_1: 0.187  loss_ce_2: 0.009379  loss_mask_2: 0.1903  loss_dice_2: 0.1791  loss_ce_3: 0.007943  loss_mask_3: 0.1916  loss_dice_3: 0.1859  loss_ce_4: 0.01026  loss_mask_4: 0.1859  loss_dice_4: 0.1867  loss_ce_5: 0.007465  loss_mask_5: 0.1914  loss_dice_5: 0.1909  loss_ce_6: 0.008471  loss_mask_6: 0.2  loss_dice_6: 0.1915  loss_ce_7: 0.009062  loss_mask_7: 0.1902  loss_dice_7: 0.1919  loss_ce_8: 0.008922  loss_mask_8: 0.1997  loss_dice_8: 0.1879  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:07] d2.utils.events INFO:  eta: 0:28:53  iter: 36239  total_loss: 5.381  loss_ce: 0.005211  loss_mask: 0.2449  loss_dice: 0.2098  loss_ce_0: 0.06342  loss_mask_0: 0.2439  loss_dice_0: 0.2184  loss_ce_1: 0.005117  loss_mask_1: 0.2441  loss_dice_1: 0.2022  loss_ce_2: 0.005504  loss_mask_2: 0.2498  loss_dice_2: 0.2092  loss_ce_3: 0.005636  loss_mask_3: 0.2369  loss_dice_3: 0.2174  loss_ce_4: 0.005529  loss_mask_4: 0.245  loss_dice_4: 0.207  loss_ce_5: 0.005108  loss_mask_5: 0.2506  loss_dice_5: 0.223  loss_ce_6: 0.005697  loss_mask_6: 0.2403  loss_dice_6: 0.2164  loss_ce_7: 0.00543  loss_mask_7: 0.2398  loss_dice_7: 0.2157  loss_ce_8: 0.004833  loss_mask_8: 0.2365  loss_dice_8: 0.2135  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:10] d2.utils.events INFO:  eta: 0:28:52  iter: 36259  total_loss: 4.135  loss_ce: 0.00487  loss_mask: 0.1924  loss_dice: 0.1584  loss_ce_0: 0.06247  loss_mask_0: 0.1961  loss_dice_0: 0.1633  loss_ce_1: 0.005159  loss_mask_1: 0.1942  loss_dice_1: 0.1575  loss_ce_2: 0.005463  loss_mask_2: 0.1936  loss_dice_2: 0.1596  loss_ce_3: 0.005692  loss_mask_3: 0.1973  loss_dice_3: 0.1546  loss_ce_4: 0.005526  loss_mask_4: 0.1871  loss_dice_4: 0.1587  loss_ce_5: 0.005327  loss_mask_5: 0.1896  loss_dice_5: 0.1614  loss_ce_6: 0.005632  loss_mask_6: 0.2096  loss_dice_6: 0.1607  loss_ce_7: 0.005246  loss_mask_7: 0.2104  loss_dice_7: 0.1671  loss_ce_8: 0.00474  loss_mask_8: 0.2117  loss_dice_8: 0.1518  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:12] d2.utils.events INFO:  eta: 0:28:49  iter: 36279  total_loss: 4.147  loss_ce: 0.004866  loss_mask: 0.2121  loss_dice: 0.147  loss_ce_0: 0.06205  loss_mask_0: 0.2109  loss_dice_0: 0.1428  loss_ce_1: 0.005274  loss_mask_1: 0.2238  loss_dice_1: 0.1478  loss_ce_2: 0.006349  loss_mask_2: 0.2058  loss_dice_2: 0.1446  loss_ce_3: 0.006403  loss_mask_3: 0.218  loss_dice_3: 0.1477  loss_ce_4: 0.005201  loss_mask_4: 0.2154  loss_dice_4: 0.1418  loss_ce_5: 0.006917  loss_mask_5: 0.2155  loss_dice_5: 0.1419  loss_ce_6: 0.005689  loss_mask_6: 0.2224  loss_dice_6: 0.143  loss_ce_7: 0.005577  loss_mask_7: 0.2242  loss_dice_7: 0.1517  loss_ce_8: 0.004463  loss_mask_8: 0.2125  loss_dice_8: 0.1452  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:15] d2.utils.events INFO:  eta: 0:28:48  iter: 36299  total_loss: 5.249  loss_ce: 0.007189  loss_mask: 0.1739  loss_dice: 0.1903  loss_ce_0: 0.06572  loss_mask_0: 0.1643  loss_dice_0: 0.1879  loss_ce_1: 0.00871  loss_mask_1: 0.1612  loss_dice_1: 0.1825  loss_ce_2: 0.02131  loss_mask_2: 0.1657  loss_dice_2: 0.1904  loss_ce_3: 0.009067  loss_mask_3: 0.1669  loss_dice_3: 0.1832  loss_ce_4: 0.01183  loss_mask_4: 0.1677  loss_dice_4: 0.1864  loss_ce_5: 0.02354  loss_mask_5: 0.1855  loss_dice_5: 0.1904  loss_ce_6: 0.007732  loss_mask_6: 0.1755  loss_dice_6: 0.1822  loss_ce_7: 0.01062  loss_mask_7: 0.1688  loss_dice_7: 0.1867  loss_ce_8: 0.007923  loss_mask_8: 0.1735  loss_dice_8: 0.1863  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:17] d2.utils.events INFO:  eta: 0:28:45  iter: 36319  total_loss: 4.262  loss_ce: 0.003752  loss_mask: 0.1757  loss_dice: 0.1814  loss_ce_0: 0.062  loss_mask_0: 0.1656  loss_dice_0: 0.1604  loss_ce_1: 0.007741  loss_mask_1: 0.1713  loss_dice_1: 0.1582  loss_ce_2: 0.005234  loss_mask_2: 0.1774  loss_dice_2: 0.1739  loss_ce_3: 0.003907  loss_mask_3: 0.1694  loss_dice_3: 0.1663  loss_ce_4: 0.003812  loss_mask_4: 0.1757  loss_dice_4: 0.1682  loss_ce_5: 0.005055  loss_mask_5: 0.1763  loss_dice_5: 0.1675  loss_ce_6: 0.003705  loss_mask_6: 0.1677  loss_dice_6: 0.1591  loss_ce_7: 0.005052  loss_mask_7: 0.1734  loss_dice_7: 0.1618  loss_ce_8: 0.003896  loss_mask_8: 0.172  loss_dice_8: 0.1691  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:20] d2.utils.events INFO:  eta: 0:28:42  iter: 36339  total_loss: 4.773  loss_ce: 0.003325  loss_mask: 0.2103  loss_dice: 0.2033  loss_ce_0: 0.06333  loss_mask_0: 0.2127  loss_dice_0: 0.1936  loss_ce_1: 0.003528  loss_mask_1: 0.2154  loss_dice_1: 0.2101  loss_ce_2: 0.003772  loss_mask_2: 0.2173  loss_dice_2: 0.2053  loss_ce_3: 0.003269  loss_mask_3: 0.2087  loss_dice_3: 0.2073  loss_ce_4: 0.003082  loss_mask_4: 0.2233  loss_dice_4: 0.2012  loss_ce_5: 0.003533  loss_mask_5: 0.2109  loss_dice_5: 0.2038  loss_ce_6: 0.003081  loss_mask_6: 0.2061  loss_dice_6: 0.2087  loss_ce_7: 0.003128  loss_mask_7: 0.2159  loss_dice_7: 0.2054  loss_ce_8: 0.003181  loss_mask_8: 0.2197  loss_dice_8: 0.1973  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:22] d2.utils.events INFO:  eta: 0:28:41  iter: 36359  total_loss: 5.278  loss_ce: 0.004862  loss_mask: 0.1889  loss_dice: 0.2285  loss_ce_0: 0.06093  loss_mask_0: 0.1929  loss_dice_0: 0.2262  loss_ce_1: 0.005183  loss_mask_1: 0.1842  loss_dice_1: 0.2306  loss_ce_2: 0.006549  loss_mask_2: 0.1878  loss_dice_2: 0.2334  loss_ce_3: 0.004435  loss_mask_3: 0.1889  loss_dice_3: 0.2357  loss_ce_4: 0.004837  loss_mask_4: 0.193  loss_dice_4: 0.2392  loss_ce_5: 0.007246  loss_mask_5: 0.1869  loss_dice_5: 0.2459  loss_ce_6: 0.004281  loss_mask_6: 0.1869  loss_dice_6: 0.2321  loss_ce_7: 0.005655  loss_mask_7: 0.191  loss_dice_7: 0.2322  loss_ce_8: 0.005445  loss_mask_8: 0.184  loss_dice_8: 0.228  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:25] d2.utils.events INFO:  eta: 0:28:41  iter: 36379  total_loss: 4.785  loss_ce: 0.01533  loss_mask: 0.1903  loss_dice: 0.1559  loss_ce_0: 0.05552  loss_mask_0: 0.1842  loss_dice_0: 0.1578  loss_ce_1: 0.04149  loss_mask_1: 0.1891  loss_dice_1: 0.1526  loss_ce_2: 0.0303  loss_mask_2: 0.1813  loss_dice_2: 0.1546  loss_ce_3: 0.01267  loss_mask_3: 0.1874  loss_dice_3: 0.1459  loss_ce_4: 0.02713  loss_mask_4: 0.1848  loss_dice_4: 0.1581  loss_ce_5: 0.02524  loss_mask_5: 0.1913  loss_dice_5: 0.156  loss_ce_6: 0.0113  loss_mask_6: 0.1835  loss_dice_6: 0.1578  loss_ce_7: 0.02641  loss_mask_7: 0.1891  loss_dice_7: 0.1504  loss_ce_8: 0.01406  loss_mask_8: 0.1893  loss_dice_8: 0.1513  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:28] d2.utils.events INFO:  eta: 0:28:39  iter: 36399  total_loss: 4.445  loss_ce: 0.004882  loss_mask: 0.1845  loss_dice: 0.1726  loss_ce_0: 0.06276  loss_mask_0: 0.1823  loss_dice_0: 0.1764  loss_ce_1: 0.005704  loss_mask_1: 0.1819  loss_dice_1: 0.169  loss_ce_2: 0.005215  loss_mask_2: 0.186  loss_dice_2: 0.1744  loss_ce_3: 0.004947  loss_mask_3: 0.1807  loss_dice_3: 0.1718  loss_ce_4: 0.005685  loss_mask_4: 0.1834  loss_dice_4: 0.1681  loss_ce_5: 0.005445  loss_mask_5: 0.1786  loss_dice_5: 0.1801  loss_ce_6: 0.004812  loss_mask_6: 0.1871  loss_dice_6: 0.1704  loss_ce_7: 0.005409  loss_mask_7: 0.1839  loss_dice_7: 0.1822  loss_ce_8: 0.004814  loss_mask_8: 0.1795  loss_dice_8: 0.1715  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:30] d2.utils.events INFO:  eta: 0:28:36  iter: 36419  total_loss: 3.804  loss_ce: 0.005042  loss_mask: 0.2108  loss_dice: 0.1218  loss_ce_0: 0.06221  loss_mask_0: 0.2073  loss_dice_0: 0.1203  loss_ce_1: 0.005243  loss_mask_1: 0.2119  loss_dice_1: 0.1203  loss_ce_2: 0.005648  loss_mask_2: 0.2149  loss_dice_2: 0.121  loss_ce_3: 0.005319  loss_mask_3: 0.2146  loss_dice_3: 0.1231  loss_ce_4: 0.005571  loss_mask_4: 0.2207  loss_dice_4: 0.1214  loss_ce_5: 0.006181  loss_mask_5: 0.2194  loss_dice_5: 0.12  loss_ce_6: 0.005059  loss_mask_6: 0.2143  loss_dice_6: 0.1177  loss_ce_7: 0.005391  loss_mask_7: 0.2116  loss_dice_7: 0.1195  loss_ce_8: 0.005188  loss_mask_8: 0.2119  loss_dice_8: 0.1221  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:33] d2.utils.events INFO:  eta: 0:28:34  iter: 36439  total_loss: 3.968  loss_ce: 0.004122  loss_mask: 0.2129  loss_dice: 0.1367  loss_ce_0: 0.06327  loss_mask_0: 0.2073  loss_dice_0: 0.1469  loss_ce_1: 0.004027  loss_mask_1: 0.2118  loss_dice_1: 0.1393  loss_ce_2: 0.004371  loss_mask_2: 0.2159  loss_dice_2: 0.1382  loss_ce_3: 0.004049  loss_mask_3: 0.2088  loss_dice_3: 0.1375  loss_ce_4: 0.004144  loss_mask_4: 0.2136  loss_dice_4: 0.1427  loss_ce_5: 0.004072  loss_mask_5: 0.207  loss_dice_5: 0.1449  loss_ce_6: 0.004079  loss_mask_6: 0.2053  loss_dice_6: 0.1401  loss_ce_7: 0.003697  loss_mask_7: 0.2182  loss_dice_7: 0.1432  loss_ce_8: 0.004069  loss_mask_8: 0.2062  loss_dice_8: 0.1373  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:35] d2.utils.events INFO:  eta: 0:28:31  iter: 36459  total_loss: 5.05  loss_ce: 0.004644  loss_mask: 0.1684  loss_dice: 0.1882  loss_ce_0: 0.06337  loss_mask_0: 0.1751  loss_dice_0: 0.1883  loss_ce_1: 0.02681  loss_mask_1: 0.1831  loss_dice_1: 0.1834  loss_ce_2: 0.01822  loss_mask_2: 0.1807  loss_dice_2: 0.1839  loss_ce_3: 0.004029  loss_mask_3: 0.1764  loss_dice_3: 0.1865  loss_ce_4: 0.01585  loss_mask_4: 0.1769  loss_dice_4: 0.1917  loss_ce_5: 0.01109  loss_mask_5: 0.1775  loss_dice_5: 0.1978  loss_ce_6: 0.003992  loss_mask_6: 0.1795  loss_dice_6: 0.1899  loss_ce_7: 0.01643  loss_mask_7: 0.1721  loss_dice_7: 0.189  loss_ce_8: 0.005107  loss_mask_8: 0.1761  loss_dice_8: 0.1842  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:38] d2.utils.events INFO:  eta: 0:28:29  iter: 36479  total_loss: 4.8  loss_ce: 0.005586  loss_mask: 0.1871  loss_dice: 0.19  loss_ce_0: 0.06182  loss_mask_0: 0.1901  loss_dice_0: 0.194  loss_ce_1: 0.007825  loss_mask_1: 0.2018  loss_dice_1: 0.197  loss_ce_2: 0.006345  loss_mask_2: 0.1915  loss_dice_2: 0.1806  loss_ce_3: 0.006102  loss_mask_3: 0.2009  loss_dice_3: 0.1971  loss_ce_4: 0.006391  loss_mask_4: 0.1851  loss_dice_4: 0.1911  loss_ce_5: 0.006275  loss_mask_5: 0.1964  loss_dice_5: 0.2035  loss_ce_6: 0.006386  loss_mask_6: 0.1949  loss_dice_6: 0.1873  loss_ce_7: 0.007369  loss_mask_7: 0.197  loss_dice_7: 0.193  loss_ce_8: 0.0062  loss_mask_8: 0.186  loss_dice_8: 0.1951  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:40] d2.utils.events INFO:  eta: 0:28:23  iter: 36499  total_loss: 4.184  loss_ce: 0.005968  loss_mask: 0.1846  loss_dice: 0.1553  loss_ce_0: 0.06198  loss_mask_0: 0.1889  loss_dice_0: 0.1644  loss_ce_1: 0.006154  loss_mask_1: 0.1892  loss_dice_1: 0.1652  loss_ce_2: 0.006554  loss_mask_2: 0.1959  loss_dice_2: 0.1636  loss_ce_3: 0.006369  loss_mask_3: 0.1985  loss_dice_3: 0.1656  loss_ce_4: 0.005564  loss_mask_4: 0.1905  loss_dice_4: 0.1663  loss_ce_5: 0.006548  loss_mask_5: 0.1866  loss_dice_5: 0.1608  loss_ce_6: 0.006343  loss_mask_6: 0.1806  loss_dice_6: 0.1633  loss_ce_7: 0.006474  loss_mask_7: 0.1971  loss_dice_7: 0.1645  loss_ce_8: 0.006805  loss_mask_8: 0.1971  loss_dice_8: 0.1605  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:43] d2.utils.events INFO:  eta: 0:28:20  iter: 36519  total_loss: 4.328  loss_ce: 0.004984  loss_mask: 0.2016  loss_dice: 0.149  loss_ce_0: 0.0623  loss_mask_0: 0.2169  loss_dice_0: 0.1533  loss_ce_1: 0.005645  loss_mask_1: 0.2094  loss_dice_1: 0.1556  loss_ce_2: 0.005332  loss_mask_2: 0.2058  loss_dice_2: 0.1495  loss_ce_3: 0.005292  loss_mask_3: 0.2063  loss_dice_3: 0.1539  loss_ce_4: 0.005353  loss_mask_4: 0.2157  loss_dice_4: 0.1491  loss_ce_5: 0.005103  loss_mask_5: 0.2163  loss_dice_5: 0.152  loss_ce_6: 0.005195  loss_mask_6: 0.2093  loss_dice_6: 0.156  loss_ce_7: 0.00575  loss_mask_7: 0.2057  loss_dice_7: 0.154  loss_ce_8: 0.005204  loss_mask_8: 0.2148  loss_dice_8: 0.153  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:45] d2.utils.events INFO:  eta: 0:28:19  iter: 36539  total_loss: 4.508  loss_ce: 0.01891  loss_mask: 0.213  loss_dice: 0.1491  loss_ce_0: 0.06208  loss_mask_0: 0.2127  loss_dice_0: 0.1577  loss_ce_1: 0.0296  loss_mask_1: 0.2181  loss_dice_1: 0.148  loss_ce_2: 0.03757  loss_mask_2: 0.2121  loss_dice_2: 0.1497  loss_ce_3: 0.0184  loss_mask_3: 0.2036  loss_dice_3: 0.1472  loss_ce_4: 0.02584  loss_mask_4: 0.2102  loss_dice_4: 0.1509  loss_ce_5: 0.04748  loss_mask_5: 0.2104  loss_dice_5: 0.15  loss_ce_6: 0.01111  loss_mask_6: 0.2107  loss_dice_6: 0.1534  loss_ce_7: 0.02242  loss_mask_7: 0.2089  loss_dice_7: 0.1536  loss_ce_8: 0.03666  loss_mask_8: 0.2214  loss_dice_8: 0.1508  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:19:48] d2.utils.events INFO:  eta: 0:28:19  iter: 36559  total_loss: 3.9  loss_ce: 0.0067  loss_mask: 0.1822  loss_dice: 0.1375  loss_ce_0: 0.05717  loss_mask_0: 0.1882  loss_dice_0: 0.1421  loss_ce_1: 0.01129  loss_mask_1: 0.1885  loss_dice_1: 0.149  loss_ce_2: 0.02126  loss_mask_2: 0.1901  loss_dice_2: 0.1435  loss_ce_3: 0.01639  loss_mask_3: 0.1822  loss_dice_3: 0.1483  loss_ce_4: 0.01326  loss_mask_4: 0.1823  loss_dice_4: 0.1448  loss_ce_5: 0.01399  loss_mask_5: 0.1869  loss_dice_5: 0.1388  loss_ce_6: 0.006631  loss_mask_6: 0.1893  loss_dice_6: 0.1457  loss_ce_7: 0.009667  loss_mask_7: 0.1847  loss_dice_7: 0.1427  loss_ce_8: 0.0148  loss_mask_8: 0.1865  loss_dice_8: 0.14  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:50] d2.utils.events INFO:  eta: 0:28:18  iter: 36579  total_loss: 4.299  loss_ce: 0.005136  loss_mask: 0.1894  loss_dice: 0.1589  loss_ce_0: 0.06236  loss_mask_0: 0.2021  loss_dice_0: 0.1625  loss_ce_1: 0.007049  loss_mask_1: 0.1959  loss_dice_1: 0.1661  loss_ce_2: 0.006646  loss_mask_2: 0.1966  loss_dice_2: 0.1666  loss_ce_3: 0.005639  loss_mask_3: 0.2007  loss_dice_3: 0.1649  loss_ce_4: 0.006737  loss_mask_4: 0.184  loss_dice_4: 0.1628  loss_ce_5: 0.006806  loss_mask_5: 0.1899  loss_dice_5: 0.1666  loss_ce_6: 0.005418  loss_mask_6: 0.1866  loss_dice_6: 0.155  loss_ce_7: 0.007405  loss_mask_7: 0.1866  loss_dice_7: 0.1622  loss_ce_8: 0.00745  loss_mask_8: 0.1866  loss_dice_8: 0.1649  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:53] d2.utils.events INFO:  eta: 0:28:16  iter: 36599  total_loss: 3.7  loss_ce: 0.004136  loss_mask: 0.2088  loss_dice: 0.1466  loss_ce_0: 0.05425  loss_mask_0: 0.2016  loss_dice_0: 0.1475  loss_ce_1: 0.004062  loss_mask_1: 0.2157  loss_dice_1: 0.1524  loss_ce_2: 0.004883  loss_mask_2: 0.2077  loss_dice_2: 0.1421  loss_ce_3: 0.004623  loss_mask_3: 0.209  loss_dice_3: 0.1449  loss_ce_4: 0.004663  loss_mask_4: 0.2036  loss_dice_4: 0.1484  loss_ce_5: 0.004567  loss_mask_5: 0.1975  loss_dice_5: 0.1478  loss_ce_6: 0.004801  loss_mask_6: 0.2107  loss_dice_6: 0.1494  loss_ce_7: 0.004464  loss_mask_7: 0.1966  loss_dice_7: 0.144  loss_ce_8: 0.004546  loss_mask_8: 0.204  loss_dice_8: 0.1448  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:56] d2.utils.events INFO:  eta: 0:28:13  iter: 36619  total_loss: 4.945  loss_ce: 0.005056  loss_mask: 0.1603  loss_dice: 0.1259  loss_ce_0: 0.0697  loss_mask_0: 0.1539  loss_dice_0: 0.1267  loss_ce_1: 0.008261  loss_mask_1: 0.1628  loss_dice_1: 0.1316  loss_ce_2: 0.005497  loss_mask_2: 0.1551  loss_dice_2: 0.1277  loss_ce_3: 0.005314  loss_mask_3: 0.1488  loss_dice_3: 0.1298  loss_ce_4: 0.00553  loss_mask_4: 0.1477  loss_dice_4: 0.1271  loss_ce_5: 0.004967  loss_mask_5: 0.1517  loss_dice_5: 0.1268  loss_ce_6: 0.004956  loss_mask_6: 0.1525  loss_dice_6: 0.134  loss_ce_7: 0.006756  loss_mask_7: 0.1603  loss_dice_7: 0.1293  loss_ce_8: 0.005886  loss_mask_8: 0.1569  loss_dice_8: 0.1301  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:19:58] d2.utils.events INFO:  eta: 0:28:11  iter: 36639  total_loss: 4.6  loss_ce: 0.0275  loss_mask: 0.2159  loss_dice: 0.1508  loss_ce_0: 0.05438  loss_mask_0: 0.2169  loss_dice_0: 0.1548  loss_ce_1: 0.02014  loss_mask_1: 0.2104  loss_dice_1: 0.1579  loss_ce_2: 0.01776  loss_mask_2: 0.2062  loss_dice_2: 0.166  loss_ce_3: 0.02091  loss_mask_3: 0.2107  loss_dice_3: 0.1581  loss_ce_4: 0.02395  loss_mask_4: 0.2068  loss_dice_4: 0.1589  loss_ce_5: 0.01596  loss_mask_5: 0.2121  loss_dice_5: 0.1599  loss_ce_6: 0.02064  loss_mask_6: 0.2149  loss_dice_6: 0.1607  loss_ce_7: 0.02275  loss_mask_7: 0.2066  loss_dice_7: 0.1615  loss_ce_8: 0.02138  loss_mask_8: 0.2077  loss_dice_8: 0.1528  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:01] d2.utils.events INFO:  eta: 0:28:09  iter: 36659  total_loss: 4.274  loss_ce: 0.04079  loss_mask: 0.1922  loss_dice: 0.1415  loss_ce_0: 0.07543  loss_mask_0: 0.1977  loss_dice_0: 0.1363  loss_ce_1: 0.06539  loss_mask_1: 0.1993  loss_dice_1: 0.1317  loss_ce_2: 0.04999  loss_mask_2: 0.1945  loss_dice_2: 0.1383  loss_ce_3: 0.02752  loss_mask_3: 0.1977  loss_dice_3: 0.1348  loss_ce_4: 0.06285  loss_mask_4: 0.1986  loss_dice_4: 0.1437  loss_ce_5: 0.04286  loss_mask_5: 0.1939  loss_dice_5: 0.1367  loss_ce_6: 0.03909  loss_mask_6: 0.1885  loss_dice_6: 0.1358  loss_ce_7: 0.07899  loss_mask_7: 0.1895  loss_dice_7: 0.1343  loss_ce_8: 0.04244  loss_mask_8: 0.203  loss_dice_8: 0.1358  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:03] d2.utils.events INFO:  eta: 0:28:07  iter: 36679  total_loss: 5.617  loss_ce: 0.01773  loss_mask: 0.2151  loss_dice: 0.1562  loss_ce_0: 0.07579  loss_mask_0: 0.2199  loss_dice_0: 0.1559  loss_ce_1: 0.02496  loss_mask_1: 0.2214  loss_dice_1: 0.1568  loss_ce_2: 0.01852  loss_mask_2: 0.2087  loss_dice_2: 0.1594  loss_ce_3: 0.01646  loss_mask_3: 0.2105  loss_dice_3: 0.1568  loss_ce_4: 0.03483  loss_mask_4: 0.2305  loss_dice_4: 0.1562  loss_ce_5: 0.01777  loss_mask_5: 0.2257  loss_dice_5: 0.1643  loss_ce_6: 0.02105  loss_mask_6: 0.2228  loss_dice_6: 0.1624  loss_ce_7: 0.02987  loss_mask_7: 0.2304  loss_dice_7: 0.1636  loss_ce_8: 0.01988  loss_mask_8: 0.2266  loss_dice_8: 0.1622  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:06] d2.utils.events INFO:  eta: 0:28:06  iter: 36699  total_loss: 4.277  loss_ce: 0.009911  loss_mask: 0.2243  loss_dice: 0.1663  loss_ce_0: 0.06628  loss_mask_0: 0.2191  loss_dice_0: 0.1619  loss_ce_1: 0.01242  loss_mask_1: 0.2265  loss_dice_1: 0.1738  loss_ce_2: 0.01395  loss_mask_2: 0.2219  loss_dice_2: 0.1628  loss_ce_3: 0.0073  loss_mask_3: 0.2174  loss_dice_3: 0.1657  loss_ce_4: 0.008831  loss_mask_4: 0.2149  loss_dice_4: 0.1697  loss_ce_5: 0.01056  loss_mask_5: 0.2177  loss_dice_5: 0.173  loss_ce_6: 0.006996  loss_mask_6: 0.2258  loss_dice_6: 0.1647  loss_ce_7: 0.01268  loss_mask_7: 0.2307  loss_dice_7: 0.172  loss_ce_8: 0.01151  loss_mask_8: 0.2119  loss_dice_8: 0.1654  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:08] d2.utils.events INFO:  eta: 0:28:04  iter: 36719  total_loss: 4.614  loss_ce: 0.01178  loss_mask: 0.2145  loss_dice: 0.1645  loss_ce_0: 0.06443  loss_mask_0: 0.2004  loss_dice_0: 0.1633  loss_ce_1: 0.01368  loss_mask_1: 0.205  loss_dice_1: 0.1558  loss_ce_2: 0.01306  loss_mask_2: 0.1925  loss_dice_2: 0.1605  loss_ce_3: 0.009666  loss_mask_3: 0.2084  loss_dice_3: 0.1478  loss_ce_4: 0.01082  loss_mask_4: 0.195  loss_dice_4: 0.1549  loss_ce_5: 0.01084  loss_mask_5: 0.2054  loss_dice_5: 0.1503  loss_ce_6: 0.008315  loss_mask_6: 0.203  loss_dice_6: 0.1547  loss_ce_7: 0.01091  loss_mask_7: 0.2023  loss_dice_7: 0.1628  loss_ce_8: 0.00951  loss_mask_8: 0.2006  loss_dice_8: 0.1567  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 16:20:11] d2.utils.events INFO:  eta: 0:28:03  iter: 36739  total_loss: 4.378  loss_ce: 0.005123  loss_mask: 0.1893  loss_dice: 0.2286  loss_ce_0: 0.06317  loss_mask_0: 0.1939  loss_dice_0: 0.214  loss_ce_1: 0.009091  loss_mask_1: 0.1854  loss_dice_1: 0.2139  loss_ce_2: 0.005186  loss_mask_2: 0.1815  loss_dice_2: 0.22  loss_ce_3: 0.004736  loss_mask_3: 0.1982  loss_dice_3: 0.2256  loss_ce_4: 0.006462  loss_mask_4: 0.1812  loss_dice_4: 0.2257  loss_ce_5: 0.004637  loss_mask_5: 0.1901  loss_dice_5: 0.2193  loss_ce_6: 0.005264  loss_mask_6: 0.189  loss_dice_6: 0.213  loss_ce_7: 0.008548  loss_mask_7: 0.1869  loss_dice_7: 0.2269  loss_ce_8: 0.00724  loss_mask_8: 0.1862  loss_dice_8: 0.2241  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 16:20:13] d2.utils.events INFO:  eta: 0:28:01  iter: 36759  total_loss: 5.879  loss_ce: 0.007701  loss_mask: 0.2066  loss_dice: 0.245  loss_ce_0: 0.05874  loss_mask_0: 0.2098  loss_dice_0: 0.2587  loss_ce_1: 0.005342  loss_mask_1: 0.2046  loss_dice_1: 0.2475  loss_ce_2: 0.005927  loss_mask_2: 0.2023  loss_dice_2: 0.2515  loss_ce_3: 0.006786  loss_mask_3: 0.2014  loss_dice_3: 0.2425  loss_ce_4: 0.006286  loss_mask_4: 0.2104  loss_dice_4: 0.238  loss_ce_5: 0.006264  loss_mask_5: 0.1957  loss_dice_5: 0.2462  loss_ce_6: 0.006988  loss_mask_6: 0.1952  loss_dice_6: 0.2365  loss_ce_7: 0.005953  loss_mask_7: 0.1961  loss_dice_7: 0.2456  loss_ce_8: 0.006875  loss_mask_8: 0.2131  loss_dice_8: 0.2447  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:16] d2.utils.events INFO:  eta: 0:27:59  iter: 36779  total_loss: 4.636  loss_ce: 0.007467  loss_mask: 0.2167  loss_dice: 0.1591  loss_ce_0: 0.05685  loss_mask_0: 0.2127  loss_dice_0: 0.159  loss_ce_1: 0.00701  loss_mask_1: 0.2018  loss_dice_1: 0.1639  loss_ce_2: 0.007593  loss_mask_2: 0.2124  loss_dice_2: 0.1659  loss_ce_3: 0.008553  loss_mask_3: 0.2031  loss_dice_3: 0.1638  loss_ce_4: 0.007552  loss_mask_4: 0.2149  loss_dice_4: 0.1611  loss_ce_5: 0.007258  loss_mask_5: 0.2129  loss_dice_5: 0.1599  loss_ce_6: 0.00822  loss_mask_6: 0.2153  loss_dice_6: 0.1634  loss_ce_7: 0.0076  loss_mask_7: 0.2194  loss_dice_7: 0.1556  loss_ce_8: 0.008586  loss_mask_8: 0.2155  loss_dice_8: 0.1571  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:19] d2.utils.events INFO:  eta: 0:27:56  iter: 36799  total_loss: 6.113  loss_ce: 0.00861  loss_mask: 0.1898  loss_dice: 0.2576  loss_ce_0: 0.0563  loss_mask_0: 0.1916  loss_dice_0: 0.2473  loss_ce_1: 0.01604  loss_mask_1: 0.1859  loss_dice_1: 0.2546  loss_ce_2: 0.008832  loss_mask_2: 0.1979  loss_dice_2: 0.2485  loss_ce_3: 0.007886  loss_mask_3: 0.192  loss_dice_3: 0.2589  loss_ce_4: 0.01203  loss_mask_4: 0.1888  loss_dice_4: 0.2566  loss_ce_5: 0.00768  loss_mask_5: 0.1899  loss_dice_5: 0.2604  loss_ce_6: 0.007696  loss_mask_6: 0.1923  loss_dice_6: 0.2459  loss_ce_7: 0.01737  loss_mask_7: 0.1961  loss_dice_7: 0.2397  loss_ce_8: 0.008249  loss_mask_8: 0.1936  loss_dice_8: 0.2599  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:21] d2.utils.events INFO:  eta: 0:27:54  iter: 36819  total_loss: 5.18  loss_ce: 0.007878  loss_mask: 0.1884  loss_dice: 0.2276  loss_ce_0: 0.05535  loss_mask_0: 0.1875  loss_dice_0: 0.2234  loss_ce_1: 0.009217  loss_mask_1: 0.1994  loss_dice_1: 0.2265  loss_ce_2: 0.007631  loss_mask_2: 0.1835  loss_dice_2: 0.2312  loss_ce_3: 0.007147  loss_mask_3: 0.1844  loss_dice_3: 0.2356  loss_ce_4: 0.006892  loss_mask_4: 0.1927  loss_dice_4: 0.2195  loss_ce_5: 0.007232  loss_mask_5: 0.1875  loss_dice_5: 0.2247  loss_ce_6: 0.007047  loss_mask_6: 0.1947  loss_dice_6: 0.2254  loss_ce_7: 0.007017  loss_mask_7: 0.1907  loss_dice_7: 0.2289  loss_ce_8: 0.007414  loss_mask_8: 0.1913  loss_dice_8: 0.2315  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:24] d2.utils.events INFO:  eta: 0:27:51  iter: 36839  total_loss: 4.44  loss_ce: 0.004781  loss_mask: 0.1707  loss_dice: 0.1847  loss_ce_0: 0.05175  loss_mask_0: 0.1715  loss_dice_0: 0.1726  loss_ce_1: 0.004385  loss_mask_1: 0.1746  loss_dice_1: 0.1719  loss_ce_2: 0.005248  loss_mask_2: 0.1715  loss_dice_2: 0.1716  loss_ce_3: 0.004793  loss_mask_3: 0.173  loss_dice_3: 0.1847  loss_ce_4: 0.00459  loss_mask_4: 0.1614  loss_dice_4: 0.1754  loss_ce_5: 0.004912  loss_mask_5: 0.1711  loss_dice_5: 0.1868  loss_ce_6: 0.005008  loss_mask_6: 0.1694  loss_dice_6: 0.179  loss_ce_7: 0.004162  loss_mask_7: 0.1645  loss_dice_7: 0.1737  loss_ce_8: 0.005215  loss_mask_8: 0.169  loss_dice_8: 0.1759  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:26] d2.utils.events INFO:  eta: 0:27:46  iter: 36859  total_loss: 4.189  loss_ce: 0.002886  loss_mask: 0.2226  loss_dice: 0.1438  loss_ce_0: 0.06413  loss_mask_0: 0.2183  loss_dice_0: 0.152  loss_ce_1: 0.00248  loss_mask_1: 0.2105  loss_dice_1: 0.1438  loss_ce_2: 0.003014  loss_mask_2: 0.2147  loss_dice_2: 0.1502  loss_ce_3: 0.003346  loss_mask_3: 0.2168  loss_dice_3: 0.1513  loss_ce_4: 0.002897  loss_mask_4: 0.2149  loss_dice_4: 0.15  loss_ce_5: 0.002904  loss_mask_5: 0.2141  loss_dice_5: 0.1555  loss_ce_6: 0.003594  loss_mask_6: 0.2193  loss_dice_6: 0.1551  loss_ce_7: 0.002281  loss_mask_7: 0.2112  loss_dice_7: 0.1517  loss_ce_8: 0.002901  loss_mask_8: 0.2071  loss_dice_8: 0.1492  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:29] d2.utils.events INFO:  eta: 0:27:45  iter: 36879  total_loss: 5.59  loss_ce: 0.008247  loss_mask: 0.1923  loss_dice: 0.1864  loss_ce_0: 0.0728  loss_mask_0: 0.1959  loss_dice_0: 0.1934  loss_ce_1: 0.02899  loss_mask_1: 0.1923  loss_dice_1: 0.1859  loss_ce_2: 0.01335  loss_mask_2: 0.1944  loss_dice_2: 0.1952  loss_ce_3: 0.006104  loss_mask_3: 0.1914  loss_dice_3: 0.1844  loss_ce_4: 0.00615  loss_mask_4: 0.1992  loss_dice_4: 0.187  loss_ce_5: 0.007454  loss_mask_5: 0.1951  loss_dice_5: 0.1832  loss_ce_6: 0.006623  loss_mask_6: 0.1949  loss_dice_6: 0.1828  loss_ce_7: 0.009023  loss_mask_7: 0.1958  loss_dice_7: 0.1875  loss_ce_8: 0.0104  loss_mask_8: 0.1877  loss_dice_8: 0.185  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:31] d2.utils.events INFO:  eta: 0:27:43  iter: 36899  total_loss: 4.451  loss_ce: 0.004658  loss_mask: 0.1707  loss_dice: 0.1686  loss_ce_0: 0.06604  loss_mask_0: 0.1783  loss_dice_0: 0.1671  loss_ce_1: 0.004255  loss_mask_1: 0.1786  loss_dice_1: 0.1656  loss_ce_2: 0.004498  loss_mask_2: 0.1835  loss_dice_2: 0.1668  loss_ce_3: 0.004539  loss_mask_3: 0.1792  loss_dice_3: 0.1685  loss_ce_4: 0.004261  loss_mask_4: 0.1748  loss_dice_4: 0.1713  loss_ce_5: 0.004804  loss_mask_5: 0.1769  loss_dice_5: 0.164  loss_ce_6: 0.00497  loss_mask_6: 0.1727  loss_dice_6: 0.1672  loss_ce_7: 0.004101  loss_mask_7: 0.1848  loss_dice_7: 0.1628  loss_ce_8: 0.005114  loss_mask_8: 0.1838  loss_dice_8: 0.1613  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:34] d2.utils.events INFO:  eta: 0:27:40  iter: 36919  total_loss: 4.099  loss_ce: 0.01962  loss_mask: 0.1754  loss_dice: 0.1462  loss_ce_0: 0.06267  loss_mask_0: 0.1792  loss_dice_0: 0.1548  loss_ce_1: 0.00872  loss_mask_1: 0.1737  loss_dice_1: 0.1508  loss_ce_2: 0.01125  loss_mask_2: 0.178  loss_dice_2: 0.1512  loss_ce_3: 0.01065  loss_mask_3: 0.1715  loss_dice_3: 0.1522  loss_ce_4: 0.009419  loss_mask_4: 0.1731  loss_dice_4: 0.1508  loss_ce_5: 0.01322  loss_mask_5: 0.1811  loss_dice_5: 0.1546  loss_ce_6: 0.009395  loss_mask_6: 0.1737  loss_dice_6: 0.1557  loss_ce_7: 0.009089  loss_mask_7: 0.1771  loss_dice_7: 0.1557  loss_ce_8: 0.014  loss_mask_8: 0.1783  loss_dice_8: 0.1574  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:36] d2.utils.events INFO:  eta: 0:27:38  iter: 36939  total_loss: 4.848  loss_ce: 0.005413  loss_mask: 0.182  loss_dice: 0.1814  loss_ce_0: 0.06206  loss_mask_0: 0.1828  loss_dice_0: 0.1851  loss_ce_1: 0.006891  loss_mask_1: 0.1769  loss_dice_1: 0.1884  loss_ce_2: 0.006078  loss_mask_2: 0.1836  loss_dice_2: 0.1822  loss_ce_3: 0.006051  loss_mask_3: 0.1929  loss_dice_3: 0.1895  loss_ce_4: 0.006047  loss_mask_4: 0.1872  loss_dice_4: 0.1873  loss_ce_5: 0.005946  loss_mask_5: 0.1861  loss_dice_5: 0.1815  loss_ce_6: 0.006101  loss_mask_6: 0.1817  loss_dice_6: 0.1777  loss_ce_7: 0.0055  loss_mask_7: 0.176  loss_dice_7: 0.1823  loss_ce_8: 0.006143  loss_mask_8: 0.1804  loss_dice_8: 0.1858  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:39] d2.utils.events INFO:  eta: 0:27:36  iter: 36959  total_loss: 5.566  loss_ce: 0.006629  loss_mask: 0.2328  loss_dice: 0.218  loss_ce_0: 0.06463  loss_mask_0: 0.2254  loss_dice_0: 0.2229  loss_ce_1: 0.005034  loss_mask_1: 0.2379  loss_dice_1: 0.2216  loss_ce_2: 0.006333  loss_mask_2: 0.2317  loss_dice_2: 0.2178  loss_ce_3: 0.006759  loss_mask_3: 0.23  loss_dice_3: 0.2295  loss_ce_4: 0.007116  loss_mask_4: 0.2296  loss_dice_4: 0.2275  loss_ce_5: 0.006914  loss_mask_5: 0.2262  loss_dice_5: 0.2136  loss_ce_6: 0.006511  loss_mask_6: 0.2268  loss_dice_6: 0.2174  loss_ce_7: 0.006119  loss_mask_7: 0.2321  loss_dice_7: 0.2129  loss_ce_8: 0.007335  loss_mask_8: 0.2334  loss_dice_8: 0.219  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:42] d2.utils.events INFO:  eta: 0:27:34  iter: 36979  total_loss: 4.996  loss_ce: 0.02235  loss_mask: 0.1415  loss_dice: 0.2198  loss_ce_0: 0.05861  loss_mask_0: 0.1317  loss_dice_0: 0.2221  loss_ce_1: 0.01032  loss_mask_1: 0.1366  loss_dice_1: 0.211  loss_ce_2: 0.01619  loss_mask_2: 0.1354  loss_dice_2: 0.2186  loss_ce_3: 0.01697  loss_mask_3: 0.1397  loss_dice_3: 0.2246  loss_ce_4: 0.01716  loss_mask_4: 0.1305  loss_dice_4: 0.2148  loss_ce_5: 0.01411  loss_mask_5: 0.1366  loss_dice_5: 0.2172  loss_ce_6: 0.01356  loss_mask_6: 0.1341  loss_dice_6: 0.218  loss_ce_7: 0.01324  loss_mask_7: 0.1354  loss_dice_7: 0.2186  loss_ce_8: 0.01566  loss_mask_8: 0.132  loss_dice_8: 0.2219  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:44] d2.utils.events INFO:  eta: 0:27:32  iter: 36999  total_loss: 4.895  loss_ce: 0.008803  loss_mask: 0.1781  loss_dice: 0.2525  loss_ce_0: 0.05698  loss_mask_0: 0.1793  loss_dice_0: 0.2784  loss_ce_1: 0.009505  loss_mask_1: 0.1818  loss_dice_1: 0.2561  loss_ce_2: 0.008049  loss_mask_2: 0.1733  loss_dice_2: 0.2635  loss_ce_3: 0.009893  loss_mask_3: 0.1883  loss_dice_3: 0.2795  loss_ce_4: 0.009256  loss_mask_4: 0.1679  loss_dice_4: 0.262  loss_ce_5: 0.008831  loss_mask_5: 0.1726  loss_dice_5: 0.2591  loss_ce_6: 0.009568  loss_mask_6: 0.1725  loss_dice_6: 0.2754  loss_ce_7: 0.008509  loss_mask_7: 0.1769  loss_dice_7: 0.2852  loss_ce_8: 0.01003  loss_mask_8: 0.1722  loss_dice_8: 0.2559  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:47] d2.utils.events INFO:  eta: 0:27:29  iter: 37019  total_loss: 5.172  loss_ce: 0.01021  loss_mask: 0.2118  loss_dice: 0.172  loss_ce_0: 0.07229  loss_mask_0: 0.2153  loss_dice_0: 0.1782  loss_ce_1: 0.00966  loss_mask_1: 0.216  loss_dice_1: 0.1813  loss_ce_2: 0.01406  loss_mask_2: 0.2064  loss_dice_2: 0.1769  loss_ce_3: 0.01096  loss_mask_3: 0.2189  loss_dice_3: 0.1752  loss_ce_4: 0.009238  loss_mask_4: 0.2182  loss_dice_4: 0.1724  loss_ce_5: 0.01115  loss_mask_5: 0.2084  loss_dice_5: 0.1739  loss_ce_6: 0.009951  loss_mask_6: 0.2223  loss_dice_6: 0.1772  loss_ce_7: 0.00748  loss_mask_7: 0.215  loss_dice_7: 0.1778  loss_ce_8: 0.01507  loss_mask_8: 0.2191  loss_dice_8: 0.1712  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:49] d2.utils.events INFO:  eta: 0:27:26  iter: 37039  total_loss: 4.531  loss_ce: 0.007806  loss_mask: 0.1719  loss_dice: 0.1904  loss_ce_0: 0.05409  loss_mask_0: 0.1744  loss_dice_0: 0.1926  loss_ce_1: 0.0101  loss_mask_1: 0.1816  loss_dice_1: 0.1884  loss_ce_2: 0.007  loss_mask_2: 0.1794  loss_dice_2: 0.1942  loss_ce_3: 0.009032  loss_mask_3: 0.1714  loss_dice_3: 0.1767  loss_ce_4: 0.008951  loss_mask_4: 0.1752  loss_dice_4: 0.1792  loss_ce_5: 0.008321  loss_mask_5: 0.1815  loss_dice_5: 0.1924  loss_ce_6: 0.009124  loss_mask_6: 0.1755  loss_dice_6: 0.1886  loss_ce_7: 0.008318  loss_mask_7: 0.1769  loss_dice_7: 0.1703  loss_ce_8: 0.006921  loss_mask_8: 0.1821  loss_dice_8: 0.1832  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:52] d2.utils.events INFO:  eta: 0:27:23  iter: 37059  total_loss: 4.66  loss_ce: 0.01003  loss_mask: 0.2292  loss_dice: 0.197  loss_ce_0: 0.05435  loss_mask_0: 0.2263  loss_dice_0: 0.1897  loss_ce_1: 0.01155  loss_mask_1: 0.2155  loss_dice_1: 0.197  loss_ce_2: 0.009341  loss_mask_2: 0.2249  loss_dice_2: 0.1927  loss_ce_3: 0.01068  loss_mask_3: 0.2297  loss_dice_3: 0.1948  loss_ce_4: 0.01172  loss_mask_4: 0.224  loss_dice_4: 0.1947  loss_ce_5: 0.009977  loss_mask_5: 0.2264  loss_dice_5: 0.1919  loss_ce_6: 0.01062  loss_mask_6: 0.2243  loss_dice_6: 0.1957  loss_ce_7: 0.01101  loss_mask_7: 0.2207  loss_dice_7: 0.1979  loss_ce_8: 0.009562  loss_mask_8: 0.2285  loss_dice_8: 0.2054  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:20:54] d2.utils.events INFO:  eta: 0:27:20  iter: 37079  total_loss: 4.466  loss_ce: 0.008948  loss_mask: 0.2079  loss_dice: 0.1931  loss_ce_0: 0.07193  loss_mask_0: 0.1867  loss_dice_0: 0.1952  loss_ce_1: 0.0101  loss_mask_1: 0.1981  loss_dice_1: 0.1953  loss_ce_2: 0.009296  loss_mask_2: 0.1891  loss_dice_2: 0.1873  loss_ce_3: 0.009078  loss_mask_3: 0.1933  loss_dice_3: 0.1909  loss_ce_4: 0.00971  loss_mask_4: 0.2005  loss_dice_4: 0.1995  loss_ce_5: 0.009995  loss_mask_5: 0.1911  loss_dice_5: 0.1901  loss_ce_6: 0.00945  loss_mask_6: 0.2028  loss_dice_6: 0.1893  loss_ce_7: 0.01039  loss_mask_7: 0.2007  loss_dice_7: 0.1876  loss_ce_8: 0.01065  loss_mask_8: 0.2043  loss_dice_8: 0.1891  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:57] d2.utils.events INFO:  eta: 0:27:17  iter: 37099  total_loss: 5.078  loss_ce: 0.006538  loss_mask: 0.2112  loss_dice: 0.1976  loss_ce_0: 0.06201  loss_mask_0: 0.2249  loss_dice_0: 0.1893  loss_ce_1: 0.007066  loss_mask_1: 0.2295  loss_dice_1: 0.1928  loss_ce_2: 0.00647  loss_mask_2: 0.2197  loss_dice_2: 0.1868  loss_ce_3: 0.006802  loss_mask_3: 0.2266  loss_dice_3: 0.1887  loss_ce_4: 0.006316  loss_mask_4: 0.2195  loss_dice_4: 0.1907  loss_ce_5: 0.006735  loss_mask_5: 0.2244  loss_dice_5: 0.19  loss_ce_6: 0.007547  loss_mask_6: 0.2164  loss_dice_6: 0.1873  loss_ce_7: 0.006178  loss_mask_7: 0.2148  loss_dice_7: 0.1913  loss_ce_8: 0.006676  loss_mask_8: 0.2267  loss_dice_8: 0.1936  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:20:59] d2.utils.events INFO:  eta: 0:27:15  iter: 37119  total_loss: 4.599  loss_ce: 0.004453  loss_mask: 0.2094  loss_dice: 0.1697  loss_ce_0: 0.05866  loss_mask_0: 0.2168  loss_dice_0: 0.1686  loss_ce_1: 0.004679  loss_mask_1: 0.2184  loss_dice_1: 0.17  loss_ce_2: 0.004186  loss_mask_2: 0.2101  loss_dice_2: 0.168  loss_ce_3: 0.005069  loss_mask_3: 0.2111  loss_dice_3: 0.1732  loss_ce_4: 0.004974  loss_mask_4: 0.2144  loss_dice_4: 0.1672  loss_ce_5: 0.004811  loss_mask_5: 0.2192  loss_dice_5: 0.1783  loss_ce_6: 0.005645  loss_mask_6: 0.2097  loss_dice_6: 0.1719  loss_ce_7: 0.004732  loss_mask_7: 0.2204  loss_dice_7: 0.1679  loss_ce_8: 0.004035  loss_mask_8: 0.2195  loss_dice_8: 0.1659  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:02] d2.utils.events INFO:  eta: 0:27:12  iter: 37139  total_loss: 4.997  loss_ce: 0.005648  loss_mask: 0.2547  loss_dice: 0.1704  loss_ce_0: 0.06214  loss_mask_0: 0.2595  loss_dice_0: 0.1615  loss_ce_1: 0.006017  loss_mask_1: 0.258  loss_dice_1: 0.1691  loss_ce_2: 0.005155  loss_mask_2: 0.255  loss_dice_2: 0.1561  loss_ce_3: 0.005985  loss_mask_3: 0.2626  loss_dice_3: 0.1696  loss_ce_4: 0.005958  loss_mask_4: 0.2574  loss_dice_4: 0.1676  loss_ce_5: 0.005757  loss_mask_5: 0.2641  loss_dice_5: 0.1624  loss_ce_6: 0.005994  loss_mask_6: 0.26  loss_dice_6: 0.1713  loss_ce_7: 0.006089  loss_mask_7: 0.251  loss_dice_7: 0.1686  loss_ce_8: 0.006049  loss_mask_8: 0.2527  loss_dice_8: 0.1737  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:04] d2.utils.events INFO:  eta: 0:27:10  iter: 37159  total_loss: 4.995  loss_ce: 0.007175  loss_mask: 0.2182  loss_dice: 0.2224  loss_ce_0: 0.06636  loss_mask_0: 0.2184  loss_dice_0: 0.2289  loss_ce_1: 0.006665  loss_mask_1: 0.2141  loss_dice_1: 0.2123  loss_ce_2: 0.006266  loss_mask_2: 0.2067  loss_dice_2: 0.2179  loss_ce_3: 0.006822  loss_mask_3: 0.2171  loss_dice_3: 0.2229  loss_ce_4: 0.006733  loss_mask_4: 0.2135  loss_dice_4: 0.2195  loss_ce_5: 0.007045  loss_mask_5: 0.2172  loss_dice_5: 0.2325  loss_ce_6: 0.007474  loss_mask_6: 0.2152  loss_dice_6: 0.2172  loss_ce_7: 0.007484  loss_mask_7: 0.2168  loss_dice_7: 0.2156  loss_ce_8: 0.007461  loss_mask_8: 0.21  loss_dice_8: 0.2201  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:07] d2.utils.events INFO:  eta: 0:27:07  iter: 37179  total_loss: 4.764  loss_ce: 0.008467  loss_mask: 0.2303  loss_dice: 0.1742  loss_ce_0: 0.06612  loss_mask_0: 0.2377  loss_dice_0: 0.1738  loss_ce_1: 0.01044  loss_mask_1: 0.2336  loss_dice_1: 0.1731  loss_ce_2: 0.01068  loss_mask_2: 0.2311  loss_dice_2: 0.1768  loss_ce_3: 0.008669  loss_mask_3: 0.23  loss_dice_3: 0.1731  loss_ce_4: 0.01321  loss_mask_4: 0.2286  loss_dice_4: 0.1646  loss_ce_5: 0.01004  loss_mask_5: 0.2231  loss_dice_5: 0.17  loss_ce_6: 0.009656  loss_mask_6: 0.2387  loss_dice_6: 0.1722  loss_ce_7: 0.0135  loss_mask_7: 0.2305  loss_dice_7: 0.1755  loss_ce_8: 0.01251  loss_mask_8: 0.2341  loss_dice_8: 0.1736  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:09] d2.utils.events INFO:  eta: 0:27:05  iter: 37199  total_loss: 4.622  loss_ce: 0.01271  loss_mask: 0.1958  loss_dice: 0.1451  loss_ce_0: 0.06118  loss_mask_0: 0.1938  loss_dice_0: 0.1444  loss_ce_1: 0.01933  loss_mask_1: 0.1917  loss_dice_1: 0.1452  loss_ce_2: 0.01727  loss_mask_2: 0.1901  loss_dice_2: 0.1476  loss_ce_3: 0.01104  loss_mask_3: 0.1948  loss_dice_3: 0.1436  loss_ce_4: 0.01077  loss_mask_4: 0.189  loss_dice_4: 0.141  loss_ce_5: 0.01462  loss_mask_5: 0.1974  loss_dice_5: 0.145  loss_ce_6: 0.01239  loss_mask_6: 0.195  loss_dice_6: 0.1462  loss_ce_7: 0.01356  loss_mask_7: 0.1845  loss_dice_7: 0.1481  loss_ce_8: 0.01605  loss_mask_8: 0.1876  loss_dice_8: 0.1427  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:12] d2.utils.events INFO:  eta: 0:27:03  iter: 37219  total_loss: 4.885  loss_ce: 0.01758  loss_mask: 0.2019  loss_dice: 0.1299  loss_ce_0: 0.05863  loss_mask_0: 0.2063  loss_dice_0: 0.1336  loss_ce_1: 0.02317  loss_mask_1: 0.2035  loss_dice_1: 0.1448  loss_ce_2: 0.0327  loss_mask_2: 0.2004  loss_dice_2: 0.1343  loss_ce_3: 0.01459  loss_mask_3: 0.217  loss_dice_3: 0.1319  loss_ce_4: 0.01571  loss_mask_4: 0.2076  loss_dice_4: 0.1308  loss_ce_5: 0.02531  loss_mask_5: 0.2016  loss_dice_5: 0.1323  loss_ce_6: 0.01505  loss_mask_6: 0.1998  loss_dice_6: 0.1298  loss_ce_7: 0.01964  loss_mask_7: 0.2042  loss_dice_7: 0.1341  loss_ce_8: 0.02805  loss_mask_8: 0.2099  loss_dice_8: 0.1358  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:21:14] d2.utils.events INFO:  eta: 0:27:00  iter: 37239  total_loss: 5.109  loss_ce: 0.009272  loss_mask: 0.1852  loss_dice: 0.2137  loss_ce_0: 0.05345  loss_mask_0: 0.1924  loss_dice_0: 0.2203  loss_ce_1: 0.01224  loss_mask_1: 0.1948  loss_dice_1: 0.2261  loss_ce_2: 0.007156  loss_mask_2: 0.1867  loss_dice_2: 0.2246  loss_ce_3: 0.009421  loss_mask_3: 0.1917  loss_dice_3: 0.2247  loss_ce_4: 0.008457  loss_mask_4: 0.1867  loss_dice_4: 0.2233  loss_ce_5: 0.007667  loss_mask_5: 0.1968  loss_dice_5: 0.2279  loss_ce_6: 0.01014  loss_mask_6: 0.1916  loss_dice_6: 0.2258  loss_ce_7: 0.0104  loss_mask_7: 0.1936  loss_dice_7: 0.2235  loss_ce_8: 0.00753  loss_mask_8: 0.1917  loss_dice_8: 0.2298  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:21:17] d2.utils.events INFO:  eta: 0:26:59  iter: 37259  total_loss: 4.409  loss_ce: 0.00802  loss_mask: 0.1867  loss_dice: 0.1789  loss_ce_0: 0.05434  loss_mask_0: 0.1888  loss_dice_0: 0.1916  loss_ce_1: 0.009125  loss_mask_1: 0.1918  loss_dice_1: 0.1928  loss_ce_2: 0.006559  loss_mask_2: 0.1826  loss_dice_2: 0.1888  loss_ce_3: 0.007783  loss_mask_3: 0.1867  loss_dice_3: 0.1972  loss_ce_4: 0.00771  loss_mask_4: 0.1854  loss_dice_4: 0.1907  loss_ce_5: 0.007082  loss_mask_5: 0.1819  loss_dice_5: 0.1926  loss_ce_6: 0.008372  loss_mask_6: 0.1915  loss_dice_6: 0.1855  loss_ce_7: 0.007768  loss_mask_7: 0.1877  loss_dice_7: 0.1961  loss_ce_8: 0.007306  loss_mask_8: 0.1962  loss_dice_8: 0.19  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:21:20] d2.utils.events INFO:  eta: 0:26:57  iter: 37279  total_loss: 4.559  loss_ce: 0.008466  loss_mask: 0.1999  loss_dice: 0.1987  loss_ce_0: 0.05636  loss_mask_0: 0.198  loss_dice_0: 0.1916  loss_ce_1: 0.007201  loss_mask_1: 0.1991  loss_dice_1: 0.2047  loss_ce_2: 0.007771  loss_mask_2: 0.2032  loss_dice_2: 0.1992  loss_ce_3: 0.008344  loss_mask_3: 0.1968  loss_dice_3: 0.2018  loss_ce_4: 0.008288  loss_mask_4: 0.2107  loss_dice_4: 0.196  loss_ce_5: 0.008391  loss_mask_5: 0.2022  loss_dice_5: 0.1979  loss_ce_6: 0.008993  loss_mask_6: 0.2043  loss_dice_6: 0.2051  loss_ce_7: 0.007404  loss_mask_7: 0.2002  loss_dice_7: 0.2012  loss_ce_8: 0.008006  loss_mask_8: 0.2023  loss_dice_8: 0.1968  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:21:22] d2.utils.events INFO:  eta: 0:26:54  iter: 37299  total_loss: 4.998  loss_ce: 0.004188  loss_mask: 0.1488  loss_dice: 0.3226  loss_ce_0: 0.05499  loss_mask_0: 0.1473  loss_dice_0: 0.3141  loss_ce_1: 0.006251  loss_mask_1: 0.1494  loss_dice_1: 0.3287  loss_ce_2: 0.003957  loss_mask_2: 0.1442  loss_dice_2: 0.3097  loss_ce_3: 0.004186  loss_mask_3: 0.1368  loss_dice_3: 0.3445  loss_ce_4: 0.00563  loss_mask_4: 0.1488  loss_dice_4: 0.3253  loss_ce_5: 0.003788  loss_mask_5: 0.1569  loss_dice_5: 0.3277  loss_ce_6: 0.003696  loss_mask_6: 0.1387  loss_dice_6: 0.3184  loss_ce_7: 0.005864  loss_mask_7: 0.1531  loss_dice_7: 0.3287  loss_ce_8: 0.003385  loss_mask_8: 0.1562  loss_dice_8: 0.3395  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:25] d2.utils.events INFO:  eta: 0:26:51  iter: 37319  total_loss: 4.35  loss_ce: 0.006406  loss_mask: 0.2191  loss_dice: 0.1686  loss_ce_0: 0.05094  loss_mask_0: 0.2151  loss_dice_0: 0.1689  loss_ce_1: 0.005806  loss_mask_1: 0.2097  loss_dice_1: 0.1682  loss_ce_2: 0.006041  loss_mask_2: 0.2225  loss_dice_2: 0.166  loss_ce_3: 0.006656  loss_mask_3: 0.2129  loss_dice_3: 0.1699  loss_ce_4: 0.005944  loss_mask_4: 0.2159  loss_dice_4: 0.1686  loss_ce_5: 0.006179  loss_mask_5: 0.2055  loss_dice_5: 0.1675  loss_ce_6: 0.00691  loss_mask_6: 0.2235  loss_dice_6: 0.1703  loss_ce_7: 0.005841  loss_mask_7: 0.2096  loss_dice_7: 0.1696  loss_ce_8: 0.006174  loss_mask_8: 0.2161  loss_dice_8: 0.1703  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:21:27] d2.utils.events INFO:  eta: 0:26:49  iter: 37339  total_loss: 5.352  loss_ce: 0.005427  loss_mask: 0.1937  loss_dice: 0.2494  loss_ce_0: 0.05026  loss_mask_0: 0.1861  loss_dice_0: 0.2499  loss_ce_1: 0.007776  loss_mask_1: 0.1971  loss_dice_1: 0.2522  loss_ce_2: 0.0226  loss_mask_2: 0.1833  loss_dice_2: 0.2486  loss_ce_3: 0.003952  loss_mask_3: 0.1909  loss_dice_3: 0.2464  loss_ce_4: 0.004228  loss_mask_4: 0.19  loss_dice_4: 0.2501  loss_ce_5: 0.01882  loss_mask_5: 0.1817  loss_dice_5: 0.2393  loss_ce_6: 0.003664  loss_mask_6: 0.1985  loss_dice_6: 0.2449  loss_ce_7: 0.005314  loss_mask_7: 0.1957  loss_dice_7: 0.2496  loss_ce_8: 0.01813  loss_mask_8: 0.1935  loss_dice_8: 0.2585  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:30] d2.utils.events INFO:  eta: 0:26:46  iter: 37359  total_loss: 4.851  loss_ce: 0.005466  loss_mask: 0.1878  loss_dice: 0.2112  loss_ce_0: 0.04761  loss_mask_0: 0.1991  loss_dice_0: 0.2026  loss_ce_1: 0.004282  loss_mask_1: 0.2011  loss_dice_1: 0.2159  loss_ce_2: 0.007205  loss_mask_2: 0.2003  loss_dice_2: 0.2096  loss_ce_3: 0.00531  loss_mask_3: 0.1889  loss_dice_3: 0.2066  loss_ce_4: 0.00734  loss_mask_4: 0.1902  loss_dice_4: 0.1988  loss_ce_5: 0.007139  loss_mask_5: 0.2018  loss_dice_5: 0.2129  loss_ce_6: 0.0053  loss_mask_6: 0.1946  loss_dice_6: 0.2153  loss_ce_7: 0.007281  loss_mask_7: 0.1944  loss_dice_7: 0.2063  loss_ce_8: 0.007679  loss_mask_8: 0.198  loss_dice_8: 0.211  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:32] d2.utils.events INFO:  eta: 0:26:42  iter: 37379  total_loss: 4.344  loss_ce: 0.005667  loss_mask: 0.2031  loss_dice: 0.1424  loss_ce_0: 0.08497  loss_mask_0: 0.2004  loss_dice_0: 0.1449  loss_ce_1: 0.004841  loss_mask_1: 0.2067  loss_dice_1: 0.1477  loss_ce_2: 0.004898  loss_mask_2: 0.2008  loss_dice_2: 0.1492  loss_ce_3: 0.005285  loss_mask_3: 0.1961  loss_dice_3: 0.1434  loss_ce_4: 0.004748  loss_mask_4: 0.2028  loss_dice_4: 0.1475  loss_ce_5: 0.004893  loss_mask_5: 0.1888  loss_dice_5: 0.1455  loss_ce_6: 0.005199  loss_mask_6: 0.2008  loss_dice_6: 0.1437  loss_ce_7: 0.004707  loss_mask_7: 0.1962  loss_dice_7: 0.1438  loss_ce_8: 0.00501  loss_mask_8: 0.2014  loss_dice_8: 0.1437  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:21:35] d2.utils.events INFO:  eta: 0:26:40  iter: 37399  total_loss: 4.982  loss_ce: 0.006258  loss_mask: 0.1788  loss_dice: 0.2136  loss_ce_0: 0.06272  loss_mask_0: 0.1784  loss_dice_0: 0.2213  loss_ce_1: 0.006793  loss_mask_1: 0.1904  loss_dice_1: 0.2227  loss_ce_2: 0.007229  loss_mask_2: 0.1811  loss_dice_2: 0.2164  loss_ce_3: 0.006072  loss_mask_3: 0.1812  loss_dice_3: 0.2197  loss_ce_4: 0.005853  loss_mask_4: 0.1764  loss_dice_4: 0.2193  loss_ce_5: 0.006144  loss_mask_5: 0.184  loss_dice_5: 0.2159  loss_ce_6: 0.005562  loss_mask_6: 0.175  loss_dice_6: 0.2071  loss_ce_7: 0.006396  loss_mask_7: 0.1776  loss_dice_7: 0.2239  loss_ce_8: 0.008422  loss_mask_8: 0.1852  loss_dice_8: 0.2246  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:37] d2.utils.events INFO:  eta: 0:26:37  iter: 37419  total_loss: 3.962  loss_ce: 0.008402  loss_mask: 0.1924  loss_dice: 0.1893  loss_ce_0: 0.06847  loss_mask_0: 0.202  loss_dice_0: 0.1914  loss_ce_1: 0.009233  loss_mask_1: 0.2053  loss_dice_1: 0.1852  loss_ce_2: 0.01094  loss_mask_2: 0.2048  loss_dice_2: 0.1874  loss_ce_3: 0.009169  loss_mask_3: 0.1981  loss_dice_3: 0.1856  loss_ce_4: 0.008645  loss_mask_4: 0.2025  loss_dice_4: 0.1916  loss_ce_5: 0.009509  loss_mask_5: 0.1946  loss_dice_5: 0.1935  loss_ce_6: 0.007508  loss_mask_6: 0.206  loss_dice_6: 0.1849  loss_ce_7: 0.008934  loss_mask_7: 0.1998  loss_dice_7: 0.1945  loss_ce_8: 0.01251  loss_mask_8: 0.2048  loss_dice_8: 0.1931  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:40] d2.utils.events INFO:  eta: 0:26:35  iter: 37439  total_loss: 4.283  loss_ce: 0.003697  loss_mask: 0.2213  loss_dice: 0.1326  loss_ce_0: 0.05677  loss_mask_0: 0.2088  loss_dice_0: 0.1359  loss_ce_1: 0.004205  loss_mask_1: 0.216  loss_dice_1: 0.1374  loss_ce_2: 0.004212  loss_mask_2: 0.2115  loss_dice_2: 0.1397  loss_ce_3: 0.00432  loss_mask_3: 0.2216  loss_dice_3: 0.1323  loss_ce_4: 0.003857  loss_mask_4: 0.2193  loss_dice_4: 0.1327  loss_ce_5: 0.00326  loss_mask_5: 0.2193  loss_dice_5: 0.1364  loss_ce_6: 0.004207  loss_mask_6: 0.2121  loss_dice_6: 0.1391  loss_ce_7: 0.00365  loss_mask_7: 0.2082  loss_dice_7: 0.1362  loss_ce_8: 0.003744  loss_mask_8: 0.2209  loss_dice_8: 0.1388  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:42] d2.utils.events INFO:  eta: 0:26:33  iter: 37459  total_loss: 4.882  loss_ce: 0.004739  loss_mask: 0.2268  loss_dice: 0.1825  loss_ce_0: 0.056  loss_mask_0: 0.2346  loss_dice_0: 0.1698  loss_ce_1: 0.005016  loss_mask_1: 0.231  loss_dice_1: 0.1735  loss_ce_2: 0.005192  loss_mask_2: 0.2371  loss_dice_2: 0.1719  loss_ce_3: 0.005546  loss_mask_3: 0.2235  loss_dice_3: 0.1757  loss_ce_4: 0.005156  loss_mask_4: 0.2317  loss_dice_4: 0.1742  loss_ce_5: 0.005173  loss_mask_5: 0.222  loss_dice_5: 0.1709  loss_ce_6: 0.005213  loss_mask_6: 0.2401  loss_dice_6: 0.1657  loss_ce_7: 0.005336  loss_mask_7: 0.2233  loss_dice_7: 0.1698  loss_ce_8: 0.004664  loss_mask_8: 0.2306  loss_dice_8: 0.1731  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:45] d2.utils.events INFO:  eta: 0:26:30  iter: 37479  total_loss: 4.674  loss_ce: 0.007087  loss_mask: 0.2098  loss_dice: 0.1465  loss_ce_0: 0.0706  loss_mask_0: 0.214  loss_dice_0: 0.146  loss_ce_1: 0.006669  loss_mask_1: 0.2074  loss_dice_1: 0.1434  loss_ce_2: 0.006473  loss_mask_2: 0.2182  loss_dice_2: 0.143  loss_ce_3: 0.007483  loss_mask_3: 0.2075  loss_dice_3: 0.1476  loss_ce_4: 0.00734  loss_mask_4: 0.2141  loss_dice_4: 0.1394  loss_ce_5: 0.007469  loss_mask_5: 0.1944  loss_dice_5: 0.1461  loss_ce_6: 0.007449  loss_mask_6: 0.2097  loss_dice_6: 0.1443  loss_ce_7: 0.006698  loss_mask_7: 0.2103  loss_dice_7: 0.1445  loss_ce_8: 0.008252  loss_mask_8: 0.2169  loss_dice_8: 0.1444  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:48] d2.utils.events INFO:  eta: 0:26:28  iter: 37499  total_loss: 4.818  loss_ce: 0.005427  loss_mask: 0.1796  loss_dice: 0.2549  loss_ce_0: 0.05309  loss_mask_0: 0.187  loss_dice_0: 0.2534  loss_ce_1: 0.004969  loss_mask_1: 0.1829  loss_dice_1: 0.2575  loss_ce_2: 0.004429  loss_mask_2: 0.1867  loss_dice_2: 0.2455  loss_ce_3: 0.005748  loss_mask_3: 0.1805  loss_dice_3: 0.2593  loss_ce_4: 0.005611  loss_mask_4: 0.1834  loss_dice_4: 0.2548  loss_ce_5: 0.005127  loss_mask_5: 0.189  loss_dice_5: 0.2615  loss_ce_6: 0.005528  loss_mask_6: 0.1806  loss_dice_6: 0.2637  loss_ce_7: 0.00471  loss_mask_7: 0.1874  loss_dice_7: 0.2749  loss_ce_8: 0.004674  loss_mask_8: 0.1847  loss_dice_8: 0.2621  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:50] d2.utils.events INFO:  eta: 0:26:26  iter: 37519  total_loss: 4.867  loss_ce: 0.001916  loss_mask: 0.2105  loss_dice: 0.1985  loss_ce_0: 0.05442  loss_mask_0: 0.2083  loss_dice_0: 0.2012  loss_ce_1: 0.003527  loss_mask_1: 0.2069  loss_dice_1: 0.1939  loss_ce_2: 0.002755  loss_mask_2: 0.2044  loss_dice_2: 0.1958  loss_ce_3: 0.002589  loss_mask_3: 0.2107  loss_dice_3: 0.1973  loss_ce_4: 0.003506  loss_mask_4: 0.2072  loss_dice_4: 0.206  loss_ce_5: 0.004585  loss_mask_5: 0.2174  loss_dice_5: 0.1961  loss_ce_6: 0.003007  loss_mask_6: 0.2056  loss_dice_6: 0.1931  loss_ce_7: 0.003125  loss_mask_7: 0.2061  loss_dice_7: 0.1951  loss_ce_8: 0.002846  loss_mask_8: 0.2118  loss_dice_8: 0.1973  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:53] d2.utils.events INFO:  eta: 0:26:23  iter: 37539  total_loss: 4.365  loss_ce: 0.006091  loss_mask: 0.1833  loss_dice: 0.1708  loss_ce_0: 0.06242  loss_mask_0: 0.1789  loss_dice_0: 0.1667  loss_ce_1: 0.004451  loss_mask_1: 0.1755  loss_dice_1: 0.1617  loss_ce_2: 0.005087  loss_mask_2: 0.1791  loss_dice_2: 0.1654  loss_ce_3: 0.005711  loss_mask_3: 0.1913  loss_dice_3: 0.1614  loss_ce_4: 0.005804  loss_mask_4: 0.1849  loss_dice_4: 0.1695  loss_ce_5: 0.006698  loss_mask_5: 0.1774  loss_dice_5: 0.1631  loss_ce_6: 0.005716  loss_mask_6: 0.1767  loss_dice_6: 0.1636  loss_ce_7: 0.004937  loss_mask_7: 0.1808  loss_dice_7: 0.1656  loss_ce_8: 0.00519  loss_mask_8: 0.1874  loss_dice_8: 0.1628  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:55] d2.utils.events INFO:  eta: 0:26:20  iter: 37559  total_loss: 6.532  loss_ce: 0.04676  loss_mask: 0.1571  loss_dice: 0.2148  loss_ce_0: 0.06283  loss_mask_0: 0.1724  loss_dice_0: 0.212  loss_ce_1: 0.1125  loss_mask_1: 0.1703  loss_dice_1: 0.2179  loss_ce_2: 0.03898  loss_mask_2: 0.1688  loss_dice_2: 0.2186  loss_ce_3: 0.03098  loss_mask_3: 0.1749  loss_dice_3: 0.2065  loss_ce_4: 0.08302  loss_mask_4: 0.1765  loss_dice_4: 0.2113  loss_ce_5: 0.03451  loss_mask_5: 0.1707  loss_dice_5: 0.2128  loss_ce_6: 0.0195  loss_mask_6: 0.1656  loss_dice_6: 0.2099  loss_ce_7: 0.07561  loss_mask_7: 0.1692  loss_dice_7: 0.2118  loss_ce_8: 0.03422  loss_mask_8: 0.1675  loss_dice_8: 0.212  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:21:58] d2.utils.events INFO:  eta: 0:26:17  iter: 37579  total_loss: 4.215  loss_ce: 0.007067  loss_mask: 0.1555  loss_dice: 0.2052  loss_ce_0: 0.07255  loss_mask_0: 0.1599  loss_dice_0: 0.1933  loss_ce_1: 0.01246  loss_mask_1: 0.1645  loss_dice_1: 0.2024  loss_ce_2: 0.007146  loss_mask_2: 0.1577  loss_dice_2: 0.1957  loss_ce_3: 0.006764  loss_mask_3: 0.1624  loss_dice_3: 0.2028  loss_ce_4: 0.009299  loss_mask_4: 0.1581  loss_dice_4: 0.1985  loss_ce_5: 0.007472  loss_mask_5: 0.1628  loss_dice_5: 0.2046  loss_ce_6: 0.007004  loss_mask_6: 0.1592  loss_dice_6: 0.2021  loss_ce_7: 0.008686  loss_mask_7: 0.1556  loss_dice_7: 0.1979  loss_ce_8: 0.007156  loss_mask_8: 0.1631  loss_dice_8: 0.1963  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:22:00] d2.utils.events INFO:  eta: 0:26:14  iter: 37599  total_loss: 4.288  loss_ce: 0.00628  loss_mask: 0.1873  loss_dice: 0.1907  loss_ce_0: 0.05905  loss_mask_0: 0.1863  loss_dice_0: 0.1862  loss_ce_1: 0.005601  loss_mask_1: 0.1895  loss_dice_1: 0.1882  loss_ce_2: 0.00547  loss_mask_2: 0.1943  loss_dice_2: 0.1863  loss_ce_3: 0.006042  loss_mask_3: 0.1884  loss_dice_3: 0.1957  loss_ce_4: 0.006134  loss_mask_4: 0.1872  loss_dice_4: 0.1909  loss_ce_5: 0.006475  loss_mask_5: 0.1839  loss_dice_5: 0.1872  loss_ce_6: 0.005696  loss_mask_6: 0.1923  loss_dice_6: 0.1776  loss_ce_7: 0.006384  loss_mask_7: 0.1918  loss_dice_7: 0.1933  loss_ce_8: 0.006447  loss_mask_8: 0.1828  loss_dice_8: 0.1874  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:22:03] d2.utils.events INFO:  eta: 0:26:12  iter: 37619  total_loss: 5.274  loss_ce: 0.0674  loss_mask: 0.1536  loss_dice: 0.2152  loss_ce_0: 0.06174  loss_mask_0: 0.1622  loss_dice_0: 0.2156  loss_ce_1: 0.05331  loss_mask_1: 0.1606  loss_dice_1: 0.2201  loss_ce_2: 0.05802  loss_mask_2: 0.1624  loss_dice_2: 0.2181  loss_ce_3: 0.05256  loss_mask_3: 0.1602  loss_dice_3: 0.2191  loss_ce_4: 0.06475  loss_mask_4: 0.1664  loss_dice_4: 0.2115  loss_ce_5: 0.06052  loss_mask_5: 0.1653  loss_dice_5: 0.2117  loss_ce_6: 0.052  loss_mask_6: 0.1665  loss_dice_6: 0.2167  loss_ce_7: 0.05895  loss_mask_7: 0.1605  loss_dice_7: 0.2253  loss_ce_8: 0.06655  loss_mask_8: 0.1623  loss_dice_8: 0.2062  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:22:05] d2.utils.events INFO:  eta: 0:26:09  iter: 37639  total_loss: 4.168  loss_ce: 0.005391  loss_mask: 0.2034  loss_dice: 0.1554  loss_ce_0: 0.06382  loss_mask_0: 0.2101  loss_dice_0: 0.1588  loss_ce_1: 0.00598  loss_mask_1: 0.205  loss_dice_1: 0.1631  loss_ce_2: 0.006607  loss_mask_2: 0.2208  loss_dice_2: 0.1573  loss_ce_3: 0.005673  loss_mask_3: 0.2065  loss_dice_3: 0.1577  loss_ce_4: 0.009058  loss_mask_4: 0.211  loss_dice_4: 0.156  loss_ce_5: 0.005796  loss_mask_5: 0.2053  loss_dice_5: 0.159  loss_ce_6: 0.006208  loss_mask_6: 0.219  loss_dice_6: 0.1499  loss_ce_7: 0.01047  loss_mask_7: 0.2067  loss_dice_7: 0.1595  loss_ce_8: 0.005844  loss_mask_8: 0.2184  loss_dice_8: 0.164  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:08] d2.utils.events INFO:  eta: 0:26:07  iter: 37659  total_loss: 4.074  loss_ce: 0.005497  loss_mask: 0.1837  loss_dice: 0.1469  loss_ce_0: 0.06619  loss_mask_0: 0.1709  loss_dice_0: 0.1435  loss_ce_1: 0.01649  loss_mask_1: 0.1765  loss_dice_1: 0.146  loss_ce_2: 0.005882  loss_mask_2: 0.1757  loss_dice_2: 0.1428  loss_ce_3: 0.005125  loss_mask_3: 0.1862  loss_dice_3: 0.1381  loss_ce_4: 0.00662  loss_mask_4: 0.1768  loss_dice_4: 0.1395  loss_ce_5: 0.006114  loss_mask_5: 0.1755  loss_dice_5: 0.1476  loss_ce_6: 0.006866  loss_mask_6: 0.1701  loss_dice_6: 0.1373  loss_ce_7: 0.01376  loss_mask_7: 0.1748  loss_dice_7: 0.1489  loss_ce_8: 0.005933  loss_mask_8: 0.1858  loss_dice_8: 0.1403  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:10] d2.utils.events INFO:  eta: 0:26:04  iter: 37679  total_loss: 3.792  loss_ce: 0.004918  loss_mask: 0.1979  loss_dice: 0.1523  loss_ce_0: 0.05788  loss_mask_0: 0.2022  loss_dice_0: 0.1482  loss_ce_1: 0.005869  loss_mask_1: 0.201  loss_dice_1: 0.1488  loss_ce_2: 0.004635  loss_mask_2: 0.1996  loss_dice_2: 0.1443  loss_ce_3: 0.004923  loss_mask_3: 0.1956  loss_dice_3: 0.1514  loss_ce_4: 0.004949  loss_mask_4: 0.1914  loss_dice_4: 0.1531  loss_ce_5: 0.005422  loss_mask_5: 0.2009  loss_dice_5: 0.1481  loss_ce_6: 0.004318  loss_mask_6: 0.1904  loss_dice_6: 0.1563  loss_ce_7: 0.005554  loss_mask_7: 0.1997  loss_dice_7: 0.1469  loss_ce_8: 0.005706  loss_mask_8: 0.1898  loss_dice_8: 0.1557  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:22:13] d2.utils.events INFO:  eta: 0:26:01  iter: 37699  total_loss: 5.596  loss_ce: 0.096  loss_mask: 0.2293  loss_dice: 0.1778  loss_ce_0: 0.06237  loss_mask_0: 0.2266  loss_dice_0: 0.1941  loss_ce_1: 0.06477  loss_mask_1: 0.2286  loss_dice_1: 0.1844  loss_ce_2: 0.09306  loss_mask_2: 0.2243  loss_dice_2: 0.1888  loss_ce_3: 0.08829  loss_mask_3: 0.214  loss_dice_3: 0.1852  loss_ce_4: 0.08512  loss_mask_4: 0.2319  loss_dice_4: 0.1829  loss_ce_5: 0.09412  loss_mask_5: 0.2234  loss_dice_5: 0.1754  loss_ce_6: 0.09554  loss_mask_6: 0.2303  loss_dice_6: 0.1772  loss_ce_7: 0.09118  loss_mask_7: 0.2234  loss_dice_7: 0.1943  loss_ce_8: 0.09916  loss_mask_8: 0.2207  loss_dice_8: 0.1878  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:15] d2.utils.events INFO:  eta: 0:25:58  iter: 37719  total_loss: 4.402  loss_ce: 0.01444  loss_mask: 0.2081  loss_dice: 0.1845  loss_ce_0: 0.06269  loss_mask_0: 0.2111  loss_dice_0: 0.1892  loss_ce_1: 0.01149  loss_mask_1: 0.2218  loss_dice_1: 0.1876  loss_ce_2: 0.01184  loss_mask_2: 0.2082  loss_dice_2: 0.1896  loss_ce_3: 0.01147  loss_mask_3: 0.2114  loss_dice_3: 0.1886  loss_ce_4: 0.01175  loss_mask_4: 0.1952  loss_dice_4: 0.1902  loss_ce_5: 0.01313  loss_mask_5: 0.2126  loss_dice_5: 0.1894  loss_ce_6: 0.01196  loss_mask_6: 0.2185  loss_dice_6: 0.183  loss_ce_7: 0.01323  loss_mask_7: 0.219  loss_dice_7: 0.1993  loss_ce_8: 0.01416  loss_mask_8: 0.2006  loss_dice_8: 0.1842  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:18] d2.utils.events INFO:  eta: 0:25:54  iter: 37739  total_loss: 4.212  loss_ce: 0.009157  loss_mask: 0.2014  loss_dice: 0.1608  loss_ce_0: 0.0617  loss_mask_0: 0.2057  loss_dice_0: 0.1597  loss_ce_1: 0.009935  loss_mask_1: 0.1986  loss_dice_1: 0.1637  loss_ce_2: 0.009535  loss_mask_2: 0.1927  loss_dice_2: 0.1596  loss_ce_3: 0.01021  loss_mask_3: 0.1985  loss_dice_3: 0.166  loss_ce_4: 0.00981  loss_mask_4: 0.1962  loss_dice_4: 0.1647  loss_ce_5: 0.01082  loss_mask_5: 0.1986  loss_dice_5: 0.1669  loss_ce_6: 0.009844  loss_mask_6: 0.1957  loss_dice_6: 0.1578  loss_ce_7: 0.01032  loss_mask_7: 0.196  loss_dice_7: 0.1651  loss_ce_8: 0.009939  loss_mask_8: 0.1957  loss_dice_8: 0.1612  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 16:22:20] d2.utils.events INFO:  eta: 0:25:51  iter: 37759  total_loss: 4.113  loss_ce: 0.008095  loss_mask: 0.1901  loss_dice: 0.1699  loss_ce_0: 0.05384  loss_mask_0: 0.1832  loss_dice_0: 0.1722  loss_ce_1: 0.008138  loss_mask_1: 0.1993  loss_dice_1: 0.1867  loss_ce_2: 0.008312  loss_mask_2: 0.1983  loss_dice_2: 0.1715  loss_ce_3: 0.008155  loss_mask_3: 0.2015  loss_dice_3: 0.1662  loss_ce_4: 0.007928  loss_mask_4: 0.2005  loss_dice_4: 0.1731  loss_ce_5: 0.008069  loss_mask_5: 0.1916  loss_dice_5: 0.1671  loss_ce_6: 0.008644  loss_mask_6: 0.1929  loss_dice_6: 0.1718  loss_ce_7: 0.007168  loss_mask_7: 0.1886  loss_dice_7: 0.1715  loss_ce_8: 0.008398  loss_mask_8: 0.203  loss_dice_8: 0.1759  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:23] d2.utils.events INFO:  eta: 0:25:46  iter: 37779  total_loss: 4.614  loss_ce: 0.003447  loss_mask: 0.1876  loss_dice: 0.1558  loss_ce_0: 0.06284  loss_mask_0: 0.1987  loss_dice_0: 0.1578  loss_ce_1: 0.002947  loss_mask_1: 0.191  loss_dice_1: 0.1501  loss_ce_2: 0.003852  loss_mask_2: 0.1926  loss_dice_2: 0.1599  loss_ce_3: 0.003605  loss_mask_3: 0.1908  loss_dice_3: 0.1503  loss_ce_4: 0.00249  loss_mask_4: 0.1937  loss_dice_4: 0.1543  loss_ce_5: 0.003376  loss_mask_5: 0.1927  loss_dice_5: 0.1557  loss_ce_6: 0.003491  loss_mask_6: 0.1917  loss_dice_6: 0.1572  loss_ce_7: 0.002201  loss_mask_7: 0.1864  loss_dice_7: 0.1529  loss_ce_8: 0.002656  loss_mask_8: 0.193  loss_dice_8: 0.1615  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:25] d2.utils.events INFO:  eta: 0:25:43  iter: 37799  total_loss: 4.551  loss_ce: 0.002751  loss_mask: 0.2378  loss_dice: 0.1994  loss_ce_0: 0.06975  loss_mask_0: 0.2269  loss_dice_0: 0.2089  loss_ce_1: 0.002378  loss_mask_1: 0.2402  loss_dice_1: 0.2055  loss_ce_2: 0.003383  loss_mask_2: 0.2389  loss_dice_2: 0.2138  loss_ce_3: 0.002795  loss_mask_3: 0.2312  loss_dice_3: 0.2049  loss_ce_4: 0.002277  loss_mask_4: 0.2201  loss_dice_4: 0.2076  loss_ce_5: 0.002748  loss_mask_5: 0.2297  loss_dice_5: 0.2102  loss_ce_6: 0.002582  loss_mask_6: 0.2332  loss_dice_6: 0.203  loss_ce_7: 0.002057  loss_mask_7: 0.2337  loss_dice_7: 0.206  loss_ce_8: 0.002309  loss_mask_8: 0.2346  loss_dice_8: 0.2096  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:28] d2.utils.events INFO:  eta: 0:25:40  iter: 37819  total_loss: 3.436  loss_ce: 0.002543  loss_mask: 0.1646  loss_dice: 0.1316  loss_ce_0: 0.05535  loss_mask_0: 0.1646  loss_dice_0: 0.1328  loss_ce_1: 0.002247  loss_mask_1: 0.1658  loss_dice_1: 0.1292  loss_ce_2: 0.002635  loss_mask_2: 0.1641  loss_dice_2: 0.1309  loss_ce_3: 0.0025  loss_mask_3: 0.1644  loss_dice_3: 0.1276  loss_ce_4: 0.001924  loss_mask_4: 0.1626  loss_dice_4: 0.1275  loss_ce_5: 0.002144  loss_mask_5: 0.1617  loss_dice_5: 0.1293  loss_ce_6: 0.002435  loss_mask_6: 0.1628  loss_dice_6: 0.1251  loss_ce_7: 0.001707  loss_mask_7: 0.1611  loss_dice_7: 0.1281  loss_ce_8: 0.001988  loss_mask_8: 0.1663  loss_dice_8: 0.1251  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:30] d2.utils.events INFO:  eta: 0:25:37  iter: 37839  total_loss: 4.282  loss_ce: 0.003548  loss_mask: 0.171  loss_dice: 0.1851  loss_ce_0: 0.07394  loss_mask_0: 0.1666  loss_dice_0: 0.1821  loss_ce_1: 0.003635  loss_mask_1: 0.1671  loss_dice_1: 0.1822  loss_ce_2: 0.003431  loss_mask_2: 0.1687  loss_dice_2: 0.1809  loss_ce_3: 0.003574  loss_mask_3: 0.1574  loss_dice_3: 0.1806  loss_ce_4: 0.003273  loss_mask_4: 0.1664  loss_dice_4: 0.1856  loss_ce_5: 0.00265  loss_mask_5: 0.1706  loss_dice_5: 0.1851  loss_ce_6: 0.004043  loss_mask_6: 0.1635  loss_dice_6: 0.1812  loss_ce_7: 0.00351  loss_mask_7: 0.161  loss_dice_7: 0.1856  loss_ce_8: 0.003004  loss_mask_8: 0.1773  loss_dice_8: 0.1947  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:22:33] d2.utils.events INFO:  eta: 0:25:36  iter: 37859  total_loss: 4.476  loss_ce: 0.004241  loss_mask: 0.1944  loss_dice: 0.1509  loss_ce_0: 0.06284  loss_mask_0: 0.1768  loss_dice_0: 0.1512  loss_ce_1: 0.004698  loss_mask_1: 0.1913  loss_dice_1: 0.158  loss_ce_2: 0.003581  loss_mask_2: 0.1837  loss_dice_2: 0.1616  loss_ce_3: 0.003924  loss_mask_3: 0.1831  loss_dice_3: 0.1544  loss_ce_4: 0.00459  loss_mask_4: 0.1904  loss_dice_4: 0.1579  loss_ce_5: 0.002658  loss_mask_5: 0.1875  loss_dice_5: 0.1553  loss_ce_6: 0.003849  loss_mask_6: 0.1847  loss_dice_6: 0.1589  loss_ce_7: 0.00388  loss_mask_7: 0.1859  loss_dice_7: 0.1519  loss_ce_8: 0.007003  loss_mask_8: 0.1873  loss_dice_8: 0.1549  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:36] d2.utils.events INFO:  eta: 0:25:33  iter: 37879  total_loss: 4.309  loss_ce: 0.0117  loss_mask: 0.2002  loss_dice: 0.1456  loss_ce_0: 0.07374  loss_mask_0: 0.2089  loss_dice_0: 0.1437  loss_ce_1: 0.004072  loss_mask_1: 0.2135  loss_dice_1: 0.1473  loss_ce_2: 0.007665  loss_mask_2: 0.2083  loss_dice_2: 0.1542  loss_ce_3: 0.01328  loss_mask_3: 0.2113  loss_dice_3: 0.1478  loss_ce_4: 0.006344  loss_mask_4: 0.2192  loss_dice_4: 0.1506  loss_ce_5: 0.007458  loss_mask_5: 0.2192  loss_dice_5: 0.147  loss_ce_6: 0.0093  loss_mask_6: 0.2113  loss_dice_6: 0.1498  loss_ce_7: 0.005825  loss_mask_7: 0.2101  loss_dice_7: 0.1495  loss_ce_8: 0.009322  loss_mask_8: 0.2057  loss_dice_8: 0.1413  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:38] d2.utils.events INFO:  eta: 0:25:31  iter: 37899  total_loss: 6.108  loss_ce: 0.005407  loss_mask: 0.2337  loss_dice: 0.1802  loss_ce_0: 0.06888  loss_mask_0: 0.241  loss_dice_0: 0.1847  loss_ce_1: 0.00282  loss_mask_1: 0.2322  loss_dice_1: 0.1884  loss_ce_2: 0.003988  loss_mask_2: 0.2311  loss_dice_2: 0.186  loss_ce_3: 0.003823  loss_mask_3: 0.2345  loss_dice_3: 0.1873  loss_ce_4: 0.003622  loss_mask_4: 0.2341  loss_dice_4: 0.1841  loss_ce_5: 0.003988  loss_mask_5: 0.2343  loss_dice_5: 0.1877  loss_ce_6: 0.004598  loss_mask_6: 0.2256  loss_dice_6: 0.1845  loss_ce_7: 0.003815  loss_mask_7: 0.2339  loss_dice_7: 0.1821  loss_ce_8: 0.003958  loss_mask_8: 0.2336  loss_dice_8: 0.1771  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:41] d2.utils.events INFO:  eta: 0:25:28  iter: 37919  total_loss: 3.692  loss_ce: 0.004813  loss_mask: 0.1842  loss_dice: 0.1596  loss_ce_0: 0.0622  loss_mask_0: 0.1777  loss_dice_0: 0.1542  loss_ce_1: 0.003776  loss_mask_1: 0.1817  loss_dice_1: 0.1647  loss_ce_2: 0.004342  loss_mask_2: 0.1788  loss_dice_2: 0.1567  loss_ce_3: 0.0036  loss_mask_3: 0.1727  loss_dice_3: 0.165  loss_ce_4: 0.003941  loss_mask_4: 0.1799  loss_dice_4: 0.1544  loss_ce_5: 0.004926  loss_mask_5: 0.179  loss_dice_5: 0.1609  loss_ce_6: 0.004346  loss_mask_6: 0.1628  loss_dice_6: 0.1522  loss_ce_7: 0.004221  loss_mask_7: 0.1772  loss_dice_7: 0.1558  loss_ce_8: 0.004776  loss_mask_8: 0.1812  loss_dice_8: 0.1656  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:43] d2.utils.events INFO:  eta: 0:25:24  iter: 37939  total_loss: 3.963  loss_ce: 0.002503  loss_mask: 0.2096  loss_dice: 0.1708  loss_ce_0: 0.06209  loss_mask_0: 0.2138  loss_dice_0: 0.1672  loss_ce_1: 0.002326  loss_mask_1: 0.2152  loss_dice_1: 0.1743  loss_ce_2: 0.003132  loss_mask_2: 0.2137  loss_dice_2: 0.1608  loss_ce_3: 0.002727  loss_mask_3: 0.2131  loss_dice_3: 0.1677  loss_ce_4: 0.002725  loss_mask_4: 0.2066  loss_dice_4: 0.1667  loss_ce_5: 0.003032  loss_mask_5: 0.218  loss_dice_5: 0.1613  loss_ce_6: 0.003106  loss_mask_6: 0.2184  loss_dice_6: 0.17  loss_ce_7: 0.002829  loss_mask_7: 0.2118  loss_dice_7: 0.166  loss_ce_8: 0.002993  loss_mask_8: 0.21  loss_dice_8: 0.1641  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:46] d2.utils.events INFO:  eta: 0:25:21  iter: 37959  total_loss: 4.898  loss_ce: 0.004944  loss_mask: 0.2084  loss_dice: 0.1612  loss_ce_0: 0.05937  loss_mask_0: 0.2213  loss_dice_0: 0.1644  loss_ce_1: 0.005771  loss_mask_1: 0.2161  loss_dice_1: 0.1679  loss_ce_2: 0.004952  loss_mask_2: 0.2065  loss_dice_2: 0.162  loss_ce_3: 0.004469  loss_mask_3: 0.2164  loss_dice_3: 0.1681  loss_ce_4: 0.006118  loss_mask_4: 0.204  loss_dice_4: 0.1613  loss_ce_5: 0.004945  loss_mask_5: 0.2018  loss_dice_5: 0.1598  loss_ce_6: 0.004554  loss_mask_6: 0.2043  loss_dice_6: 0.1633  loss_ce_7: 0.005894  loss_mask_7: 0.2051  loss_dice_7: 0.1659  loss_ce_8: 0.005808  loss_mask_8: 0.2111  loss_dice_8: 0.1678  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:48] d2.utils.events INFO:  eta: 0:25:17  iter: 37979  total_loss: 4.866  loss_ce: 0.003452  loss_mask: 0.2079  loss_dice: 0.1862  loss_ce_0: 0.06912  loss_mask_0: 0.1911  loss_dice_0: 0.1897  loss_ce_1: 0.003727  loss_mask_1: 0.2001  loss_dice_1: 0.1867  loss_ce_2: 0.004231  loss_mask_2: 0.1999  loss_dice_2: 0.1864  loss_ce_3: 0.003289  loss_mask_3: 0.2118  loss_dice_3: 0.1891  loss_ce_4: 0.003519  loss_mask_4: 0.2049  loss_dice_4: 0.1935  loss_ce_5: 0.004407  loss_mask_5: 0.2024  loss_dice_5: 0.1884  loss_ce_6: 0.004881  loss_mask_6: 0.2021  loss_dice_6: 0.1864  loss_ce_7: 0.003947  loss_mask_7: 0.2056  loss_dice_7: 0.1901  loss_ce_8: 0.005135  loss_mask_8: 0.201  loss_dice_8: 0.1871  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:51] d2.utils.events INFO:  eta: 0:25:15  iter: 37999  total_loss: 3.818  loss_ce: 0.003041  loss_mask: 0.1842  loss_dice: 0.1828  loss_ce_0: 0.05611  loss_mask_0: 0.1871  loss_dice_0: 0.1723  loss_ce_1: 0.003047  loss_mask_1: 0.1914  loss_dice_1: 0.172  loss_ce_2: 0.003456  loss_mask_2: 0.1884  loss_dice_2: 0.175  loss_ce_3: 0.003148  loss_mask_3: 0.1897  loss_dice_3: 0.1756  loss_ce_4: 0.003516  loss_mask_4: 0.1846  loss_dice_4: 0.1686  loss_ce_5: 0.003071  loss_mask_5: 0.1875  loss_dice_5: 0.1731  loss_ce_6: 0.003275  loss_mask_6: 0.1897  loss_dice_6: 0.1806  loss_ce_7: 0.003344  loss_mask_7: 0.1886  loss_dice_7: 0.1675  loss_ce_8: 0.003208  loss_mask_8: 0.1791  loss_dice_8: 0.1718  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:53] d2.utils.events INFO:  eta: 0:25:12  iter: 38019  total_loss: 4.499  loss_ce: 0.01427  loss_mask: 0.151  loss_dice: 0.1968  loss_ce_0: 0.06196  loss_mask_0: 0.1539  loss_dice_0: 0.1893  loss_ce_1: 0.029  loss_mask_1: 0.1535  loss_dice_1: 0.192  loss_ce_2: 0.03193  loss_mask_2: 0.1545  loss_dice_2: 0.1848  loss_ce_3: 0.008519  loss_mask_3: 0.154  loss_dice_3: 0.1883  loss_ce_4: 0.02283  loss_mask_4: 0.1505  loss_dice_4: 0.1859  loss_ce_5: 0.03412  loss_mask_5: 0.1526  loss_dice_5: 0.1967  loss_ce_6: 0.0115  loss_mask_6: 0.1533  loss_dice_6: 0.1947  loss_ce_7: 0.02861  loss_mask_7: 0.1581  loss_dice_7: 0.195  loss_ce_8: 0.03364  loss_mask_8: 0.1579  loss_dice_8: 0.1944  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:22:56] d2.utils.events INFO:  eta: 0:25:10  iter: 38039  total_loss: 3.922  loss_ce: 0.002543  loss_mask: 0.2227  loss_dice: 0.1579  loss_ce_0: 0.05592  loss_mask_0: 0.2164  loss_dice_0: 0.1538  loss_ce_1: 0.002929  loss_mask_1: 0.2086  loss_dice_1: 0.1568  loss_ce_2: 0.003325  loss_mask_2: 0.2212  loss_dice_2: 0.1589  loss_ce_3: 0.002543  loss_mask_3: 0.2184  loss_dice_3: 0.1506  loss_ce_4: 0.003517  loss_mask_4: 0.2264  loss_dice_4: 0.1552  loss_ce_5: 0.00288  loss_mask_5: 0.2189  loss_dice_5: 0.1579  loss_ce_6: 0.002431  loss_mask_6: 0.2168  loss_dice_6: 0.1611  loss_ce_7: 0.00326  loss_mask_7: 0.2195  loss_dice_7: 0.1581  loss_ce_8: 0.002929  loss_mask_8: 0.2151  loss_dice_8: 0.1497  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:22:58] d2.utils.events INFO:  eta: 0:25:08  iter: 38059  total_loss: 4.23  loss_ce: 0.07506  loss_mask: 0.2024  loss_dice: 0.1309  loss_ce_0: 0.0519  loss_mask_0: 0.2031  loss_dice_0: 0.1277  loss_ce_1: 0.03216  loss_mask_1: 0.2001  loss_dice_1: 0.1285  loss_ce_2: 0.0243  loss_mask_2: 0.1965  loss_dice_2: 0.1261  loss_ce_3: 0.08095  loss_mask_3: 0.2061  loss_dice_3: 0.1304  loss_ce_4: 0.05858  loss_mask_4: 0.2007  loss_dice_4: 0.1257  loss_ce_5: 0.0374  loss_mask_5: 0.1955  loss_dice_5: 0.1287  loss_ce_6: 0.07939  loss_mask_6: 0.2021  loss_dice_6: 0.1297  loss_ce_7: 0.03887  loss_mask_7: 0.1963  loss_dice_7: 0.1308  loss_ce_8: 0.02172  loss_mask_8: 0.2072  loss_dice_8: 0.1322  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:01] d2.utils.events INFO:  eta: 0:25:06  iter: 38079  total_loss: 3.732  loss_ce: 0.002894  loss_mask: 0.1953  loss_dice: 0.1527  loss_ce_0: 0.07175  loss_mask_0: 0.1883  loss_dice_0: 0.1516  loss_ce_1: 0.004161  loss_mask_1: 0.1854  loss_dice_1: 0.1563  loss_ce_2: 0.00399  loss_mask_2: 0.1891  loss_dice_2: 0.1538  loss_ce_3: 0.002732  loss_mask_3: 0.1901  loss_dice_3: 0.1501  loss_ce_4: 0.00386  loss_mask_4: 0.1822  loss_dice_4: 0.155  loss_ce_5: 0.003195  loss_mask_5: 0.1851  loss_dice_5: 0.1417  loss_ce_6: 0.003041  loss_mask_6: 0.1862  loss_dice_6: 0.1603  loss_ce_7: 0.004365  loss_mask_7: 0.1884  loss_dice_7: 0.146  loss_ce_8: 0.003749  loss_mask_8: 0.1879  loss_dice_8: 0.156  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:03] d2.utils.events INFO:  eta: 0:25:04  iter: 38099  total_loss: 4.135  loss_ce: 0.002765  loss_mask: 0.1779  loss_dice: 0.2096  loss_ce_0: 0.06444  loss_mask_0: 0.1859  loss_dice_0: 0.2049  loss_ce_1: 0.005039  loss_mask_1: 0.1783  loss_dice_1: 0.21  loss_ce_2: 0.003147  loss_mask_2: 0.1816  loss_dice_2: 0.212  loss_ce_3: 0.002435  loss_mask_3: 0.1876  loss_dice_3: 0.2045  loss_ce_4: 0.003345  loss_mask_4: 0.1822  loss_dice_4: 0.2079  loss_ce_5: 0.002899  loss_mask_5: 0.1833  loss_dice_5: 0.205  loss_ce_6: 0.002428  loss_mask_6: 0.1805  loss_dice_6: 0.2163  loss_ce_7: 0.003778  loss_mask_7: 0.1799  loss_dice_7: 0.2153  loss_ce_8: 0.003176  loss_mask_8: 0.1797  loss_dice_8: 0.2144  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:06] d2.utils.events INFO:  eta: 0:25:02  iter: 38119  total_loss: 3.778  loss_ce: 0.003259  loss_mask: 0.1904  loss_dice: 0.1118  loss_ce_0: 0.06245  loss_mask_0: 0.19  loss_dice_0: 0.113  loss_ce_1: 0.003974  loss_mask_1: 0.1941  loss_dice_1: 0.1171  loss_ce_2: 0.003194  loss_mask_2: 0.1845  loss_dice_2: 0.1142  loss_ce_3: 0.002719  loss_mask_3: 0.1949  loss_dice_3: 0.1149  loss_ce_4: 0.00349  loss_mask_4: 0.1884  loss_dice_4: 0.1169  loss_ce_5: 0.003233  loss_mask_5: 0.1833  loss_dice_5: 0.1131  loss_ce_6: 0.00299  loss_mask_6: 0.197  loss_dice_6: 0.1109  loss_ce_7: 0.003634  loss_mask_7: 0.1935  loss_dice_7: 0.1157  loss_ce_8: 0.003208  loss_mask_8: 0.1946  loss_dice_8: 0.1187  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:08] d2.utils.events INFO:  eta: 0:24:59  iter: 38139  total_loss: 3.946  loss_ce: 0.003204  loss_mask: 0.2118  loss_dice: 0.1568  loss_ce_0: 0.06227  loss_mask_0: 0.2093  loss_dice_0: 0.1577  loss_ce_1: 0.004226  loss_mask_1: 0.2146  loss_dice_1: 0.1522  loss_ce_2: 0.003109  loss_mask_2: 0.2237  loss_dice_2: 0.1574  loss_ce_3: 0.003031  loss_mask_3: 0.2094  loss_dice_3: 0.1607  loss_ce_4: 0.003733  loss_mask_4: 0.2203  loss_dice_4: 0.1616  loss_ce_5: 0.003046  loss_mask_5: 0.217  loss_dice_5: 0.1567  loss_ce_6: 0.003167  loss_mask_6: 0.2216  loss_dice_6: 0.1605  loss_ce_7: 0.003586  loss_mask_7: 0.2062  loss_dice_7: 0.1531  loss_ce_8: 0.002987  loss_mask_8: 0.2122  loss_dice_8: 0.1548  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:11] d2.utils.events INFO:  eta: 0:24:56  iter: 38159  total_loss: 4.199  loss_ce: 0.001561  loss_mask: 0.2012  loss_dice: 0.188  loss_ce_0: 0.06249  loss_mask_0: 0.1953  loss_dice_0: 0.1825  loss_ce_1: 0.002977  loss_mask_1: 0.2006  loss_dice_1: 0.1872  loss_ce_2: 0.001958  loss_mask_2: 0.1961  loss_dice_2: 0.1861  loss_ce_3: 0.00185  loss_mask_3: 0.1977  loss_dice_3: 0.1868  loss_ce_4: 0.001935  loss_mask_4: 0.2005  loss_dice_4: 0.1763  loss_ce_5: 0.001825  loss_mask_5: 0.2003  loss_dice_5: 0.1852  loss_ce_6: 0.001766  loss_mask_6: 0.2024  loss_dice_6: 0.1875  loss_ce_7: 0.001874  loss_mask_7: 0.1999  loss_dice_7: 0.181  loss_ce_8: 0.002159  loss_mask_8: 0.1978  loss_dice_8: 0.1867  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:14] d2.utils.events INFO:  eta: 0:24:54  iter: 38179  total_loss: 4.275  loss_ce: 0.01098  loss_mask: 0.1998  loss_dice: 0.1807  loss_ce_0: 0.06317  loss_mask_0: 0.2064  loss_dice_0: 0.1808  loss_ce_1: 0.00964  loss_mask_1: 0.2057  loss_dice_1: 0.168  loss_ce_2: 0.008711  loss_mask_2: 0.2042  loss_dice_2: 0.1762  loss_ce_3: 0.009047  loss_mask_3: 0.2122  loss_dice_3: 0.1809  loss_ce_4: 0.009194  loss_mask_4: 0.2043  loss_dice_4: 0.1733  loss_ce_5: 0.00802  loss_mask_5: 0.1992  loss_dice_5: 0.1738  loss_ce_6: 0.009115  loss_mask_6: 0.1958  loss_dice_6: 0.1749  loss_ce_7: 0.008419  loss_mask_7: 0.1941  loss_dice_7: 0.1762  loss_ce_8: 0.007164  loss_mask_8: 0.2077  loss_dice_8: 0.1732  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:16] d2.utils.events INFO:  eta: 0:24:50  iter: 38199  total_loss: 3.84  loss_ce: 0.008463  loss_mask: 0.2092  loss_dice: 0.1585  loss_ce_0: 0.06266  loss_mask_0: 0.2027  loss_dice_0: 0.1689  loss_ce_1: 0.007326  loss_mask_1: 0.2077  loss_dice_1: 0.1649  loss_ce_2: 0.007102  loss_mask_2: 0.2068  loss_dice_2: 0.1598  loss_ce_3: 0.009379  loss_mask_3: 0.2145  loss_dice_3: 0.1606  loss_ce_4: 0.009664  loss_mask_4: 0.1993  loss_dice_4: 0.1578  loss_ce_5: 0.008808  loss_mask_5: 0.2114  loss_dice_5: 0.1637  loss_ce_6: 0.009764  loss_mask_6: 0.203  loss_dice_6: 0.1633  loss_ce_7: 0.008981  loss_mask_7: 0.2056  loss_dice_7: 0.1609  loss_ce_8: 0.007404  loss_mask_8: 0.204  loss_dice_8: 0.1588  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:19] d2.utils.events INFO:  eta: 0:24:47  iter: 38219  total_loss: 4.045  loss_ce: 0.005702  loss_mask: 0.1928  loss_dice: 0.1383  loss_ce_0: 0.062  loss_mask_0: 0.1982  loss_dice_0: 0.1464  loss_ce_1: 0.00566  loss_mask_1: 0.2006  loss_dice_1: 0.1422  loss_ce_2: 0.004712  loss_mask_2: 0.1964  loss_dice_2: 0.1422  loss_ce_3: 0.006365  loss_mask_3: 0.1957  loss_dice_3: 0.1409  loss_ce_4: 0.006061  loss_mask_4: 0.1985  loss_dice_4: 0.1382  loss_ce_5: 0.005658  loss_mask_5: 0.201  loss_dice_5: 0.1423  loss_ce_6: 0.006654  loss_mask_6: 0.1923  loss_dice_6: 0.1406  loss_ce_7: 0.006203  loss_mask_7: 0.1947  loss_dice_7: 0.1459  loss_ce_8: 0.004951  loss_mask_8: 0.1995  loss_dice_8: 0.1412  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:21] d2.utils.events INFO:  eta: 0:24:45  iter: 38239  total_loss: 3.871  loss_ce: 0.009042  loss_mask: 0.1749  loss_dice: 0.1737  loss_ce_0: 0.05992  loss_mask_0: 0.1669  loss_dice_0: 0.1726  loss_ce_1: 0.005938  loss_mask_1: 0.1697  loss_dice_1: 0.1752  loss_ce_2: 0.005608  loss_mask_2: 0.1709  loss_dice_2: 0.1698  loss_ce_3: 0.009419  loss_mask_3: 0.1714  loss_dice_3: 0.176  loss_ce_4: 0.007121  loss_mask_4: 0.1719  loss_dice_4: 0.1703  loss_ce_5: 0.007028  loss_mask_5: 0.1714  loss_dice_5: 0.1743  loss_ce_6: 0.009859  loss_mask_6: 0.1671  loss_dice_6: 0.1681  loss_ce_7: 0.006825  loss_mask_7: 0.1614  loss_dice_7: 0.1793  loss_ce_8: 0.005759  loss_mask_8: 0.1747  loss_dice_8: 0.1761  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:24] d2.utils.events INFO:  eta: 0:24:43  iter: 38259  total_loss: 4.491  loss_ce: 0.01904  loss_mask: 0.1894  loss_dice: 0.2243  loss_ce_0: 0.0675  loss_mask_0: 0.1852  loss_dice_0: 0.2213  loss_ce_1: 0.01882  loss_mask_1: 0.1895  loss_dice_1: 0.2247  loss_ce_2: 0.02478  loss_mask_2: 0.1872  loss_dice_2: 0.2171  loss_ce_3: 0.01946  loss_mask_3: 0.1864  loss_dice_3: 0.2234  loss_ce_4: 0.01605  loss_mask_4: 0.188  loss_dice_4: 0.2194  loss_ce_5: 0.02224  loss_mask_5: 0.184  loss_dice_5: 0.2233  loss_ce_6: 0.01777  loss_mask_6: 0.183  loss_dice_6: 0.2257  loss_ce_7: 0.01675  loss_mask_7: 0.1854  loss_dice_7: 0.2246  loss_ce_8: 0.02172  loss_mask_8: 0.1855  loss_dice_8: 0.2177  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:26] d2.utils.events INFO:  eta: 0:24:41  iter: 38279  total_loss: 4.73  loss_ce: 0.01936  loss_mask: 0.2137  loss_dice: 0.1663  loss_ce_0: 0.068  loss_mask_0: 0.2008  loss_dice_0: 0.1642  loss_ce_1: 0.01998  loss_mask_1: 0.2093  loss_dice_1: 0.1678  loss_ce_2: 0.02735  loss_mask_2: 0.2027  loss_dice_2: 0.1737  loss_ce_3: 0.02404  loss_mask_3: 0.1998  loss_dice_3: 0.1613  loss_ce_4: 0.0292  loss_mask_4: 0.2106  loss_dice_4: 0.1686  loss_ce_5: 0.02517  loss_mask_5: 0.2149  loss_dice_5: 0.1629  loss_ce_6: 0.02323  loss_mask_6: 0.2091  loss_dice_6: 0.1608  loss_ce_7: 0.02623  loss_mask_7: 0.2072  loss_dice_7: 0.1661  loss_ce_8: 0.01899  loss_mask_8: 0.2112  loss_dice_8: 0.1623  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:29] d2.utils.events INFO:  eta: 0:24:39  iter: 38299  total_loss: 4.123  loss_ce: 0.006297  loss_mask: 0.1932  loss_dice: 0.1671  loss_ce_0: 0.06546  loss_mask_0: 0.195  loss_dice_0: 0.165  loss_ce_1: 0.009055  loss_mask_1: 0.2034  loss_dice_1: 0.1679  loss_ce_2: 0.006744  loss_mask_2: 0.203  loss_dice_2: 0.1697  loss_ce_3: 0.007477  loss_mask_3: 0.199  loss_dice_3: 0.1681  loss_ce_4: 0.007407  loss_mask_4: 0.2055  loss_dice_4: 0.169  loss_ce_5: 0.006311  loss_mask_5: 0.2009  loss_dice_5: 0.1654  loss_ce_6: 0.007836  loss_mask_6: 0.1935  loss_dice_6: 0.1696  loss_ce_7: 0.008533  loss_mask_7: 0.1931  loss_dice_7: 0.1612  loss_ce_8: 0.006575  loss_mask_8: 0.1899  loss_dice_8: 0.1702  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:31] d2.utils.events INFO:  eta: 0:24:38  iter: 38319  total_loss: 4.666  loss_ce: 0.005974  loss_mask: 0.1319  loss_dice: 0.2195  loss_ce_0: 0.06446  loss_mask_0: 0.1354  loss_dice_0: 0.2255  loss_ce_1: 0.01553  loss_mask_1: 0.1343  loss_dice_1: 0.2304  loss_ce_2: 0.005034  loss_mask_2: 0.1459  loss_dice_2: 0.2261  loss_ce_3: 0.006231  loss_mask_3: 0.1441  loss_dice_3: 0.2218  loss_ce_4: 0.007018  loss_mask_4: 0.1375  loss_dice_4: 0.2261  loss_ce_5: 0.005038  loss_mask_5: 0.141  loss_dice_5: 0.2246  loss_ce_6: 0.007199  loss_mask_6: 0.1352  loss_dice_6: 0.2232  loss_ce_7: 0.007185  loss_mask_7: 0.1394  loss_dice_7: 0.2296  loss_ce_8: 0.005679  loss_mask_8: 0.138  loss_dice_8: 0.2228  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:34] d2.utils.events INFO:  eta: 0:24:34  iter: 38339  total_loss: 4.419  loss_ce: 0.003142  loss_mask: 0.2095  loss_dice: 0.162  loss_ce_0: 0.06389  loss_mask_0: 0.2009  loss_dice_0: 0.1522  loss_ce_1: 0.005318  loss_mask_1: 0.2117  loss_dice_1: 0.1599  loss_ce_2: 0.003328  loss_mask_2: 0.1985  loss_dice_2: 0.162  loss_ce_3: 0.001797  loss_mask_3: 0.2089  loss_dice_3: 0.1616  loss_ce_4: 0.002079  loss_mask_4: 0.2095  loss_dice_4: 0.1623  loss_ce_5: 0.002535  loss_mask_5: 0.2005  loss_dice_5: 0.1632  loss_ce_6: 0.002219  loss_mask_6: 0.2182  loss_dice_6: 0.1627  loss_ce_7: 0.002573  loss_mask_7: 0.2047  loss_dice_7: 0.1561  loss_ce_8: 0.003082  loss_mask_8: 0.205  loss_dice_8: 0.1607  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:36] d2.utils.events INFO:  eta: 0:24:31  iter: 38359  total_loss: 4.115  loss_ce: 0.005114  loss_mask: 0.1499  loss_dice: 0.233  loss_ce_0: 0.06063  loss_mask_0: 0.1462  loss_dice_0: 0.2133  loss_ce_1: 0.00946  loss_mask_1: 0.1547  loss_dice_1: 0.2276  loss_ce_2: 0.004476  loss_mask_2: 0.1527  loss_dice_2: 0.2181  loss_ce_3: 0.005787  loss_mask_3: 0.1497  loss_dice_3: 0.2324  loss_ce_4: 0.00589  loss_mask_4: 0.1498  loss_dice_4: 0.2199  loss_ce_5: 0.003866  loss_mask_5: 0.1392  loss_dice_5: 0.2239  loss_ce_6: 0.006406  loss_mask_6: 0.1397  loss_dice_6: 0.241  loss_ce_7: 0.005414  loss_mask_7: 0.1443  loss_dice_7: 0.2167  loss_ce_8: 0.003981  loss_mask_8: 0.1487  loss_dice_8: 0.2361  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:39] d2.utils.events INFO:  eta: 0:24:28  iter: 38379  total_loss: 4.191  loss_ce: 0.001666  loss_mask: 0.1986  loss_dice: 0.2181  loss_ce_0: 0.06148  loss_mask_0: 0.1871  loss_dice_0: 0.2186  loss_ce_1: 0.001002  loss_mask_1: 0.1951  loss_dice_1: 0.2132  loss_ce_2: 0.001381  loss_mask_2: 0.1893  loss_dice_2: 0.2214  loss_ce_3: 0.0009986  loss_mask_3: 0.1955  loss_dice_3: 0.2082  loss_ce_4: 0.001055  loss_mask_4: 0.1967  loss_dice_4: 0.2086  loss_ce_5: 0.001092  loss_mask_5: 0.1956  loss_dice_5: 0.2102  loss_ce_6: 0.001364  loss_mask_6: 0.1841  loss_dice_6: 0.2153  loss_ce_7: 0.001072  loss_mask_7: 0.1953  loss_dice_7: 0.2157  loss_ce_8: 0.001454  loss_mask_8: 0.1961  loss_dice_8: 0.2166  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:41] d2.utils.events INFO:  eta: 0:24:24  iter: 38399  total_loss: 4.331  loss_ce: 0.001633  loss_mask: 0.207  loss_dice: 0.1751  loss_ce_0: 0.07193  loss_mask_0: 0.2084  loss_dice_0: 0.1719  loss_ce_1: 0.001924  loss_mask_1: 0.2227  loss_dice_1: 0.1659  loss_ce_2: 0.001917  loss_mask_2: 0.2218  loss_dice_2: 0.1663  loss_ce_3: 0.001398  loss_mask_3: 0.2081  loss_dice_3: 0.1719  loss_ce_4: 0.001392  loss_mask_4: 0.2165  loss_dice_4: 0.1668  loss_ce_5: 0.001746  loss_mask_5: 0.2176  loss_dice_5: 0.1672  loss_ce_6: 0.001731  loss_mask_6: 0.2237  loss_dice_6: 0.1628  loss_ce_7: 0.001423  loss_mask_7: 0.209  loss_dice_7: 0.1687  loss_ce_8: 0.002087  loss_mask_8: 0.2151  loss_dice_8: 0.1661  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:44] d2.utils.events INFO:  eta: 0:24:22  iter: 38419  total_loss: 4.716  loss_ce: 0.005765  loss_mask: 0.1783  loss_dice: 0.2259  loss_ce_0: 0.06644  loss_mask_0: 0.1699  loss_dice_0: 0.2134  loss_ce_1: 0.005151  loss_mask_1: 0.1719  loss_dice_1: 0.2105  loss_ce_2: 0.004763  loss_mask_2: 0.171  loss_dice_2: 0.2277  loss_ce_3: 0.006033  loss_mask_3: 0.1691  loss_dice_3: 0.2257  loss_ce_4: 0.006479  loss_mask_4: 0.1757  loss_dice_4: 0.2236  loss_ce_5: 0.005685  loss_mask_5: 0.1672  loss_dice_5: 0.2091  loss_ce_6: 0.007005  loss_mask_6: 0.1641  loss_dice_6: 0.215  loss_ce_7: 0.005739  loss_mask_7: 0.1696  loss_dice_7: 0.2207  loss_ce_8: 0.005201  loss_mask_8: 0.1765  loss_dice_8: 0.2196  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:46] d2.utils.events INFO:  eta: 0:24:18  iter: 38439  total_loss: 4.318  loss_ce: 0.003275  loss_mask: 0.1871  loss_dice: 0.1957  loss_ce_0: 0.06069  loss_mask_0: 0.193  loss_dice_0: 0.1967  loss_ce_1: 0.004311  loss_mask_1: 0.1972  loss_dice_1: 0.2016  loss_ce_2: 0.003019  loss_mask_2: 0.1872  loss_dice_2: 0.2056  loss_ce_3: 0.003253  loss_mask_3: 0.194  loss_dice_3: 0.2009  loss_ce_4: 0.004025  loss_mask_4: 0.1891  loss_dice_4: 0.1976  loss_ce_5: 0.00356  loss_mask_5: 0.1907  loss_dice_5: 0.2064  loss_ce_6: 0.003547  loss_mask_6: 0.1967  loss_dice_6: 0.1901  loss_ce_7: 0.004611  loss_mask_7: 0.1872  loss_dice_7: 0.1981  loss_ce_8: 0.003337  loss_mask_8: 0.19  loss_dice_8: 0.204  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:49] d2.utils.events INFO:  eta: 0:24:15  iter: 38459  total_loss: 4.829  loss_ce: 0.004122  loss_mask: 0.2298  loss_dice: 0.2019  loss_ce_0: 0.05492  loss_mask_0: 0.2193  loss_dice_0: 0.2015  loss_ce_1: 0.004745  loss_mask_1: 0.2186  loss_dice_1: 0.2044  loss_ce_2: 0.005725  loss_mask_2: 0.2201  loss_dice_2: 0.2055  loss_ce_3: 0.004282  loss_mask_3: 0.2297  loss_dice_3: 0.1989  loss_ce_4: 0.00495  loss_mask_4: 0.2222  loss_dice_4: 0.2054  loss_ce_5: 0.00738  loss_mask_5: 0.2167  loss_dice_5: 0.2047  loss_ce_6: 0.004989  loss_mask_6: 0.2232  loss_dice_6: 0.2052  loss_ce_7: 0.005438  loss_mask_7: 0.227  loss_dice_7: 0.1959  loss_ce_8: 0.006687  loss_mask_8: 0.2043  loss_dice_8: 0.2034  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:23:51] d2.utils.events INFO:  eta: 0:24:12  iter: 38479  total_loss: 4.152  loss_ce: 0.007479  loss_mask: 0.2146  loss_dice: 0.1653  loss_ce_0: 0.06748  loss_mask_0: 0.2081  loss_dice_0: 0.1613  loss_ce_1: 0.005536  loss_mask_1: 0.2122  loss_dice_1: 0.1648  loss_ce_2: 0.0077  loss_mask_2: 0.1975  loss_dice_2: 0.1678  loss_ce_3: 0.007961  loss_mask_3: 0.21  loss_dice_3: 0.159  loss_ce_4: 0.00687  loss_mask_4: 0.2069  loss_dice_4: 0.1599  loss_ce_5: 0.009133  loss_mask_5: 0.2091  loss_dice_5: 0.1654  loss_ce_6: 0.00904  loss_mask_6: 0.2166  loss_dice_6: 0.1628  loss_ce_7: 0.005953  loss_mask_7: 0.2111  loss_dice_7: 0.1692  loss_ce_8: 0.00826  loss_mask_8: 0.2088  loss_dice_8: 0.1633  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:23:54] d2.utils.events INFO:  eta: 0:24:10  iter: 38499  total_loss: 4.565  loss_ce: 0.0009531  loss_mask: 0.234  loss_dice: 0.1839  loss_ce_0: 0.05873  loss_mask_0: 0.217  loss_dice_0: 0.1804  loss_ce_1: 0.0012  loss_mask_1: 0.2213  loss_dice_1: 0.1866  loss_ce_2: 0.0007587  loss_mask_2: 0.2293  loss_dice_2: 0.1806  loss_ce_3: 0.000802  loss_mask_3: 0.2267  loss_dice_3: 0.1854  loss_ce_4: 0.00102  loss_mask_4: 0.2185  loss_dice_4: 0.1759  loss_ce_5: 0.0008753  loss_mask_5: 0.2156  loss_dice_5: 0.1766  loss_ce_6: 0.0008821  loss_mask_6: 0.2169  loss_dice_6: 0.1862  loss_ce_7: 0.001414  loss_mask_7: 0.2276  loss_dice_7: 0.1816  loss_ce_8: 0.0008657  loss_mask_8: 0.2201  loss_dice_8: 0.1867  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:57] d2.utils.events INFO:  eta: 0:24:08  iter: 38519  total_loss: 3.973  loss_ce: 0.003107  loss_mask: 0.2271  loss_dice: 0.1577  loss_ce_0: 0.06204  loss_mask_0: 0.226  loss_dice_0: 0.1621  loss_ce_1: 0.002302  loss_mask_1: 0.2234  loss_dice_1: 0.1665  loss_ce_2: 0.003934  loss_mask_2: 0.223  loss_dice_2: 0.1581  loss_ce_3: 0.002786  loss_mask_3: 0.2304  loss_dice_3: 0.1646  loss_ce_4: 0.002657  loss_mask_4: 0.2215  loss_dice_4: 0.159  loss_ce_5: 0.003408  loss_mask_5: 0.2243  loss_dice_5: 0.1548  loss_ce_6: 0.002929  loss_mask_6: 0.2293  loss_dice_6: 0.1654  loss_ce_7: 0.001944  loss_mask_7: 0.215  loss_dice_7: 0.1532  loss_ce_8: 0.003508  loss_mask_8: 0.2306  loss_dice_8: 0.1657  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:23:59] d2.utils.events INFO:  eta: 0:24:08  iter: 38539  total_loss: 3.847  loss_ce: 0.002277  loss_mask: 0.2189  loss_dice: 0.1306  loss_ce_0: 0.06805  loss_mask_0: 0.2076  loss_dice_0: 0.1346  loss_ce_1: 0.002028  loss_mask_1: 0.2151  loss_dice_1: 0.1265  loss_ce_2: 0.002731  loss_mask_2: 0.2116  loss_dice_2: 0.1366  loss_ce_3: 0.001768  loss_mask_3: 0.2111  loss_dice_3: 0.1277  loss_ce_4: 0.001759  loss_mask_4: 0.2124  loss_dice_4: 0.1326  loss_ce_5: 0.002084  loss_mask_5: 0.2138  loss_dice_5: 0.1287  loss_ce_6: 0.001792  loss_mask_6: 0.2077  loss_dice_6: 0.133  loss_ce_7: 0.001353  loss_mask_7: 0.2102  loss_dice_7: 0.126  loss_ce_8: 0.002432  loss_mask_8: 0.2168  loss_dice_8: 0.1296  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:02] d2.utils.events INFO:  eta: 0:24:06  iter: 38559  total_loss: 4.326  loss_ce: 0.005069  loss_mask: 0.217  loss_dice: 0.1556  loss_ce_0: 0.06044  loss_mask_0: 0.2194  loss_dice_0: 0.1553  loss_ce_1: 0.004627  loss_mask_1: 0.2146  loss_dice_1: 0.1546  loss_ce_2: 0.005097  loss_mask_2: 0.2203  loss_dice_2: 0.1559  loss_ce_3: 0.005265  loss_mask_3: 0.2158  loss_dice_3: 0.1569  loss_ce_4: 0.004927  loss_mask_4: 0.2128  loss_dice_4: 0.1539  loss_ce_5: 0.005369  loss_mask_5: 0.2191  loss_dice_5: 0.1552  loss_ce_6: 0.005531  loss_mask_6: 0.219  loss_dice_6: 0.1556  loss_ce_7: 0.004971  loss_mask_7: 0.2104  loss_dice_7: 0.1611  loss_ce_8: 0.005635  loss_mask_8: 0.2084  loss_dice_8: 0.1584  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:04] d2.utils.events INFO:  eta: 0:24:04  iter: 38579  total_loss: 5.166  loss_ce: 0.02616  loss_mask: 0.184  loss_dice: 0.2256  loss_ce_0: 0.0622  loss_mask_0: 0.1706  loss_dice_0: 0.2211  loss_ce_1: 0.009007  loss_mask_1: 0.1774  loss_dice_1: 0.229  loss_ce_2: 0.03023  loss_mask_2: 0.1876  loss_dice_2: 0.2174  loss_ce_3: 0.02669  loss_mask_3: 0.1744  loss_dice_3: 0.2118  loss_ce_4: 0.02366  loss_mask_4: 0.1756  loss_dice_4: 0.2059  loss_ce_5: 0.03964  loss_mask_5: 0.1809  loss_dice_5: 0.2176  loss_ce_6: 0.02308  loss_mask_6: 0.1709  loss_dice_6: 0.2261  loss_ce_7: 0.0261  loss_mask_7: 0.181  loss_dice_7: 0.2198  loss_ce_8: 0.03562  loss_mask_8: 0.1781  loss_dice_8: 0.2121  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:07] d2.utils.events INFO:  eta: 0:24:01  iter: 38599  total_loss: 4.433  loss_ce: 0.005724  loss_mask: 0.2018  loss_dice: 0.1718  loss_ce_0: 0.06039  loss_mask_0: 0.1909  loss_dice_0: 0.1722  loss_ce_1: 0.004445  loss_mask_1: 0.2106  loss_dice_1: 0.168  loss_ce_2: 0.00615  loss_mask_2: 0.2021  loss_dice_2: 0.1679  loss_ce_3: 0.006213  loss_mask_3: 0.201  loss_dice_3: 0.172  loss_ce_4: 0.004733  loss_mask_4: 0.2147  loss_dice_4: 0.1715  loss_ce_5: 0.007112  loss_mask_5: 0.2026  loss_dice_5: 0.1696  loss_ce_6: 0.006702  loss_mask_6: 0.2018  loss_dice_6: 0.1677  loss_ce_7: 0.006496  loss_mask_7: 0.2066  loss_dice_7: 0.162  loss_ce_8: 0.006428  loss_mask_8: 0.2138  loss_dice_8: 0.171  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:09] d2.utils.events INFO:  eta: 0:23:59  iter: 38619  total_loss: 4.133  loss_ce: 0.005091  loss_mask: 0.2384  loss_dice: 0.1459  loss_ce_0: 0.05682  loss_mask_0: 0.2358  loss_dice_0: 0.1383  loss_ce_1: 0.003816  loss_mask_1: 0.2369  loss_dice_1: 0.143  loss_ce_2: 0.006066  loss_mask_2: 0.2365  loss_dice_2: 0.1425  loss_ce_3: 0.006088  loss_mask_3: 0.2341  loss_dice_3: 0.1486  loss_ce_4: 0.006111  loss_mask_4: 0.2255  loss_dice_4: 0.14  loss_ce_5: 0.006771  loss_mask_5: 0.2312  loss_dice_5: 0.1514  loss_ce_6: 0.006601  loss_mask_6: 0.2279  loss_dice_6: 0.1504  loss_ce_7: 0.006201  loss_mask_7: 0.2311  loss_dice_7: 0.1467  loss_ce_8: 0.009166  loss_mask_8: 0.2231  loss_dice_8: 0.1415  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:12] d2.utils.events INFO:  eta: 0:23:57  iter: 38639  total_loss: 4.07  loss_ce: 0.004874  loss_mask: 0.2216  loss_dice: 0.1435  loss_ce_0: 0.06951  loss_mask_0: 0.2337  loss_dice_0: 0.1488  loss_ce_1: 0.003765  loss_mask_1: 0.2318  loss_dice_1: 0.1488  loss_ce_2: 0.005135  loss_mask_2: 0.222  loss_dice_2: 0.1437  loss_ce_3: 0.006576  loss_mask_3: 0.2297  loss_dice_3: 0.1453  loss_ce_4: 0.00514  loss_mask_4: 0.2166  loss_dice_4: 0.1478  loss_ce_5: 0.005816  loss_mask_5: 0.2238  loss_dice_5: 0.1436  loss_ce_6: 0.007017  loss_mask_6: 0.2202  loss_dice_6: 0.1419  loss_ce_7: 0.005093  loss_mask_7: 0.2326  loss_dice_7: 0.1521  loss_ce_8: 0.00512  loss_mask_8: 0.2262  loss_dice_8: 0.145  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:14] d2.utils.events INFO:  eta: 0:23:52  iter: 38659  total_loss: 4.428  loss_ce: 0.002986  loss_mask: 0.2221  loss_dice: 0.1957  loss_ce_0: 0.05711  loss_mask_0: 0.2263  loss_dice_0: 0.1979  loss_ce_1: 0.003083  loss_mask_1: 0.2223  loss_dice_1: 0.1895  loss_ce_2: 0.002761  loss_mask_2: 0.2242  loss_dice_2: 0.193  loss_ce_3: 0.003591  loss_mask_3: 0.2187  loss_dice_3: 0.1911  loss_ce_4: 0.003349  loss_mask_4: 0.223  loss_dice_4: 0.1993  loss_ce_5: 0.002622  loss_mask_5: 0.2212  loss_dice_5: 0.1997  loss_ce_6: 0.002468  loss_mask_6: 0.2296  loss_dice_6: 0.1893  loss_ce_7: 0.002504  loss_mask_7: 0.2292  loss_dice_7: 0.1972  loss_ce_8: 0.00362  loss_mask_8: 0.2277  loss_dice_8: 0.1915  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:17] d2.utils.events INFO:  eta: 0:23:51  iter: 38679  total_loss: 4.28  loss_ce: 0.008636  loss_mask: 0.1958  loss_dice: 0.1672  loss_ce_0: 0.06682  loss_mask_0: 0.2068  loss_dice_0: 0.1725  loss_ce_1: 0.008725  loss_mask_1: 0.2075  loss_dice_1: 0.17  loss_ce_2: 0.0133  loss_mask_2: 0.2034  loss_dice_2: 0.1628  loss_ce_3: 0.008467  loss_mask_3: 0.1944  loss_dice_3: 0.1672  loss_ce_4: 0.008862  loss_mask_4: 0.2062  loss_dice_4: 0.1666  loss_ce_5: 0.009031  loss_mask_5: 0.1913  loss_dice_5: 0.1722  loss_ce_6: 0.009005  loss_mask_6: 0.2002  loss_dice_6: 0.1669  loss_ce_7: 0.00956  loss_mask_7: 0.2073  loss_dice_7: 0.1663  loss_ce_8: 0.01401  loss_mask_8: 0.2005  loss_dice_8: 0.1666  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:20] d2.utils.events INFO:  eta: 0:23:49  iter: 38699  total_loss: 4.315  loss_ce: 0.008204  loss_mask: 0.2057  loss_dice: 0.1522  loss_ce_0: 0.06495  loss_mask_0: 0.2031  loss_dice_0: 0.1544  loss_ce_1: 0.007143  loss_mask_1: 0.194  loss_dice_1: 0.1516  loss_ce_2: 0.008209  loss_mask_2: 0.1977  loss_dice_2: 0.1528  loss_ce_3: 0.008028  loss_mask_3: 0.2014  loss_dice_3: 0.1547  loss_ce_4: 0.007811  loss_mask_4: 0.2046  loss_dice_4: 0.1546  loss_ce_5: 0.008951  loss_mask_5: 0.1987  loss_dice_5: 0.1504  loss_ce_6: 0.008076  loss_mask_6: 0.2016  loss_dice_6: 0.152  loss_ce_7: 0.007795  loss_mask_7: 0.1988  loss_dice_7: 0.1528  loss_ce_8: 0.008998  loss_mask_8: 0.1971  loss_dice_8: 0.1506  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:22] d2.utils.events INFO:  eta: 0:23:49  iter: 38719  total_loss: 4.033  loss_ce: 0.005066  loss_mask: 0.2244  loss_dice: 0.1444  loss_ce_0: 0.05839  loss_mask_0: 0.2283  loss_dice_0: 0.1448  loss_ce_1: 0.005452  loss_mask_1: 0.2321  loss_dice_1: 0.1483  loss_ce_2: 0.005757  loss_mask_2: 0.2267  loss_dice_2: 0.1509  loss_ce_3: 0.005344  loss_mask_3: 0.2269  loss_dice_3: 0.149  loss_ce_4: 0.005836  loss_mask_4: 0.2279  loss_dice_4: 0.1457  loss_ce_5: 0.005523  loss_mask_5: 0.2182  loss_dice_5: 0.1512  loss_ce_6: 0.005173  loss_mask_6: 0.2306  loss_dice_6: 0.1475  loss_ce_7: 0.005365  loss_mask_7: 0.2295  loss_dice_7: 0.1476  loss_ce_8: 0.005544  loss_mask_8: 0.2341  loss_dice_8: 0.1469  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:25] d2.utils.events INFO:  eta: 0:23:47  iter: 38739  total_loss: 4.78  loss_ce: 0.005289  loss_mask: 0.1965  loss_dice: 0.1895  loss_ce_0: 0.06082  loss_mask_0: 0.1916  loss_dice_0: 0.1926  loss_ce_1: 0.01098  loss_mask_1: 0.189  loss_dice_1: 0.1936  loss_ce_2: 0.00545  loss_mask_2: 0.1946  loss_dice_2: 0.1921  loss_ce_3: 0.005241  loss_mask_3: 0.1928  loss_dice_3: 0.1911  loss_ce_4: 0.007177  loss_mask_4: 0.1886  loss_dice_4: 0.1872  loss_ce_5: 0.00597  loss_mask_5: 0.1901  loss_dice_5: 0.1852  loss_ce_6: 0.005095  loss_mask_6: 0.1876  loss_dice_6: 0.1925  loss_ce_7: 0.005743  loss_mask_7: 0.1957  loss_dice_7: 0.1921  loss_ce_8: 0.005167  loss_mask_8: 0.1905  loss_dice_8: 0.196  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:27] d2.utils.events INFO:  eta: 0:23:46  iter: 38759  total_loss: 4.702  loss_ce: 0.009632  loss_mask: 0.1955  loss_dice: 0.246  loss_ce_0: 0.05998  loss_mask_0: 0.1974  loss_dice_0: 0.2387  loss_ce_1: 0.01294  loss_mask_1: 0.1978  loss_dice_1: 0.2415  loss_ce_2: 0.01382  loss_mask_2: 0.1975  loss_dice_2: 0.2438  loss_ce_3: 0.01017  loss_mask_3: 0.2043  loss_dice_3: 0.2415  loss_ce_4: 0.0134  loss_mask_4: 0.1948  loss_dice_4: 0.2362  loss_ce_5: 0.01393  loss_mask_5: 0.2019  loss_dice_5: 0.2371  loss_ce_6: 0.009237  loss_mask_6: 0.1902  loss_dice_6: 0.2417  loss_ce_7: 0.009507  loss_mask_7: 0.1924  loss_dice_7: 0.2351  loss_ce_8: 0.01735  loss_mask_8: 0.1965  loss_dice_8: 0.2428  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:30] d2.utils.events INFO:  eta: 0:23:45  iter: 38779  total_loss: 4.659  loss_ce: 0.01086  loss_mask: 0.2095  loss_dice: 0.1853  loss_ce_0: 0.06193  loss_mask_0: 0.2176  loss_dice_0: 0.1939  loss_ce_1: 0.0123  loss_mask_1: 0.2102  loss_dice_1: 0.1903  loss_ce_2: 0.01334  loss_mask_2: 0.209  loss_dice_2: 0.1953  loss_ce_3: 0.01089  loss_mask_3: 0.2083  loss_dice_3: 0.1869  loss_ce_4: 0.01331  loss_mask_4: 0.2029  loss_dice_4: 0.1847  loss_ce_5: 0.01438  loss_mask_5: 0.2157  loss_dice_5: 0.1807  loss_ce_6: 0.009705  loss_mask_6: 0.2169  loss_dice_6: 0.1924  loss_ce_7: 0.01805  loss_mask_7: 0.2083  loss_dice_7: 0.1879  loss_ce_8: 0.01672  loss_mask_8: 0.2023  loss_dice_8: 0.1919  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:32] d2.utils.events INFO:  eta: 0:23:43  iter: 38799  total_loss: 4.382  loss_ce: 0.002106  loss_mask: 0.2191  loss_dice: 0.2122  loss_ce_0: 0.05503  loss_mask_0: 0.2164  loss_dice_0: 0.2142  loss_ce_1: 0.006251  loss_mask_1: 0.2306  loss_dice_1: 0.2144  loss_ce_2: 0.006755  loss_mask_2: 0.2234  loss_dice_2: 0.2194  loss_ce_3: 0.003809  loss_mask_3: 0.2149  loss_dice_3: 0.2161  loss_ce_4: 0.003364  loss_mask_4: 0.2207  loss_dice_4: 0.2176  loss_ce_5: 0.005165  loss_mask_5: 0.2244  loss_dice_5: 0.216  loss_ce_6: 0.001091  loss_mask_6: 0.2213  loss_dice_6: 0.2136  loss_ce_7: 0.001965  loss_mask_7: 0.2312  loss_dice_7: 0.2179  loss_ce_8: 0.006701  loss_mask_8: 0.2296  loss_dice_8: 0.2129  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:35] d2.utils.events INFO:  eta: 0:23:41  iter: 38819  total_loss: 5.977  loss_ce: 0.009957  loss_mask: 0.2172  loss_dice: 0.1995  loss_ce_0: 0.06218  loss_mask_0: 0.2131  loss_dice_0: 0.2049  loss_ce_1: 0.01236  loss_mask_1: 0.2086  loss_dice_1: 0.2036  loss_ce_2: 0.01196  loss_mask_2: 0.2182  loss_dice_2: 0.1968  loss_ce_3: 0.0119  loss_mask_3: 0.2098  loss_dice_3: 0.202  loss_ce_4: 0.009595  loss_mask_4: 0.2088  loss_dice_4: 0.1973  loss_ce_5: 0.01144  loss_mask_5: 0.2135  loss_dice_5: 0.1919  loss_ce_6: 0.009569  loss_mask_6: 0.2134  loss_dice_6: 0.1984  loss_ce_7: 0.008564  loss_mask_7: 0.2113  loss_dice_7: 0.2017  loss_ce_8: 0.01006  loss_mask_8: 0.2122  loss_dice_8: 0.2039  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:38] d2.utils.events INFO:  eta: 0:23:39  iter: 38839  total_loss: 4.475  loss_ce: 0.00798  loss_mask: 0.1742  loss_dice: 0.2267  loss_ce_0: 0.05814  loss_mask_0: 0.1704  loss_dice_0: 0.2267  loss_ce_1: 0.007406  loss_mask_1: 0.172  loss_dice_1: 0.2317  loss_ce_2: 0.007601  loss_mask_2: 0.1829  loss_dice_2: 0.2357  loss_ce_3: 0.007322  loss_mask_3: 0.1708  loss_dice_3: 0.2177  loss_ce_4: 0.007554  loss_mask_4: 0.174  loss_dice_4: 0.227  loss_ce_5: 0.007677  loss_mask_5: 0.1754  loss_dice_5: 0.2345  loss_ce_6: 0.007204  loss_mask_6: 0.1814  loss_dice_6: 0.2299  loss_ce_7: 0.006341  loss_mask_7: 0.1721  loss_dice_7: 0.2203  loss_ce_8: 0.008077  loss_mask_8: 0.1719  loss_dice_8: 0.2228  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:40] d2.utils.events INFO:  eta: 0:23:36  iter: 38859  total_loss: 4.197  loss_ce: 0.009375  loss_mask: 0.1887  loss_dice: 0.169  loss_ce_0: 0.06227  loss_mask_0: 0.1988  loss_dice_0: 0.1694  loss_ce_1: 0.009847  loss_mask_1: 0.1987  loss_dice_1: 0.1673  loss_ce_2: 0.0107  loss_mask_2: 0.1859  loss_dice_2: 0.1633  loss_ce_3: 0.009367  loss_mask_3: 0.1981  loss_dice_3: 0.167  loss_ce_4: 0.009225  loss_mask_4: 0.204  loss_dice_4: 0.1655  loss_ce_5: 0.008962  loss_mask_5: 0.1979  loss_dice_5: 0.1656  loss_ce_6: 0.009294  loss_mask_6: 0.2054  loss_dice_6: 0.1668  loss_ce_7: 0.008319  loss_mask_7: 0.2001  loss_dice_7: 0.164  loss_ce_8: 0.01133  loss_mask_8: 0.2027  loss_dice_8: 0.1653  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:43] d2.utils.events INFO:  eta: 0:23:34  iter: 38879  total_loss: 4.031  loss_ce: 0.007702  loss_mask: 0.1954  loss_dice: 0.1887  loss_ce_0: 0.06044  loss_mask_0: 0.2013  loss_dice_0: 0.1853  loss_ce_1: 0.006187  loss_mask_1: 0.1957  loss_dice_1: 0.1883  loss_ce_2: 0.007087  loss_mask_2: 0.2003  loss_dice_2: 0.1904  loss_ce_3: 0.006991  loss_mask_3: 0.202  loss_dice_3: 0.1887  loss_ce_4: 0.006699  loss_mask_4: 0.2043  loss_dice_4: 0.1883  loss_ce_5: 0.007286  loss_mask_5: 0.2032  loss_dice_5: 0.1942  loss_ce_6: 0.00699  loss_mask_6: 0.2035  loss_dice_6: 0.1953  loss_ce_7: 0.006028  loss_mask_7: 0.206  loss_dice_7: 0.1909  loss_ce_8: 0.007735  loss_mask_8: 0.2088  loss_dice_8: 0.1828  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:45] d2.utils.events INFO:  eta: 0:23:30  iter: 38899  total_loss: 4.47  loss_ce: 0.01105  loss_mask: 0.1984  loss_dice: 0.2172  loss_ce_0: 0.06339  loss_mask_0: 0.1911  loss_dice_0: 0.2138  loss_ce_1: 0.0184  loss_mask_1: 0.1877  loss_dice_1: 0.2115  loss_ce_2: 0.009951  loss_mask_2: 0.1946  loss_dice_2: 0.2113  loss_ce_3: 0.01293  loss_mask_3: 0.2026  loss_dice_3: 0.2093  loss_ce_4: 0.01337  loss_mask_4: 0.1932  loss_dice_4: 0.2099  loss_ce_5: 0.008511  loss_mask_5: 0.1954  loss_dice_5: 0.2283  loss_ce_6: 0.012  loss_mask_6: 0.1895  loss_dice_6: 0.2025  loss_ce_7: 0.0164  loss_mask_7: 0.1962  loss_dice_7: 0.2176  loss_ce_8: 0.008509  loss_mask_8: 0.1973  loss_dice_8: 0.2084  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:48] d2.utils.events INFO:  eta: 0:23:28  iter: 38919  total_loss: 4.462  loss_ce: 0.004718  loss_mask: 0.1933  loss_dice: 0.1889  loss_ce_0: 0.06206  loss_mask_0: 0.2042  loss_dice_0: 0.1783  loss_ce_1: 0.003276  loss_mask_1: 0.211  loss_dice_1: 0.1896  loss_ce_2: 0.004862  loss_mask_2: 0.2051  loss_dice_2: 0.1842  loss_ce_3: 0.00458  loss_mask_3: 0.2085  loss_dice_3: 0.1795  loss_ce_4: 0.004074  loss_mask_4: 0.2022  loss_dice_4: 0.1858  loss_ce_5: 0.004801  loss_mask_5: 0.1997  loss_dice_5: 0.1893  loss_ce_6: 0.004353  loss_mask_6: 0.2038  loss_dice_6: 0.1794  loss_ce_7: 0.003531  loss_mask_7: 0.1836  loss_dice_7: 0.1765  loss_ce_8: 0.004386  loss_mask_8: 0.1996  loss_dice_8: 0.1848  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:24:50] d2.utils.events INFO:  eta: 0:23:25  iter: 38939  total_loss: 4.474  loss_ce: 0.004629  loss_mask: 0.2021  loss_dice: 0.1588  loss_ce_0: 0.06193  loss_mask_0: 0.1884  loss_dice_0: 0.1655  loss_ce_1: 0.003437  loss_mask_1: 0.1944  loss_dice_1: 0.167  loss_ce_2: 0.004484  loss_mask_2: 0.191  loss_dice_2: 0.1685  loss_ce_3: 0.004653  loss_mask_3: 0.1901  loss_dice_3: 0.1697  loss_ce_4: 0.00396  loss_mask_4: 0.1899  loss_dice_4: 0.1636  loss_ce_5: 0.004682  loss_mask_5: 0.1922  loss_dice_5: 0.164  loss_ce_6: 0.004408  loss_mask_6: 0.1946  loss_dice_6: 0.1585  loss_ce_7: 0.003452  loss_mask_7: 0.1992  loss_dice_7: 0.1676  loss_ce_8: 0.004214  loss_mask_8: 0.2028  loss_dice_8: 0.1624  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:53] d2.utils.events INFO:  eta: 0:23:22  iter: 38959  total_loss: 4.396  loss_ce: 0.005511  loss_mask: 0.1918  loss_dice: 0.1894  loss_ce_0: 0.06186  loss_mask_0: 0.1957  loss_dice_0: 0.1984  loss_ce_1: 0.006737  loss_mask_1: 0.1956  loss_dice_1: 0.1889  loss_ce_2: 0.006614  loss_mask_2: 0.196  loss_dice_2: 0.2035  loss_ce_3: 0.006038  loss_mask_3: 0.2012  loss_dice_3: 0.1945  loss_ce_4: 0.006427  loss_mask_4: 0.1955  loss_dice_4: 0.1917  loss_ce_5: 0.005841  loss_mask_5: 0.1931  loss_dice_5: 0.2125  loss_ce_6: 0.005431  loss_mask_6: 0.2014  loss_dice_6: 0.1909  loss_ce_7: 0.005545  loss_mask_7: 0.1945  loss_dice_7: 0.1986  loss_ce_8: 0.006343  loss_mask_8: 0.2023  loss_dice_8: 0.1929  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:55] d2.utils.events INFO:  eta: 0:23:19  iter: 38979  total_loss: 5.157  loss_ce: 0.005767  loss_mask: 0.1931  loss_dice: 0.2122  loss_ce_0: 0.063  loss_mask_0: 0.1887  loss_dice_0: 0.2183  loss_ce_1: 0.00484  loss_mask_1: 0.1986  loss_dice_1: 0.2201  loss_ce_2: 0.006069  loss_mask_2: 0.1891  loss_dice_2: 0.2138  loss_ce_3: 0.006263  loss_mask_3: 0.2037  loss_dice_3: 0.2214  loss_ce_4: 0.0056  loss_mask_4: 0.1906  loss_dice_4: 0.2094  loss_ce_5: 0.006103  loss_mask_5: 0.1924  loss_dice_5: 0.2192  loss_ce_6: 0.007089  loss_mask_6: 0.1985  loss_dice_6: 0.2153  loss_ce_7: 0.005216  loss_mask_7: 0.1971  loss_dice_7: 0.2114  loss_ce_8: 0.005729  loss_mask_8: 0.1902  loss_dice_8: 0.2029  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:24:58] d2.utils.events INFO:  eta: 0:23:16  iter: 38999  total_loss: 4.603  loss_ce: 0.004881  loss_mask: 0.2267  loss_dice: 0.2126  loss_ce_0: 0.06403  loss_mask_0: 0.2227  loss_dice_0: 0.2018  loss_ce_1: 0.003617  loss_mask_1: 0.2243  loss_dice_1: 0.2044  loss_ce_2: 0.004749  loss_mask_2: 0.2211  loss_dice_2: 0.2065  loss_ce_3: 0.004597  loss_mask_3: 0.2246  loss_dice_3: 0.2126  loss_ce_4: 0.004262  loss_mask_4: 0.222  loss_dice_4: 0.2047  loss_ce_5: 0.004598  loss_mask_5: 0.221  loss_dice_5: 0.2026  loss_ce_6: 0.005893  loss_mask_6: 0.229  loss_dice_6: 0.2117  loss_ce_7: 0.004089  loss_mask_7: 0.2181  loss_dice_7: 0.2066  loss_ce_8: 0.004312  loss_mask_8: 0.2215  loss_dice_8: 0.2037  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:25:00] d2.utils.events INFO:  eta: 0:23:14  iter: 39019  total_loss: 4.22  loss_ce: 0.01015  loss_mask: 0.1872  loss_dice: 0.1721  loss_ce_0: 0.05811  loss_mask_0: 0.1833  loss_dice_0: 0.1652  loss_ce_1: 0.008904  loss_mask_1: 0.1801  loss_dice_1: 0.1713  loss_ce_2: 0.01079  loss_mask_2: 0.1744  loss_dice_2: 0.1645  loss_ce_3: 0.007028  loss_mask_3: 0.1845  loss_dice_3: 0.1715  loss_ce_4: 0.008218  loss_mask_4: 0.1772  loss_dice_4: 0.1593  loss_ce_5: 0.00869  loss_mask_5: 0.1822  loss_dice_5: 0.1637  loss_ce_6: 0.006306  loss_mask_6: 0.1843  loss_dice_6: 0.1632  loss_ce_7: 0.006279  loss_mask_7: 0.1805  loss_dice_7: 0.1727  loss_ce_8: 0.006436  loss_mask_8: 0.1854  loss_dice_8: 0.1694  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:25:03] d2.utils.events INFO:  eta: 0:23:11  iter: 39039  total_loss: 3.825  loss_ce: 0.004613  loss_mask: 0.1777  loss_dice: 0.1806  loss_ce_0: 0.06762  loss_mask_0: 0.1649  loss_dice_0: 0.175  loss_ce_1: 0.003896  loss_mask_1: 0.1727  loss_dice_1: 0.1712  loss_ce_2: 0.004592  loss_mask_2: 0.1791  loss_dice_2: 0.1778  loss_ce_3: 0.004057  loss_mask_3: 0.1682  loss_dice_3: 0.1748  loss_ce_4: 0.003942  loss_mask_4: 0.1657  loss_dice_4: 0.1725  loss_ce_5: 0.004111  loss_mask_5: 0.1755  loss_dice_5: 0.1822  loss_ce_6: 0.00394  loss_mask_6: 0.1646  loss_dice_6: 0.1776  loss_ce_7: 0.003893  loss_mask_7: 0.1718  loss_dice_7: 0.1765  loss_ce_8: 0.004158  loss_mask_8: 0.1746  loss_dice_8: 0.1801  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:05] d2.utils.events INFO:  eta: 0:23:09  iter: 39059  total_loss: 4.792  loss_ce: 0.004719  loss_mask: 0.1716  loss_dice: 0.1718  loss_ce_0: 0.06576  loss_mask_0: 0.1662  loss_dice_0: 0.1668  loss_ce_1: 0.005869  loss_mask_1: 0.1619  loss_dice_1: 0.1695  loss_ce_2: 0.005225  loss_mask_2: 0.1729  loss_dice_2: 0.1709  loss_ce_3: 0.004567  loss_mask_3: 0.1633  loss_dice_3: 0.1743  loss_ce_4: 0.004701  loss_mask_4: 0.1734  loss_dice_4: 0.1707  loss_ce_5: 0.004787  loss_mask_5: 0.1646  loss_dice_5: 0.1801  loss_ce_6: 0.005078  loss_mask_6: 0.1711  loss_dice_6: 0.1679  loss_ce_7: 0.005062  loss_mask_7: 0.1567  loss_dice_7: 0.17  loss_ce_8: 0.005299  loss_mask_8: 0.1646  loss_dice_8: 0.173  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:25:08] d2.utils.events INFO:  eta: 0:23:06  iter: 39079  total_loss: 4.717  loss_ce: 0.01272  loss_mask: 0.1846  loss_dice: 0.2271  loss_ce_0: 0.06488  loss_mask_0: 0.1939  loss_dice_0: 0.2334  loss_ce_1: 0.01265  loss_mask_1: 0.1975  loss_dice_1: 0.2337  loss_ce_2: 0.01706  loss_mask_2: 0.1966  loss_dice_2: 0.2227  loss_ce_3: 0.01335  loss_mask_3: 0.1897  loss_dice_3: 0.2308  loss_ce_4: 0.008532  loss_mask_4: 0.1781  loss_dice_4: 0.2275  loss_ce_5: 0.01356  loss_mask_5: 0.1861  loss_dice_5: 0.2286  loss_ce_6: 0.0116  loss_mask_6: 0.2007  loss_dice_6: 0.2309  loss_ce_7: 0.007731  loss_mask_7: 0.1847  loss_dice_7: 0.222  loss_ce_8: 0.01034  loss_mask_8: 0.1867  loss_dice_8: 0.2254  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:10] d2.utils.events INFO:  eta: 0:23:02  iter: 39099  total_loss: 4.278  loss_ce: 0.003977  loss_mask: 0.1932  loss_dice: 0.1694  loss_ce_0: 0.0586  loss_mask_0: 0.1931  loss_dice_0: 0.1692  loss_ce_1: 0.003577  loss_mask_1: 0.1925  loss_dice_1: 0.1699  loss_ce_2: 0.002953  loss_mask_2: 0.1858  loss_dice_2: 0.1749  loss_ce_3: 0.00289  loss_mask_3: 0.2071  loss_dice_3: 0.173  loss_ce_4: 0.003331  loss_mask_4: 0.1931  loss_dice_4: 0.1748  loss_ce_5: 0.002777  loss_mask_5: 0.1982  loss_dice_5: 0.174  loss_ce_6: 0.002543  loss_mask_6: 0.1905  loss_dice_6: 0.1711  loss_ce_7: 0.003143  loss_mask_7: 0.1914  loss_dice_7: 0.1685  loss_ce_8: 0.002772  loss_mask_8: 0.1951  loss_dice_8: 0.1709  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:13] d2.utils.events INFO:  eta: 0:22:59  iter: 39119  total_loss: 4.425  loss_ce: 0.003939  loss_mask: 0.1904  loss_dice: 0.1866  loss_ce_0: 0.0658  loss_mask_0: 0.1807  loss_dice_0: 0.1793  loss_ce_1: 0.003557  loss_mask_1: 0.1912  loss_dice_1: 0.1713  loss_ce_2: 0.004039  loss_mask_2: 0.1908  loss_dice_2: 0.1749  loss_ce_3: 0.003681  loss_mask_3: 0.1881  loss_dice_3: 0.1793  loss_ce_4: 0.004056  loss_mask_4: 0.1863  loss_dice_4: 0.1914  loss_ce_5: 0.003672  loss_mask_5: 0.1917  loss_dice_5: 0.1837  loss_ce_6: 0.003646  loss_mask_6: 0.1912  loss_dice_6: 0.1791  loss_ce_7: 0.004012  loss_mask_7: 0.1806  loss_dice_7: 0.1873  loss_ce_8: 0.003882  loss_mask_8: 0.1815  loss_dice_8: 0.1811  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:15] d2.utils.events INFO:  eta: 0:22:57  iter: 39139  total_loss: 4.589  loss_ce: 0.003744  loss_mask: 0.2186  loss_dice: 0.1709  loss_ce_0: 0.06202  loss_mask_0: 0.2287  loss_dice_0: 0.1747  loss_ce_1: 0.003814  loss_mask_1: 0.2143  loss_dice_1: 0.1697  loss_ce_2: 0.004072  loss_mask_2: 0.2212  loss_dice_2: 0.1779  loss_ce_3: 0.004016  loss_mask_3: 0.2175  loss_dice_3: 0.171  loss_ce_4: 0.003709  loss_mask_4: 0.2142  loss_dice_4: 0.173  loss_ce_5: 0.003801  loss_mask_5: 0.223  loss_dice_5: 0.1678  loss_ce_6: 0.003671  loss_mask_6: 0.2148  loss_dice_6: 0.1697  loss_ce_7: 0.003829  loss_mask_7: 0.2176  loss_dice_7: 0.1698  loss_ce_8: 0.003851  loss_mask_8: 0.2169  loss_dice_8: 0.1708  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:18] d2.utils.events INFO:  eta: 0:22:55  iter: 39159  total_loss: 4.32  loss_ce: 0.004311  loss_mask: 0.2046  loss_dice: 0.1753  loss_ce_0: 0.06189  loss_mask_0: 0.2192  loss_dice_0: 0.1795  loss_ce_1: 0.005166  loss_mask_1: 0.2202  loss_dice_1: 0.1699  loss_ce_2: 0.004547  loss_mask_2: 0.2116  loss_dice_2: 0.1747  loss_ce_3: 0.004261  loss_mask_3: 0.2143  loss_dice_3: 0.1663  loss_ce_4: 0.004604  loss_mask_4: 0.2222  loss_dice_4: 0.1768  loss_ce_5: 0.004296  loss_mask_5: 0.2083  loss_dice_5: 0.1756  loss_ce_6: 0.004191  loss_mask_6: 0.2139  loss_dice_6: 0.1768  loss_ce_7: 0.004667  loss_mask_7: 0.2167  loss_dice_7: 0.1741  loss_ce_8: 0.004278  loss_mask_8: 0.225  loss_dice_8: 0.1741  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:21] d2.utils.events INFO:  eta: 0:22:52  iter: 39179  total_loss: 3.927  loss_ce: 0.00328  loss_mask: 0.187  loss_dice: 0.1785  loss_ce_0: 0.06403  loss_mask_0: 0.1904  loss_dice_0: 0.1892  loss_ce_1: 0.004048  loss_mask_1: 0.1925  loss_dice_1: 0.1893  loss_ce_2: 0.003547  loss_mask_2: 0.1909  loss_dice_2: 0.1923  loss_ce_3: 0.003409  loss_mask_3: 0.1944  loss_dice_3: 0.1902  loss_ce_4: 0.003647  loss_mask_4: 0.1928  loss_dice_4: 0.2036  loss_ce_5: 0.003371  loss_mask_5: 0.1902  loss_dice_5: 0.1927  loss_ce_6: 0.003634  loss_mask_6: 0.1944  loss_dice_6: 0.187  loss_ce_7: 0.003739  loss_mask_7: 0.1863  loss_dice_7: 0.1876  loss_ce_8: 0.003441  loss_mask_8: 0.1859  loss_dice_8: 0.1795  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:25:23] d2.utils.events INFO:  eta: 0:22:49  iter: 39199  total_loss: 4.119  loss_ce: 0.0033  loss_mask: 0.2201  loss_dice: 0.1562  loss_ce_0: 0.05911  loss_mask_0: 0.2104  loss_dice_0: 0.1627  loss_ce_1: 0.002878  loss_mask_1: 0.2045  loss_dice_1: 0.1608  loss_ce_2: 0.002969  loss_mask_2: 0.226  loss_dice_2: 0.1589  loss_ce_3: 0.003337  loss_mask_3: 0.2125  loss_dice_3: 0.1647  loss_ce_4: 0.003385  loss_mask_4: 0.2076  loss_dice_4: 0.1682  loss_ce_5: 0.003164  loss_mask_5: 0.219  loss_dice_5: 0.1644  loss_ce_6: 0.003349  loss_mask_6: 0.2168  loss_dice_6: 0.1648  loss_ce_7: 0.003125  loss_mask_7: 0.2187  loss_dice_7: 0.1595  loss_ce_8: 0.00304  loss_mask_8: 0.2181  loss_dice_8: 0.1604  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:26] d2.utils.events INFO:  eta: 0:22:47  iter: 39219  total_loss: 3.997  loss_ce: 0.003297  loss_mask: 0.2088  loss_dice: 0.1229  loss_ce_0: 0.05823  loss_mask_0: 0.1881  loss_dice_0: 0.1287  loss_ce_1: 0.002921  loss_mask_1: 0.19  loss_dice_1: 0.1231  loss_ce_2: 0.003008  loss_mask_2: 0.1817  loss_dice_2: 0.13  loss_ce_3: 0.003054  loss_mask_3: 0.1903  loss_dice_3: 0.1258  loss_ce_4: 0.003354  loss_mask_4: 0.202  loss_dice_4: 0.1277  loss_ce_5: 0.003002  loss_mask_5: 0.1877  loss_dice_5: 0.1326  loss_ce_6: 0.00311  loss_mask_6: 0.1965  loss_dice_6: 0.1284  loss_ce_7: 0.003023  loss_mask_7: 0.1994  loss_dice_7: 0.1294  loss_ce_8: 0.003033  loss_mask_8: 0.1922  loss_dice_8: 0.1284  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:28] d2.utils.events INFO:  eta: 0:22:46  iter: 39239  total_loss: 4.282  loss_ce: 0.004686  loss_mask: 0.2006  loss_dice: 0.1845  loss_ce_0: 0.05415  loss_mask_0: 0.1982  loss_dice_0: 0.1865  loss_ce_1: 0.003951  loss_mask_1: 0.208  loss_dice_1: 0.1906  loss_ce_2: 0.004519  loss_mask_2: 0.1996  loss_dice_2: 0.189  loss_ce_3: 0.004706  loss_mask_3: 0.2001  loss_dice_3: 0.1912  loss_ce_4: 0.004598  loss_mask_4: 0.2083  loss_dice_4: 0.1895  loss_ce_5: 0.004769  loss_mask_5: 0.2029  loss_dice_5: 0.1986  loss_ce_6: 0.004459  loss_mask_6: 0.2078  loss_dice_6: 0.197  loss_ce_7: 0.004203  loss_mask_7: 0.2008  loss_dice_7: 0.1914  loss_ce_8: 0.005071  loss_mask_8: 0.2021  loss_dice_8: 0.1922  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:25:31] d2.utils.events INFO:  eta: 0:22:43  iter: 39259  total_loss: 4.519  loss_ce: 0.005856  loss_mask: 0.2142  loss_dice: 0.1707  loss_ce_0: 0.07459  loss_mask_0: 0.2233  loss_dice_0: 0.1748  loss_ce_1: 0.005228  loss_mask_1: 0.2108  loss_dice_1: 0.1691  loss_ce_2: 0.005419  loss_mask_2: 0.2191  loss_dice_2: 0.17  loss_ce_3: 0.00713  loss_mask_3: 0.2241  loss_dice_3: 0.1727  loss_ce_4: 0.005234  loss_mask_4: 0.2222  loss_dice_4: 0.1702  loss_ce_5: 0.005964  loss_mask_5: 0.2122  loss_dice_5: 0.1652  loss_ce_6: 0.006498  loss_mask_6: 0.217  loss_dice_6: 0.1706  loss_ce_7: 0.005151  loss_mask_7: 0.2181  loss_dice_7: 0.167  loss_ce_8: 0.005634  loss_mask_8: 0.2237  loss_dice_8: 0.1677  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:33] d2.utils.events INFO:  eta: 0:22:39  iter: 39279  total_loss: 4.066  loss_ce: 0.00689  loss_mask: 0.1846  loss_dice: 0.1459  loss_ce_0: 0.06948  loss_mask_0: 0.1935  loss_dice_0: 0.1448  loss_ce_1: 0.008825  loss_mask_1: 0.1758  loss_dice_1: 0.148  loss_ce_2: 0.00634  loss_mask_2: 0.1814  loss_dice_2: 0.151  loss_ce_3: 0.006636  loss_mask_3: 0.1849  loss_dice_3: 0.1481  loss_ce_4: 0.006936  loss_mask_4: 0.1811  loss_dice_4: 0.1477  loss_ce_5: 0.006834  loss_mask_5: 0.1844  loss_dice_5: 0.1528  loss_ce_6: 0.006174  loss_mask_6: 0.1947  loss_dice_6: 0.1516  loss_ce_7: 0.007218  loss_mask_7: 0.196  loss_dice_7: 0.1511  loss_ce_8: 0.006839  loss_mask_8: 0.1897  loss_dice_8: 0.1475  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:36] d2.utils.events INFO:  eta: 0:22:35  iter: 39299  total_loss: 3.811  loss_ce: 0.004763  loss_mask: 0.1674  loss_dice: 0.1661  loss_ce_0: 0.05685  loss_mask_0: 0.1599  loss_dice_0: 0.1727  loss_ce_1: 0.00757  loss_mask_1: 0.1598  loss_dice_1: 0.1695  loss_ce_2: 0.005014  loss_mask_2: 0.1641  loss_dice_2: 0.1719  loss_ce_3: 0.0047  loss_mask_3: 0.1543  loss_dice_3: 0.1718  loss_ce_4: 0.006695  loss_mask_4: 0.1655  loss_dice_4: 0.1674  loss_ce_5: 0.004996  loss_mask_5: 0.1594  loss_dice_5: 0.1631  loss_ce_6: 0.004921  loss_mask_6: 0.1591  loss_dice_6: 0.1697  loss_ce_7: 0.00775  loss_mask_7: 0.1639  loss_dice_7: 0.172  loss_ce_8: 0.005958  loss_mask_8: 0.1564  loss_dice_8: 0.1665  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:38] d2.utils.events INFO:  eta: 0:22:31  iter: 39319  total_loss: 3.815  loss_ce: 0.003389  loss_mask: 0.182  loss_dice: 0.1529  loss_ce_0: 0.05202  loss_mask_0: 0.1899  loss_dice_0: 0.1549  loss_ce_1: 0.004278  loss_mask_1: 0.1873  loss_dice_1: 0.1462  loss_ce_2: 0.003895  loss_mask_2: 0.1852  loss_dice_2: 0.1609  loss_ce_3: 0.002997  loss_mask_3: 0.1885  loss_dice_3: 0.1619  loss_ce_4: 0.00367  loss_mask_4: 0.1875  loss_dice_4: 0.1583  loss_ce_5: 0.003393  loss_mask_5: 0.1876  loss_dice_5: 0.1535  loss_ce_6: 0.002933  loss_mask_6: 0.1869  loss_dice_6: 0.1544  loss_ce_7: 0.004144  loss_mask_7: 0.1876  loss_dice_7: 0.1523  loss_ce_8: 0.00389  loss_mask_8: 0.1877  loss_dice_8: 0.1593  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:41] d2.utils.events INFO:  eta: 0:22:29  iter: 39339  total_loss: 4.553  loss_ce: 0.003885  loss_mask: 0.2094  loss_dice: 0.1507  loss_ce_0: 0.0495  loss_mask_0: 0.1919  loss_dice_0: 0.1588  loss_ce_1: 0.00554  loss_mask_1: 0.2067  loss_dice_1: 0.159  loss_ce_2: 0.003992  loss_mask_2: 0.2025  loss_dice_2: 0.1536  loss_ce_3: 0.003527  loss_mask_3: 0.2029  loss_dice_3: 0.1607  loss_ce_4: 0.00344  loss_mask_4: 0.2029  loss_dice_4: 0.1594  loss_ce_5: 0.004466  loss_mask_5: 0.1988  loss_dice_5: 0.1576  loss_ce_6: 0.003702  loss_mask_6: 0.2006  loss_dice_6: 0.1627  loss_ce_7: 0.004032  loss_mask_7: 0.1983  loss_dice_7: 0.159  loss_ce_8: 0.00463  loss_mask_8: 0.2009  loss_dice_8: 0.1584  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:43] d2.utils.events INFO:  eta: 0:22:28  iter: 39359  total_loss: 4.711  loss_ce: 0.003333  loss_mask: 0.143  loss_dice: 0.2141  loss_ce_0: 0.07358  loss_mask_0: 0.157  loss_dice_0: 0.2245  loss_ce_1: 0.004486  loss_mask_1: 0.1577  loss_dice_1: 0.2108  loss_ce_2: 0.00354  loss_mask_2: 0.1544  loss_dice_2: 0.2227  loss_ce_3: 0.003601  loss_mask_3: 0.1543  loss_dice_3: 0.2184  loss_ce_4: 0.003929  loss_mask_4: 0.1507  loss_dice_4: 0.2118  loss_ce_5: 0.003671  loss_mask_5: 0.1504  loss_dice_5: 0.2156  loss_ce_6: 0.003535  loss_mask_6: 0.1596  loss_dice_6: 0.2085  loss_ce_7: 0.004506  loss_mask_7: 0.1517  loss_dice_7: 0.2187  loss_ce_8: 0.004003  loss_mask_8: 0.1571  loss_dice_8: 0.2174  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:25:46] d2.utils.events INFO:  eta: 0:22:25  iter: 39379  total_loss: 4.328  loss_ce: 0.002579  loss_mask: 0.2005  loss_dice: 0.1958  loss_ce_0: 0.05603  loss_mask_0: 0.1989  loss_dice_0: 0.1894  loss_ce_1: 0.00272  loss_mask_1: 0.2008  loss_dice_1: 0.1958  loss_ce_2: 0.002719  loss_mask_2: 0.2066  loss_dice_2: 0.1929  loss_ce_3: 0.00251  loss_mask_3: 0.2036  loss_dice_3: 0.1942  loss_ce_4: 0.002661  loss_mask_4: 0.1978  loss_dice_4: 0.1912  loss_ce_5: 0.002512  loss_mask_5: 0.2067  loss_dice_5: 0.204  loss_ce_6: 0.002633  loss_mask_6: 0.2096  loss_dice_6: 0.195  loss_ce_7: 0.002578  loss_mask_7: 0.1968  loss_dice_7: 0.1952  loss_ce_8: 0.002795  loss_mask_8: 0.2111  loss_dice_8: 0.1924  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:25:48] d2.utils.events INFO:  eta: 0:22:23  iter: 39399  total_loss: 4.219  loss_ce: 0.002286  loss_mask: 0.1678  loss_dice: 0.1618  loss_ce_0: 0.069  loss_mask_0: 0.1714  loss_dice_0: 0.168  loss_ce_1: 0.002959  loss_mask_1: 0.1652  loss_dice_1: 0.1673  loss_ce_2: 0.002414  loss_mask_2: 0.1747  loss_dice_2: 0.1666  loss_ce_3: 0.002229  loss_mask_3: 0.1737  loss_dice_3: 0.1685  loss_ce_4: 0.002827  loss_mask_4: 0.1764  loss_dice_4: 0.1647  loss_ce_5: 0.00219  loss_mask_5: 0.1847  loss_dice_5: 0.16  loss_ce_6: 0.002445  loss_mask_6: 0.1808  loss_dice_6: 0.172  loss_ce_7: 0.002855  loss_mask_7: 0.1747  loss_dice_7: 0.1732  loss_ce_8: 0.002399  loss_mask_8: 0.17  loss_dice_8: 0.166  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:25:51] d2.utils.events INFO:  eta: 0:22:20  iter: 39419  total_loss: 4.505  loss_ce: 0.003857  loss_mask: 0.1725  loss_dice: 0.1613  loss_ce_0: 0.05607  loss_mask_0: 0.1729  loss_dice_0: 0.1709  loss_ce_1: 0.009914  loss_mask_1: 0.1689  loss_dice_1: 0.1733  loss_ce_2: 0.004851  loss_mask_2: 0.1721  loss_dice_2: 0.1636  loss_ce_3: 0.004624  loss_mask_3: 0.1788  loss_dice_3: 0.1684  loss_ce_4: 0.003827  loss_mask_4: 0.171  loss_dice_4: 0.1694  loss_ce_5: 0.005022  loss_mask_5: 0.1761  loss_dice_5: 0.1726  loss_ce_6: 0.00497  loss_mask_6: 0.1777  loss_dice_6: 0.1648  loss_ce_7: 0.004368  loss_mask_7: 0.17  loss_dice_7: 0.1662  loss_ce_8: 0.005717  loss_mask_8: 0.1706  loss_dice_8: 0.176  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:25:53] d2.utils.events INFO:  eta: 0:22:18  iter: 39439  total_loss: 4.771  loss_ce: 0.002586  loss_mask: 0.1957  loss_dice: 0.222  loss_ce_0: 0.0695  loss_mask_0: 0.1896  loss_dice_0: 0.229  loss_ce_1: 0.002276  loss_mask_1: 0.197  loss_dice_1: 0.2247  loss_ce_2: 0.002825  loss_mask_2: 0.2  loss_dice_2: 0.2229  loss_ce_3: 0.002652  loss_mask_3: 0.2011  loss_dice_3: 0.2346  loss_ce_4: 0.00267  loss_mask_4: 0.2032  loss_dice_4: 0.2271  loss_ce_5: 0.002473  loss_mask_5: 0.204  loss_dice_5: 0.2261  loss_ce_6: 0.002462  loss_mask_6: 0.2003  loss_dice_6: 0.2201  loss_ce_7: 0.002572  loss_mask_7: 0.1961  loss_dice_7: 0.2296  loss_ce_8: 0.002512  loss_mask_8: 0.1959  loss_dice_8: 0.2286  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:56] d2.utils.events INFO:  eta: 0:22:16  iter: 39459  total_loss: 4.338  loss_ce: 0.003392  loss_mask: 0.1858  loss_dice: 0.19  loss_ce_0: 0.06843  loss_mask_0: 0.1792  loss_dice_0: 0.1919  loss_ce_1: 0.002994  loss_mask_1: 0.1787  loss_dice_1: 0.1894  loss_ce_2: 0.003014  loss_mask_2: 0.1827  loss_dice_2: 0.189  loss_ce_3: 0.002906  loss_mask_3: 0.1905  loss_dice_3: 0.1894  loss_ce_4: 0.003537  loss_mask_4: 0.1843  loss_dice_4: 0.1876  loss_ce_5: 0.003129  loss_mask_5: 0.1824  loss_dice_5: 0.1895  loss_ce_6: 0.0032  loss_mask_6: 0.185  loss_dice_6: 0.1875  loss_ce_7: 0.003481  loss_mask_7: 0.1898  loss_dice_7: 0.1896  loss_ce_8: 0.003065  loss_mask_8: 0.189  loss_dice_8: 0.1911  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:25:58] d2.utils.events INFO:  eta: 0:22:14  iter: 39479  total_loss: 3.869  loss_ce: 0.001971  loss_mask: 0.1988  loss_dice: 0.1349  loss_ce_0: 0.05538  loss_mask_0: 0.2005  loss_dice_0: 0.1356  loss_ce_1: 0.002181  loss_mask_1: 0.2161  loss_dice_1: 0.1348  loss_ce_2: 0.002246  loss_mask_2: 0.1994  loss_dice_2: 0.1308  loss_ce_3: 0.001909  loss_mask_3: 0.2046  loss_dice_3: 0.1361  loss_ce_4: 0.002004  loss_mask_4: 0.1998  loss_dice_4: 0.1289  loss_ce_5: 0.002043  loss_mask_5: 0.2091  loss_dice_5: 0.1336  loss_ce_6: 0.002103  loss_mask_6: 0.1995  loss_dice_6: 0.1358  loss_ce_7: 0.001979  loss_mask_7: 0.2071  loss_dice_7: 0.1327  loss_ce_8: 0.002255  loss_mask_8: 0.2019  loss_dice_8: 0.1376  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:01] d2.utils.events INFO:  eta: 0:22:11  iter: 39499  total_loss: 4.422  loss_ce: 0.002138  loss_mask: 0.2332  loss_dice: 0.1667  loss_ce_0: 0.06895  loss_mask_0: 0.2291  loss_dice_0: 0.1655  loss_ce_1: 0.002612  loss_mask_1: 0.2292  loss_dice_1: 0.1697  loss_ce_2: 0.002514  loss_mask_2: 0.2334  loss_dice_2: 0.1633  loss_ce_3: 0.002496  loss_mask_3: 0.231  loss_dice_3: 0.1688  loss_ce_4: 0.002467  loss_mask_4: 0.2343  loss_dice_4: 0.1719  loss_ce_5: 0.002311  loss_mask_5: 0.2376  loss_dice_5: 0.1667  loss_ce_6: 0.002495  loss_mask_6: 0.2283  loss_dice_6: 0.161  loss_ce_7: 0.002718  loss_mask_7: 0.2141  loss_dice_7: 0.1657  loss_ce_8: 0.002399  loss_mask_8: 0.2285  loss_dice_8: 0.1654  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:26:03] d2.utils.events INFO:  eta: 0:22:08  iter: 39519  total_loss: 4.21  loss_ce: 0.0022  loss_mask: 0.2022  loss_dice: 0.1612  loss_ce_0: 0.06596  loss_mask_0: 0.2145  loss_dice_0: 0.1696  loss_ce_1: 0.002361  loss_mask_1: 0.2059  loss_dice_1: 0.1761  loss_ce_2: 0.00222  loss_mask_2: 0.2072  loss_dice_2: 0.1654  loss_ce_3: 0.002604  loss_mask_3: 0.2157  loss_dice_3: 0.1674  loss_ce_4: 0.0024  loss_mask_4: 0.2092  loss_dice_4: 0.1641  loss_ce_5: 0.002111  loss_mask_5: 0.2118  loss_dice_5: 0.1684  loss_ce_6: 0.002346  loss_mask_6: 0.2176  loss_dice_6: 0.1644  loss_ce_7: 0.002415  loss_mask_7: 0.2106  loss_dice_7: 0.167  loss_ce_8: 0.002233  loss_mask_8: 0.2113  loss_dice_8: 0.1637  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:26:06] d2.utils.events INFO:  eta: 0:22:05  iter: 39539  total_loss: 4.831  loss_ce: 0.004305  loss_mask: 0.1447  loss_dice: 0.2423  loss_ce_0: 0.06266  loss_mask_0: 0.1351  loss_dice_0: 0.2365  loss_ce_1: 0.002742  loss_mask_1: 0.1402  loss_dice_1: 0.2501  loss_ce_2: 0.002763  loss_mask_2: 0.1456  loss_dice_2: 0.2363  loss_ce_3: 0.003591  loss_mask_3: 0.1428  loss_dice_3: 0.2325  loss_ce_4: 0.003178  loss_mask_4: 0.1386  loss_dice_4: 0.2355  loss_ce_5: 0.003995  loss_mask_5: 0.1449  loss_dice_5: 0.2408  loss_ce_6: 0.003829  loss_mask_6: 0.1405  loss_dice_6: 0.2364  loss_ce_7: 0.002814  loss_mask_7: 0.1486  loss_dice_7: 0.2381  loss_ce_8: 0.003613  loss_mask_8: 0.1404  loss_dice_8: 0.2312  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:09] d2.utils.events INFO:  eta: 0:22:03  iter: 39559  total_loss: 3.613  loss_ce: 0.00135  loss_mask: 0.1866  loss_dice: 0.1301  loss_ce_0: 0.06431  loss_mask_0: 0.1897  loss_dice_0: 0.1282  loss_ce_1: 0.001807  loss_mask_1: 0.1807  loss_dice_1: 0.1314  loss_ce_2: 0.001304  loss_mask_2: 0.1793  loss_dice_2: 0.1305  loss_ce_3: 0.001137  loss_mask_3: 0.186  loss_dice_3: 0.1297  loss_ce_4: 0.001206  loss_mask_4: 0.1776  loss_dice_4: 0.1306  loss_ce_5: 0.001203  loss_mask_5: 0.1851  loss_dice_5: 0.1307  loss_ce_6: 0.001459  loss_mask_6: 0.1841  loss_dice_6: 0.133  loss_ce_7: 0.001561  loss_mask_7: 0.1784  loss_dice_7: 0.1293  loss_ce_8: 0.001226  loss_mask_8: 0.1782  loss_dice_8: 0.1313  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:11] d2.utils.events INFO:  eta: 0:22:00  iter: 39579  total_loss: 4  loss_ce: 0.001368  loss_mask: 0.1748  loss_dice: 0.1717  loss_ce_0: 0.06483  loss_mask_0: 0.1796  loss_dice_0: 0.1777  loss_ce_1: 0.001908  loss_mask_1: 0.1733  loss_dice_1: 0.1717  loss_ce_2: 0.00136  loss_mask_2: 0.1804  loss_dice_2: 0.1773  loss_ce_3: 0.001264  loss_mask_3: 0.1737  loss_dice_3: 0.1685  loss_ce_4: 0.001348  loss_mask_4: 0.1759  loss_dice_4: 0.1819  loss_ce_5: 0.001073  loss_mask_5: 0.1761  loss_dice_5: 0.1643  loss_ce_6: 0.001387  loss_mask_6: 0.1796  loss_dice_6: 0.1769  loss_ce_7: 0.001343  loss_mask_7: 0.1799  loss_dice_7: 0.1766  loss_ce_8: 0.001165  loss_mask_8: 0.1773  loss_dice_8: 0.1756  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:14] d2.utils.events INFO:  eta: 0:21:56  iter: 39599  total_loss: 4.411  loss_ce: 0.001054  loss_mask: 0.2046  loss_dice: 0.1929  loss_ce_0: 0.06208  loss_mask_0: 0.2081  loss_dice_0: 0.2012  loss_ce_1: 0.001223  loss_mask_1: 0.1954  loss_dice_1: 0.2044  loss_ce_2: 0.0007767  loss_mask_2: 0.2051  loss_dice_2: 0.201  loss_ce_3: 0.0007287  loss_mask_3: 0.2008  loss_dice_3: 0.2081  loss_ce_4: 0.0007845  loss_mask_4: 0.1993  loss_dice_4: 0.1928  loss_ce_5: 0.0007058  loss_mask_5: 0.2066  loss_dice_5: 0.1904  loss_ce_6: 0.001009  loss_mask_6: 0.1991  loss_dice_6: 0.1992  loss_ce_7: 0.0008671  loss_mask_7: 0.2081  loss_dice_7: 0.2017  loss_ce_8: 0.0007553  loss_mask_8: 0.2037  loss_dice_8: 0.2084  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:16] d2.utils.events INFO:  eta: 0:21:53  iter: 39619  total_loss: 4.51  loss_ce: 0.002051  loss_mask: 0.1979  loss_dice: 0.1594  loss_ce_0: 0.06208  loss_mask_0: 0.1856  loss_dice_0: 0.1576  loss_ce_1: 0.004809  loss_mask_1: 0.1925  loss_dice_1: 0.164  loss_ce_2: 0.002424  loss_mask_2: 0.1875  loss_dice_2: 0.1638  loss_ce_3: 0.001978  loss_mask_3: 0.1929  loss_dice_3: 0.1599  loss_ce_4: 0.002444  loss_mask_4: 0.191  loss_dice_4: 0.1621  loss_ce_5: 0.001981  loss_mask_5: 0.188  loss_dice_5: 0.1588  loss_ce_6: 0.001792  loss_mask_6: 0.1886  loss_dice_6: 0.1577  loss_ce_7: 0.002324  loss_mask_7: 0.1854  loss_dice_7: 0.1593  loss_ce_8: 0.002992  loss_mask_8: 0.1868  loss_dice_8: 0.1616  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:19] d2.utils.events INFO:  eta: 0:21:49  iter: 39639  total_loss: 3.74  loss_ce: 0.001408  loss_mask: 0.1784  loss_dice: 0.1923  loss_ce_0: 0.06182  loss_mask_0: 0.1759  loss_dice_0: 0.1908  loss_ce_1: 0.001542  loss_mask_1: 0.1738  loss_dice_1: 0.1908  loss_ce_2: 0.00142  loss_mask_2: 0.1715  loss_dice_2: 0.1902  loss_ce_3: 0.001207  loss_mask_3: 0.1755  loss_dice_3: 0.1916  loss_ce_4: 0.001264  loss_mask_4: 0.1736  loss_dice_4: 0.1892  loss_ce_5: 0.001389  loss_mask_5: 0.1696  loss_dice_5: 0.1888  loss_ce_6: 0.001412  loss_mask_6: 0.1759  loss_dice_6: 0.1851  loss_ce_7: 0.001398  loss_mask_7: 0.1682  loss_dice_7: 0.1872  loss_ce_8: 0.001281  loss_mask_8: 0.1717  loss_dice_8: 0.187  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:21] d2.utils.events INFO:  eta: 0:21:47  iter: 39659  total_loss: 4.724  loss_ce: 0.005583  loss_mask: 0.1506  loss_dice: 0.1713  loss_ce_0: 0.06147  loss_mask_0: 0.1558  loss_dice_0: 0.1735  loss_ce_1: 0.01894  loss_mask_1: 0.1567  loss_dice_1: 0.1704  loss_ce_2: 0.0066  loss_mask_2: 0.1609  loss_dice_2: 0.1806  loss_ce_3: 0.005241  loss_mask_3: 0.1613  loss_dice_3: 0.1735  loss_ce_4: 0.008148  loss_mask_4: 0.1581  loss_dice_4: 0.1738  loss_ce_5: 0.005832  loss_mask_5: 0.1644  loss_dice_5: 0.1793  loss_ce_6: 0.004283  loss_mask_6: 0.157  loss_dice_6: 0.1801  loss_ce_7: 0.007689  loss_mask_7: 0.162  loss_dice_7: 0.1733  loss_ce_8: 0.005496  loss_mask_8: 0.1547  loss_dice_8: 0.178  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:24] d2.utils.events INFO:  eta: 0:21:43  iter: 39679  total_loss: 4.328  loss_ce: 0.00191  loss_mask: 0.2173  loss_dice: 0.1501  loss_ce_0: 0.06564  loss_mask_0: 0.2175  loss_dice_0: 0.1512  loss_ce_1: 0.002149  loss_mask_1: 0.2317  loss_dice_1: 0.1588  loss_ce_2: 0.001725  loss_mask_2: 0.2269  loss_dice_2: 0.1596  loss_ce_3: 0.001514  loss_mask_3: 0.2234  loss_dice_3: 0.1571  loss_ce_4: 0.001496  loss_mask_4: 0.216  loss_dice_4: 0.1536  loss_ce_5: 0.001741  loss_mask_5: 0.2265  loss_dice_5: 0.1548  loss_ce_6: 0.001269  loss_mask_6: 0.2268  loss_dice_6: 0.155  loss_ce_7: 0.001706  loss_mask_7: 0.2282  loss_dice_7: 0.1529  loss_ce_8: 0.001991  loss_mask_8: 0.2228  loss_dice_8: 0.152  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:26] d2.utils.events INFO:  eta: 0:21:38  iter: 39699  total_loss: 5.206  loss_ce: 0.003159  loss_mask: 0.2161  loss_dice: 0.1736  loss_ce_0: 0.06826  loss_mask_0: 0.2134  loss_dice_0: 0.1772  loss_ce_1: 0.07315  loss_mask_1: 0.2117  loss_dice_1: 0.173  loss_ce_2: 0.003143  loss_mask_2: 0.1993  loss_dice_2: 0.1754  loss_ce_3: 0.003436  loss_mask_3: 0.2019  loss_dice_3: 0.1868  loss_ce_4: 0.004271  loss_mask_4: 0.2088  loss_dice_4: 0.1762  loss_ce_5: 0.002843  loss_mask_5: 0.2204  loss_dice_5: 0.1753  loss_ce_6: 0.001994  loss_mask_6: 0.2034  loss_dice_6: 0.1795  loss_ce_7: 0.003757  loss_mask_7: 0.2136  loss_dice_7: 0.1774  loss_ce_8: 0.003798  loss_mask_8: 0.2057  loss_dice_8: 0.1753  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:29] d2.utils.events INFO:  eta: 0:21:34  iter: 39719  total_loss: 5.189  loss_ce: 0.006723  loss_mask: 0.1895  loss_dice: 0.1927  loss_ce_0: 0.06214  loss_mask_0: 0.1824  loss_dice_0: 0.1916  loss_ce_1: 0.005693  loss_mask_1: 0.187  loss_dice_1: 0.1904  loss_ce_2: 0.009406  loss_mask_2: 0.1865  loss_dice_2: 0.1908  loss_ce_3: 0.007866  loss_mask_3: 0.1998  loss_dice_3: 0.2014  loss_ce_4: 0.007185  loss_mask_4: 0.1999  loss_dice_4: 0.1955  loss_ce_5: 0.006218  loss_mask_5: 0.187  loss_dice_5: 0.2026  loss_ce_6: 0.003938  loss_mask_6: 0.1848  loss_dice_6: 0.1899  loss_ce_7: 0.00555  loss_mask_7: 0.1841  loss_dice_7: 0.1953  loss_ce_8: 0.009507  loss_mask_8: 0.197  loss_dice_8: 0.1991  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:26:31] d2.utils.events INFO:  eta: 0:21:31  iter: 39739  total_loss: 4.879  loss_ce: 0.01855  loss_mask: 0.1884  loss_dice: 0.2235  loss_ce_0: 0.062  loss_mask_0: 0.1858  loss_dice_0: 0.2146  loss_ce_1: 0.01014  loss_mask_1: 0.1839  loss_dice_1: 0.2155  loss_ce_2: 0.02177  loss_mask_2: 0.1863  loss_dice_2: 0.2094  loss_ce_3: 0.01877  loss_mask_3: 0.1836  loss_dice_3: 0.2157  loss_ce_4: 0.01871  loss_mask_4: 0.1857  loss_dice_4: 0.2248  loss_ce_5: 0.02595  loss_mask_5: 0.1916  loss_dice_5: 0.2149  loss_ce_6: 0.01689  loss_mask_6: 0.1809  loss_dice_6: 0.2136  loss_ce_7: 0.01499  loss_mask_7: 0.1866  loss_dice_7: 0.2134  loss_ce_8: 0.02609  loss_mask_8: 0.188  loss_dice_8: 0.2253  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:26:34] d2.utils.events INFO:  eta: 0:21:28  iter: 39759  total_loss: 5.258  loss_ce: 0.006958  loss_mask: 0.2295  loss_dice: 0.1722  loss_ce_0: 0.0619  loss_mask_0: 0.229  loss_dice_0: 0.1658  loss_ce_1: 0.01001  loss_mask_1: 0.2339  loss_dice_1: 0.164  loss_ce_2: 0.009885  loss_mask_2: 0.2363  loss_dice_2: 0.166  loss_ce_3: 0.008269  loss_mask_3: 0.2329  loss_dice_3: 0.1644  loss_ce_4: 0.008243  loss_mask_4: 0.2337  loss_dice_4: 0.1703  loss_ce_5: 0.009703  loss_mask_5: 0.2292  loss_dice_5: 0.1685  loss_ce_6: 0.00638  loss_mask_6: 0.2249  loss_dice_6: 0.1687  loss_ce_7: 0.007131  loss_mask_7: 0.2326  loss_dice_7: 0.1696  loss_ce_8: 0.01106  loss_mask_8: 0.238  loss_dice_8: 0.1727  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:26:36] d2.utils.events INFO:  eta: 0:21:25  iter: 39779  total_loss: 4.573  loss_ce: 0.002754  loss_mask: 0.1847  loss_dice: 0.2676  loss_ce_0: 0.05977  loss_mask_0: 0.1924  loss_dice_0: 0.2716  loss_ce_1: 0.002613  loss_mask_1: 0.1744  loss_dice_1: 0.2663  loss_ce_2: 0.002726  loss_mask_2: 0.177  loss_dice_2: 0.2638  loss_ce_3: 0.003261  loss_mask_3: 0.1833  loss_dice_3: 0.2688  loss_ce_4: 0.003625  loss_mask_4: 0.186  loss_dice_4: 0.2706  loss_ce_5: 0.002833  loss_mask_5: 0.1767  loss_dice_5: 0.2724  loss_ce_6: 0.0029  loss_mask_6: 0.1845  loss_dice_6: 0.2603  loss_ce_7: 0.003042  loss_mask_7: 0.1822  loss_dice_7: 0.2673  loss_ce_8: 0.002852  loss_mask_8: 0.1731  loss_dice_8: 0.2686  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:39] d2.utils.events INFO:  eta: 0:21:23  iter: 39799  total_loss: 4.211  loss_ce: 0.002712  loss_mask: 0.2055  loss_dice: 0.149  loss_ce_0: 0.06322  loss_mask_0: 0.2069  loss_dice_0: 0.1521  loss_ce_1: 0.003035  loss_mask_1: 0.2022  loss_dice_1: 0.1474  loss_ce_2: 0.002759  loss_mask_2: 0.2063  loss_dice_2: 0.142  loss_ce_3: 0.003153  loss_mask_3: 0.207  loss_dice_3: 0.1461  loss_ce_4: 0.003656  loss_mask_4: 0.2004  loss_dice_4: 0.1489  loss_ce_5: 0.003217  loss_mask_5: 0.1989  loss_dice_5: 0.1518  loss_ce_6: 0.002488  loss_mask_6: 0.2077  loss_dice_6: 0.1481  loss_ce_7: 0.002657  loss_mask_7: 0.1991  loss_dice_7: 0.145  loss_ce_8: 0.002921  loss_mask_8: 0.1971  loss_dice_8: 0.1508  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:41] d2.utils.events INFO:  eta: 0:21:20  iter: 39819  total_loss: 4.276  loss_ce: 0.002516  loss_mask: 0.2184  loss_dice: 0.1574  loss_ce_0: 0.06227  loss_mask_0: 0.2064  loss_dice_0: 0.1519  loss_ce_1: 0.002002  loss_mask_1: 0.2087  loss_dice_1: 0.1555  loss_ce_2: 0.002447  loss_mask_2: 0.2106  loss_dice_2: 0.1516  loss_ce_3: 0.002543  loss_mask_3: 0.2017  loss_dice_3: 0.1553  loss_ce_4: 0.003045  loss_mask_4: 0.2037  loss_dice_4: 0.1503  loss_ce_5: 0.00229  loss_mask_5: 0.2047  loss_dice_5: 0.1537  loss_ce_6: 0.002292  loss_mask_6: 0.2045  loss_dice_6: 0.1514  loss_ce_7: 0.00234  loss_mask_7: 0.2071  loss_dice_7: 0.1585  loss_ce_8: 0.002152  loss_mask_8: 0.2095  loss_dice_8: 0.1536  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:26:44] d2.utils.events INFO:  eta: 0:21:17  iter: 39839  total_loss: 4.445  loss_ce: 0.001632  loss_mask: 0.1875  loss_dice: 0.1879  loss_ce_0: 0.06159  loss_mask_0: 0.1886  loss_dice_0: 0.1886  loss_ce_1: 0.001411  loss_mask_1: 0.1969  loss_dice_1: 0.1905  loss_ce_2: 0.001317  loss_mask_2: 0.1912  loss_dice_2: 0.1916  loss_ce_3: 0.001491  loss_mask_3: 0.1922  loss_dice_3: 0.1848  loss_ce_4: 0.00177  loss_mask_4: 0.1914  loss_dice_4: 0.1865  loss_ce_5: 0.001362  loss_mask_5: 0.1891  loss_dice_5: 0.181  loss_ce_6: 0.001827  loss_mask_6: 0.1886  loss_dice_6: 0.1896  loss_ce_7: 0.001631  loss_mask_7: 0.1935  loss_dice_7: 0.1883  loss_ce_8: 0.001261  loss_mask_8: 0.1882  loss_dice_8: 0.19  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:26:46] d2.utils.events INFO:  eta: 0:21:14  iter: 39859  total_loss: 4.347  loss_ce: 0.002439  loss_mask: 0.2286  loss_dice: 0.194  loss_ce_0: 0.05523  loss_mask_0: 0.2321  loss_dice_0: 0.1822  loss_ce_1: 0.001934  loss_mask_1: 0.2311  loss_dice_1: 0.1893  loss_ce_2: 0.002409  loss_mask_2: 0.2398  loss_dice_2: 0.1833  loss_ce_3: 0.002191  loss_mask_3: 0.2274  loss_dice_3: 0.1882  loss_ce_4: 0.002541  loss_mask_4: 0.2331  loss_dice_4: 0.1884  loss_ce_5: 0.002484  loss_mask_5: 0.2419  loss_dice_5: 0.1866  loss_ce_6: 0.00267  loss_mask_6: 0.2339  loss_dice_6: 0.1933  loss_ce_7: 0.002094  loss_mask_7: 0.232  loss_dice_7: 0.188  loss_ce_8: 0.003172  loss_mask_8: 0.2309  loss_dice_8: 0.193  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:26:49] d2.utils.events INFO:  eta: 0:21:10  iter: 39879  total_loss: 5.457  loss_ce: 0.00491  loss_mask: 0.1908  loss_dice: 0.1902  loss_ce_0: 0.05138  loss_mask_0: 0.1834  loss_dice_0: 0.1859  loss_ce_1: 0.009466  loss_mask_1: 0.1858  loss_dice_1: 0.1923  loss_ce_2: 0.00482  loss_mask_2: 0.1793  loss_dice_2: 0.1929  loss_ce_3: 0.004048  loss_mask_3: 0.183  loss_dice_3: 0.1941  loss_ce_4: 0.00627  loss_mask_4: 0.1845  loss_dice_4: 0.1932  loss_ce_5: 0.005736  loss_mask_5: 0.1836  loss_dice_5: 0.1901  loss_ce_6: 0.007906  loss_mask_6: 0.1864  loss_dice_6: 0.1939  loss_ce_7: 0.01272  loss_mask_7: 0.1846  loss_dice_7: 0.192  loss_ce_8: 0.007197  loss_mask_8: 0.2011  loss_dice_8: 0.1924  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:51] d2.utils.events INFO:  eta: 0:21:07  iter: 39899  total_loss: 4.306  loss_ce: 0.002473  loss_mask: 0.2367  loss_dice: 0.1546  loss_ce_0: 0.07032  loss_mask_0: 0.243  loss_dice_0: 0.1611  loss_ce_1: 0.003429  loss_mask_1: 0.2429  loss_dice_1: 0.1537  loss_ce_2: 0.002849  loss_mask_2: 0.2388  loss_dice_2: 0.1589  loss_ce_3: 0.002131  loss_mask_3: 0.236  loss_dice_3: 0.1554  loss_ce_4: 0.002947  loss_mask_4: 0.2304  loss_dice_4: 0.1496  loss_ce_5: 0.002737  loss_mask_5: 0.2318  loss_dice_5: 0.1513  loss_ce_6: 0.005041  loss_mask_6: 0.2332  loss_dice_6: 0.1512  loss_ce_7: 0.006071  loss_mask_7: 0.2304  loss_dice_7: 0.1585  loss_ce_8: 0.003902  loss_mask_8: 0.2285  loss_dice_8: 0.1554  time: 0.1260  data_time: 0.0012  lr: 0.0001  max_mem: 3105M
[04/13 16:26:54] d2.utils.events INFO:  eta: 0:21:06  iter: 39919  total_loss: 4.581  loss_ce: 0.002364  loss_mask: 0.22  loss_dice: 0.1904  loss_ce_0: 0.05829  loss_mask_0: 0.2128  loss_dice_0: 0.1926  loss_ce_1: 0.002222  loss_mask_1: 0.217  loss_dice_1: 0.1919  loss_ce_2: 0.002406  loss_mask_2: 0.2128  loss_dice_2: 0.1933  loss_ce_3: 0.001923  loss_mask_3: 0.2272  loss_dice_3: 0.1939  loss_ce_4: 0.002548  loss_mask_4: 0.2188  loss_dice_4: 0.1902  loss_ce_5: 0.0023  loss_mask_5: 0.2136  loss_dice_5: 0.1921  loss_ce_6: 0.002991  loss_mask_6: 0.2137  loss_dice_6: 0.194  loss_ce_7: 0.003112  loss_mask_7: 0.2212  loss_dice_7: 0.194  loss_ce_8: 0.003577  loss_mask_8: 0.2119  loss_dice_8: 0.1885  time: 0.1260  data_time: 0.0014  lr: 0.0001  max_mem: 3105M
[04/13 16:26:56] d2.utils.events INFO:  eta: 0:21:04  iter: 39939  total_loss: 4.205  loss_ce: 0.00148  loss_mask: 0.2528  loss_dice: 0.1682  loss_ce_0: 0.0647  loss_mask_0: 0.2412  loss_dice_0: 0.1648  loss_ce_1: 0.001638  loss_mask_1: 0.2457  loss_dice_1: 0.1631  loss_ce_2: 0.00182  loss_mask_2: 0.2547  loss_dice_2: 0.1587  loss_ce_3: 0.001261  loss_mask_3: 0.2426  loss_dice_3: 0.1671  loss_ce_4: 0.001979  loss_mask_4: 0.2465  loss_dice_4: 0.1595  loss_ce_5: 0.001749  loss_mask_5: 0.2518  loss_dice_5: 0.1661  loss_ce_6: 0.002202  loss_mask_6: 0.2471  loss_dice_6: 0.1545  loss_ce_7: 0.002271  loss_mask_7: 0.2404  loss_dice_7: 0.161  loss_ce_8: 0.002475  loss_mask_8: 0.2565  loss_dice_8: 0.1648  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:26:59] d2.utils.events INFO:  eta: 0:21:01  iter: 39959  total_loss: 4.523  loss_ce: 0.0009477  loss_mask: 0.241  loss_dice: 0.1892  loss_ce_0: 0.05904  loss_mask_0: 0.2379  loss_dice_0: 0.1896  loss_ce_1: 0.0012  loss_mask_1: 0.225  loss_dice_1: 0.1868  loss_ce_2: 0.001108  loss_mask_2: 0.2287  loss_dice_2: 0.1895  loss_ce_3: 0.001017  loss_mask_3: 0.2193  loss_dice_3: 0.1935  loss_ce_4: 0.00106  loss_mask_4: 0.2303  loss_dice_4: 0.1871  loss_ce_5: 0.001051  loss_mask_5: 0.2239  loss_dice_5: 0.1898  loss_ce_6: 0.00128  loss_mask_6: 0.2348  loss_dice_6: 0.191  loss_ce_7: 0.0009886  loss_mask_7: 0.2235  loss_dice_7: 0.1831  loss_ce_8: 0.001589  loss_mask_8: 0.2201  loss_dice_8: 0.1884  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:27:01] d2.utils.events INFO:  eta: 0:20:59  iter: 39979  total_loss: 3.475  loss_ce: 0.00124  loss_mask: 0.1813  loss_dice: 0.1355  loss_ce_0: 0.07053  loss_mask_0: 0.185  loss_dice_0: 0.1403  loss_ce_1: 0.001294  loss_mask_1: 0.1821  loss_dice_1: 0.1355  loss_ce_2: 0.00145  loss_mask_2: 0.1813  loss_dice_2: 0.1404  loss_ce_3: 0.001065  loss_mask_3: 0.1823  loss_dice_3: 0.1342  loss_ce_4: 0.001502  loss_mask_4: 0.1869  loss_dice_4: 0.1369  loss_ce_5: 0.001304  loss_mask_5: 0.186  loss_dice_5: 0.1414  loss_ce_6: 0.001839  loss_mask_6: 0.1877  loss_dice_6: 0.1409  loss_ce_7: 0.001752  loss_mask_7: 0.1797  loss_dice_7: 0.1432  loss_ce_8: 0.001917  loss_mask_8: 0.1834  loss_dice_8: 0.1401  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:27:04] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0039999.pth
[04/13 16:27:04] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 16:27:04] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 16:27:04] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 16:27:04] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 16:27:04] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 16:27:08] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0005 s/iter. Inference: 0.0528 s/iter. Eval: 0.2286 s/iter. Total: 0.2820 s/iter. ETA=0:03:58
[04/13 16:27:13] d2.evaluation.evaluator INFO: Inference done 29/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2293 s/iter. Total: 0.2831 s/iter. ETA=0:03:54
[04/13 16:27:18] d2.evaluation.evaluator INFO: Inference done 47/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2295 s/iter. Total: 0.2833 s/iter. ETA=0:03:49
[04/13 16:27:23] d2.evaluation.evaluator INFO: Inference done 65/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2319 s/iter. Total: 0.2857 s/iter. ETA=0:03:45
[04/13 16:27:28] d2.evaluation.evaluator INFO: Inference done 83/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2319 s/iter. Total: 0.2857 s/iter. ETA=0:03:40
[04/13 16:27:33] d2.evaluation.evaluator INFO: Inference done 101/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2319 s/iter. Total: 0.2857 s/iter. ETA=0:03:35
[04/13 16:27:38] d2.evaluation.evaluator INFO: Inference done 119/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2321 s/iter. Total: 0.2858 s/iter. ETA=0:03:30
[04/13 16:27:44] d2.evaluation.evaluator INFO: Inference done 137/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2321 s/iter. Total: 0.2859 s/iter. ETA=0:03:25
[04/13 16:27:49] d2.evaluation.evaluator INFO: Inference done 155/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2322 s/iter. Total: 0.2859 s/iter. ETA=0:03:20
[04/13 16:27:54] d2.evaluation.evaluator INFO: Inference done 173/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2324 s/iter. Total: 0.2861 s/iter. ETA=0:03:15
[04/13 16:27:59] d2.evaluation.evaluator INFO: Inference done 191/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2324 s/iter. Total: 0.2861 s/iter. ETA=0:03:10
[04/13 16:28:04] d2.evaluation.evaluator INFO: Inference done 209/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2325 s/iter. Total: 0.2863 s/iter. ETA=0:03:05
[04/13 16:28:09] d2.evaluation.evaluator INFO: Inference done 227/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2326 s/iter. Total: 0.2864 s/iter. ETA=0:03:00
[04/13 16:28:15] d2.evaluation.evaluator INFO: Inference done 245/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2326 s/iter. Total: 0.2864 s/iter. ETA=0:02:54
[04/13 16:28:20] d2.evaluation.evaluator INFO: Inference done 263/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2327 s/iter. Total: 0.2864 s/iter. ETA=0:02:49
[04/13 16:28:25] d2.evaluation.evaluator INFO: Inference done 281/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2331 s/iter. Total: 0.2868 s/iter. ETA=0:02:44
[04/13 16:28:30] d2.evaluation.evaluator INFO: Inference done 299/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2332 s/iter. Total: 0.2869 s/iter. ETA=0:02:39
[04/13 16:28:35] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2332 s/iter. Total: 0.2869 s/iter. ETA=0:02:34
[04/13 16:28:40] d2.evaluation.evaluator INFO: Inference done 335/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2331 s/iter. Total: 0.2869 s/iter. ETA=0:02:29
[04/13 16:28:46] d2.evaluation.evaluator INFO: Inference done 353/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2331 s/iter. Total: 0.2869 s/iter. ETA=0:02:24
[04/13 16:28:51] d2.evaluation.evaluator INFO: Inference done 371/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2331 s/iter. Total: 0.2869 s/iter. ETA=0:02:19
[04/13 16:28:56] d2.evaluation.evaluator INFO: Inference done 389/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2331 s/iter. Total: 0.2869 s/iter. ETA=0:02:13
[04/13 16:29:01] d2.evaluation.evaluator INFO: Inference done 407/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2332 s/iter. Total: 0.2869 s/iter. ETA=0:02:08
[04/13 16:29:06] d2.evaluation.evaluator INFO: Inference done 425/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2331 s/iter. Total: 0.2869 s/iter. ETA=0:02:03
[04/13 16:29:11] d2.evaluation.evaluator INFO: Inference done 443/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2331 s/iter. Total: 0.2868 s/iter. ETA=0:01:58
[04/13 16:29:17] d2.evaluation.evaluator INFO: Inference done 461/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2331 s/iter. Total: 0.2868 s/iter. ETA=0:01:53
[04/13 16:29:22] d2.evaluation.evaluator INFO: Inference done 479/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2331 s/iter. Total: 0.2868 s/iter. ETA=0:01:48
[04/13 16:29:27] d2.evaluation.evaluator INFO: Inference done 497/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2331 s/iter. Total: 0.2869 s/iter. ETA=0:01:42
[04/13 16:29:32] d2.evaluation.evaluator INFO: Inference done 515/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2334 s/iter. Total: 0.2871 s/iter. ETA=0:01:37
[04/13 16:29:37] d2.evaluation.evaluator INFO: Inference done 533/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2334 s/iter. Total: 0.2871 s/iter. ETA=0:01:32
[04/13 16:29:43] d2.evaluation.evaluator INFO: Inference done 551/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2334 s/iter. Total: 0.2872 s/iter. ETA=0:01:27
[04/13 16:29:48] d2.evaluation.evaluator INFO: Inference done 569/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2335 s/iter. Total: 0.2872 s/iter. ETA=0:01:22
[04/13 16:29:53] d2.evaluation.evaluator INFO: Inference done 586/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2338 s/iter. Total: 0.2875 s/iter. ETA=0:01:17
[04/13 16:29:58] d2.evaluation.evaluator INFO: Inference done 603/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2340 s/iter. Total: 0.2878 s/iter. ETA=0:01:12
[04/13 16:30:03] d2.evaluation.evaluator INFO: Inference done 620/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2343 s/iter. Total: 0.2880 s/iter. ETA=0:01:07
[04/13 16:30:08] d2.evaluation.evaluator INFO: Inference done 637/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2346 s/iter. Total: 0.2883 s/iter. ETA=0:01:03
[04/13 16:30:13] d2.evaluation.evaluator INFO: Inference done 654/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2349 s/iter. Total: 0.2886 s/iter. ETA=0:00:58
[04/13 16:30:18] d2.evaluation.evaluator INFO: Inference done 671/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2351 s/iter. Total: 0.2888 s/iter. ETA=0:00:53
[04/13 16:30:23] d2.evaluation.evaluator INFO: Inference done 688/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2352 s/iter. Total: 0.2890 s/iter. ETA=0:00:48
[04/13 16:30:28] d2.evaluation.evaluator INFO: Inference done 705/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2354 s/iter. Total: 0.2891 s/iter. ETA=0:00:43
[04/13 16:30:33] d2.evaluation.evaluator INFO: Inference done 722/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2356 s/iter. Total: 0.2893 s/iter. ETA=0:00:38
[04/13 16:30:38] d2.evaluation.evaluator INFO: Inference done 739/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2358 s/iter. Total: 0.2895 s/iter. ETA=0:00:33
[04/13 16:30:43] d2.evaluation.evaluator INFO: Inference done 756/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2361 s/iter. Total: 0.2898 s/iter. ETA=0:00:28
[04/13 16:30:48] d2.evaluation.evaluator INFO: Inference done 773/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2362 s/iter. Total: 0.2900 s/iter. ETA=0:00:24
[04/13 16:30:54] d2.evaluation.evaluator INFO: Inference done 790/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2363 s/iter. Total: 0.2901 s/iter. ETA=0:00:19
[04/13 16:30:59] d2.evaluation.evaluator INFO: Inference done 807/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2365 s/iter. Total: 0.2902 s/iter. ETA=0:00:14
[04/13 16:31:04] d2.evaluation.evaluator INFO: Inference done 824/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2366 s/iter. Total: 0.2903 s/iter. ETA=0:00:09
[04/13 16:31:09] d2.evaluation.evaluator INFO: Inference done 841/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2367 s/iter. Total: 0.2905 s/iter. ETA=0:00:04
[04/13 16:31:13] d2.evaluation.evaluator INFO: Total inference time: 0:04:07.383459 (0.290697 s / iter per device, on 1 devices)
[04/13 16:31:13] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052836 s / iter per device, on 1 devices)
[04/13 16:31:14] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 16:31:14] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 16:31:16] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 16:31:16] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 16:31:16] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.27 seconds.
[04/13 16:31:16] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 16:31:16] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.04 seconds.
[04/13 16:31:16] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 16:31:16] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 16:31:16] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 16:31:20] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 16:31:22] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 1.46 seconds.
[04/13 16:31:22] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 16:31:22] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.04 seconds.
[04/13 16:31:22] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 88.927 | 88.927 | 88.927 |  nan  |  nan  | 88.927 |
[04/13 16:31:22] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 16:31:22] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 89.284 | defect     | 88.570 |
[04/13 16:31:22] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 16:31:22] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 16:31:22] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 16:31:22] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 16:31:22] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 16:31:22] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 16:31:22] d2.evaluation.testing INFO: copypaste: 88.9268,88.9268,88.9268,nan,nan,88.9268
[04/13 16:31:22] d2.utils.events INFO:  eta: 0:20:57  iter: 39999  total_loss: 4.021  loss_ce: 0.0008872  loss_mask: 0.2194  loss_dice: 0.1653  loss_ce_0: 0.06554  loss_mask_0: 0.2208  loss_dice_0: 0.1652  loss_ce_1: 0.001175  loss_mask_1: 0.2189  loss_dice_1: 0.1639  loss_ce_2: 0.001202  loss_mask_2: 0.2124  loss_dice_2: 0.1552  loss_ce_3: 0.001083  loss_mask_3: 0.2276  loss_dice_3: 0.1659  loss_ce_4: 0.001192  loss_mask_4: 0.216  loss_dice_4: 0.1564  loss_ce_5: 0.001159  loss_mask_5: 0.209  loss_dice_5: 0.155  loss_ce_6: 0.001346  loss_mask_6: 0.2124  loss_dice_6: 0.1595  loss_ce_7: 0.001393  loss_mask_7: 0.2204  loss_dice_7: 0.1642  loss_ce_8: 0.001575  loss_mask_8: 0.2238  loss_dice_8: 0.1636  time: 0.1260  data_time: 0.0013  lr: 0.0001  max_mem: 3105M
[04/13 16:31:24] d2.utils.events INFO:  eta: 0:20:54  iter: 40019  total_loss: 5.004  loss_ce: 0.001837  loss_mask: 0.197  loss_dice: 0.1813  loss_ce_0: 0.06211  loss_mask_0: 0.1947  loss_dice_0: 0.1786  loss_ce_1: 0.002403  loss_mask_1: 0.1928  loss_dice_1: 0.173  loss_ce_2: 0.002887  loss_mask_2: 0.2041  loss_dice_2: 0.1757  loss_ce_3: 0.002402  loss_mask_3: 0.2002  loss_dice_3: 0.1769  loss_ce_4: 0.002142  loss_mask_4: 0.2027  loss_dice_4: 0.1785  loss_ce_5: 0.002687  loss_mask_5: 0.1935  loss_dice_5: 0.1756  loss_ce_6: 0.002303  loss_mask_6: 0.2  loss_dice_6: 0.1712  loss_ce_7: 0.002062  loss_mask_7: 0.1962  loss_dice_7: 0.1731  loss_ce_8: 0.002851  loss_mask_8: 0.19  loss_dice_8: 0.1743  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:31:27] d2.utils.events INFO:  eta: 0:20:50  iter: 40039  total_loss: 4.373  loss_ce: 0.0008887  loss_mask: 0.2629  loss_dice: 0.1788  loss_ce_0: 0.06189  loss_mask_0: 0.2463  loss_dice_0: 0.1805  loss_ce_1: 0.001063  loss_mask_1: 0.263  loss_dice_1: 0.1786  loss_ce_2: 0.0009866  loss_mask_2: 0.2599  loss_dice_2: 0.1724  loss_ce_3: 0.001021  loss_mask_3: 0.2608  loss_dice_3: 0.1815  loss_ce_4: 0.00106  loss_mask_4: 0.2431  loss_dice_4: 0.1773  loss_ce_5: 0.001042  loss_mask_5: 0.2514  loss_dice_5: 0.177  loss_ce_6: 0.00128  loss_mask_6: 0.2603  loss_dice_6: 0.1803  loss_ce_7: 0.001329  loss_mask_7: 0.2518  loss_dice_7: 0.1752  loss_ce_8: 0.001545  loss_mask_8: 0.2569  loss_dice_8: 0.1794  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:31:29] d2.utils.events INFO:  eta: 0:20:46  iter: 40059  total_loss: 4.206  loss_ce: 0.001421  loss_mask: 0.1894  loss_dice: 0.1881  loss_ce_0: 0.06215  loss_mask_0: 0.1873  loss_dice_0: 0.1837  loss_ce_1: 0.001501  loss_mask_1: 0.1868  loss_dice_1: 0.1924  loss_ce_2: 0.001236  loss_mask_2: 0.1842  loss_dice_2: 0.1948  loss_ce_3: 0.001579  loss_mask_3: 0.2048  loss_dice_3: 0.1877  loss_ce_4: 0.001392  loss_mask_4: 0.1909  loss_dice_4: 0.1824  loss_ce_5: 0.001022  loss_mask_5: 0.1985  loss_dice_5: 0.188  loss_ce_6: 0.001482  loss_mask_6: 0.1953  loss_dice_6: 0.1912  loss_ce_7: 0.00144  loss_mask_7: 0.1866  loss_dice_7: 0.1907  loss_ce_8: 0.001481  loss_mask_8: 0.1947  loss_dice_8: 0.1889  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:31:32] d2.utils.events INFO:  eta: 0:20:45  iter: 40079  total_loss: 4.707  loss_ce: 0.002052  loss_mask: 0.1998  loss_dice: 0.18  loss_ce_0: 0.06199  loss_mask_0: 0.1924  loss_dice_0: 0.173  loss_ce_1: 0.004094  loss_mask_1: 0.1865  loss_dice_1: 0.1777  loss_ce_2: 0.002533  loss_mask_2: 0.2069  loss_dice_2: 0.1723  loss_ce_3: 0.001917  loss_mask_3: 0.202  loss_dice_3: 0.1822  loss_ce_4: 0.002541  loss_mask_4: 0.1961  loss_dice_4: 0.1857  loss_ce_5: 0.002255  loss_mask_5: 0.1852  loss_dice_5: 0.1823  loss_ce_6: 0.001841  loss_mask_6: 0.1968  loss_dice_6: 0.1786  loss_ce_7: 0.001827  loss_mask_7: 0.1781  loss_dice_7: 0.1741  loss_ce_8: 0.002726  loss_mask_8: 0.1821  loss_dice_8: 0.1765  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:31:34] d2.utils.events INFO:  eta: 0:20:44  iter: 40099  total_loss: 4.069  loss_ce: 0.0008505  loss_mask: 0.2237  loss_dice: 0.1688  loss_ce_0: 0.06178  loss_mask_0: 0.215  loss_dice_0: 0.1705  loss_ce_1: 0.001081  loss_mask_1: 0.2149  loss_dice_1: 0.1689  loss_ce_2: 0.000963  loss_mask_2: 0.2234  loss_dice_2: 0.1662  loss_ce_3: 0.0009503  loss_mask_3: 0.2061  loss_dice_3: 0.1644  loss_ce_4: 0.000976  loss_mask_4: 0.2186  loss_dice_4: 0.1635  loss_ce_5: 0.0008915  loss_mask_5: 0.2232  loss_dice_5: 0.1634  loss_ce_6: 0.001183  loss_mask_6: 0.2029  loss_dice_6: 0.167  loss_ce_7: 0.001132  loss_mask_7: 0.2131  loss_dice_7: 0.1739  loss_ce_8: 0.001304  loss_mask_8: 0.2152  loss_dice_8: 0.1702  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:31:37] d2.utils.events INFO:  eta: 0:20:42  iter: 40119  total_loss: 4.71  loss_ce: 0.001551  loss_mask: 0.1878  loss_dice: 0.1563  loss_ce_0: 0.06182  loss_mask_0: 0.1833  loss_dice_0: 0.1551  loss_ce_1: 0.001865  loss_mask_1: 0.1866  loss_dice_1: 0.1529  loss_ce_2: 0.00192  loss_mask_2: 0.1899  loss_dice_2: 0.1581  loss_ce_3: 0.001786  loss_mask_3: 0.1843  loss_dice_3: 0.1563  loss_ce_4: 0.001591  loss_mask_4: 0.1882  loss_dice_4: 0.161  loss_ce_5: 0.001709  loss_mask_5: 0.1831  loss_dice_5: 0.1606  loss_ce_6: 0.001997  loss_mask_6: 0.1768  loss_dice_6: 0.1653  loss_ce_7: 0.001471  loss_mask_7: 0.1857  loss_dice_7: 0.1535  loss_ce_8: 0.002577  loss_mask_8: 0.1917  loss_dice_8: 0.1542  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:31:39] d2.utils.events INFO:  eta: 0:20:38  iter: 40139  total_loss: 3.67  loss_ce: 0.0009781  loss_mask: 0.1844  loss_dice: 0.1989  loss_ce_0: 0.06186  loss_mask_0: 0.1728  loss_dice_0: 0.1954  loss_ce_1: 0.001026  loss_mask_1: 0.1833  loss_dice_1: 0.1925  loss_ce_2: 0.001058  loss_mask_2: 0.1775  loss_dice_2: 0.1905  loss_ce_3: 0.001037  loss_mask_3: 0.1845  loss_dice_3: 0.1846  loss_ce_4: 0.0009632  loss_mask_4: 0.1791  loss_dice_4: 0.191  loss_ce_5: 0.0009563  loss_mask_5: 0.1784  loss_dice_5: 0.1975  loss_ce_6: 0.001267  loss_mask_6: 0.1783  loss_dice_6: 0.1992  loss_ce_7: 0.0009566  loss_mask_7: 0.18  loss_dice_7: 0.1986  loss_ce_8: 0.001339  loss_mask_8: 0.1683  loss_dice_8: 0.1908  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:31:42] d2.utils.events INFO:  eta: 0:20:35  iter: 40159  total_loss: 4.45  loss_ce: 0.002565  loss_mask: 0.1408  loss_dice: 0.233  loss_ce_0: 0.06108  loss_mask_0: 0.1367  loss_dice_0: 0.2153  loss_ce_1: 0.003321  loss_mask_1: 0.1276  loss_dice_1: 0.2344  loss_ce_2: 0.002319  loss_mask_2: 0.1345  loss_dice_2: 0.2219  loss_ce_3: 0.002447  loss_mask_3: 0.1372  loss_dice_3: 0.2337  loss_ce_4: 0.003655  loss_mask_4: 0.1353  loss_dice_4: 0.2253  loss_ce_5: 0.002313  loss_mask_5: 0.1317  loss_dice_5: 0.2322  loss_ce_6: 0.0025  loss_mask_6: 0.144  loss_dice_6: 0.2294  loss_ce_7: 0.00378  loss_mask_7: 0.1324  loss_dice_7: 0.2226  loss_ce_8: 0.002896  loss_mask_8: 0.1352  loss_dice_8: 0.2309  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:31:44] d2.utils.events INFO:  eta: 0:20:33  iter: 40179  total_loss: 4.462  loss_ce: 0.001007  loss_mask: 0.2149  loss_dice: 0.1869  loss_ce_0: 0.06311  loss_mask_0: 0.2173  loss_dice_0: 0.1842  loss_ce_1: 0.001219  loss_mask_1: 0.2097  loss_dice_1: 0.1905  loss_ce_2: 0.001249  loss_mask_2: 0.2054  loss_dice_2: 0.1862  loss_ce_3: 0.001048  loss_mask_3: 0.2132  loss_dice_3: 0.1878  loss_ce_4: 0.001195  loss_mask_4: 0.2052  loss_dice_4: 0.18  loss_ce_5: 0.001077  loss_mask_5: 0.2132  loss_dice_5: 0.188  loss_ce_6: 0.001239  loss_mask_6: 0.2042  loss_dice_6: 0.1871  loss_ce_7: 0.0009949  loss_mask_7: 0.2123  loss_dice_7: 0.1867  loss_ce_8: 0.001297  loss_mask_8: 0.2092  loss_dice_8: 0.1901  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:31:47] d2.utils.events INFO:  eta: 0:20:30  iter: 40199  total_loss: 4.904  loss_ce: 0.0008145  loss_mask: 0.2248  loss_dice: 0.2107  loss_ce_0: 0.0628  loss_mask_0: 0.2174  loss_dice_0: 0.209  loss_ce_1: 0.001  loss_mask_1: 0.2159  loss_dice_1: 0.2129  loss_ce_2: 0.0008248  loss_mask_2: 0.2288  loss_dice_2: 0.2071  loss_ce_3: 0.000939  loss_mask_3: 0.2346  loss_dice_3: 0.2125  loss_ce_4: 0.0009539  loss_mask_4: 0.2204  loss_dice_4: 0.205  loss_ce_5: 0.0009178  loss_mask_5: 0.2177  loss_dice_5: 0.21  loss_ce_6: 0.001191  loss_mask_6: 0.2252  loss_dice_6: 0.2092  loss_ce_7: 0.001138  loss_mask_7: 0.221  loss_dice_7: 0.2034  loss_ce_8: 0.001416  loss_mask_8: 0.2299  loss_dice_8: 0.2082  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:31:49] d2.utils.events INFO:  eta: 0:20:29  iter: 40219  total_loss: 3.89  loss_ce: 0.001011  loss_mask: 0.2082  loss_dice: 0.1488  loss_ce_0: 0.06245  loss_mask_0: 0.1989  loss_dice_0: 0.1519  loss_ce_1: 0.001109  loss_mask_1: 0.1871  loss_dice_1: 0.1408  loss_ce_2: 0.0009395  loss_mask_2: 0.2005  loss_dice_2: 0.1487  loss_ce_3: 0.0009438  loss_mask_3: 0.1993  loss_dice_3: 0.1469  loss_ce_4: 0.001032  loss_mask_4: 0.1998  loss_dice_4: 0.1509  loss_ce_5: 0.00102  loss_mask_5: 0.2041  loss_dice_5: 0.1496  loss_ce_6: 0.001166  loss_mask_6: 0.1981  loss_dice_6: 0.1446  loss_ce_7: 0.001312  loss_mask_7: 0.1993  loss_dice_7: 0.1524  loss_ce_8: 0.001396  loss_mask_8: 0.1943  loss_dice_8: 0.1422  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:31:52] d2.utils.events INFO:  eta: 0:20:27  iter: 40239  total_loss: 4.104  loss_ce: 0.001002  loss_mask: 0.2071  loss_dice: 0.1591  loss_ce_0: 0.06139  loss_mask_0: 0.2199  loss_dice_0: 0.1658  loss_ce_1: 0.00105  loss_mask_1: 0.2089  loss_dice_1: 0.1629  loss_ce_2: 0.0009248  loss_mask_2: 0.2253  loss_dice_2: 0.1604  loss_ce_3: 0.0009686  loss_mask_3: 0.2135  loss_dice_3: 0.1615  loss_ce_4: 0.00108  loss_mask_4: 0.2152  loss_dice_4: 0.1709  loss_ce_5: 0.0009241  loss_mask_5: 0.2128  loss_dice_5: 0.1616  loss_ce_6: 0.001248  loss_mask_6: 0.2167  loss_dice_6: 0.1664  loss_ce_7: 0.001406  loss_mask_7: 0.2141  loss_dice_7: 0.164  loss_ce_8: 0.001406  loss_mask_8: 0.2109  loss_dice_8: 0.1639  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:31:55] d2.utils.events INFO:  eta: 0:20:25  iter: 40259  total_loss: 3.815  loss_ce: 0.001006  loss_mask: 0.2137  loss_dice: 0.1379  loss_ce_0: 0.06181  loss_mask_0: 0.2179  loss_dice_0: 0.1346  loss_ce_1: 0.000915  loss_mask_1: 0.2206  loss_dice_1: 0.1372  loss_ce_2: 0.0007597  loss_mask_2: 0.2189  loss_dice_2: 0.1386  loss_ce_3: 0.001009  loss_mask_3: 0.2216  loss_dice_3: 0.1424  loss_ce_4: 0.0009699  loss_mask_4: 0.2207  loss_dice_4: 0.1392  loss_ce_5: 0.0008648  loss_mask_5: 0.2141  loss_dice_5: 0.1345  loss_ce_6: 0.001204  loss_mask_6: 0.2114  loss_dice_6: 0.1352  loss_ce_7: 0.001352  loss_mask_7: 0.22  loss_dice_7: 0.1405  loss_ce_8: 0.001414  loss_mask_8: 0.2106  loss_dice_8: 0.1412  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:31:57] d2.utils.events INFO:  eta: 0:20:22  iter: 40279  total_loss: 4.159  loss_ce: 0.0008078  loss_mask: 0.1786  loss_dice: 0.1762  loss_ce_0: 0.06183  loss_mask_0: 0.1798  loss_dice_0: 0.1755  loss_ce_1: 0.0008005  loss_mask_1: 0.1876  loss_dice_1: 0.178  loss_ce_2: 0.0006671  loss_mask_2: 0.1905  loss_dice_2: 0.1747  loss_ce_3: 0.0008106  loss_mask_3: 0.1858  loss_dice_3: 0.1743  loss_ce_4: 0.0007872  loss_mask_4: 0.1861  loss_dice_4: 0.1758  loss_ce_5: 0.0008183  loss_mask_5: 0.184  loss_dice_5: 0.1721  loss_ce_6: 0.001039  loss_mask_6: 0.1785  loss_dice_6: 0.1766  loss_ce_7: 0.00106  loss_mask_7: 0.1853  loss_dice_7: 0.1774  loss_ce_8: 0.001332  loss_mask_8: 0.1818  loss_dice_8: 0.1731  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:32:00] d2.utils.events INFO:  eta: 0:20:20  iter: 40299  total_loss: 4.312  loss_ce: 0.001233  loss_mask: 0.1791  loss_dice: 0.1841  loss_ce_0: 0.06241  loss_mask_0: 0.1882  loss_dice_0: 0.1825  loss_ce_1: 0.001082  loss_mask_1: 0.1809  loss_dice_1: 0.1866  loss_ce_2: 0.0009797  loss_mask_2: 0.1881  loss_dice_2: 0.186  loss_ce_3: 0.001191  loss_mask_3: 0.1879  loss_dice_3: 0.1853  loss_ce_4: 0.001043  loss_mask_4: 0.1793  loss_dice_4: 0.1827  loss_ce_5: 0.001034  loss_mask_5: 0.18  loss_dice_5: 0.1876  loss_ce_6: 0.001291  loss_mask_6: 0.1783  loss_dice_6: 0.1833  loss_ce_7: 0.001291  loss_mask_7: 0.1785  loss_dice_7: 0.1779  loss_ce_8: 0.001405  loss_mask_8: 0.1842  loss_dice_8: 0.1827  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:02] d2.utils.events INFO:  eta: 0:20:18  iter: 40319  total_loss: 4.689  loss_ce: 0.002574  loss_mask: 0.2066  loss_dice: 0.1441  loss_ce_0: 0.06193  loss_mask_0: 0.1923  loss_dice_0: 0.147  loss_ce_1: 0.002504  loss_mask_1: 0.1922  loss_dice_1: 0.1458  loss_ce_2: 0.001456  loss_mask_2: 0.2016  loss_dice_2: 0.1394  loss_ce_3: 0.002197  loss_mask_3: 0.2023  loss_dice_3: 0.1437  loss_ce_4: 0.002464  loss_mask_4: 0.1999  loss_dice_4: 0.1507  loss_ce_5: 0.00158  loss_mask_5: 0.2021  loss_dice_5: 0.1409  loss_ce_6: 0.00191  loss_mask_6: 0.2027  loss_dice_6: 0.1416  loss_ce_7: 0.002376  loss_mask_7: 0.204  loss_dice_7: 0.1459  loss_ce_8: 0.001883  loss_mask_8: 0.2067  loss_dice_8: 0.1441  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:32:05] d2.utils.events INFO:  eta: 0:20:15  iter: 40339  total_loss: 4.652  loss_ce: 0.00177  loss_mask: 0.2128  loss_dice: 0.1736  loss_ce_0: 0.06148  loss_mask_0: 0.2018  loss_dice_0: 0.1757  loss_ce_1: 0.00138  loss_mask_1: 0.2138  loss_dice_1: 0.1773  loss_ce_2: 0.001133  loss_mask_2: 0.2144  loss_dice_2: 0.1761  loss_ce_3: 0.001387  loss_mask_3: 0.2064  loss_dice_3: 0.1799  loss_ce_4: 0.001463  loss_mask_4: 0.2127  loss_dice_4: 0.1791  loss_ce_5: 0.001181  loss_mask_5: 0.2093  loss_dice_5: 0.1733  loss_ce_6: 0.001442  loss_mask_6: 0.2019  loss_dice_6: 0.1799  loss_ce_7: 0.001395  loss_mask_7: 0.2038  loss_dice_7: 0.1774  loss_ce_8: 0.001444  loss_mask_8: 0.2115  loss_dice_8: 0.1865  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:07] d2.utils.events INFO:  eta: 0:20:12  iter: 40359  total_loss: 4.533  loss_ce: 0.01529  loss_mask: 0.171  loss_dice: 0.2008  loss_ce_0: 0.06246  loss_mask_0: 0.1677  loss_dice_0: 0.193  loss_ce_1: 0.009432  loss_mask_1: 0.1692  loss_dice_1: 0.1987  loss_ce_2: 0.01222  loss_mask_2: 0.1576  loss_dice_2: 0.1944  loss_ce_3: 0.01084  loss_mask_3: 0.1662  loss_dice_3: 0.2014  loss_ce_4: 0.01033  loss_mask_4: 0.1669  loss_dice_4: 0.191  loss_ce_5: 0.01605  loss_mask_5: 0.1657  loss_dice_5: 0.1957  loss_ce_6: 0.012  loss_mask_6: 0.1614  loss_dice_6: 0.1916  loss_ce_7: 0.006641  loss_mask_7: 0.1699  loss_dice_7: 0.1826  loss_ce_8: 0.01617  loss_mask_8: 0.1689  loss_dice_8: 0.1963  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:10] d2.utils.events INFO:  eta: 0:20:10  iter: 40379  total_loss: 4.733  loss_ce: 0.001304  loss_mask: 0.1888  loss_dice: 0.1831  loss_ce_0: 0.0614  loss_mask_0: 0.185  loss_dice_0: 0.1769  loss_ce_1: 0.001682  loss_mask_1: 0.1785  loss_dice_1: 0.1796  loss_ce_2: 0.001124  loss_mask_2: 0.1785  loss_dice_2: 0.1739  loss_ce_3: 0.001235  loss_mask_3: 0.1755  loss_dice_3: 0.1752  loss_ce_4: 0.001624  loss_mask_4: 0.1749  loss_dice_4: 0.189  loss_ce_5: 0.001083  loss_mask_5: 0.1742  loss_dice_5: 0.1857  loss_ce_6: 0.001397  loss_mask_6: 0.1804  loss_dice_6: 0.1794  loss_ce_7: 0.001845  loss_mask_7: 0.1715  loss_dice_7: 0.1755  loss_ce_8: 0.001409  loss_mask_8: 0.1818  loss_dice_8: 0.1814  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:32:12] d2.utils.events INFO:  eta: 0:20:08  iter: 40399  total_loss: 3.912  loss_ce: 0.0009568  loss_mask: 0.2104  loss_dice: 0.1583  loss_ce_0: 0.06245  loss_mask_0: 0.2168  loss_dice_0: 0.1519  loss_ce_1: 0.001112  loss_mask_1: 0.2078  loss_dice_1: 0.1566  loss_ce_2: 0.0009388  loss_mask_2: 0.2152  loss_dice_2: 0.1533  loss_ce_3: 0.001054  loss_mask_3: 0.2088  loss_dice_3: 0.1589  loss_ce_4: 0.001021  loss_mask_4: 0.2106  loss_dice_4: 0.1526  loss_ce_5: 0.00104  loss_mask_5: 0.2097  loss_dice_5: 0.156  loss_ce_6: 0.001357  loss_mask_6: 0.2101  loss_dice_6: 0.1576  loss_ce_7: 0.001253  loss_mask_7: 0.2125  loss_dice_7: 0.144  loss_ce_8: 0.001322  loss_mask_8: 0.2061  loss_dice_8: 0.1539  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:32:15] d2.utils.events INFO:  eta: 0:20:06  iter: 40419  total_loss: 4.08  loss_ce: 0.001471  loss_mask: 0.1965  loss_dice: 0.1367  loss_ce_0: 0.06133  loss_mask_0: 0.1916  loss_dice_0: 0.1445  loss_ce_1: 0.001782  loss_mask_1: 0.1932  loss_dice_1: 0.1383  loss_ce_2: 0.001833  loss_mask_2: 0.1907  loss_dice_2: 0.1382  loss_ce_3: 0.001824  loss_mask_3: 0.1798  loss_dice_3: 0.1363  loss_ce_4: 0.001774  loss_mask_4: 0.1913  loss_dice_4: 0.1417  loss_ce_5: 0.001561  loss_mask_5: 0.1872  loss_dice_5: 0.1406  loss_ce_6: 0.001881  loss_mask_6: 0.1981  loss_dice_6: 0.1409  loss_ce_7: 0.001901  loss_mask_7: 0.1929  loss_dice_7: 0.1374  loss_ce_8: 0.002203  loss_mask_8: 0.1827  loss_dice_8: 0.1445  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:17] d2.utils.events INFO:  eta: 0:20:03  iter: 40439  total_loss: 4.37  loss_ce: 0.001147  loss_mask: 0.1962  loss_dice: 0.1547  loss_ce_0: 0.06122  loss_mask_0: 0.1789  loss_dice_0: 0.1537  loss_ce_1: 0.00113  loss_mask_1: 0.1825  loss_dice_1: 0.155  loss_ce_2: 0.001054  loss_mask_2: 0.1911  loss_dice_2: 0.1675  loss_ce_3: 0.001233  loss_mask_3: 0.1874  loss_dice_3: 0.1556  loss_ce_4: 0.001114  loss_mask_4: 0.1863  loss_dice_4: 0.1543  loss_ce_5: 0.001057  loss_mask_5: 0.1929  loss_dice_5: 0.1587  loss_ce_6: 0.001409  loss_mask_6: 0.1896  loss_dice_6: 0.1604  loss_ce_7: 0.001237  loss_mask_7: 0.1941  loss_dice_7: 0.1495  loss_ce_8: 0.00125  loss_mask_8: 0.1945  loss_dice_8: 0.1599  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:20] d2.utils.events INFO:  eta: 0:20:00  iter: 40459  total_loss: 3.928  loss_ce: 0.0008686  loss_mask: 0.2007  loss_dice: 0.1432  loss_ce_0: 0.06184  loss_mask_0: 0.1975  loss_dice_0: 0.1442  loss_ce_1: 0.0009772  loss_mask_1: 0.2056  loss_dice_1: 0.1438  loss_ce_2: 0.0008334  loss_mask_2: 0.2053  loss_dice_2: 0.1465  loss_ce_3: 0.001051  loss_mask_3: 0.1993  loss_dice_3: 0.147  loss_ce_4: 0.0009133  loss_mask_4: 0.1973  loss_dice_4: 0.1472  loss_ce_5: 0.0008693  loss_mask_5: 0.205  loss_dice_5: 0.1499  loss_ce_6: 0.001231  loss_mask_6: 0.2017  loss_dice_6: 0.1424  loss_ce_7: 0.001095  loss_mask_7: 0.1987  loss_dice_7: 0.1458  loss_ce_8: 0.001363  loss_mask_8: 0.1994  loss_dice_8: 0.1537  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:32:22] d2.utils.events INFO:  eta: 0:19:57  iter: 40479  total_loss: 4.536  loss_ce: 0.0008432  loss_mask: 0.1973  loss_dice: 0.2041  loss_ce_0: 0.06259  loss_mask_0: 0.2051  loss_dice_0: 0.1974  loss_ce_1: 0.0009287  loss_mask_1: 0.2004  loss_dice_1: 0.1991  loss_ce_2: 0.0008202  loss_mask_2: 0.1953  loss_dice_2: 0.2056  loss_ce_3: 0.000982  loss_mask_3: 0.2024  loss_dice_3: 0.1945  loss_ce_4: 0.0009087  loss_mask_4: 0.1948  loss_dice_4: 0.194  loss_ce_5: 0.0008594  loss_mask_5: 0.1993  loss_dice_5: 0.1981  loss_ce_6: 0.001188  loss_mask_6: 0.1958  loss_dice_6: 0.2033  loss_ce_7: 0.0009649  loss_mask_7: 0.1958  loss_dice_7: 0.1922  loss_ce_8: 0.001167  loss_mask_8: 0.2006  loss_dice_8: 0.202  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:25] d2.utils.events INFO:  eta: 0:19:55  iter: 40499  total_loss: 4.179  loss_ce: 0.001348  loss_mask: 0.1589  loss_dice: 0.2303  loss_ce_0: 0.06098  loss_mask_0: 0.1702  loss_dice_0: 0.2311  loss_ce_1: 0.001215  loss_mask_1: 0.1542  loss_dice_1: 0.2205  loss_ce_2: 0.001095  loss_mask_2: 0.1602  loss_dice_2: 0.2251  loss_ce_3: 0.001298  loss_mask_3: 0.1555  loss_dice_3: 0.2286  loss_ce_4: 0.001267  loss_mask_4: 0.1593  loss_dice_4: 0.2283  loss_ce_5: 0.00102  loss_mask_5: 0.1631  loss_dice_5: 0.2318  loss_ce_6: 0.001286  loss_mask_6: 0.152  loss_dice_6: 0.2248  loss_ce_7: 0.001168  loss_mask_7: 0.1694  loss_dice_7: 0.2303  loss_ce_8: 0.001169  loss_mask_8: 0.145  loss_dice_8: 0.2267  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:32:27] d2.utils.events INFO:  eta: 0:19:52  iter: 40519  total_loss: 4.08  loss_ce: 0.0008907  loss_mask: 0.2179  loss_dice: 0.146  loss_ce_0: 0.06182  loss_mask_0: 0.217  loss_dice_0: 0.1496  loss_ce_1: 0.0009949  loss_mask_1: 0.2162  loss_dice_1: 0.1457  loss_ce_2: 0.0009266  loss_mask_2: 0.2141  loss_dice_2: 0.143  loss_ce_3: 0.0009613  loss_mask_3: 0.2141  loss_dice_3: 0.1395  loss_ce_4: 0.0008599  loss_mask_4: 0.2139  loss_dice_4: 0.1399  loss_ce_5: 0.0008659  loss_mask_5: 0.2161  loss_dice_5: 0.1474  loss_ce_6: 0.001156  loss_mask_6: 0.2157  loss_dice_6: 0.1437  loss_ce_7: 0.0009596  loss_mask_7: 0.2218  loss_dice_7: 0.1442  loss_ce_8: 0.001283  loss_mask_8: 0.2205  loss_dice_8: 0.1457  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:30] d2.utils.events INFO:  eta: 0:19:50  iter: 40539  total_loss: 3.921  loss_ce: 0.001074  loss_mask: 0.186  loss_dice: 0.1342  loss_ce_0: 0.06072  loss_mask_0: 0.1891  loss_dice_0: 0.1353  loss_ce_1: 0.0009628  loss_mask_1: 0.1903  loss_dice_1: 0.1309  loss_ce_2: 0.0008165  loss_mask_2: 0.1882  loss_dice_2: 0.1333  loss_ce_3: 0.001158  loss_mask_3: 0.1891  loss_dice_3: 0.1317  loss_ce_4: 0.001042  loss_mask_4: 0.1904  loss_dice_4: 0.1289  loss_ce_5: 0.000861  loss_mask_5: 0.188  loss_dice_5: 0.1339  loss_ce_6: 0.001298  loss_mask_6: 0.1871  loss_dice_6: 0.1304  loss_ce_7: 0.001036  loss_mask_7: 0.1955  loss_dice_7: 0.1347  loss_ce_8: 0.001182  loss_mask_8: 0.1954  loss_dice_8: 0.1349  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:33] d2.utils.events INFO:  eta: 0:19:47  iter: 40559  total_loss: 4.296  loss_ce: 0.001037  loss_mask: 0.1721  loss_dice: 0.1484  loss_ce_0: 0.06285  loss_mask_0: 0.1624  loss_dice_0: 0.1662  loss_ce_1: 0.001176  loss_mask_1: 0.1699  loss_dice_1: 0.1662  loss_ce_2: 0.001195  loss_mask_2: 0.1641  loss_dice_2: 0.1648  loss_ce_3: 0.001027  loss_mask_3: 0.1709  loss_dice_3: 0.1662  loss_ce_4: 0.001185  loss_mask_4: 0.1667  loss_dice_4: 0.1574  loss_ce_5: 0.00129  loss_mask_5: 0.1636  loss_dice_5: 0.1646  loss_ce_6: 0.001194  loss_mask_6: 0.1684  loss_dice_6: 0.1647  loss_ce_7: 0.001386  loss_mask_7: 0.1674  loss_dice_7: 0.169  loss_ce_8: 0.001656  loss_mask_8: 0.1614  loss_dice_8: 0.167  time: 0.1260  data_time: 0.0014  lr: 1e-05  max_mem: 3105M
[04/13 16:32:35] d2.utils.events INFO:  eta: 0:19:44  iter: 40579  total_loss: 4.685  loss_ce: 0.0008815  loss_mask: 0.2273  loss_dice: 0.185  loss_ce_0: 0.06101  loss_mask_0: 0.2316  loss_dice_0: 0.1965  loss_ce_1: 0.0009745  loss_mask_1: 0.2236  loss_dice_1: 0.19  loss_ce_2: 0.0009649  loss_mask_2: 0.2322  loss_dice_2: 0.1922  loss_ce_3: 0.001043  loss_mask_3: 0.2256  loss_dice_3: 0.1889  loss_ce_4: 0.0009702  loss_mask_4: 0.2329  loss_dice_4: 0.1846  loss_ce_5: 0.0008959  loss_mask_5: 0.2379  loss_dice_5: 0.1882  loss_ce_6: 0.001252  loss_mask_6: 0.2335  loss_dice_6: 0.1879  loss_ce_7: 0.0008922  loss_mask_7: 0.2361  loss_dice_7: 0.1867  loss_ce_8: 0.001115  loss_mask_8: 0.2279  loss_dice_8: 0.1918  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:38] d2.utils.events INFO:  eta: 0:19:42  iter: 40599  total_loss: 4.312  loss_ce: 0.001202  loss_mask: 0.2328  loss_dice: 0.1652  loss_ce_0: 0.06078  loss_mask_0: 0.2248  loss_dice_0: 0.1644  loss_ce_1: 0.0009949  loss_mask_1: 0.2304  loss_dice_1: 0.1689  loss_ce_2: 0.0008484  loss_mask_2: 0.2299  loss_dice_2: 0.1662  loss_ce_3: 0.001173  loss_mask_3: 0.2301  loss_dice_3: 0.1655  loss_ce_4: 0.001044  loss_mask_4: 0.2425  loss_dice_4: 0.1641  loss_ce_5: 0.0007595  loss_mask_5: 0.2285  loss_dice_5: 0.1712  loss_ce_6: 0.001162  loss_mask_6: 0.2253  loss_dice_6: 0.1638  loss_ce_7: 0.0009721  loss_mask_7: 0.2208  loss_dice_7: 0.1681  loss_ce_8: 0.00117  loss_mask_8: 0.2209  loss_dice_8: 0.1663  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:32:40] d2.utils.events INFO:  eta: 0:19:40  iter: 40619  total_loss: 3.662  loss_ce: 0.0007564  loss_mask: 0.1897  loss_dice: 0.1205  loss_ce_0: 0.06306  loss_mask_0: 0.2026  loss_dice_0: 0.1256  loss_ce_1: 0.0008754  loss_mask_1: 0.1961  loss_dice_1: 0.1206  loss_ce_2: 0.0006846  loss_mask_2: 0.1992  loss_dice_2: 0.1207  loss_ce_3: 0.0008388  loss_mask_3: 0.1987  loss_dice_3: 0.1228  loss_ce_4: 0.0007827  loss_mask_4: 0.202  loss_dice_4: 0.1218  loss_ce_5: 0.0006911  loss_mask_5: 0.1973  loss_dice_5: 0.1305  loss_ce_6: 0.001086  loss_mask_6: 0.1955  loss_dice_6: 0.1239  loss_ce_7: 0.001016  loss_mask_7: 0.1976  loss_dice_7: 0.1217  loss_ce_8: 0.001119  loss_mask_8: 0.2023  loss_dice_8: 0.1267  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:32:43] d2.utils.events INFO:  eta: 0:19:38  iter: 40639  total_loss: 4.15  loss_ce: 0.001303  loss_mask: 0.2007  loss_dice: 0.13  loss_ce_0: 0.06179  loss_mask_0: 0.2006  loss_dice_0: 0.1322  loss_ce_1: 0.001117  loss_mask_1: 0.2071  loss_dice_1: 0.1399  loss_ce_2: 0.001036  loss_mask_2: 0.2063  loss_dice_2: 0.1369  loss_ce_3: 0.001205  loss_mask_3: 0.202  loss_dice_3: 0.1347  loss_ce_4: 0.001182  loss_mask_4: 0.1978  loss_dice_4: 0.132  loss_ce_5: 0.001101  loss_mask_5: 0.2067  loss_dice_5: 0.1391  loss_ce_6: 0.001323  loss_mask_6: 0.2125  loss_dice_6: 0.1437  loss_ce_7: 0.001391  loss_mask_7: 0.2086  loss_dice_7: 0.1406  loss_ce_8: 0.001354  loss_mask_8: 0.2087  loss_dice_8: 0.1394  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:45] d2.utils.events INFO:  eta: 0:19:35  iter: 40659  total_loss: 3.907  loss_ce: 0.00102  loss_mask: 0.187  loss_dice: 0.1759  loss_ce_0: 0.06103  loss_mask_0: 0.1925  loss_dice_0: 0.1809  loss_ce_1: 0.001013  loss_mask_1: 0.1999  loss_dice_1: 0.177  loss_ce_2: 0.001004  loss_mask_2: 0.1952  loss_dice_2: 0.1725  loss_ce_3: 0.001068  loss_mask_3: 0.1935  loss_dice_3: 0.1741  loss_ce_4: 0.0009892  loss_mask_4: 0.1848  loss_dice_4: 0.1629  loss_ce_5: 0.0009746  loss_mask_5: 0.194  loss_dice_5: 0.1821  loss_ce_6: 0.00114  loss_mask_6: 0.1877  loss_dice_6: 0.1761  loss_ce_7: 0.0007673  loss_mask_7: 0.1893  loss_dice_7: 0.1741  loss_ce_8: 0.001046  loss_mask_8: 0.1947  loss_dice_8: 0.1735  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:48] d2.utils.events INFO:  eta: 0:19:33  iter: 40679  total_loss: 4.222  loss_ce: 0.000965  loss_mask: 0.2026  loss_dice: 0.2154  loss_ce_0: 0.06068  loss_mask_0: 0.1979  loss_dice_0: 0.2034  loss_ce_1: 0.001018  loss_mask_1: 0.1929  loss_dice_1: 0.2194  loss_ce_2: 0.001024  loss_mask_2: 0.1973  loss_dice_2: 0.218  loss_ce_3: 0.0009741  loss_mask_3: 0.2008  loss_dice_3: 0.2107  loss_ce_4: 0.001102  loss_mask_4: 0.2021  loss_dice_4: 0.2139  loss_ce_5: 0.0009574  loss_mask_5: 0.1943  loss_dice_5: 0.2104  loss_ce_6: 0.001054  loss_mask_6: 0.1934  loss_dice_6: 0.2149  loss_ce_7: 0.0009206  loss_mask_7: 0.1993  loss_dice_7: 0.2149  loss_ce_8: 0.001159  loss_mask_8: 0.1947  loss_dice_8: 0.211  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:50] d2.utils.events INFO:  eta: 0:19:31  iter: 40699  total_loss: 4.286  loss_ce: 0.0006311  loss_mask: 0.2167  loss_dice: 0.1542  loss_ce_0: 0.06016  loss_mask_0: 0.2189  loss_dice_0: 0.155  loss_ce_1: 0.0007859  loss_mask_1: 0.2289  loss_dice_1: 0.1538  loss_ce_2: 0.0005598  loss_mask_2: 0.2228  loss_dice_2: 0.1547  loss_ce_3: 0.0007207  loss_mask_3: 0.2117  loss_dice_3: 0.1516  loss_ce_4: 0.0006966  loss_mask_4: 0.2169  loss_dice_4: 0.1563  loss_ce_5: 0.000583  loss_mask_5: 0.2259  loss_dice_5: 0.1513  loss_ce_6: 0.0009405  loss_mask_6: 0.2216  loss_dice_6: 0.1513  loss_ce_7: 0.0005323  loss_mask_7: 0.2249  loss_dice_7: 0.1533  loss_ce_8: 0.0008175  loss_mask_8: 0.2236  loss_dice_8: 0.1519  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:53] d2.utils.events INFO:  eta: 0:19:29  iter: 40719  total_loss: 4.333  loss_ce: 0.0009016  loss_mask: 0.148  loss_dice: 0.1971  loss_ce_0: 0.0618  loss_mask_0: 0.1467  loss_dice_0: 0.1997  loss_ce_1: 0.002365  loss_mask_1: 0.1504  loss_dice_1: 0.2041  loss_ce_2: 0.001255  loss_mask_2: 0.1558  loss_dice_2: 0.2144  loss_ce_3: 0.0008578  loss_mask_3: 0.1494  loss_dice_3: 0.2006  loss_ce_4: 0.001123  loss_mask_4: 0.1493  loss_dice_4: 0.1951  loss_ce_5: 0.0008124  loss_mask_5: 0.1514  loss_dice_5: 0.2108  loss_ce_6: 0.001034  loss_mask_6: 0.1525  loss_dice_6: 0.2037  loss_ce_7: 0.001243  loss_mask_7: 0.1472  loss_dice_7: 0.1942  loss_ce_8: 0.001354  loss_mask_8: 0.1553  loss_dice_8: 0.2099  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:32:55] d2.utils.events INFO:  eta: 0:19:26  iter: 40739  total_loss: 3.916  loss_ce: 0.0007389  loss_mask: 0.2154  loss_dice: 0.1392  loss_ce_0: 0.06052  loss_mask_0: 0.2074  loss_dice_0: 0.1464  loss_ce_1: 0.0006949  loss_mask_1: 0.2167  loss_dice_1: 0.139  loss_ce_2: 0.0004858  loss_mask_2: 0.2136  loss_dice_2: 0.1396  loss_ce_3: 0.0008376  loss_mask_3: 0.2133  loss_dice_3: 0.1434  loss_ce_4: 0.0006725  loss_mask_4: 0.2171  loss_dice_4: 0.1417  loss_ce_5: 0.0005732  loss_mask_5: 0.2156  loss_dice_5: 0.1453  loss_ce_6: 0.0009558  loss_mask_6: 0.2182  loss_dice_6: 0.1418  loss_ce_7: 0.0007729  loss_mask_7: 0.1988  loss_dice_7: 0.1399  loss_ce_8: 0.000972  loss_mask_8: 0.2125  loss_dice_8: 0.1409  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:32:58] d2.utils.events INFO:  eta: 0:19:24  iter: 40759  total_loss: 5.176  loss_ce: 0.003199  loss_mask: 0.1823  loss_dice: 0.1864  loss_ce_0: 0.0605  loss_mask_0: 0.1927  loss_dice_0: 0.2037  loss_ce_1: 0.00503  loss_mask_1: 0.1846  loss_dice_1: 0.1898  loss_ce_2: 0.0166  loss_mask_2: 0.1825  loss_dice_2: 0.1895  loss_ce_3: 0.003015  loss_mask_3: 0.1751  loss_dice_3: 0.1959  loss_ce_4: 0.004217  loss_mask_4: 0.1856  loss_dice_4: 0.1931  loss_ce_5: 0.02017  loss_mask_5: 0.1857  loss_dice_5: 0.196  loss_ce_6: 0.002939  loss_mask_6: 0.1822  loss_dice_6: 0.195  loss_ce_7: 0.005438  loss_mask_7: 0.1932  loss_dice_7: 0.2003  loss_ce_8: 0.0112  loss_mask_8: 0.1826  loss_dice_8: 0.1961  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:00] d2.utils.events INFO:  eta: 0:19:21  iter: 40779  total_loss: 4.426  loss_ce: 0.001287  loss_mask: 0.1789  loss_dice: 0.2118  loss_ce_0: 0.06033  loss_mask_0: 0.1859  loss_dice_0: 0.22  loss_ce_1: 0.002187  loss_mask_1: 0.171  loss_dice_1: 0.2205  loss_ce_2: 0.001334  loss_mask_2: 0.1837  loss_dice_2: 0.2161  loss_ce_3: 0.001212  loss_mask_3: 0.1815  loss_dice_3: 0.2028  loss_ce_4: 0.00142  loss_mask_4: 0.1833  loss_dice_4: 0.2106  loss_ce_5: 0.001265  loss_mask_5: 0.1841  loss_dice_5: 0.2068  loss_ce_6: 0.00122  loss_mask_6: 0.1743  loss_dice_6: 0.2073  loss_ce_7: 0.001331  loss_mask_7: 0.1896  loss_dice_7: 0.2108  loss_ce_8: 0.00151  loss_mask_8: 0.1729  loss_dice_8: 0.209  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:03] d2.utils.events INFO:  eta: 0:19:19  iter: 40799  total_loss: 4.869  loss_ce: 0.0007292  loss_mask: 0.2041  loss_dice: 0.2129  loss_ce_0: 0.06013  loss_mask_0: 0.2073  loss_dice_0: 0.2069  loss_ce_1: 0.0006812  loss_mask_1: 0.2115  loss_dice_1: 0.2044  loss_ce_2: 0.000547  loss_mask_2: 0.2067  loss_dice_2: 0.2078  loss_ce_3: 0.000758  loss_mask_3: 0.2135  loss_dice_3: 0.2104  loss_ce_4: 0.0006671  loss_mask_4: 0.2071  loss_dice_4: 0.2209  loss_ce_5: 0.0006054  loss_mask_5: 0.2146  loss_dice_5: 0.2211  loss_ce_6: 0.000927  loss_mask_6: 0.2095  loss_dice_6: 0.2112  loss_ce_7: 0.0007183  loss_mask_7: 0.206  loss_dice_7: 0.2066  loss_ce_8: 0.0009606  loss_mask_8: 0.2016  loss_dice_8: 0.2156  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:05] d2.utils.events INFO:  eta: 0:19:16  iter: 40819  total_loss: 3.905  loss_ce: 0.001114  loss_mask: 0.1701  loss_dice: 0.1461  loss_ce_0: 0.05964  loss_mask_0: 0.1814  loss_dice_0: 0.1436  loss_ce_1: 0.0009697  loss_mask_1: 0.1844  loss_dice_1: 0.1487  loss_ce_2: 0.000769  loss_mask_2: 0.1766  loss_dice_2: 0.1456  loss_ce_3: 0.0009665  loss_mask_3: 0.1779  loss_dice_3: 0.1532  loss_ce_4: 0.001057  loss_mask_4: 0.1768  loss_dice_4: 0.1396  loss_ce_5: 0.0007262  loss_mask_5: 0.1702  loss_dice_5: 0.1435  loss_ce_6: 0.0009933  loss_mask_6: 0.1843  loss_dice_6: 0.1425  loss_ce_7: 0.0009674  loss_mask_7: 0.1745  loss_dice_7: 0.147  loss_ce_8: 0.001057  loss_mask_8: 0.1817  loss_dice_8: 0.1503  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:08] d2.utils.events INFO:  eta: 0:19:14  iter: 40839  total_loss: 4.083  loss_ce: 0.0008709  loss_mask: 0.1865  loss_dice: 0.1428  loss_ce_0: 0.05968  loss_mask_0: 0.2002  loss_dice_0: 0.1441  loss_ce_1: 0.0007436  loss_mask_1: 0.1984  loss_dice_1: 0.1366  loss_ce_2: 0.0006895  loss_mask_2: 0.1918  loss_dice_2: 0.1416  loss_ce_3: 0.0008488  loss_mask_3: 0.1873  loss_dice_3: 0.1409  loss_ce_4: 0.000782  loss_mask_4: 0.2023  loss_dice_4: 0.1397  loss_ce_5: 0.0006538  loss_mask_5: 0.1927  loss_dice_5: 0.1377  loss_ce_6: 0.0009309  loss_mask_6: 0.199  loss_dice_6: 0.1458  loss_ce_7: 0.000779  loss_mask_7: 0.1951  loss_dice_7: 0.1377  loss_ce_8: 0.000904  loss_mask_8: 0.2057  loss_dice_8: 0.1486  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:10] d2.utils.events INFO:  eta: 0:19:11  iter: 40859  total_loss: 4.363  loss_ce: 0.0007646  loss_mask: 0.2286  loss_dice: 0.1704  loss_ce_0: 0.05941  loss_mask_0: 0.2276  loss_dice_0: 0.162  loss_ce_1: 0.0007281  loss_mask_1: 0.231  loss_dice_1: 0.1657  loss_ce_2: 0.0006055  loss_mask_2: 0.2315  loss_dice_2: 0.1669  loss_ce_3: 0.0008229  loss_mask_3: 0.2357  loss_dice_3: 0.1709  loss_ce_4: 0.000793  loss_mask_4: 0.2215  loss_dice_4: 0.1746  loss_ce_5: 0.0006609  loss_mask_5: 0.2299  loss_dice_5: 0.1638  loss_ce_6: 0.0009908  loss_mask_6: 0.2352  loss_dice_6: 0.1661  loss_ce_7: 0.001051  loss_mask_7: 0.2288  loss_dice_7: 0.1582  loss_ce_8: 0.0009232  loss_mask_8: 0.233  loss_dice_8: 0.157  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:13] d2.utils.events INFO:  eta: 0:19:10  iter: 40879  total_loss: 4.343  loss_ce: 0.001195  loss_mask: 0.1891  loss_dice: 0.1559  loss_ce_0: 0.05905  loss_mask_0: 0.1961  loss_dice_0: 0.1589  loss_ce_1: 0.001162  loss_mask_1: 0.1989  loss_dice_1: 0.1587  loss_ce_2: 0.0007831  loss_mask_2: 0.199  loss_dice_2: 0.1628  loss_ce_3: 0.001038  loss_mask_3: 0.1943  loss_dice_3: 0.1611  loss_ce_4: 0.001294  loss_mask_4: 0.1959  loss_dice_4: 0.1606  loss_ce_5: 0.0008182  loss_mask_5: 0.2043  loss_dice_5: 0.1631  loss_ce_6: 0.00114  loss_mask_6: 0.1952  loss_dice_6: 0.1598  loss_ce_7: 0.0009695  loss_mask_7: 0.1969  loss_dice_7: 0.1548  loss_ce_8: 0.001053  loss_mask_8: 0.1919  loss_dice_8: 0.1601  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:16] d2.utils.events INFO:  eta: 0:19:08  iter: 40899  total_loss: 4.316  loss_ce: 0.001905  loss_mask: 0.1486  loss_dice: 0.1931  loss_ce_0: 0.0585  loss_mask_0: 0.1448  loss_dice_0: 0.1869  loss_ce_1: 0.00565  loss_mask_1: 0.1537  loss_dice_1: 0.1889  loss_ce_2: 0.001671  loss_mask_2: 0.1463  loss_dice_2: 0.1949  loss_ce_3: 0.002086  loss_mask_3: 0.1452  loss_dice_3: 0.1893  loss_ce_4: 0.002784  loss_mask_4: 0.1465  loss_dice_4: 0.1913  loss_ce_5: 0.002068  loss_mask_5: 0.1535  loss_dice_5: 0.1908  loss_ce_6: 0.001832  loss_mask_6: 0.1489  loss_dice_6: 0.1844  loss_ce_7: 0.003071  loss_mask_7: 0.1446  loss_dice_7: 0.1869  loss_ce_8: 0.002066  loss_mask_8: 0.1534  loss_dice_8: 0.1878  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:33:18] d2.utils.events INFO:  eta: 0:19:04  iter: 40919  total_loss: 4.191  loss_ce: 0.000853  loss_mask: 0.2144  loss_dice: 0.1437  loss_ce_0: 0.05825  loss_mask_0: 0.2195  loss_dice_0: 0.1469  loss_ce_1: 0.0007895  loss_mask_1: 0.2268  loss_dice_1: 0.1498  loss_ce_2: 0.0006794  loss_mask_2: 0.2219  loss_dice_2: 0.1485  loss_ce_3: 0.0007915  loss_mask_3: 0.2161  loss_dice_3: 0.1521  loss_ce_4: 0.0009123  loss_mask_4: 0.2268  loss_dice_4: 0.1482  loss_ce_5: 0.0007653  loss_mask_5: 0.2245  loss_dice_5: 0.1546  loss_ce_6: 0.001103  loss_mask_6: 0.2215  loss_dice_6: 0.1509  loss_ce_7: 0.0009341  loss_mask_7: 0.2248  loss_dice_7: 0.1474  loss_ce_8: 0.001074  loss_mask_8: 0.2176  loss_dice_8: 0.149  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:21] d2.utils.events INFO:  eta: 0:19:01  iter: 40939  total_loss: 4.11  loss_ce: 0.0008075  loss_mask: 0.2107  loss_dice: 0.1693  loss_ce_0: 0.06193  loss_mask_0: 0.1972  loss_dice_0: 0.158  loss_ce_1: 0.0009015  loss_mask_1: 0.1997  loss_dice_1: 0.1552  loss_ce_2: 0.0008002  loss_mask_2: 0.1927  loss_dice_2: 0.1573  loss_ce_3: 0.0008015  loss_mask_3: 0.2038  loss_dice_3: 0.161  loss_ce_4: 0.0008556  loss_mask_4: 0.1999  loss_dice_4: 0.1675  loss_ce_5: 0.0008688  loss_mask_5: 0.2  loss_dice_5: 0.1517  loss_ce_6: 0.0009648  loss_mask_6: 0.2005  loss_dice_6: 0.1663  loss_ce_7: 0.001035  loss_mask_7: 0.2036  loss_dice_7: 0.1618  loss_ce_8: 0.001166  loss_mask_8: 0.191  loss_dice_8: 0.1509  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:23] d2.utils.events INFO:  eta: 0:18:59  iter: 40959  total_loss: 4.335  loss_ce: 0.0008101  loss_mask: 0.2335  loss_dice: 0.1421  loss_ce_0: 0.05814  loss_mask_0: 0.2272  loss_dice_0: 0.1384  loss_ce_1: 0.0008206  loss_mask_1: 0.2208  loss_dice_1: 0.1385  loss_ce_2: 0.0006845  loss_mask_2: 0.2229  loss_dice_2: 0.1384  loss_ce_3: 0.0008182  loss_mask_3: 0.2269  loss_dice_3: 0.1395  loss_ce_4: 0.000873  loss_mask_4: 0.2331  loss_dice_4: 0.1366  loss_ce_5: 0.0007476  loss_mask_5: 0.2184  loss_dice_5: 0.1397  loss_ce_6: 0.00099  loss_mask_6: 0.225  loss_dice_6: 0.1358  loss_ce_7: 0.001008  loss_mask_7: 0.2269  loss_dice_7: 0.1381  loss_ce_8: 0.0009108  loss_mask_8: 0.2272  loss_dice_8: 0.1466  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:26] d2.utils.events INFO:  eta: 0:18:56  iter: 40979  total_loss: 4.365  loss_ce: 0.009045  loss_mask: 0.1753  loss_dice: 0.2039  loss_ce_0: 0.06589  loss_mask_0: 0.1727  loss_dice_0: 0.2092  loss_ce_1: 0.003921  loss_mask_1: 0.1683  loss_dice_1: 0.2079  loss_ce_2: 0.003934  loss_mask_2: 0.1692  loss_dice_2: 0.2095  loss_ce_3: 0.006598  loss_mask_3: 0.1678  loss_dice_3: 0.2079  loss_ce_4: 0.004423  loss_mask_4: 0.1786  loss_dice_4: 0.2051  loss_ce_5: 0.005999  loss_mask_5: 0.1728  loss_dice_5: 0.2014  loss_ce_6: 0.006155  loss_mask_6: 0.1705  loss_dice_6: 0.2031  loss_ce_7: 0.003176  loss_mask_7: 0.1742  loss_dice_7: 0.2013  loss_ce_8: 0.01124  loss_mask_8: 0.1746  loss_dice_8: 0.2153  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:28] d2.utils.events INFO:  eta: 0:18:55  iter: 40999  total_loss: 3.875  loss_ce: 0.0009873  loss_mask: 0.1807  loss_dice: 0.1424  loss_ce_0: 0.06591  loss_mask_0: 0.1991  loss_dice_0: 0.1424  loss_ce_1: 0.0007996  loss_mask_1: 0.1889  loss_dice_1: 0.1393  loss_ce_2: 0.0007308  loss_mask_2: 0.1899  loss_dice_2: 0.1377  loss_ce_3: 0.0009194  loss_mask_3: 0.1943  loss_dice_3: 0.1387  loss_ce_4: 0.0009097  loss_mask_4: 0.1871  loss_dice_4: 0.141  loss_ce_5: 0.0007423  loss_mask_5: 0.195  loss_dice_5: 0.1458  loss_ce_6: 0.001048  loss_mask_6: 0.1947  loss_dice_6: 0.1463  loss_ce_7: 0.0009799  loss_mask_7: 0.1915  loss_dice_7: 0.1433  loss_ce_8: 0.001066  loss_mask_8: 0.1924  loss_dice_8: 0.143  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:31] d2.utils.events INFO:  eta: 0:18:53  iter: 41019  total_loss: 3.964  loss_ce: 0.001063  loss_mask: 0.189  loss_dice: 0.1463  loss_ce_0: 0.06557  loss_mask_0: 0.1971  loss_dice_0: 0.1495  loss_ce_1: 0.001074  loss_mask_1: 0.1889  loss_dice_1: 0.1411  loss_ce_2: 0.001138  loss_mask_2: 0.1959  loss_dice_2: 0.1511  loss_ce_3: 0.0009114  loss_mask_3: 0.199  loss_dice_3: 0.1437  loss_ce_4: 0.001157  loss_mask_4: 0.1968  loss_dice_4: 0.1488  loss_ce_5: 0.001277  loss_mask_5: 0.2  loss_dice_5: 0.1453  loss_ce_6: 0.001174  loss_mask_6: 0.1962  loss_dice_6: 0.1438  loss_ce_7: 0.001253  loss_mask_7: 0.1975  loss_dice_7: 0.1448  loss_ce_8: 0.001438  loss_mask_8: 0.1937  loss_dice_8: 0.152  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:33] d2.utils.events INFO:  eta: 0:18:51  iter: 41039  total_loss: 4.095  loss_ce: 0.001392  loss_mask: 0.1691  loss_dice: 0.189  loss_ce_0: 0.06188  loss_mask_0: 0.1739  loss_dice_0: 0.2066  loss_ce_1: 0.0012  loss_mask_1: 0.1781  loss_dice_1: 0.2113  loss_ce_2: 0.001018  loss_mask_2: 0.1719  loss_dice_2: 0.2012  loss_ce_3: 0.001188  loss_mask_3: 0.1697  loss_dice_3: 0.2027  loss_ce_4: 0.001355  loss_mask_4: 0.1699  loss_dice_4: 0.1956  loss_ce_5: 0.0009304  loss_mask_5: 0.1716  loss_dice_5: 0.1911  loss_ce_6: 0.001143  loss_mask_6: 0.1718  loss_dice_6: 0.2019  loss_ce_7: 0.001109  loss_mask_7: 0.1659  loss_dice_7: 0.1939  loss_ce_8: 0.00124  loss_mask_8: 0.1753  loss_dice_8: 0.2016  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:36] d2.utils.events INFO:  eta: 0:18:48  iter: 41059  total_loss: 3.836  loss_ce: 0.0007751  loss_mask: 0.2083  loss_dice: 0.1415  loss_ce_0: 0.06551  loss_mask_0: 0.2012  loss_dice_0: 0.1404  loss_ce_1: 0.0007336  loss_mask_1: 0.1998  loss_dice_1: 0.1406  loss_ce_2: 0.000769  loss_mask_2: 0.2133  loss_dice_2: 0.1372  loss_ce_3: 0.0007971  loss_mask_3: 0.2143  loss_dice_3: 0.143  loss_ce_4: 0.0008677  loss_mask_4: 0.2119  loss_dice_4: 0.1374  loss_ce_5: 0.000827  loss_mask_5: 0.2091  loss_dice_5: 0.1379  loss_ce_6: 0.001001  loss_mask_6: 0.2066  loss_dice_6: 0.1406  loss_ce_7: 0.001058  loss_mask_7: 0.2138  loss_dice_7: 0.1456  loss_ce_8: 0.001095  loss_mask_8: 0.2134  loss_dice_8: 0.1352  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:38] d2.utils.events INFO:  eta: 0:18:45  iter: 41079  total_loss: 3.841  loss_ce: 0.001002  loss_mask: 0.1992  loss_dice: 0.1459  loss_ce_0: 0.05902  loss_mask_0: 0.2098  loss_dice_0: 0.1478  loss_ce_1: 0.0008157  loss_mask_1: 0.2028  loss_dice_1: 0.1439  loss_ce_2: 0.0006996  loss_mask_2: 0.2086  loss_dice_2: 0.1475  loss_ce_3: 0.0009427  loss_mask_3: 0.2112  loss_dice_3: 0.1454  loss_ce_4: 0.000772  loss_mask_4: 0.2044  loss_dice_4: 0.1439  loss_ce_5: 0.0007301  loss_mask_5: 0.2124  loss_dice_5: 0.1518  loss_ce_6: 0.001054  loss_mask_6: 0.21  loss_dice_6: 0.1436  loss_ce_7: 0.0009413  loss_mask_7: 0.2116  loss_dice_7: 0.1435  loss_ce_8: 0.001014  loss_mask_8: 0.1994  loss_dice_8: 0.1423  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:41] d2.utils.events INFO:  eta: 0:18:42  iter: 41099  total_loss: 3.905  loss_ce: 0.0008991  loss_mask: 0.198  loss_dice: 0.1516  loss_ce_0: 0.05894  loss_mask_0: 0.2068  loss_dice_0: 0.1628  loss_ce_1: 0.0007754  loss_mask_1: 0.1982  loss_dice_1: 0.1594  loss_ce_2: 0.0006475  loss_mask_2: 0.2057  loss_dice_2: 0.1577  loss_ce_3: 0.0009528  loss_mask_3: 0.2057  loss_dice_3: 0.1604  loss_ce_4: 0.000817  loss_mask_4: 0.1955  loss_dice_4: 0.1617  loss_ce_5: 0.0006887  loss_mask_5: 0.1994  loss_dice_5: 0.1582  loss_ce_6: 0.001074  loss_mask_6: 0.1867  loss_dice_6: 0.1519  loss_ce_7: 0.0008837  loss_mask_7: 0.1926  loss_dice_7: 0.1543  loss_ce_8: 0.0008055  loss_mask_8: 0.1957  loss_dice_8: 0.1585  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:33:43] d2.utils.events INFO:  eta: 0:18:39  iter: 41119  total_loss: 5.098  loss_ce: 0.0009977  loss_mask: 0.2419  loss_dice: 0.1392  loss_ce_0: 0.06189  loss_mask_0: 0.2367  loss_dice_0: 0.134  loss_ce_1: 0.0009846  loss_mask_1: 0.2376  loss_dice_1: 0.1387  loss_ce_2: 0.001126  loss_mask_2: 0.2312  loss_dice_2: 0.1324  loss_ce_3: 0.0009527  loss_mask_3: 0.2287  loss_dice_3: 0.137  loss_ce_4: 0.00115  loss_mask_4: 0.2331  loss_dice_4: 0.1325  loss_ce_5: 0.001126  loss_mask_5: 0.2283  loss_dice_5: 0.14  loss_ce_6: 0.001276  loss_mask_6: 0.2272  loss_dice_6: 0.1348  loss_ce_7: 0.001421  loss_mask_7: 0.2393  loss_dice_7: 0.1329  loss_ce_8: 0.001379  loss_mask_8: 0.2324  loss_dice_8: 0.1372  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:46] d2.utils.events INFO:  eta: 0:18:37  iter: 41139  total_loss: 4.342  loss_ce: 0.0009404  loss_mask: 0.17  loss_dice: 0.2356  loss_ce_0: 0.05893  loss_mask_0: 0.1712  loss_dice_0: 0.2313  loss_ce_1: 0.0009206  loss_mask_1: 0.174  loss_dice_1: 0.2331  loss_ce_2: 0.0008249  loss_mask_2: 0.1648  loss_dice_2: 0.2351  loss_ce_3: 0.0009351  loss_mask_3: 0.1662  loss_dice_3: 0.2273  loss_ce_4: 0.001072  loss_mask_4: 0.1789  loss_dice_4: 0.2284  loss_ce_5: 0.0008242  loss_mask_5: 0.179  loss_dice_5: 0.2364  loss_ce_6: 0.001059  loss_mask_6: 0.1736  loss_dice_6: 0.2386  loss_ce_7: 0.001105  loss_mask_7: 0.1688  loss_dice_7: 0.2279  loss_ce_8: 0.001087  loss_mask_8: 0.1723  loss_dice_8: 0.2363  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:48] d2.utils.events INFO:  eta: 0:18:35  iter: 41159  total_loss: 4.626  loss_ce: 0.0009993  loss_mask: 0.2193  loss_dice: 0.2013  loss_ce_0: 0.05859  loss_mask_0: 0.2183  loss_dice_0: 0.2139  loss_ce_1: 0.0009921  loss_mask_1: 0.2071  loss_dice_1: 0.1946  loss_ce_2: 0.0007491  loss_mask_2: 0.2153  loss_dice_2: 0.1981  loss_ce_3: 0.0009123  loss_mask_3: 0.2157  loss_dice_3: 0.2002  loss_ce_4: 0.001084  loss_mask_4: 0.2094  loss_dice_4: 0.1984  loss_ce_5: 0.00083  loss_mask_5: 0.2131  loss_dice_5: 0.1914  loss_ce_6: 0.001105  loss_mask_6: 0.2108  loss_dice_6: 0.2007  loss_ce_7: 0.0007969  loss_mask_7: 0.2227  loss_dice_7: 0.2007  loss_ce_8: 0.001017  loss_mask_8: 0.2161  loss_dice_8: 0.191  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:51] d2.utils.events INFO:  eta: 0:18:33  iter: 41179  total_loss: 3.65  loss_ce: 0.0008078  loss_mask: 0.1853  loss_dice: 0.1269  loss_ce_0: 0.05796  loss_mask_0: 0.1864  loss_dice_0: 0.1268  loss_ce_1: 0.0007704  loss_mask_1: 0.1908  loss_dice_1: 0.1307  loss_ce_2: 0.0007646  loss_mask_2: 0.188  loss_dice_2: 0.125  loss_ce_3: 0.0007585  loss_mask_3: 0.1924  loss_dice_3: 0.1275  loss_ce_4: 0.0008231  loss_mask_4: 0.1917  loss_dice_4: 0.1291  loss_ce_5: 0.0007697  loss_mask_5: 0.1891  loss_dice_5: 0.1306  loss_ce_6: 0.0008943  loss_mask_6: 0.1901  loss_dice_6: 0.129  loss_ce_7: 0.00094  loss_mask_7: 0.1899  loss_dice_7: 0.1285  loss_ce_8: 0.001033  loss_mask_8: 0.1967  loss_dice_8: 0.129  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:33:53] d2.utils.events INFO:  eta: 0:18:30  iter: 41199  total_loss: 4.147  loss_ce: 0.0012  loss_mask: 0.2038  loss_dice: 0.1349  loss_ce_0: 0.06611  loss_mask_0: 0.195  loss_dice_0: 0.1352  loss_ce_1: 0.0009862  loss_mask_1: 0.1994  loss_dice_1: 0.1417  loss_ce_2: 0.001183  loss_mask_2: 0.1953  loss_dice_2: 0.1358  loss_ce_3: 0.001184  loss_mask_3: 0.1988  loss_dice_3: 0.1348  loss_ce_4: 0.001057  loss_mask_4: 0.1979  loss_dice_4: 0.1412  loss_ce_5: 0.001403  loss_mask_5: 0.199  loss_dice_5: 0.14  loss_ce_6: 0.001322  loss_mask_6: 0.1968  loss_dice_6: 0.1383  loss_ce_7: 0.001403  loss_mask_7: 0.201  loss_dice_7: 0.1372  loss_ce_8: 0.001486  loss_mask_8: 0.1995  loss_dice_8: 0.1365  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:33:56] d2.utils.events INFO:  eta: 0:18:28  iter: 41219  total_loss: 4.013  loss_ce: 0.002036  loss_mask: 0.1501  loss_dice: 0.1755  loss_ce_0: 0.05832  loss_mask_0: 0.1654  loss_dice_0: 0.1709  loss_ce_1: 0.00266  loss_mask_1: 0.1561  loss_dice_1: 0.1677  loss_ce_2: 0.002264  loss_mask_2: 0.1561  loss_dice_2: 0.1791  loss_ce_3: 0.001893  loss_mask_3: 0.1618  loss_dice_3: 0.1716  loss_ce_4: 0.001784  loss_mask_4: 0.1541  loss_dice_4: 0.1723  loss_ce_5: 0.002034  loss_mask_5: 0.1569  loss_dice_5: 0.1749  loss_ce_6: 0.001562  loss_mask_6: 0.1561  loss_dice_6: 0.1701  loss_ce_7: 0.001633  loss_mask_7: 0.161  loss_dice_7: 0.1692  loss_ce_8: 0.002334  loss_mask_8: 0.1542  loss_dice_8: 0.1762  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:33:58] d2.utils.events INFO:  eta: 0:18:25  iter: 41239  total_loss: 4.43  loss_ce: 0.001136  loss_mask: 0.179  loss_dice: 0.2297  loss_ce_0: 0.06187  loss_mask_0: 0.181  loss_dice_0: 0.2213  loss_ce_1: 0.001078  loss_mask_1: 0.1792  loss_dice_1: 0.2315  loss_ce_2: 0.0008586  loss_mask_2: 0.1737  loss_dice_2: 0.2183  loss_ce_3: 0.00103  loss_mask_3: 0.1739  loss_dice_3: 0.2274  loss_ce_4: 0.001071  loss_mask_4: 0.1709  loss_dice_4: 0.2209  loss_ce_5: 0.0009675  loss_mask_5: 0.1758  loss_dice_5: 0.2265  loss_ce_6: 0.001062  loss_mask_6: 0.1861  loss_dice_6: 0.2235  loss_ce_7: 0.001121  loss_mask_7: 0.1756  loss_dice_7: 0.2205  loss_ce_8: 0.001168  loss_mask_8: 0.1695  loss_dice_8: 0.2202  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:01] d2.utils.events INFO:  eta: 0:18:22  iter: 41259  total_loss: 4.198  loss_ce: 0.003865  loss_mask: 0.2101  loss_dice: 0.1564  loss_ce_0: 0.05818  loss_mask_0: 0.2049  loss_dice_0: 0.1523  loss_ce_1: 0.002803  loss_mask_1: 0.2153  loss_dice_1: 0.1523  loss_ce_2: 0.002148  loss_mask_2: 0.214  loss_dice_2: 0.15  loss_ce_3: 0.003377  loss_mask_3: 0.2088  loss_dice_3: 0.1527  loss_ce_4: 0.002466  loss_mask_4: 0.2076  loss_dice_4: 0.1463  loss_ce_5: 0.001962  loss_mask_5: 0.218  loss_dice_5: 0.1552  loss_ce_6: 0.00277  loss_mask_6: 0.1964  loss_dice_6: 0.1485  loss_ce_7: 0.002747  loss_mask_7: 0.2088  loss_dice_7: 0.1585  loss_ce_8: 0.002237  loss_mask_8: 0.2054  loss_dice_8: 0.1563  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:34:04] d2.utils.events INFO:  eta: 0:18:19  iter: 41279  total_loss: 4.175  loss_ce: 0.0008833  loss_mask: 0.2179  loss_dice: 0.173  loss_ce_0: 0.0581  loss_mask_0: 0.2184  loss_dice_0: 0.1723  loss_ce_1: 0.0007749  loss_mask_1: 0.21  loss_dice_1: 0.1713  loss_ce_2: 0.0008568  loss_mask_2: 0.2022  loss_dice_2: 0.1686  loss_ce_3: 0.0009496  loss_mask_3: 0.2075  loss_dice_3: 0.1641  loss_ce_4: 0.0009668  loss_mask_4: 0.2128  loss_dice_4: 0.1712  loss_ce_5: 0.0008965  loss_mask_5: 0.2077  loss_dice_5: 0.1647  loss_ce_6: 0.001171  loss_mask_6: 0.2083  loss_dice_6: 0.1667  loss_ce_7: 0.001096  loss_mask_7: 0.2284  loss_dice_7: 0.1697  loss_ce_8: 0.001112  loss_mask_8: 0.2228  loss_dice_8: 0.1748  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:06] d2.utils.events INFO:  eta: 0:18:17  iter: 41299  total_loss: 4.353  loss_ce: 0.001465  loss_mask: 0.202  loss_dice: 0.1587  loss_ce_0: 0.05808  loss_mask_0: 0.1894  loss_dice_0: 0.1552  loss_ce_1: 0.001324  loss_mask_1: 0.1955  loss_dice_1: 0.1562  loss_ce_2: 0.0012  loss_mask_2: 0.1937  loss_dice_2: 0.1532  loss_ce_3: 0.001289  loss_mask_3: 0.2005  loss_dice_3: 0.1566  loss_ce_4: 0.001286  loss_mask_4: 0.2018  loss_dice_4: 0.1541  loss_ce_5: 0.001178  loss_mask_5: 0.194  loss_dice_5: 0.1497  loss_ce_6: 0.001443  loss_mask_6: 0.1957  loss_dice_6: 0.1538  loss_ce_7: 0.001375  loss_mask_7: 0.1917  loss_dice_7: 0.158  loss_ce_8: 0.001336  loss_mask_8: 0.2041  loss_dice_8: 0.1519  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:09] d2.utils.events INFO:  eta: 0:18:14  iter: 41319  total_loss: 4.168  loss_ce: 0.001179  loss_mask: 0.2007  loss_dice: 0.159  loss_ce_0: 0.05813  loss_mask_0: 0.2104  loss_dice_0: 0.16  loss_ce_1: 0.0009014  loss_mask_1: 0.2081  loss_dice_1: 0.1551  loss_ce_2: 0.001023  loss_mask_2: 0.2058  loss_dice_2: 0.1562  loss_ce_3: 0.001112  loss_mask_3: 0.2151  loss_dice_3: 0.155  loss_ce_4: 0.001033  loss_mask_4: 0.2052  loss_dice_4: 0.1581  loss_ce_5: 0.001111  loss_mask_5: 0.2075  loss_dice_5: 0.152  loss_ce_6: 0.001206  loss_mask_6: 0.2074  loss_dice_6: 0.1552  loss_ce_7: 0.001222  loss_mask_7: 0.209  loss_dice_7: 0.1608  loss_ce_8: 0.001191  loss_mask_8: 0.2104  loss_dice_8: 0.1554  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:11] d2.utils.events INFO:  eta: 0:18:11  iter: 41339  total_loss: 4.411  loss_ce: 0.001225  loss_mask: 0.1886  loss_dice: 0.2063  loss_ce_0: 0.05798  loss_mask_0: 0.1936  loss_dice_0: 0.2073  loss_ce_1: 0.0009056  loss_mask_1: 0.1907  loss_dice_1: 0.2179  loss_ce_2: 0.0009967  loss_mask_2: 0.1814  loss_dice_2: 0.2054  loss_ce_3: 0.001078  loss_mask_3: 0.1863  loss_dice_3: 0.2037  loss_ce_4: 0.00122  loss_mask_4: 0.1903  loss_dice_4: 0.2063  loss_ce_5: 0.001108  loss_mask_5: 0.1792  loss_dice_5: 0.1992  loss_ce_6: 0.001153  loss_mask_6: 0.1885  loss_dice_6: 0.2027  loss_ce_7: 0.001218  loss_mask_7: 0.1776  loss_dice_7: 0.204  loss_ce_8: 0.001136  loss_mask_8: 0.1916  loss_dice_8: 0.2052  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:14] d2.utils.events INFO:  eta: 0:18:09  iter: 41359  total_loss: 3.562  loss_ce: 0.00105  loss_mask: 0.1783  loss_dice: 0.1747  loss_ce_0: 0.06559  loss_mask_0: 0.1806  loss_dice_0: 0.1765  loss_ce_1: 0.0008548  loss_mask_1: 0.1763  loss_dice_1: 0.1682  loss_ce_2: 0.0008987  loss_mask_2: 0.1798  loss_dice_2: 0.164  loss_ce_3: 0.001052  loss_mask_3: 0.1849  loss_dice_3: 0.1652  loss_ce_4: 0.0009558  loss_mask_4: 0.1863  loss_dice_4: 0.172  loss_ce_5: 0.001032  loss_mask_5: 0.1787  loss_dice_5: 0.171  loss_ce_6: 0.001213  loss_mask_6: 0.1802  loss_dice_6: 0.179  loss_ce_7: 0.001255  loss_mask_7: 0.175  loss_dice_7: 0.1715  loss_ce_8: 0.001197  loss_mask_8: 0.1733  loss_dice_8: 0.1741  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:16] d2.utils.events INFO:  eta: 0:18:06  iter: 41379  total_loss: 3.881  loss_ce: 0.0009808  loss_mask: 0.1843  loss_dice: 0.1374  loss_ce_0: 0.06491  loss_mask_0: 0.1773  loss_dice_0: 0.1407  loss_ce_1: 0.001117  loss_mask_1: 0.1821  loss_dice_1: 0.1431  loss_ce_2: 0.0008752  loss_mask_2: 0.1799  loss_dice_2: 0.141  loss_ce_3: 0.0008867  loss_mask_3: 0.1761  loss_dice_3: 0.1436  loss_ce_4: 0.0011  loss_mask_4: 0.1839  loss_dice_4: 0.1413  loss_ce_5: 0.00105  loss_mask_5: 0.1837  loss_dice_5: 0.1407  loss_ce_6: 0.001051  loss_mask_6: 0.1812  loss_dice_6: 0.1436  loss_ce_7: 0.00113  loss_mask_7: 0.1826  loss_dice_7: 0.1402  loss_ce_8: 0.001176  loss_mask_8: 0.1839  loss_dice_8: 0.1445  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:34:19] d2.utils.events INFO:  eta: 0:18:04  iter: 41399  total_loss: 4.175  loss_ce: 0.001008  loss_mask: 0.1832  loss_dice: 0.1538  loss_ce_0: 0.05893  loss_mask_0: 0.1891  loss_dice_0: 0.1524  loss_ce_1: 0.0008027  loss_mask_1: 0.1884  loss_dice_1: 0.1481  loss_ce_2: 0.0008357  loss_mask_2: 0.1976  loss_dice_2: 0.1556  loss_ce_3: 0.0009695  loss_mask_3: 0.1877  loss_dice_3: 0.1426  loss_ce_4: 0.0009531  loss_mask_4: 0.1909  loss_dice_4: 0.1534  loss_ce_5: 0.0009302  loss_mask_5: 0.1912  loss_dice_5: 0.1517  loss_ce_6: 0.001062  loss_mask_6: 0.1948  loss_dice_6: 0.1494  loss_ce_7: 0.001089  loss_mask_7: 0.1928  loss_dice_7: 0.1575  loss_ce_8: 0.001096  loss_mask_8: 0.1869  loss_dice_8: 0.1602  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:34:21] d2.utils.events INFO:  eta: 0:18:01  iter: 41419  total_loss: 3.799  loss_ce: 0.0009633  loss_mask: 0.211  loss_dice: 0.1334  loss_ce_0: 0.06472  loss_mask_0: 0.2117  loss_dice_0: 0.1342  loss_ce_1: 0.0009377  loss_mask_1: 0.2175  loss_dice_1: 0.1312  loss_ce_2: 0.0009643  loss_mask_2: 0.2042  loss_dice_2: 0.1334  loss_ce_3: 0.0009373  loss_mask_3: 0.2034  loss_dice_3: 0.1365  loss_ce_4: 0.0009855  loss_mask_4: 0.2059  loss_dice_4: 0.1339  loss_ce_5: 0.001076  loss_mask_5: 0.204  loss_dice_5: 0.1345  loss_ce_6: 0.001181  loss_mask_6: 0.2099  loss_dice_6: 0.1353  loss_ce_7: 0.001192  loss_mask_7: 0.206  loss_dice_7: 0.1285  loss_ce_8: 0.001224  loss_mask_8: 0.2109  loss_dice_8: 0.1313  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:34:24] d2.utils.events INFO:  eta: 0:17:59  iter: 41439  total_loss: 4.131  loss_ce: 0.001161  loss_mask: 0.2106  loss_dice: 0.177  loss_ce_0: 0.06358  loss_mask_0: 0.1966  loss_dice_0: 0.1714  loss_ce_1: 0.001334  loss_mask_1: 0.2174  loss_dice_1: 0.1716  loss_ce_2: 0.001285  loss_mask_2: 0.2053  loss_dice_2: 0.1806  loss_ce_3: 0.001094  loss_mask_3: 0.21  loss_dice_3: 0.179  loss_ce_4: 0.001491  loss_mask_4: 0.2034  loss_dice_4: 0.1827  loss_ce_5: 0.001273  loss_mask_5: 0.2129  loss_dice_5: 0.1817  loss_ce_6: 0.001268  loss_mask_6: 0.2193  loss_dice_6: 0.1707  loss_ce_7: 0.001561  loss_mask_7: 0.2034  loss_dice_7: 0.1705  loss_ce_8: 0.001376  loss_mask_8: 0.204  loss_dice_8: 0.186  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:26] d2.utils.events INFO:  eta: 0:17:56  iter: 41459  total_loss: 3.755  loss_ce: 0.0013  loss_mask: 0.2122  loss_dice: 0.1416  loss_ce_0: 0.06284  loss_mask_0: 0.2057  loss_dice_0: 0.1438  loss_ce_1: 0.0009105  loss_mask_1: 0.2068  loss_dice_1: 0.1401  loss_ce_2: 0.0009665  loss_mask_2: 0.2059  loss_dice_2: 0.1488  loss_ce_3: 0.001048  loss_mask_3: 0.2017  loss_dice_3: 0.1441  loss_ce_4: 0.001032  loss_mask_4: 0.2126  loss_dice_4: 0.1464  loss_ce_5: 0.0009388  loss_mask_5: 0.2057  loss_dice_5: 0.1444  loss_ce_6: 0.001011  loss_mask_6: 0.2072  loss_dice_6: 0.141  loss_ce_7: 0.001141  loss_mask_7: 0.2152  loss_dice_7: 0.1507  loss_ce_8: 0.001174  loss_mask_8: 0.1985  loss_dice_8: 0.1436  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:29] d2.utils.events INFO:  eta: 0:17:53  iter: 41479  total_loss: 4.826  loss_ce: 0.002495  loss_mask: 0.1773  loss_dice: 0.1828  loss_ce_0: 0.0616  loss_mask_0: 0.2027  loss_dice_0: 0.1807  loss_ce_1: 0.001423  loss_mask_1: 0.1935  loss_dice_1: 0.1841  loss_ce_2: 0.001672  loss_mask_2: 0.1914  loss_dice_2: 0.182  loss_ce_3: 0.002206  loss_mask_3: 0.1858  loss_dice_3: 0.1851  loss_ce_4: 0.00176  loss_mask_4: 0.1869  loss_dice_4: 0.1831  loss_ce_5: 0.001696  loss_mask_5: 0.1948  loss_dice_5: 0.1875  loss_ce_6: 0.001723  loss_mask_6: 0.1939  loss_dice_6: 0.1986  loss_ce_7: 0.001377  loss_mask_7: 0.1821  loss_dice_7: 0.1855  loss_ce_8: 0.002216  loss_mask_8: 0.18  loss_dice_8: 0.188  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:31] d2.utils.events INFO:  eta: 0:17:51  iter: 41499  total_loss: 4.63  loss_ce: 0.001064  loss_mask: 0.2477  loss_dice: 0.1678  loss_ce_0: 0.06177  loss_mask_0: 0.2514  loss_dice_0: 0.1699  loss_ce_1: 0.0009243  loss_mask_1: 0.2548  loss_dice_1: 0.1617  loss_ce_2: 0.0009603  loss_mask_2: 0.2612  loss_dice_2: 0.1695  loss_ce_3: 0.001091  loss_mask_3: 0.2559  loss_dice_3: 0.1675  loss_ce_4: 0.0009714  loss_mask_4: 0.2497  loss_dice_4: 0.1695  loss_ce_5: 0.0009646  loss_mask_5: 0.254  loss_dice_5: 0.1689  loss_ce_6: 0.001097  loss_mask_6: 0.2539  loss_dice_6: 0.1674  loss_ce_7: 0.001104  loss_mask_7: 0.2599  loss_dice_7: 0.1697  loss_ce_8: 0.001192  loss_mask_8: 0.2668  loss_dice_8: 0.1714  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:34] d2.utils.events INFO:  eta: 0:17:48  iter: 41519  total_loss: 3.668  loss_ce: 0.000878  loss_mask: 0.2103  loss_dice: 0.1292  loss_ce_0: 0.06177  loss_mask_0: 0.2099  loss_dice_0: 0.1306  loss_ce_1: 0.0008674  loss_mask_1: 0.2041  loss_dice_1: 0.1295  loss_ce_2: 0.0009208  loss_mask_2: 0.216  loss_dice_2: 0.1249  loss_ce_3: 0.0008536  loss_mask_3: 0.2106  loss_dice_3: 0.1301  loss_ce_4: 0.0008949  loss_mask_4: 0.2109  loss_dice_4: 0.1295  loss_ce_5: 0.0009262  loss_mask_5: 0.2051  loss_dice_5: 0.1328  loss_ce_6: 0.001022  loss_mask_6: 0.206  loss_dice_6: 0.132  loss_ce_7: 0.001081  loss_mask_7: 0.2104  loss_dice_7: 0.1285  loss_ce_8: 0.001077  loss_mask_8: 0.2132  loss_dice_8: 0.1352  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:36] d2.utils.events INFO:  eta: 0:17:45  iter: 41539  total_loss: 4.3  loss_ce: 0.001877  loss_mask: 0.1812  loss_dice: 0.2146  loss_ce_0: 0.06163  loss_mask_0: 0.1914  loss_dice_0: 0.2162  loss_ce_1: 0.003437  loss_mask_1: 0.1974  loss_dice_1: 0.2159  loss_ce_2: 0.001736  loss_mask_2: 0.1828  loss_dice_2: 0.2181  loss_ce_3: 0.001384  loss_mask_3: 0.1826  loss_dice_3: 0.2095  loss_ce_4: 0.001856  loss_mask_4: 0.1834  loss_dice_4: 0.2219  loss_ce_5: 0.001459  loss_mask_5: 0.1905  loss_dice_5: 0.2185  loss_ce_6: 0.001401  loss_mask_6: 0.1945  loss_dice_6: 0.2127  loss_ce_7: 0.001955  loss_mask_7: 0.1881  loss_dice_7: 0.2135  loss_ce_8: 0.001802  loss_mask_8: 0.1864  loss_dice_8: 0.2054  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:39] d2.utils.events INFO:  eta: 0:17:43  iter: 41559  total_loss: 3.831  loss_ce: 0.0009538  loss_mask: 0.2006  loss_dice: 0.1436  loss_ce_0: 0.06209  loss_mask_0: 0.1809  loss_dice_0: 0.1385  loss_ce_1: 0.0008866  loss_mask_1: 0.1871  loss_dice_1: 0.139  loss_ce_2: 0.001008  loss_mask_2: 0.1979  loss_dice_2: 0.1373  loss_ce_3: 0.0007438  loss_mask_3: 0.1973  loss_dice_3: 0.1396  loss_ce_4: 0.000931  loss_mask_4: 0.1965  loss_dice_4: 0.1374  loss_ce_5: 0.001048  loss_mask_5: 0.2032  loss_dice_5: 0.1452  loss_ce_6: 0.0009521  loss_mask_6: 0.1896  loss_dice_6: 0.1409  loss_ce_7: 0.000972  loss_mask_7: 0.1846  loss_dice_7: 0.1434  loss_ce_8: 0.001141  loss_mask_8: 0.2034  loss_dice_8: 0.1454  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:42] d2.utils.events INFO:  eta: 0:17:41  iter: 41579  total_loss: 4.556  loss_ce: 0.001217  loss_mask: 0.2221  loss_dice: 0.1588  loss_ce_0: 0.06159  loss_mask_0: 0.2247  loss_dice_0: 0.1548  loss_ce_1: 0.001385  loss_mask_1: 0.2218  loss_dice_1: 0.1594  loss_ce_2: 0.0009577  loss_mask_2: 0.223  loss_dice_2: 0.1561  loss_ce_3: 0.0009935  loss_mask_3: 0.2135  loss_dice_3: 0.1578  loss_ce_4: 0.001166  loss_mask_4: 0.2307  loss_dice_4: 0.1597  loss_ce_5: 0.0009153  loss_mask_5: 0.2171  loss_dice_5: 0.1543  loss_ce_6: 0.001148  loss_mask_6: 0.2214  loss_dice_6: 0.1527  loss_ce_7: 0.001279  loss_mask_7: 0.2206  loss_dice_7: 0.1552  loss_ce_8: 0.001082  loss_mask_8: 0.2252  loss_dice_8: 0.1598  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:44] d2.utils.events INFO:  eta: 0:17:38  iter: 41599  total_loss: 4.002  loss_ce: 0.001059  loss_mask: 0.2003  loss_dice: 0.1545  loss_ce_0: 0.06101  loss_mask_0: 0.1906  loss_dice_0: 0.1586  loss_ce_1: 0.0009912  loss_mask_1: 0.193  loss_dice_1: 0.1543  loss_ce_2: 0.0008754  loss_mask_2: 0.1963  loss_dice_2: 0.1507  loss_ce_3: 0.001119  loss_mask_3: 0.1982  loss_dice_3: 0.158  loss_ce_4: 0.001137  loss_mask_4: 0.1993  loss_dice_4: 0.1558  loss_ce_5: 0.0008982  loss_mask_5: 0.1931  loss_dice_5: 0.1546  loss_ce_6: 0.001217  loss_mask_6: 0.2055  loss_dice_6: 0.1529  loss_ce_7: 0.001348  loss_mask_7: 0.1932  loss_dice_7: 0.1489  loss_ce_8: 0.001038  loss_mask_8: 0.1983  loss_dice_8: 0.1525  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:47] d2.utils.events INFO:  eta: 0:17:36  iter: 41619  total_loss: 4.13  loss_ce: 0.001136  loss_mask: 0.2072  loss_dice: 0.1576  loss_ce_0: 0.06038  loss_mask_0: 0.1966  loss_dice_0: 0.161  loss_ce_1: 0.001157  loss_mask_1: 0.1851  loss_dice_1: 0.1575  loss_ce_2: 0.001  loss_mask_2: 0.1873  loss_dice_2: 0.1637  loss_ce_3: 0.001081  loss_mask_3: 0.198  loss_dice_3: 0.1582  loss_ce_4: 0.001164  loss_mask_4: 0.1939  loss_dice_4: 0.1549  loss_ce_5: 0.0009547  loss_mask_5: 0.1951  loss_dice_5: 0.1597  loss_ce_6: 0.001187  loss_mask_6: 0.1937  loss_dice_6: 0.164  loss_ce_7: 0.001159  loss_mask_7: 0.1866  loss_dice_7: 0.1618  loss_ce_8: 0.001129  loss_mask_8: 0.2035  loss_dice_8: 0.1582  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:49] d2.utils.events INFO:  eta: 0:17:33  iter: 41639  total_loss: 3.831  loss_ce: 0.001247  loss_mask: 0.1605  loss_dice: 0.1801  loss_ce_0: 0.05999  loss_mask_0: 0.1629  loss_dice_0: 0.1737  loss_ce_1: 0.001268  loss_mask_1: 0.1748  loss_dice_1: 0.182  loss_ce_2: 0.001161  loss_mask_2: 0.162  loss_dice_2: 0.1822  loss_ce_3: 0.001158  loss_mask_3: 0.1677  loss_dice_3: 0.1859  loss_ce_4: 0.001311  loss_mask_4: 0.1662  loss_dice_4: 0.1826  loss_ce_5: 0.001113  loss_mask_5: 0.1623  loss_dice_5: 0.184  loss_ce_6: 0.001217  loss_mask_6: 0.1663  loss_dice_6: 0.1808  loss_ce_7: 0.001357  loss_mask_7: 0.1658  loss_dice_7: 0.1829  loss_ce_8: 0.001282  loss_mask_8: 0.1582  loss_dice_8: 0.1833  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:52] d2.utils.events INFO:  eta: 0:17:31  iter: 41659  total_loss: 4.477  loss_ce: 0.001362  loss_mask: 0.168  loss_dice: 0.199  loss_ce_0: 0.06375  loss_mask_0: 0.173  loss_dice_0: 0.1909  loss_ce_1: 0.000966  loss_mask_1: 0.1685  loss_dice_1: 0.1828  loss_ce_2: 0.001105  loss_mask_2: 0.1759  loss_dice_2: 0.1906  loss_ce_3: 0.001302  loss_mask_3: 0.169  loss_dice_3: 0.1991  loss_ce_4: 0.00108  loss_mask_4: 0.1728  loss_dice_4: 0.1938  loss_ce_5: 0.001171  loss_mask_5: 0.1789  loss_dice_5: 0.1924  loss_ce_6: 0.001457  loss_mask_6: 0.1815  loss_dice_6: 0.1855  loss_ce_7: 0.001311  loss_mask_7: 0.1696  loss_dice_7: 0.1999  loss_ce_8: 0.001331  loss_mask_8: 0.1725  loss_dice_8: 0.2014  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:54] d2.utils.events INFO:  eta: 0:17:29  iter: 41679  total_loss: 3.539  loss_ce: 0.0009367  loss_mask: 0.1981  loss_dice: 0.1417  loss_ce_0: 0.06337  loss_mask_0: 0.1998  loss_dice_0: 0.1454  loss_ce_1: 0.0007669  loss_mask_1: 0.2053  loss_dice_1: 0.1505  loss_ce_2: 0.0008491  loss_mask_2: 0.198  loss_dice_2: 0.1385  loss_ce_3: 0.0008205  loss_mask_3: 0.1982  loss_dice_3: 0.1402  loss_ce_4: 0.000924  loss_mask_4: 0.1944  loss_dice_4: 0.1429  loss_ce_5: 0.0008206  loss_mask_5: 0.2029  loss_dice_5: 0.1421  loss_ce_6: 0.0009622  loss_mask_6: 0.1962  loss_dice_6: 0.1424  loss_ce_7: 0.001067  loss_mask_7: 0.2018  loss_dice_7: 0.1453  loss_ce_8: 0.001002  loss_mask_8: 0.1965  loss_dice_8: 0.1406  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:57] d2.utils.events INFO:  eta: 0:17:26  iter: 41699  total_loss: 3.664  loss_ce: 0.0008911  loss_mask: 0.1734  loss_dice: 0.1423  loss_ce_0: 0.06328  loss_mask_0: 0.1794  loss_dice_0: 0.1373  loss_ce_1: 0.00106  loss_mask_1: 0.173  loss_dice_1: 0.1404  loss_ce_2: 0.0009544  loss_mask_2: 0.179  loss_dice_2: 0.1413  loss_ce_3: 0.0008947  loss_mask_3: 0.1727  loss_dice_3: 0.139  loss_ce_4: 0.00113  loss_mask_4: 0.1778  loss_dice_4: 0.1386  loss_ce_5: 0.0009246  loss_mask_5: 0.1867  loss_dice_5: 0.138  loss_ce_6: 0.001086  loss_mask_6: 0.1688  loss_dice_6: 0.1354  loss_ce_7: 0.001137  loss_mask_7: 0.1669  loss_dice_7: 0.1456  loss_ce_8: 0.001124  loss_mask_8: 0.1739  loss_dice_8: 0.1377  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:34:59] d2.utils.events INFO:  eta: 0:17:23  iter: 41719  total_loss: 3.953  loss_ce: 0.001013  loss_mask: 0.2185  loss_dice: 0.1656  loss_ce_0: 0.06069  loss_mask_0: 0.215  loss_dice_0: 0.1669  loss_ce_1: 0.0007846  loss_mask_1: 0.2187  loss_dice_1: 0.1675  loss_ce_2: 0.000741  loss_mask_2: 0.2205  loss_dice_2: 0.1695  loss_ce_3: 0.0009719  loss_mask_3: 0.2195  loss_dice_3: 0.1624  loss_ce_4: 0.0008879  loss_mask_4: 0.2237  loss_dice_4: 0.1658  loss_ce_5: 0.00078  loss_mask_5: 0.212  loss_dice_5: 0.1693  loss_ce_6: 0.00105  loss_mask_6: 0.2163  loss_dice_6: 0.1625  loss_ce_7: 0.0008713  loss_mask_7: 0.2262  loss_dice_7: 0.1672  loss_ce_8: 0.0009985  loss_mask_8: 0.2117  loss_dice_8: 0.172  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:35:02] d2.utils.events INFO:  eta: 0:17:21  iter: 41739  total_loss: 4.481  loss_ce: 0.00309  loss_mask: 0.1724  loss_dice: 0.1853  loss_ce_0: 0.06055  loss_mask_0: 0.1715  loss_dice_0: 0.1844  loss_ce_1: 0.001355  loss_mask_1: 0.1598  loss_dice_1: 0.1812  loss_ce_2: 0.001354  loss_mask_2: 0.1781  loss_dice_2: 0.1961  loss_ce_3: 0.002486  loss_mask_3: 0.1666  loss_dice_3: 0.1863  loss_ce_4: 0.002209  loss_mask_4: 0.1709  loss_dice_4: 0.1857  loss_ce_5: 0.001327  loss_mask_5: 0.1682  loss_dice_5: 0.1893  loss_ce_6: 0.002679  loss_mask_6: 0.1767  loss_dice_6: 0.1836  loss_ce_7: 0.0026  loss_mask_7: 0.1673  loss_dice_7: 0.1843  loss_ce_8: 0.001629  loss_mask_8: 0.1686  loss_dice_8: 0.1907  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:04] d2.utils.events INFO:  eta: 0:17:19  iter: 41759  total_loss: 3.647  loss_ce: 0.001023  loss_mask: 0.1876  loss_dice: 0.1474  loss_ce_0: 0.06378  loss_mask_0: 0.1957  loss_dice_0: 0.1467  loss_ce_1: 0.0009235  loss_mask_1: 0.1889  loss_dice_1: 0.1408  loss_ce_2: 0.0008734  loss_mask_2: 0.1912  loss_dice_2: 0.142  loss_ce_3: 0.0008674  loss_mask_3: 0.1825  loss_dice_3: 0.1423  loss_ce_4: 0.0008707  loss_mask_4: 0.1908  loss_dice_4: 0.1464  loss_ce_5: 0.0008431  loss_mask_5: 0.1955  loss_dice_5: 0.1415  loss_ce_6: 0.001041  loss_mask_6: 0.1955  loss_dice_6: 0.1436  loss_ce_7: 0.0009412  loss_mask_7: 0.1862  loss_dice_7: 0.1418  loss_ce_8: 0.00104  loss_mask_8: 0.1916  loss_dice_8: 0.1426  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:07] d2.utils.events INFO:  eta: 0:17:16  iter: 41779  total_loss: 4.146  loss_ce: 0.002047  loss_mask: 0.1777  loss_dice: 0.1935  loss_ce_0: 0.06287  loss_mask_0: 0.1789  loss_dice_0: 0.1907  loss_ce_1: 0.0008297  loss_mask_1: 0.1937  loss_dice_1: 0.2029  loss_ce_2: 0.0008502  loss_mask_2: 0.176  loss_dice_2: 0.1894  loss_ce_3: 0.002183  loss_mask_3: 0.1853  loss_dice_3: 0.1961  loss_ce_4: 0.001276  loss_mask_4: 0.1745  loss_dice_4: 0.1964  loss_ce_5: 0.001037  loss_mask_5: 0.1789  loss_dice_5: 0.1916  loss_ce_6: 0.002152  loss_mask_6: 0.1836  loss_dice_6: 0.1837  loss_ce_7: 0.0009685  loss_mask_7: 0.1926  loss_dice_7: 0.1954  loss_ce_8: 0.001162  loss_mask_8: 0.18  loss_dice_8: 0.1938  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:09] d2.utils.events INFO:  eta: 0:17:13  iter: 41799  total_loss: 4.11  loss_ce: 0.001543  loss_mask: 0.1837  loss_dice: 0.17  loss_ce_0: 0.06233  loss_mask_0: 0.1812  loss_dice_0: 0.1657  loss_ce_1: 0.00152  loss_mask_1: 0.1769  loss_dice_1: 0.1706  loss_ce_2: 0.001279  loss_mask_2: 0.1846  loss_dice_2: 0.1661  loss_ce_3: 0.001471  loss_mask_3: 0.1821  loss_dice_3: 0.1621  loss_ce_4: 0.001396  loss_mask_4: 0.1868  loss_dice_4: 0.1693  loss_ce_5: 0.001161  loss_mask_5: 0.1873  loss_dice_5: 0.174  loss_ce_6: 0.001378  loss_mask_6: 0.176  loss_dice_6: 0.1682  loss_ce_7: 0.001346  loss_mask_7: 0.1809  loss_dice_7: 0.172  loss_ce_8: 0.001277  loss_mask_8: 0.1863  loss_dice_8: 0.164  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:12] d2.utils.events INFO:  eta: 0:17:11  iter: 41819  total_loss: 3.7  loss_ce: 0.001365  loss_mask: 0.1802  loss_dice: 0.1398  loss_ce_0: 0.0618  loss_mask_0: 0.181  loss_dice_0: 0.1409  loss_ce_1: 0.001636  loss_mask_1: 0.1757  loss_dice_1: 0.1449  loss_ce_2: 0.001208  loss_mask_2: 0.1708  loss_dice_2: 0.1431  loss_ce_3: 0.001185  loss_mask_3: 0.1713  loss_dice_3: 0.145  loss_ce_4: 0.00129  loss_mask_4: 0.175  loss_dice_4: 0.1439  loss_ce_5: 0.0009973  loss_mask_5: 0.1768  loss_dice_5: 0.145  loss_ce_6: 0.001068  loss_mask_6: 0.1772  loss_dice_6: 0.1456  loss_ce_7: 0.001489  loss_mask_7: 0.1776  loss_dice_7: 0.1449  loss_ce_8: 0.001172  loss_mask_8: 0.1742  loss_dice_8: 0.1423  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:35:14] d2.utils.events INFO:  eta: 0:17:08  iter: 41839  total_loss: 4.202  loss_ce: 0.000844  loss_mask: 0.2257  loss_dice: 0.1497  loss_ce_0: 0.06187  loss_mask_0: 0.2232  loss_dice_0: 0.1455  loss_ce_1: 0.0009137  loss_mask_1: 0.224  loss_dice_1: 0.1512  loss_ce_2: 0.0008097  loss_mask_2: 0.2341  loss_dice_2: 0.1561  loss_ce_3: 0.000761  loss_mask_3: 0.2143  loss_dice_3: 0.1526  loss_ce_4: 0.000947  loss_mask_4: 0.2192  loss_dice_4: 0.1509  loss_ce_5: 0.0008739  loss_mask_5: 0.2317  loss_dice_5: 0.157  loss_ce_6: 0.0009519  loss_mask_6: 0.2322  loss_dice_6: 0.1573  loss_ce_7: 0.0009943  loss_mask_7: 0.2171  loss_dice_7: 0.1569  loss_ce_8: 0.001074  loss_mask_8: 0.2227  loss_dice_8: 0.1518  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:35:17] d2.utils.events INFO:  eta: 0:17:05  iter: 41859  total_loss: 4.183  loss_ce: 0.001824  loss_mask: 0.2015  loss_dice: 0.1658  loss_ce_0: 0.06186  loss_mask_0: 0.2038  loss_dice_0: 0.166  loss_ce_1: 0.002845  loss_mask_1: 0.2074  loss_dice_1: 0.1616  loss_ce_2: 0.005043  loss_mask_2: 0.2054  loss_dice_2: 0.1646  loss_ce_3: 0.002292  loss_mask_3: 0.2059  loss_dice_3: 0.1697  loss_ce_4: 0.002184  loss_mask_4: 0.2078  loss_dice_4: 0.162  loss_ce_5: 0.002317  loss_mask_5: 0.197  loss_dice_5: 0.1674  loss_ce_6: 0.001456  loss_mask_6: 0.1997  loss_dice_6: 0.1701  loss_ce_7: 0.001714  loss_mask_7: 0.2059  loss_dice_7: 0.154  loss_ce_8: 0.003629  loss_mask_8: 0.2071  loss_dice_8: 0.1635  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:19] d2.utils.events INFO:  eta: 0:17:02  iter: 41879  total_loss: 3.869  loss_ce: 0.0009614  loss_mask: 0.2254  loss_dice: 0.1465  loss_ce_0: 0.06179  loss_mask_0: 0.2353  loss_dice_0: 0.1441  loss_ce_1: 0.0009688  loss_mask_1: 0.2328  loss_dice_1: 0.1443  loss_ce_2: 0.001057  loss_mask_2: 0.2277  loss_dice_2: 0.1404  loss_ce_3: 0.0009676  loss_mask_3: 0.2316  loss_dice_3: 0.1445  loss_ce_4: 0.001027  loss_mask_4: 0.2356  loss_dice_4: 0.1466  loss_ce_5: 0.0009961  loss_mask_5: 0.2152  loss_dice_5: 0.1389  loss_ce_6: 0.001031  loss_mask_6: 0.2304  loss_dice_6: 0.1481  loss_ce_7: 0.001062  loss_mask_7: 0.2289  loss_dice_7: 0.1428  loss_ce_8: 0.001138  loss_mask_8: 0.2342  loss_dice_8: 0.143  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:22] d2.utils.events INFO:  eta: 0:16:59  iter: 41899  total_loss: 4.039  loss_ce: 0.001113  loss_mask: 0.1773  loss_dice: 0.1739  loss_ce_0: 0.062  loss_mask_0: 0.1763  loss_dice_0: 0.1746  loss_ce_1: 0.001113  loss_mask_1: 0.1725  loss_dice_1: 0.174  loss_ce_2: 0.001016  loss_mask_2: 0.1835  loss_dice_2: 0.1719  loss_ce_3: 0.001099  loss_mask_3: 0.1879  loss_dice_3: 0.1689  loss_ce_4: 0.001166  loss_mask_4: 0.1742  loss_dice_4: 0.1749  loss_ce_5: 0.001008  loss_mask_5: 0.1798  loss_dice_5: 0.177  loss_ce_6: 0.001187  loss_mask_6: 0.1812  loss_dice_6: 0.1698  loss_ce_7: 0.001116  loss_mask_7: 0.1799  loss_dice_7: 0.1668  loss_ce_8: 0.001159  loss_mask_8: 0.1794  loss_dice_8: 0.1673  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:24] d2.utils.events INFO:  eta: 0:16:56  iter: 41919  total_loss: 4.031  loss_ce: 0.001708  loss_mask: 0.1689  loss_dice: 0.1295  loss_ce_0: 0.06163  loss_mask_0: 0.1709  loss_dice_0: 0.1269  loss_ce_1: 0.001914  loss_mask_1: 0.1646  loss_dice_1: 0.1272  loss_ce_2: 0.002237  loss_mask_2: 0.1682  loss_dice_2: 0.1221  loss_ce_3: 0.002207  loss_mask_3: 0.1673  loss_dice_3: 0.126  loss_ce_4: 0.001984  loss_mask_4: 0.1618  loss_dice_4: 0.1302  loss_ce_5: 0.002549  loss_mask_5: 0.1689  loss_dice_5: 0.1297  loss_ce_6: 0.001853  loss_mask_6: 0.1659  loss_dice_6: 0.1294  loss_ce_7: 0.002248  loss_mask_7: 0.1702  loss_dice_7: 0.1275  loss_ce_8: 0.002416  loss_mask_8: 0.1778  loss_dice_8: 0.1297  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:27] d2.utils.events INFO:  eta: 0:16:53  iter: 41939  total_loss: 3.836  loss_ce: 0.001209  loss_mask: 0.2072  loss_dice: 0.1367  loss_ce_0: 0.06225  loss_mask_0: 0.2112  loss_dice_0: 0.1308  loss_ce_1: 0.001222  loss_mask_1: 0.2115  loss_dice_1: 0.1277  loss_ce_2: 0.001283  loss_mask_2: 0.2171  loss_dice_2: 0.137  loss_ce_3: 0.001198  loss_mask_3: 0.2103  loss_dice_3: 0.1268  loss_ce_4: 0.001224  loss_mask_4: 0.2091  loss_dice_4: 0.1314  loss_ce_5: 0.001192  loss_mask_5: 0.2133  loss_dice_5: 0.1311  loss_ce_6: 0.001299  loss_mask_6: 0.2023  loss_dice_6: 0.1321  loss_ce_7: 0.001457  loss_mask_7: 0.2023  loss_dice_7: 0.1345  loss_ce_8: 0.001344  loss_mask_8: 0.1909  loss_dice_8: 0.1336  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:29] d2.utils.events INFO:  eta: 0:16:51  iter: 41959  total_loss: 4.216  loss_ce: 0.0009168  loss_mask: 0.2052  loss_dice: 0.1681  loss_ce_0: 0.06265  loss_mask_0: 0.1983  loss_dice_0: 0.1686  loss_ce_1: 0.0008484  loss_mask_1: 0.1936  loss_dice_1: 0.1713  loss_ce_2: 0.0007772  loss_mask_2: 0.206  loss_dice_2: 0.1737  loss_ce_3: 0.0009385  loss_mask_3: 0.2024  loss_dice_3: 0.1713  loss_ce_4: 0.0008429  loss_mask_4: 0.2039  loss_dice_4: 0.1691  loss_ce_5: 0.0007912  loss_mask_5: 0.2109  loss_dice_5: 0.1653  loss_ce_6: 0.001084  loss_mask_6: 0.2079  loss_dice_6: 0.1683  loss_ce_7: 0.0009266  loss_mask_7: 0.1948  loss_dice_7: 0.176  loss_ce_8: 0.000923  loss_mask_8: 0.2013  loss_dice_8: 0.1594  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:32] d2.utils.events INFO:  eta: 0:16:49  iter: 41979  total_loss: 4.914  loss_ce: 0.001095  loss_mask: 0.2018  loss_dice: 0.2532  loss_ce_0: 0.06225  loss_mask_0: 0.1985  loss_dice_0: 0.2468  loss_ce_1: 0.001182  loss_mask_1: 0.2011  loss_dice_1: 0.2691  loss_ce_2: 0.000862  loss_mask_2: 0.1904  loss_dice_2: 0.2603  loss_ce_3: 0.0009036  loss_mask_3: 0.2022  loss_dice_3: 0.2659  loss_ce_4: 0.001105  loss_mask_4: 0.2011  loss_dice_4: 0.2493  loss_ce_5: 0.0008868  loss_mask_5: 0.2032  loss_dice_5: 0.2761  loss_ce_6: 0.001076  loss_mask_6: 0.1978  loss_dice_6: 0.2782  loss_ce_7: 0.001294  loss_mask_7: 0.2029  loss_dice_7: 0.2535  loss_ce_8: 0.001063  loss_mask_8: 0.1981  loss_dice_8: 0.261  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:34] d2.utils.events INFO:  eta: 0:16:46  iter: 41999  total_loss: 4.779  loss_ce: 0.003901  loss_mask: 0.1308  loss_dice: 0.3098  loss_ce_0: 0.06167  loss_mask_0: 0.1319  loss_dice_0: 0.2931  loss_ce_1: 0.007666  loss_mask_1: 0.1296  loss_dice_1: 0.2964  loss_ce_2: 0.002522  loss_mask_2: 0.1313  loss_dice_2: 0.2944  loss_ce_3: 0.003309  loss_mask_3: 0.1287  loss_dice_3: 0.2844  loss_ce_4: 0.005822  loss_mask_4: 0.1346  loss_dice_4: 0.2958  loss_ce_5: 0.002388  loss_mask_5: 0.1359  loss_dice_5: 0.2972  loss_ce_6: 0.002993  loss_mask_6: 0.1281  loss_dice_6: 0.2842  loss_ce_7: 0.004937  loss_mask_7: 0.1287  loss_dice_7: 0.298  loss_ce_8: 0.002511  loss_mask_8: 0.1231  loss_dice_8: 0.2886  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:37] d2.utils.events INFO:  eta: 0:16:43  iter: 42019  total_loss: 4.081  loss_ce: 0.001054  loss_mask: 0.1664  loss_dice: 0.2122  loss_ce_0: 0.06177  loss_mask_0: 0.1593  loss_dice_0: 0.2157  loss_ce_1: 0.0009574  loss_mask_1: 0.1602  loss_dice_1: 0.2163  loss_ce_2: 0.001041  loss_mask_2: 0.1641  loss_dice_2: 0.2134  loss_ce_3: 0.001003  loss_mask_3: 0.1613  loss_dice_3: 0.2111  loss_ce_4: 0.001028  loss_mask_4: 0.1665  loss_dice_4: 0.2154  loss_ce_5: 0.001042  loss_mask_5: 0.1496  loss_dice_5: 0.2145  loss_ce_6: 0.001098  loss_mask_6: 0.1622  loss_dice_6: 0.2223  loss_ce_7: 0.001272  loss_mask_7: 0.1629  loss_dice_7: 0.2111  loss_ce_8: 0.001172  loss_mask_8: 0.1671  loss_dice_8: 0.2191  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:35:39] d2.utils.events INFO:  eta: 0:16:41  iter: 42039  total_loss: 4.286  loss_ce: 0.001137  loss_mask: 0.1652  loss_dice: 0.1559  loss_ce_0: 0.0618  loss_mask_0: 0.1702  loss_dice_0: 0.1622  loss_ce_1: 0.001255  loss_mask_1: 0.1777  loss_dice_1: 0.1631  loss_ce_2: 0.001329  loss_mask_2: 0.1754  loss_dice_2: 0.1651  loss_ce_3: 0.001151  loss_mask_3: 0.169  loss_dice_3: 0.1617  loss_ce_4: 0.001308  loss_mask_4: 0.176  loss_dice_4: 0.1606  loss_ce_5: 0.001142  loss_mask_5: 0.1659  loss_dice_5: 0.1477  loss_ce_6: 0.001229  loss_mask_6: 0.169  loss_dice_6: 0.1557  loss_ce_7: 0.001511  loss_mask_7: 0.1713  loss_dice_7: 0.1606  loss_ce_8: 0.001368  loss_mask_8: 0.1648  loss_dice_8: 0.1513  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:35:42] d2.utils.events INFO:  eta: 0:16:39  iter: 42059  total_loss: 4.407  loss_ce: 0.001593  loss_mask: 0.2013  loss_dice: 0.1487  loss_ce_0: 0.06205  loss_mask_0: 0.2021  loss_dice_0: 0.1429  loss_ce_1: 0.002545  loss_mask_1: 0.195  loss_dice_1: 0.1419  loss_ce_2: 0.001956  loss_mask_2: 0.1985  loss_dice_2: 0.1457  loss_ce_3: 0.001636  loss_mask_3: 0.1936  loss_dice_3: 0.1541  loss_ce_4: 0.001749  loss_mask_4: 0.2076  loss_dice_4: 0.1485  loss_ce_5: 0.001727  loss_mask_5: 0.2059  loss_dice_5: 0.1462  loss_ce_6: 0.001759  loss_mask_6: 0.2091  loss_dice_6: 0.1449  loss_ce_7: 0.001694  loss_mask_7: 0.2104  loss_dice_7: 0.1454  loss_ce_8: 0.001701  loss_mask_8: 0.2172  loss_dice_8: 0.1511  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:35:44] d2.utils.events INFO:  eta: 0:16:37  iter: 42079  total_loss: 4.625  loss_ce: 0.001666  loss_mask: 0.1814  loss_dice: 0.1714  loss_ce_0: 0.06183  loss_mask_0: 0.1797  loss_dice_0: 0.1688  loss_ce_1: 0.001119  loss_mask_1: 0.1735  loss_dice_1: 0.1638  loss_ce_2: 0.00129  loss_mask_2: 0.1816  loss_dice_2: 0.1708  loss_ce_3: 0.00177  loss_mask_3: 0.1815  loss_dice_3: 0.1659  loss_ce_4: 0.00128  loss_mask_4: 0.1793  loss_dice_4: 0.1613  loss_ce_5: 0.001358  loss_mask_5: 0.1821  loss_dice_5: 0.1635  loss_ce_6: 0.001697  loss_mask_6: 0.1768  loss_dice_6: 0.1664  loss_ce_7: 0.001404  loss_mask_7: 0.1852  loss_dice_7: 0.1591  loss_ce_8: 0.001458  loss_mask_8: 0.184  loss_dice_8: 0.1671  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:35:47] d2.utils.events INFO:  eta: 0:16:34  iter: 42099  total_loss: 3.722  loss_ce: 0.001078  loss_mask: 0.2194  loss_dice: 0.1379  loss_ce_0: 0.0617  loss_mask_0: 0.2193  loss_dice_0: 0.1378  loss_ce_1: 0.001002  loss_mask_1: 0.2227  loss_dice_1: 0.1401  loss_ce_2: 0.001055  loss_mask_2: 0.225  loss_dice_2: 0.1426  loss_ce_3: 0.001216  loss_mask_3: 0.2224  loss_dice_3: 0.14  loss_ce_4: 0.001088  loss_mask_4: 0.223  loss_dice_4: 0.1434  loss_ce_5: 0.001051  loss_mask_5: 0.2165  loss_dice_5: 0.1361  loss_ce_6: 0.001171  loss_mask_6: 0.2215  loss_dice_6: 0.142  loss_ce_7: 0.001167  loss_mask_7: 0.2159  loss_dice_7: 0.1371  loss_ce_8: 0.001198  loss_mask_8: 0.2236  loss_dice_8: 0.1374  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:50] d2.utils.events INFO:  eta: 0:16:31  iter: 42119  total_loss: 5.265  loss_ce: 0.002393  loss_mask: 0.179  loss_dice: 0.1911  loss_ce_0: 0.06154  loss_mask_0: 0.1745  loss_dice_0: 0.1665  loss_ce_1: 0.003218  loss_mask_1: 0.1716  loss_dice_1: 0.175  loss_ce_2: 0.002537  loss_mask_2: 0.1795  loss_dice_2: 0.1744  loss_ce_3: 0.00216  loss_mask_3: 0.1686  loss_dice_3: 0.1739  loss_ce_4: 0.00267  loss_mask_4: 0.1741  loss_dice_4: 0.1799  loss_ce_5: 0.002581  loss_mask_5: 0.1727  loss_dice_5: 0.1776  loss_ce_6: 0.001728  loss_mask_6: 0.174  loss_dice_6: 0.1735  loss_ce_7: 0.002413  loss_mask_7: 0.1686  loss_dice_7: 0.1801  loss_ce_8: 0.002442  loss_mask_8: 0.1737  loss_dice_8: 0.173  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:52] d2.utils.events INFO:  eta: 0:16:29  iter: 42139  total_loss: 3.987  loss_ce: 0.001226  loss_mask: 0.2131  loss_dice: 0.1617  loss_ce_0: 0.061  loss_mask_0: 0.2182  loss_dice_0: 0.1505  loss_ce_1: 0.0009934  loss_mask_1: 0.2287  loss_dice_1: 0.155  loss_ce_2: 0.001093  loss_mask_2: 0.2186  loss_dice_2: 0.1584  loss_ce_3: 0.00118  loss_mask_3: 0.2206  loss_dice_3: 0.1583  loss_ce_4: 0.001088  loss_mask_4: 0.2219  loss_dice_4: 0.1576  loss_ce_5: 0.00123  loss_mask_5: 0.217  loss_dice_5: 0.1571  loss_ce_6: 0.001148  loss_mask_6: 0.2198  loss_dice_6: 0.1651  loss_ce_7: 0.001167  loss_mask_7: 0.2245  loss_dice_7: 0.158  loss_ce_8: 0.001282  loss_mask_8: 0.2222  loss_dice_8: 0.16  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:55] d2.utils.events INFO:  eta: 0:16:26  iter: 42159  total_loss: 4.995  loss_ce: 0.002096  loss_mask: 0.1779  loss_dice: 0.1792  loss_ce_0: 0.06049  loss_mask_0: 0.1774  loss_dice_0: 0.1784  loss_ce_1: 0.001309  loss_mask_1: 0.1862  loss_dice_1: 0.1883  loss_ce_2: 0.001456  loss_mask_2: 0.176  loss_dice_2: 0.1827  loss_ce_3: 0.001523  loss_mask_3: 0.1896  loss_dice_3: 0.1823  loss_ce_4: 0.001782  loss_mask_4: 0.1874  loss_dice_4: 0.1732  loss_ce_5: 0.001559  loss_mask_5: 0.1842  loss_dice_5: 0.1732  loss_ce_6: 0.001604  loss_mask_6: 0.1865  loss_dice_6: 0.1829  loss_ce_7: 0.001895  loss_mask_7: 0.1824  loss_dice_7: 0.1847  loss_ce_8: 0.001529  loss_mask_8: 0.1775  loss_dice_8: 0.1787  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:35:57] d2.utils.events INFO:  eta: 0:16:24  iter: 42179  total_loss: 4.607  loss_ce: 0.00188  loss_mask: 0.1628  loss_dice: 0.1543  loss_ce_0: 0.06349  loss_mask_0: 0.1687  loss_dice_0: 0.1526  loss_ce_1: 0.002113  loss_mask_1: 0.1624  loss_dice_1: 0.1563  loss_ce_2: 0.001882  loss_mask_2: 0.1663  loss_dice_2: 0.1581  loss_ce_3: 0.002422  loss_mask_3: 0.1751  loss_dice_3: 0.1501  loss_ce_4: 0.002027  loss_mask_4: 0.1699  loss_dice_4: 0.1626  loss_ce_5: 0.00189  loss_mask_5: 0.1545  loss_dice_5: 0.1573  loss_ce_6: 0.002212  loss_mask_6: 0.1628  loss_dice_6: 0.1618  loss_ce_7: 0.002544  loss_mask_7: 0.1621  loss_dice_7: 0.1597  loss_ce_8: 0.002002  loss_mask_8: 0.1544  loss_dice_8: 0.1633  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:00] d2.utils.events INFO:  eta: 0:16:22  iter: 42199  total_loss: 4.046  loss_ce: 0.001824  loss_mask: 0.1889  loss_dice: 0.1492  loss_ce_0: 0.06002  loss_mask_0: 0.1951  loss_dice_0: 0.1489  loss_ce_1: 0.001546  loss_mask_1: 0.1879  loss_dice_1: 0.1549  loss_ce_2: 0.001571  loss_mask_2: 0.1929  loss_dice_2: 0.1514  loss_ce_3: 0.001812  loss_mask_3: 0.1857  loss_dice_3: 0.1477  loss_ce_4: 0.001976  loss_mask_4: 0.1974  loss_dice_4: 0.1445  loss_ce_5: 0.001599  loss_mask_5: 0.183  loss_dice_5: 0.1521  loss_ce_6: 0.001582  loss_mask_6: 0.1889  loss_dice_6: 0.1532  loss_ce_7: 0.001882  loss_mask_7: 0.1872  loss_dice_7: 0.1551  loss_ce_8: 0.001616  loss_mask_8: 0.1864  loss_dice_8: 0.1447  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:02] d2.utils.events INFO:  eta: 0:16:19  iter: 42219  total_loss: 4.386  loss_ce: 0.001389  loss_mask: 0.1853  loss_dice: 0.1676  loss_ce_0: 0.0618  loss_mask_0: 0.18  loss_dice_0: 0.1702  loss_ce_1: 0.001285  loss_mask_1: 0.1797  loss_dice_1: 0.1717  loss_ce_2: 0.001395  loss_mask_2: 0.1862  loss_dice_2: 0.1693  loss_ce_3: 0.001387  loss_mask_3: 0.1901  loss_dice_3: 0.1755  loss_ce_4: 0.001392  loss_mask_4: 0.1883  loss_dice_4: 0.1711  loss_ce_5: 0.001289  loss_mask_5: 0.1904  loss_dice_5: 0.1807  loss_ce_6: 0.001316  loss_mask_6: 0.1762  loss_dice_6: 0.1696  loss_ce_7: 0.001959  loss_mask_7: 0.1917  loss_dice_7: 0.175  loss_ce_8: 0.001417  loss_mask_8: 0.1859  loss_dice_8: 0.1865  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:05] d2.utils.events INFO:  eta: 0:16:17  iter: 42239  total_loss: 4.043  loss_ce: 0.001612  loss_mask: 0.2011  loss_dice: 0.1487  loss_ce_0: 0.05995  loss_mask_0: 0.2117  loss_dice_0: 0.1521  loss_ce_1: 0.001184  loss_mask_1: 0.2052  loss_dice_1: 0.15  loss_ce_2: 0.001311  loss_mask_2: 0.2022  loss_dice_2: 0.1371  loss_ce_3: 0.001519  loss_mask_3: 0.1993  loss_dice_3: 0.1505  loss_ce_4: 0.001343  loss_mask_4: 0.2009  loss_dice_4: 0.1556  loss_ce_5: 0.001224  loss_mask_5: 0.204  loss_dice_5: 0.1524  loss_ce_6: 0.001424  loss_mask_6: 0.2144  loss_dice_6: 0.1465  loss_ce_7: 0.001463  loss_mask_7: 0.1979  loss_dice_7: 0.1511  loss_ce_8: 0.001446  loss_mask_8: 0.214  loss_dice_8: 0.1519  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:07] d2.utils.events INFO:  eta: 0:16:14  iter: 42259  total_loss: 3.882  loss_ce: 0.001371  loss_mask: 0.2021  loss_dice: 0.1477  loss_ce_0: 0.05983  loss_mask_0: 0.2114  loss_dice_0: 0.1501  loss_ce_1: 0.001138  loss_mask_1: 0.2016  loss_dice_1: 0.1549  loss_ce_2: 0.001255  loss_mask_2: 0.1997  loss_dice_2: 0.1488  loss_ce_3: 0.001522  loss_mask_3: 0.2036  loss_dice_3: 0.1537  loss_ce_4: 0.00138  loss_mask_4: 0.2084  loss_dice_4: 0.1563  loss_ce_5: 0.001302  loss_mask_5: 0.1988  loss_dice_5: 0.1476  loss_ce_6: 0.001457  loss_mask_6: 0.2007  loss_dice_6: 0.1516  loss_ce_7: 0.001525  loss_mask_7: 0.212  loss_dice_7: 0.1548  loss_ce_8: 0.001298  loss_mask_8: 0.2031  loss_dice_8: 0.1518  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:10] d2.utils.events INFO:  eta: 0:16:12  iter: 42279  total_loss: 3.701  loss_ce: 0.001819  loss_mask: 0.1788  loss_dice: 0.1828  loss_ce_0: 0.06179  loss_mask_0: 0.1868  loss_dice_0: 0.1767  loss_ce_1: 0.001935  loss_mask_1: 0.1857  loss_dice_1: 0.1838  loss_ce_2: 0.002096  loss_mask_2: 0.1896  loss_dice_2: 0.1701  loss_ce_3: 0.002007  loss_mask_3: 0.1888  loss_dice_3: 0.1762  loss_ce_4: 0.001986  loss_mask_4: 0.1858  loss_dice_4: 0.1677  loss_ce_5: 0.001949  loss_mask_5: 0.1853  loss_dice_5: 0.1759  loss_ce_6: 0.00162  loss_mask_6: 0.1837  loss_dice_6: 0.1835  loss_ce_7: 0.001973  loss_mask_7: 0.1849  loss_dice_7: 0.1768  loss_ce_8: 0.002037  loss_mask_8: 0.1902  loss_dice_8: 0.1778  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:13] d2.utils.events INFO:  eta: 0:16:10  iter: 42299  total_loss: 4.282  loss_ce: 0.002011  loss_mask: 0.1988  loss_dice: 0.1519  loss_ce_0: 0.06178  loss_mask_0: 0.2063  loss_dice_0: 0.1571  loss_ce_1: 0.002215  loss_mask_1: 0.2035  loss_dice_1: 0.1592  loss_ce_2: 0.002156  loss_mask_2: 0.2011  loss_dice_2: 0.151  loss_ce_3: 0.002558  loss_mask_3: 0.2008  loss_dice_3: 0.1637  loss_ce_4: 0.002207  loss_mask_4: 0.206  loss_dice_4: 0.1573  loss_ce_5: 0.002121  loss_mask_5: 0.2068  loss_dice_5: 0.1481  loss_ce_6: 0.002577  loss_mask_6: 0.2095  loss_dice_6: 0.1599  loss_ce_7: 0.002688  loss_mask_7: 0.193  loss_dice_7: 0.1529  loss_ce_8: 0.002279  loss_mask_8: 0.2005  loss_dice_8: 0.1593  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:15] d2.utils.events INFO:  eta: 0:16:08  iter: 42319  total_loss: 4.167  loss_ce: 0.0008838  loss_mask: 0.2106  loss_dice: 0.1707  loss_ce_0: 0.06039  loss_mask_0: 0.2074  loss_dice_0: 0.1645  loss_ce_1: 0.0008481  loss_mask_1: 0.2058  loss_dice_1: 0.1698  loss_ce_2: 0.0008378  loss_mask_2: 0.2058  loss_dice_2: 0.1663  loss_ce_3: 0.0009141  loss_mask_3: 0.2074  loss_dice_3: 0.1702  loss_ce_4: 0.0008868  loss_mask_4: 0.2081  loss_dice_4: 0.1734  loss_ce_5: 0.0008529  loss_mask_5: 0.2085  loss_dice_5: 0.1701  loss_ce_6: 0.001081  loss_mask_6: 0.2113  loss_dice_6: 0.1721  loss_ce_7: 0.001111  loss_mask_7: 0.2092  loss_dice_7: 0.1728  loss_ce_8: 0.0009992  loss_mask_8: 0.2038  loss_dice_8: 0.169  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:36:18] d2.utils.events INFO:  eta: 0:16:05  iter: 42339  total_loss: 4.586  loss_ce: 0.008166  loss_mask: 0.1593  loss_dice: 0.1812  loss_ce_0: 0.06307  loss_mask_0: 0.1487  loss_dice_0: 0.1812  loss_ce_1: 0.008091  loss_mask_1: 0.1556  loss_dice_1: 0.183  loss_ce_2: 0.01062  loss_mask_2: 0.1513  loss_dice_2: 0.181  loss_ce_3: 0.006488  loss_mask_3: 0.1485  loss_dice_3: 0.1831  loss_ce_4: 0.006072  loss_mask_4: 0.1627  loss_dice_4: 0.1858  loss_ce_5: 0.00935  loss_mask_5: 0.1475  loss_dice_5: 0.1895  loss_ce_6: 0.005902  loss_mask_6: 0.1518  loss_dice_6: 0.1822  loss_ce_7: 0.004965  loss_mask_7: 0.1505  loss_dice_7: 0.1811  loss_ce_8: 0.008559  loss_mask_8: 0.1543  loss_dice_8: 0.1844  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:20] d2.utils.events INFO:  eta: 0:16:02  iter: 42359  total_loss: 3.873  loss_ce: 0.001975  loss_mask: 0.2147  loss_dice: 0.1443  loss_ce_0: 0.06062  loss_mask_0: 0.2144  loss_dice_0: 0.1524  loss_ce_1: 0.00121  loss_mask_1: 0.2123  loss_dice_1: 0.1448  loss_ce_2: 0.001619  loss_mask_2: 0.2044  loss_dice_2: 0.1492  loss_ce_3: 0.001675  loss_mask_3: 0.1974  loss_dice_3: 0.151  loss_ce_4: 0.001572  loss_mask_4: 0.2039  loss_dice_4: 0.1482  loss_ce_5: 0.001602  loss_mask_5: 0.1988  loss_dice_5: 0.147  loss_ce_6: 0.001568  loss_mask_6: 0.2054  loss_dice_6: 0.1471  loss_ce_7: 0.001567  loss_mask_7: 0.2033  loss_dice_7: 0.1505  loss_ce_8: 0.001736  loss_mask_8: 0.2064  loss_dice_8: 0.1488  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:23] d2.utils.events INFO:  eta: 0:15:59  iter: 42379  total_loss: 4.277  loss_ce: 0.007993  loss_mask: 0.1411  loss_dice: 0.2629  loss_ce_0: 0.06041  loss_mask_0: 0.1453  loss_dice_0: 0.2616  loss_ce_1: 0.004384  loss_mask_1: 0.1416  loss_dice_1: 0.264  loss_ce_2: 0.004513  loss_mask_2: 0.1408  loss_dice_2: 0.2684  loss_ce_3: 0.006914  loss_mask_3: 0.1452  loss_dice_3: 0.2684  loss_ce_4: 0.005616  loss_mask_4: 0.1418  loss_dice_4: 0.2629  loss_ce_5: 0.005657  loss_mask_5: 0.1402  loss_dice_5: 0.2515  loss_ce_6: 0.006959  loss_mask_6: 0.1415  loss_dice_6: 0.2549  loss_ce_7: 0.005426  loss_mask_7: 0.1435  loss_dice_7: 0.2827  loss_ce_8: 0.004907  loss_mask_8: 0.1397  loss_dice_8: 0.2651  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:36:25] d2.utils.events INFO:  eta: 0:15:57  iter: 42399  total_loss: 3.492  loss_ce: 0.001275  loss_mask: 0.213  loss_dice: 0.1222  loss_ce_0: 0.06326  loss_mask_0: 0.2116  loss_dice_0: 0.1236  loss_ce_1: 0.001293  loss_mask_1: 0.2176  loss_dice_1: 0.1214  loss_ce_2: 0.001399  loss_mask_2: 0.2165  loss_dice_2: 0.1225  loss_ce_3: 0.001163  loss_mask_3: 0.2118  loss_dice_3: 0.1248  loss_ce_4: 0.001275  loss_mask_4: 0.2139  loss_dice_4: 0.1253  loss_ce_5: 0.001244  loss_mask_5: 0.2044  loss_dice_5: 0.1232  loss_ce_6: 0.001305  loss_mask_6: 0.2147  loss_dice_6: 0.1187  loss_ce_7: 0.001814  loss_mask_7: 0.2129  loss_dice_7: 0.1215  loss_ce_8: 0.001376  loss_mask_8: 0.2179  loss_dice_8: 0.121  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:28] d2.utils.events INFO:  eta: 0:15:54  iter: 42419  total_loss: 4.315  loss_ce: 0.001384  loss_mask: 0.1883  loss_dice: 0.2066  loss_ce_0: 0.06057  loss_mask_0: 0.1988  loss_dice_0: 0.2107  loss_ce_1: 0.001259  loss_mask_1: 0.1867  loss_dice_1: 0.1993  loss_ce_2: 0.001236  loss_mask_2: 0.1914  loss_dice_2: 0.2064  loss_ce_3: 0.001396  loss_mask_3: 0.1941  loss_dice_3: 0.1976  loss_ce_4: 0.001368  loss_mask_4: 0.1937  loss_dice_4: 0.1995  loss_ce_5: 0.001165  loss_mask_5: 0.1927  loss_dice_5: 0.2014  loss_ce_6: 0.001445  loss_mask_6: 0.1881  loss_dice_6: 0.2024  loss_ce_7: 0.001707  loss_mask_7: 0.1972  loss_dice_7: 0.1969  loss_ce_8: 0.001232  loss_mask_8: 0.1913  loss_dice_8: 0.1998  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:36:30] d2.utils.events INFO:  eta: 0:15:52  iter: 42439  total_loss: 3.668  loss_ce: 0.00122  loss_mask: 0.2022  loss_dice: 0.1331  loss_ce_0: 0.06339  loss_mask_0: 0.2073  loss_dice_0: 0.1412  loss_ce_1: 0.001107  loss_mask_1: 0.211  loss_dice_1: 0.1383  loss_ce_2: 0.001267  loss_mask_2: 0.2034  loss_dice_2: 0.1361  loss_ce_3: 0.001305  loss_mask_3: 0.1993  loss_dice_3: 0.1352  loss_ce_4: 0.00123  loss_mask_4: 0.2016  loss_dice_4: 0.1385  loss_ce_5: 0.001201  loss_mask_5: 0.21  loss_dice_5: 0.135  loss_ce_6: 0.001266  loss_mask_6: 0.2089  loss_dice_6: 0.1319  loss_ce_7: 0.001442  loss_mask_7: 0.2118  loss_dice_7: 0.1327  loss_ce_8: 0.00132  loss_mask_8: 0.2041  loss_dice_8: 0.1339  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:33] d2.utils.events INFO:  eta: 0:15:49  iter: 42459  total_loss: 3.823  loss_ce: 0.002043  loss_mask: 0.1876  loss_dice: 0.1657  loss_ce_0: 0.06323  loss_mask_0: 0.1828  loss_dice_0: 0.1593  loss_ce_1: 0.002097  loss_mask_1: 0.1818  loss_dice_1: 0.1628  loss_ce_2: 0.001987  loss_mask_2: 0.1839  loss_dice_2: 0.1585  loss_ce_3: 0.002021  loss_mask_3: 0.1913  loss_dice_3: 0.164  loss_ce_4: 0.00229  loss_mask_4: 0.1905  loss_dice_4: 0.1575  loss_ce_5: 0.001895  loss_mask_5: 0.1901  loss_dice_5: 0.1633  loss_ce_6: 0.001763  loss_mask_6: 0.1825  loss_dice_6: 0.163  loss_ce_7: 0.002437  loss_mask_7: 0.1787  loss_dice_7: 0.1709  loss_ce_8: 0.001978  loss_mask_8: 0.175  loss_dice_8: 0.1563  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:35] d2.utils.events INFO:  eta: 0:15:48  iter: 42479  total_loss: 3.508  loss_ce: 0.0009059  loss_mask: 0.212  loss_dice: 0.1298  loss_ce_0: 0.06068  loss_mask_0: 0.22  loss_dice_0: 0.1289  loss_ce_1: 0.001  loss_mask_1: 0.2143  loss_dice_1: 0.1306  loss_ce_2: 0.001023  loss_mask_2: 0.2097  loss_dice_2: 0.1317  loss_ce_3: 0.0009568  loss_mask_3: 0.2121  loss_dice_3: 0.1256  loss_ce_4: 0.001  loss_mask_4: 0.216  loss_dice_4: 0.1326  loss_ce_5: 0.001031  loss_mask_5: 0.2234  loss_dice_5: 0.1255  loss_ce_6: 0.00106  loss_mask_6: 0.2117  loss_dice_6: 0.1352  loss_ce_7: 0.001316  loss_mask_7: 0.2186  loss_dice_7: 0.1286  loss_ce_8: 0.001213  loss_mask_8: 0.2196  loss_dice_8: 0.1257  time: 0.1260  data_time: 0.0014  lr: 1e-05  max_mem: 3105M
[04/13 16:36:38] d2.utils.events INFO:  eta: 0:15:45  iter: 42499  total_loss: 4.437  loss_ce: 0.005436  loss_mask: 0.1467  loss_dice: 0.1942  loss_ce_0: 0.06046  loss_mask_0: 0.1474  loss_dice_0: 0.2025  loss_ce_1: 0.005413  loss_mask_1: 0.1436  loss_dice_1: 0.2007  loss_ce_2: 0.003288  loss_mask_2: 0.1531  loss_dice_2: 0.1998  loss_ce_3: 0.005497  loss_mask_3: 0.1463  loss_dice_3: 0.1991  loss_ce_4: 0.005007  loss_mask_4: 0.1526  loss_dice_4: 0.2019  loss_ce_5: 0.002902  loss_mask_5: 0.1441  loss_dice_5: 0.1986  loss_ce_6: 0.004237  loss_mask_6: 0.1513  loss_dice_6: 0.2012  loss_ce_7: 0.004295  loss_mask_7: 0.1471  loss_dice_7: 0.1958  loss_ce_8: 0.003558  loss_mask_8: 0.1433  loss_dice_8: 0.2028  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:40] d2.utils.events INFO:  eta: 0:15:43  iter: 42519  total_loss: 4.442  loss_ce: 0.001684  loss_mask: 0.1519  loss_dice: 0.1578  loss_ce_0: 0.06178  loss_mask_0: 0.1482  loss_dice_0: 0.1598  loss_ce_1: 0.001793  loss_mask_1: 0.1643  loss_dice_1: 0.1576  loss_ce_2: 0.001572  loss_mask_2: 0.1608  loss_dice_2: 0.1606  loss_ce_3: 0.001979  loss_mask_3: 0.1598  loss_dice_3: 0.16  loss_ce_4: 0.002162  loss_mask_4: 0.155  loss_dice_4: 0.1591  loss_ce_5: 0.001619  loss_mask_5: 0.1574  loss_dice_5: 0.1613  loss_ce_6: 0.001836  loss_mask_6: 0.1583  loss_dice_6: 0.1598  loss_ce_7: 0.002031  loss_mask_7: 0.1659  loss_dice_7: 0.1586  loss_ce_8: 0.001585  loss_mask_8: 0.1637  loss_dice_8: 0.1639  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:36:43] d2.utils.events INFO:  eta: 0:15:40  iter: 42539  total_loss: 3.799  loss_ce: 0.001047  loss_mask: 0.1966  loss_dice: 0.1567  loss_ce_0: 0.06343  loss_mask_0: 0.2152  loss_dice_0: 0.1578  loss_ce_1: 0.001283  loss_mask_1: 0.2052  loss_dice_1: 0.1526  loss_ce_2: 0.0009779  loss_mask_2: 0.2041  loss_dice_2: 0.1586  loss_ce_3: 0.0009664  loss_mask_3: 0.1999  loss_dice_3: 0.1564  loss_ce_4: 0.001395  loss_mask_4: 0.202  loss_dice_4: 0.1581  loss_ce_5: 0.00105  loss_mask_5: 0.2098  loss_dice_5: 0.1618  loss_ce_6: 0.001168  loss_mask_6: 0.2126  loss_dice_6: 0.1581  loss_ce_7: 0.001635  loss_mask_7: 0.2012  loss_dice_7: 0.1597  loss_ce_8: 0.001239  loss_mask_8: 0.198  loss_dice_8: 0.1558  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:46] d2.utils.events INFO:  eta: 0:15:38  iter: 42559  total_loss: 4.044  loss_ce: 0.001261  loss_mask: 0.1769  loss_dice: 0.1449  loss_ce_0: 0.06326  loss_mask_0: 0.1814  loss_dice_0: 0.149  loss_ce_1: 0.001244  loss_mask_1: 0.183  loss_dice_1: 0.1451  loss_ce_2: 0.001667  loss_mask_2: 0.1833  loss_dice_2: 0.1477  loss_ce_3: 0.00112  loss_mask_3: 0.1787  loss_dice_3: 0.1461  loss_ce_4: 0.001494  loss_mask_4: 0.1764  loss_dice_4: 0.1469  loss_ce_5: 0.001501  loss_mask_5: 0.1756  loss_dice_5: 0.1453  loss_ce_6: 0.001189  loss_mask_6: 0.1804  loss_dice_6: 0.1457  loss_ce_7: 0.001381  loss_mask_7: 0.1766  loss_dice_7: 0.1484  loss_ce_8: 0.0014  loss_mask_8: 0.177  loss_dice_8: 0.1492  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:48] d2.utils.events INFO:  eta: 0:15:34  iter: 42579  total_loss: 3.696  loss_ce: 0.001156  loss_mask: 0.2028  loss_dice: 0.1526  loss_ce_0: 0.063  loss_mask_0: 0.2084  loss_dice_0: 0.1518  loss_ce_1: 0.0009788  loss_mask_1: 0.1989  loss_dice_1: 0.1518  loss_ce_2: 0.00113  loss_mask_2: 0.2142  loss_dice_2: 0.1541  loss_ce_3: 0.001104  loss_mask_3: 0.2159  loss_dice_3: 0.1495  loss_ce_4: 0.0009988  loss_mask_4: 0.2143  loss_dice_4: 0.1492  loss_ce_5: 0.0009934  loss_mask_5: 0.2116  loss_dice_5: 0.1565  loss_ce_6: 0.001144  loss_mask_6: 0.2072  loss_dice_6: 0.158  loss_ce_7: 0.001157  loss_mask_7: 0.2147  loss_dice_7: 0.1521  loss_ce_8: 0.001155  loss_mask_8: 0.2042  loss_dice_8: 0.1505  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:51] d2.utils.events INFO:  eta: 0:15:33  iter: 42599  total_loss: 4.355  loss_ce: 0.0012  loss_mask: 0.1729  loss_dice: 0.2093  loss_ce_0: 0.06061  loss_mask_0: 0.1718  loss_dice_0: 0.2022  loss_ce_1: 0.001226  loss_mask_1: 0.1647  loss_dice_1: 0.21  loss_ce_2: 0.00125  loss_mask_2: 0.1565  loss_dice_2: 0.2065  loss_ce_3: 0.001165  loss_mask_3: 0.1657  loss_dice_3: 0.2132  loss_ce_4: 0.001285  loss_mask_4: 0.1625  loss_dice_4: 0.213  loss_ce_5: 0.001075  loss_mask_5: 0.1689  loss_dice_5: 0.2084  loss_ce_6: 0.001255  loss_mask_6: 0.1687  loss_dice_6: 0.2083  loss_ce_7: 0.001334  loss_mask_7: 0.1689  loss_dice_7: 0.1989  loss_ce_8: 0.001211  loss_mask_8: 0.1733  loss_dice_8: 0.2023  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:36:53] d2.utils.events INFO:  eta: 0:15:30  iter: 42619  total_loss: 4.299  loss_ce: 0.001091  loss_mask: 0.1567  loss_dice: 0.1961  loss_ce_0: 0.06109  loss_mask_0: 0.1619  loss_dice_0: 0.2004  loss_ce_1: 0.001048  loss_mask_1: 0.1614  loss_dice_1: 0.2034  loss_ce_2: 0.0009843  loss_mask_2: 0.1646  loss_dice_2: 0.2102  loss_ce_3: 0.001012  loss_mask_3: 0.1683  loss_dice_3: 0.2045  loss_ce_4: 0.001246  loss_mask_4: 0.1628  loss_dice_4: 0.2012  loss_ce_5: 0.000828  loss_mask_5: 0.1733  loss_dice_5: 0.2155  loss_ce_6: 0.00108  loss_mask_6: 0.1627  loss_dice_6: 0.2019  loss_ce_7: 0.00104  loss_mask_7: 0.1673  loss_dice_7: 0.2059  loss_ce_8: 0.001037  loss_mask_8: 0.1734  loss_dice_8: 0.2003  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:36:56] d2.utils.events INFO:  eta: 0:15:27  iter: 42639  total_loss: 4.491  loss_ce: 0.001269  loss_mask: 0.2293  loss_dice: 0.1766  loss_ce_0: 0.06177  loss_mask_0: 0.227  loss_dice_0: 0.175  loss_ce_1: 0.0008727  loss_mask_1: 0.2253  loss_dice_1: 0.1725  loss_ce_2: 0.0008623  loss_mask_2: 0.2224  loss_dice_2: 0.1753  loss_ce_3: 0.0009947  loss_mask_3: 0.229  loss_dice_3: 0.1714  loss_ce_4: 0.0009362  loss_mask_4: 0.2314  loss_dice_4: 0.171  loss_ce_5: 0.0008757  loss_mask_5: 0.2321  loss_dice_5: 0.1743  loss_ce_6: 0.001053  loss_mask_6: 0.2278  loss_dice_6: 0.1764  loss_ce_7: 0.001019  loss_mask_7: 0.2239  loss_dice_7: 0.1695  loss_ce_8: 0.001023  loss_mask_8: 0.226  loss_dice_8: 0.1787  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:36:58] d2.utils.events INFO:  eta: 0:15:25  iter: 42659  total_loss: 3.655  loss_ce: 0.001075  loss_mask: 0.1835  loss_dice: 0.1376  loss_ce_0: 0.06193  loss_mask_0: 0.1731  loss_dice_0: 0.1336  loss_ce_1: 0.001286  loss_mask_1: 0.1768  loss_dice_1: 0.129  loss_ce_2: 0.00116  loss_mask_2: 0.1777  loss_dice_2: 0.1359  loss_ce_3: 0.001135  loss_mask_3: 0.1802  loss_dice_3: 0.1333  loss_ce_4: 0.001072  loss_mask_4: 0.174  loss_dice_4: 0.1356  loss_ce_5: 0.00102  loss_mask_5: 0.1852  loss_dice_5: 0.1374  loss_ce_6: 0.001186  loss_mask_6: 0.1849  loss_dice_6: 0.1414  loss_ce_7: 0.001284  loss_mask_7: 0.1768  loss_dice_7: 0.1327  loss_ce_8: 0.001242  loss_mask_8: 0.1708  loss_dice_8: 0.137  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:01] d2.utils.events INFO:  eta: 0:15:23  iter: 42679  total_loss: 4.039  loss_ce: 0.0007157  loss_mask: 0.2254  loss_dice: 0.1582  loss_ce_0: 0.06179  loss_mask_0: 0.2177  loss_dice_0: 0.151  loss_ce_1: 0.0008356  loss_mask_1: 0.2284  loss_dice_1: 0.162  loss_ce_2: 0.0007369  loss_mask_2: 0.2299  loss_dice_2: 0.1645  loss_ce_3: 0.0008662  loss_mask_3: 0.2252  loss_dice_3: 0.1584  loss_ce_4: 0.0008678  loss_mask_4: 0.2202  loss_dice_4: 0.1565  loss_ce_5: 0.000875  loss_mask_5: 0.2271  loss_dice_5: 0.159  loss_ce_6: 0.0009699  loss_mask_6: 0.2216  loss_dice_6: 0.1649  loss_ce_7: 0.001069  loss_mask_7: 0.2159  loss_dice_7: 0.1607  loss_ce_8: 0.0009954  loss_mask_8: 0.2208  loss_dice_8: 0.157  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:03] d2.utils.events INFO:  eta: 0:15:21  iter: 42699  total_loss: 4.334  loss_ce: 0.001166  loss_mask: 0.1956  loss_dice: 0.1841  loss_ce_0: 0.06193  loss_mask_0: 0.1761  loss_dice_0: 0.1723  loss_ce_1: 0.000832  loss_mask_1: 0.1803  loss_dice_1: 0.1841  loss_ce_2: 0.0007707  loss_mask_2: 0.196  loss_dice_2: 0.1815  loss_ce_3: 0.00109  loss_mask_3: 0.1878  loss_dice_3: 0.1839  loss_ce_4: 0.0009002  loss_mask_4: 0.1827  loss_dice_4: 0.1798  loss_ce_5: 0.0007665  loss_mask_5: 0.1828  loss_dice_5: 0.1733  loss_ce_6: 0.001112  loss_mask_6: 0.181  loss_dice_6: 0.1792  loss_ce_7: 0.001123  loss_mask_7: 0.1868  loss_dice_7: 0.1751  loss_ce_8: 0.0009481  loss_mask_8: 0.1871  loss_dice_8: 0.1782  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:06] d2.utils.events INFO:  eta: 0:15:19  iter: 42719  total_loss: 4.463  loss_ce: 0.001055  loss_mask: 0.2257  loss_dice: 0.1555  loss_ce_0: 0.06194  loss_mask_0: 0.2241  loss_dice_0: 0.1579  loss_ce_1: 0.001017  loss_mask_1: 0.2168  loss_dice_1: 0.1531  loss_ce_2: 0.00111  loss_mask_2: 0.2293  loss_dice_2: 0.1571  loss_ce_3: 0.0008781  loss_mask_3: 0.2269  loss_dice_3: 0.1521  loss_ce_4: 0.001177  loss_mask_4: 0.2185  loss_dice_4: 0.148  loss_ce_5: 0.001054  loss_mask_5: 0.2142  loss_dice_5: 0.1565  loss_ce_6: 0.0009713  loss_mask_6: 0.2117  loss_dice_6: 0.1583  loss_ce_7: 0.001206  loss_mask_7: 0.2154  loss_dice_7: 0.1553  loss_ce_8: 0.00128  loss_mask_8: 0.225  loss_dice_8: 0.1567  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:08] d2.utils.events INFO:  eta: 0:15:17  iter: 42739  total_loss: 3.879  loss_ce: 0.0007724  loss_mask: 0.1868  loss_dice: 0.172  loss_ce_0: 0.06201  loss_mask_0: 0.1967  loss_dice_0: 0.167  loss_ce_1: 0.0007519  loss_mask_1: 0.1909  loss_dice_1: 0.1676  loss_ce_2: 0.0007887  loss_mask_2: 0.1901  loss_dice_2: 0.1608  loss_ce_3: 0.0007495  loss_mask_3: 0.1949  loss_dice_3: 0.1703  loss_ce_4: 0.0009921  loss_mask_4: 0.1877  loss_dice_4: 0.1662  loss_ce_5: 0.0008364  loss_mask_5: 0.1904  loss_dice_5: 0.1804  loss_ce_6: 0.001065  loss_mask_6: 0.1977  loss_dice_6: 0.1741  loss_ce_7: 0.00105  loss_mask_7: 0.1864  loss_dice_7: 0.1738  loss_ce_8: 0.001038  loss_mask_8: 0.1968  loss_dice_8: 0.1728  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:11] d2.utils.events INFO:  eta: 0:15:14  iter: 42759  total_loss: 4.02  loss_ce: 0.0008232  loss_mask: 0.1883  loss_dice: 0.1627  loss_ce_0: 0.06188  loss_mask_0: 0.1948  loss_dice_0: 0.1617  loss_ce_1: 0.0007605  loss_mask_1: 0.1979  loss_dice_1: 0.1645  loss_ce_2: 0.0007614  loss_mask_2: 0.1926  loss_dice_2: 0.1611  loss_ce_3: 0.0008233  loss_mask_3: 0.1893  loss_dice_3: 0.16  loss_ce_4: 0.0008647  loss_mask_4: 0.1902  loss_dice_4: 0.1663  loss_ce_5: 0.0008224  loss_mask_5: 0.2048  loss_dice_5: 0.1717  loss_ce_6: 0.0009198  loss_mask_6: 0.2055  loss_dice_6: 0.1584  loss_ce_7: 0.001063  loss_mask_7: 0.1941  loss_dice_7: 0.1582  loss_ce_8: 0.001021  loss_mask_8: 0.1955  loss_dice_8: 0.162  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:13] d2.utils.events INFO:  eta: 0:15:11  iter: 42779  total_loss: 3.629  loss_ce: 0.0006226  loss_mask: 0.1798  loss_dice: 0.1648  loss_ce_0: 0.06176  loss_mask_0: 0.1715  loss_dice_0: 0.1648  loss_ce_1: 0.000713  loss_mask_1: 0.181  loss_dice_1: 0.1704  loss_ce_2: 0.0005161  loss_mask_2: 0.1783  loss_dice_2: 0.1732  loss_ce_3: 0.0006409  loss_mask_3: 0.1787  loss_dice_3: 0.1641  loss_ce_4: 0.0007326  loss_mask_4: 0.1793  loss_dice_4: 0.1684  loss_ce_5: 0.0006699  loss_mask_5: 0.1855  loss_dice_5: 0.1687  loss_ce_6: 0.0007908  loss_mask_6: 0.1837  loss_dice_6: 0.1669  loss_ce_7: 0.0008565  loss_mask_7: 0.1912  loss_dice_7: 0.1637  loss_ce_8: 0.0008971  loss_mask_8: 0.1873  loss_dice_8: 0.1696  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:16] d2.utils.events INFO:  eta: 0:15:09  iter: 42799  total_loss: 4.804  loss_ce: 0.0009209  loss_mask: 0.1796  loss_dice: 0.2327  loss_ce_0: 0.06191  loss_mask_0: 0.1786  loss_dice_0: 0.2233  loss_ce_1: 0.0007638  loss_mask_1: 0.1837  loss_dice_1: 0.2243  loss_ce_2: 0.0007184  loss_mask_2: 0.1863  loss_dice_2: 0.2396  loss_ce_3: 0.000828  loss_mask_3: 0.188  loss_dice_3: 0.2323  loss_ce_4: 0.0007288  loss_mask_4: 0.1751  loss_dice_4: 0.2256  loss_ce_5: 0.0007494  loss_mask_5: 0.1878  loss_dice_5: 0.2366  loss_ce_6: 0.0008641  loss_mask_6: 0.1883  loss_dice_6: 0.227  loss_ce_7: 0.0006602  loss_mask_7: 0.187  loss_dice_7: 0.2297  loss_ce_8: 0.001005  loss_mask_8: 0.179  loss_dice_8: 0.2194  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:37:19] d2.utils.events INFO:  eta: 0:15:06  iter: 42819  total_loss: 4.13  loss_ce: 0.0005501  loss_mask: 0.2081  loss_dice: 0.1802  loss_ce_0: 0.06173  loss_mask_0: 0.2032  loss_dice_0: 0.1834  loss_ce_1: 0.0006029  loss_mask_1: 0.2067  loss_dice_1: 0.1839  loss_ce_2: 0.0004308  loss_mask_2: 0.213  loss_dice_2: 0.1851  loss_ce_3: 0.0005437  loss_mask_3: 0.2121  loss_dice_3: 0.1915  loss_ce_4: 0.0005836  loss_mask_4: 0.1999  loss_dice_4: 0.1866  loss_ce_5: 0.0004622  loss_mask_5: 0.2067  loss_dice_5: 0.1844  loss_ce_6: 0.0007433  loss_mask_6: 0.2154  loss_dice_6: 0.1831  loss_ce_7: 0.0006758  loss_mask_7: 0.208  loss_dice_7: 0.1824  loss_ce_8: 0.0007192  loss_mask_8: 0.1965  loss_dice_8: 0.1815  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:21] d2.utils.events INFO:  eta: 0:15:05  iter: 42839  total_loss: 4.71  loss_ce: 0.0007991  loss_mask: 0.2102  loss_dice: 0.1849  loss_ce_0: 0.06122  loss_mask_0: 0.2063  loss_dice_0: 0.1864  loss_ce_1: 0.000795  loss_mask_1: 0.1988  loss_dice_1: 0.186  loss_ce_2: 0.0006054  loss_mask_2: 0.2083  loss_dice_2: 0.2057  loss_ce_3: 0.000736  loss_mask_3: 0.2008  loss_dice_3: 0.1953  loss_ce_4: 0.0007713  loss_mask_4: 0.2069  loss_dice_4: 0.1979  loss_ce_5: 0.0006046  loss_mask_5: 0.2088  loss_dice_5: 0.2032  loss_ce_6: 0.0008622  loss_mask_6: 0.2122  loss_dice_6: 0.1901  loss_ce_7: 0.0007289  loss_mask_7: 0.1985  loss_dice_7: 0.1992  loss_ce_8: 0.0007985  loss_mask_8: 0.2017  loss_dice_8: 0.2054  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:24] d2.utils.events INFO:  eta: 0:15:02  iter: 42859  total_loss: 4.763  loss_ce: 0.001087  loss_mask: 0.1854  loss_dice: 0.1911  loss_ce_0: 0.06054  loss_mask_0: 0.1944  loss_dice_0: 0.1898  loss_ce_1: 0.0009464  loss_mask_1: 0.1831  loss_dice_1: 0.1875  loss_ce_2: 0.0007288  loss_mask_2: 0.1949  loss_dice_2: 0.1889  loss_ce_3: 0.001011  loss_mask_3: 0.1967  loss_dice_3: 0.1874  loss_ce_4: 0.00145  loss_mask_4: 0.1924  loss_dice_4: 0.189  loss_ce_5: 0.0008659  loss_mask_5: 0.1895  loss_dice_5: 0.1881  loss_ce_6: 0.001248  loss_mask_6: 0.1875  loss_dice_6: 0.1838  loss_ce_7: 0.002023  loss_mask_7: 0.1894  loss_dice_7: 0.1819  loss_ce_8: 0.001078  loss_mask_8: 0.1886  loss_dice_8: 0.1826  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:26] d2.utils.events INFO:  eta: 0:15:01  iter: 42879  total_loss: 4.544  loss_ce: 0.0006134  loss_mask: 0.2123  loss_dice: 0.1945  loss_ce_0: 0.06021  loss_mask_0: 0.2265  loss_dice_0: 0.1958  loss_ce_1: 0.0006433  loss_mask_1: 0.234  loss_dice_1: 0.2011  loss_ce_2: 0.0004269  loss_mask_2: 0.224  loss_dice_2: 0.2012  loss_ce_3: 0.0005958  loss_mask_3: 0.2081  loss_dice_3: 0.1957  loss_ce_4: 0.0006039  loss_mask_4: 0.2192  loss_dice_4: 0.1909  loss_ce_5: 0.0004454  loss_mask_5: 0.2276  loss_dice_5: 0.2027  loss_ce_6: 0.0007152  loss_mask_6: 0.2249  loss_dice_6: 0.1936  loss_ce_7: 0.0006397  loss_mask_7: 0.2409  loss_dice_7: 0.2007  loss_ce_8: 0.0005955  loss_mask_8: 0.2276  loss_dice_8: 0.1984  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:37:29] d2.utils.events INFO:  eta: 0:14:58  iter: 42899  total_loss: 4.665  loss_ce: 0.0009523  loss_mask: 0.2049  loss_dice: 0.1891  loss_ce_0: 0.0639  loss_mask_0: 0.2067  loss_dice_0: 0.1936  loss_ce_1: 0.001238  loss_mask_1: 0.2084  loss_dice_1: 0.194  loss_ce_2: 0.0009493  loss_mask_2: 0.2087  loss_dice_2: 0.2003  loss_ce_3: 0.000993  loss_mask_3: 0.2115  loss_dice_3: 0.1943  loss_ce_4: 0.001287  loss_mask_4: 0.2039  loss_dice_4: 0.201  loss_ce_5: 0.0008951  loss_mask_5: 0.204  loss_dice_5: 0.1991  loss_ce_6: 0.001229  loss_mask_6: 0.2007  loss_dice_6: 0.1904  loss_ce_7: 0.001309  loss_mask_7: 0.2104  loss_dice_7: 0.1923  loss_ce_8: 0.001097  loss_mask_8: 0.2014  loss_dice_8: 0.2044  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:37:31] d2.utils.events INFO:  eta: 0:14:56  iter: 42919  total_loss: 3.835  loss_ce: 0.0005448  loss_mask: 0.1766  loss_dice: 0.1536  loss_ce_0: 0.05931  loss_mask_0: 0.1844  loss_dice_0: 0.1574  loss_ce_1: 0.0005961  loss_mask_1: 0.1756  loss_dice_1: 0.1611  loss_ce_2: 0.0004217  loss_mask_2: 0.1833  loss_dice_2: 0.1608  loss_ce_3: 0.0004964  loss_mask_3: 0.18  loss_dice_3: 0.1597  loss_ce_4: 0.0005821  loss_mask_4: 0.1822  loss_dice_4: 0.1594  loss_ce_5: 0.0005498  loss_mask_5: 0.1813  loss_dice_5: 0.1541  loss_ce_6: 0.0006653  loss_mask_6: 0.1724  loss_dice_6: 0.1568  loss_ce_7: 0.0007884  loss_mask_7: 0.1855  loss_dice_7: 0.1592  loss_ce_8: 0.0008386  loss_mask_8: 0.1822  loss_dice_8: 0.1577  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:34] d2.utils.events INFO:  eta: 0:14:54  iter: 42939  total_loss: 3.962  loss_ce: 0.001062  loss_mask: 0.1858  loss_dice: 0.1515  loss_ce_0: 0.05891  loss_mask_0: 0.1919  loss_dice_0: 0.1545  loss_ce_1: 0.0006299  loss_mask_1: 0.1865  loss_dice_1: 0.1511  loss_ce_2: 0.0006423  loss_mask_2: 0.1864  loss_dice_2: 0.152  loss_ce_3: 0.0008197  loss_mask_3: 0.1826  loss_dice_3: 0.1598  loss_ce_4: 0.0008702  loss_mask_4: 0.1887  loss_dice_4: 0.1444  loss_ce_5: 0.0007819  loss_mask_5: 0.1842  loss_dice_5: 0.1534  loss_ce_6: 0.00101  loss_mask_6: 0.1864  loss_dice_6: 0.1503  loss_ce_7: 0.001125  loss_mask_7: 0.1893  loss_dice_7: 0.1476  loss_ce_8: 0.0009915  loss_mask_8: 0.1909  loss_dice_8: 0.1514  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:36] d2.utils.events INFO:  eta: 0:14:52  iter: 42959  total_loss: 4.336  loss_ce: 0.0005627  loss_mask: 0.1966  loss_dice: 0.2009  loss_ce_0: 0.05881  loss_mask_0: 0.1995  loss_dice_0: 0.2017  loss_ce_1: 0.0005219  loss_mask_1: 0.1856  loss_dice_1: 0.1941  loss_ce_2: 0.0003738  loss_mask_2: 0.1872  loss_dice_2: 0.1968  loss_ce_3: 0.000487  loss_mask_3: 0.1921  loss_dice_3: 0.1967  loss_ce_4: 0.0005452  loss_mask_4: 0.1889  loss_dice_4: 0.1964  loss_ce_5: 0.0004697  loss_mask_5: 0.1887  loss_dice_5: 0.1975  loss_ce_6: 0.0006672  loss_mask_6: 0.185  loss_dice_6: 0.2063  loss_ce_7: 0.00071  loss_mask_7: 0.1802  loss_dice_7: 0.1908  loss_ce_8: 0.0006963  loss_mask_8: 0.1856  loss_dice_8: 0.1974  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:39] d2.utils.events INFO:  eta: 0:14:50  iter: 42979  total_loss: 4.242  loss_ce: 0.0005701  loss_mask: 0.1803  loss_dice: 0.1878  loss_ce_0: 0.0588  loss_mask_0: 0.1923  loss_dice_0: 0.1877  loss_ce_1: 0.0006637  loss_mask_1: 0.1792  loss_dice_1: 0.1922  loss_ce_2: 0.0004805  loss_mask_2: 0.187  loss_dice_2: 0.1923  loss_ce_3: 0.0005572  loss_mask_3: 0.1994  loss_dice_3: 0.1942  loss_ce_4: 0.000704  loss_mask_4: 0.1939  loss_dice_4: 0.1918  loss_ce_5: 0.0005342  loss_mask_5: 0.1894  loss_dice_5: 0.1905  loss_ce_6: 0.0007065  loss_mask_6: 0.1986  loss_dice_6: 0.1931  loss_ce_7: 0.0008252  loss_mask_7: 0.1959  loss_dice_7: 0.1919  loss_ce_8: 0.0007726  loss_mask_8: 0.1852  loss_dice_8: 0.1867  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:42] d2.utils.events INFO:  eta: 0:14:47  iter: 42999  total_loss: 4.781  loss_ce: 0.006014  loss_mask: 0.1777  loss_dice: 0.2141  loss_ce_0: 0.05845  loss_mask_0: 0.1807  loss_dice_0: 0.2055  loss_ce_1: 0.00145  loss_mask_1: 0.1772  loss_dice_1: 0.215  loss_ce_2: 0.00259  loss_mask_2: 0.1727  loss_dice_2: 0.2147  loss_ce_3: 0.005125  loss_mask_3: 0.1768  loss_dice_3: 0.2199  loss_ce_4: 0.00264  loss_mask_4: 0.1753  loss_dice_4: 0.2196  loss_ce_5: 0.002246  loss_mask_5: 0.173  loss_dice_5: 0.2215  loss_ce_6: 0.004354  loss_mask_6: 0.169  loss_dice_6: 0.2077  loss_ce_7: 0.002769  loss_mask_7: 0.1708  loss_dice_7: 0.2196  loss_ce_8: 0.002635  loss_mask_8: 0.1694  loss_dice_8: 0.2102  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:37:44] d2.utils.events INFO:  eta: 0:14:45  iter: 43019  total_loss: 3.95  loss_ce: 0.0006754  loss_mask: 0.2169  loss_dice: 0.1583  loss_ce_0: 0.0582  loss_mask_0: 0.2172  loss_dice_0: 0.1559  loss_ce_1: 0.0005917  loss_mask_1: 0.2144  loss_dice_1: 0.16  loss_ce_2: 0.0004157  loss_mask_2: 0.2199  loss_dice_2: 0.1569  loss_ce_3: 0.0005933  loss_mask_3: 0.2172  loss_dice_3: 0.1622  loss_ce_4: 0.0005607  loss_mask_4: 0.2216  loss_dice_4: 0.1654  loss_ce_5: 0.0004889  loss_mask_5: 0.2227  loss_dice_5: 0.171  loss_ce_6: 0.0007015  loss_mask_6: 0.2161  loss_dice_6: 0.1654  loss_ce_7: 0.0006183  loss_mask_7: 0.2221  loss_dice_7: 0.1619  loss_ce_8: 0.0007613  loss_mask_8: 0.2131  loss_dice_8: 0.1631  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:47] d2.utils.events INFO:  eta: 0:14:42  iter: 43039  total_loss: 3.685  loss_ce: 0.0007608  loss_mask: 0.2097  loss_dice: 0.1486  loss_ce_0: 0.06187  loss_mask_0: 0.2032  loss_dice_0: 0.1499  loss_ce_1: 0.0007121  loss_mask_1: 0.208  loss_dice_1: 0.1544  loss_ce_2: 0.0006008  loss_mask_2: 0.2044  loss_dice_2: 0.1448  loss_ce_3: 0.0007143  loss_mask_3: 0.1997  loss_dice_3: 0.1503  loss_ce_4: 0.0007646  loss_mask_4: 0.1968  loss_dice_4: 0.1442  loss_ce_5: 0.0006362  loss_mask_5: 0.2111  loss_dice_5: 0.149  loss_ce_6: 0.0008507  loss_mask_6: 0.2116  loss_dice_6: 0.1464  loss_ce_7: 0.0008847  loss_mask_7: 0.2052  loss_dice_7: 0.1482  loss_ce_8: 0.0008748  loss_mask_8: 0.2039  loss_dice_8: 0.1485  time: 0.1260  data_time: 0.0014  lr: 1e-05  max_mem: 3105M
[04/13 16:37:49] d2.utils.events INFO:  eta: 0:14:40  iter: 43059  total_loss: 4.014  loss_ce: 0.000688  loss_mask: 0.2193  loss_dice: 0.1576  loss_ce_0: 0.05804  loss_mask_0: 0.2274  loss_dice_0: 0.1519  loss_ce_1: 0.0006171  loss_mask_1: 0.2324  loss_dice_1: 0.1502  loss_ce_2: 0.0006358  loss_mask_2: 0.2251  loss_dice_2: 0.1519  loss_ce_3: 0.0006609  loss_mask_3: 0.2254  loss_dice_3: 0.148  loss_ce_4: 0.0006915  loss_mask_4: 0.2289  loss_dice_4: 0.1559  loss_ce_5: 0.0006897  loss_mask_5: 0.2201  loss_dice_5: 0.1549  loss_ce_6: 0.0008568  loss_mask_6: 0.2209  loss_dice_6: 0.1547  loss_ce_7: 0.0009011  loss_mask_7: 0.224  loss_dice_7: 0.1548  loss_ce_8: 0.0008634  loss_mask_8: 0.2208  loss_dice_8: 0.1503  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:52] d2.utils.events INFO:  eta: 0:14:38  iter: 43079  total_loss: 3.939  loss_ce: 0.0005936  loss_mask: 0.2144  loss_dice: 0.1448  loss_ce_0: 0.05795  loss_mask_0: 0.2144  loss_dice_0: 0.1482  loss_ce_1: 0.0005848  loss_mask_1: 0.2139  loss_dice_1: 0.1531  loss_ce_2: 0.0004094  loss_mask_2: 0.2073  loss_dice_2: 0.1491  loss_ce_3: 0.000559  loss_mask_3: 0.2088  loss_dice_3: 0.1462  loss_ce_4: 0.0005907  loss_mask_4: 0.2049  loss_dice_4: 0.154  loss_ce_5: 0.0004899  loss_mask_5: 0.217  loss_dice_5: 0.1491  loss_ce_6: 0.0006969  loss_mask_6: 0.2181  loss_dice_6: 0.1592  loss_ce_7: 0.0007577  loss_mask_7: 0.2123  loss_dice_7: 0.144  loss_ce_8: 0.0007649  loss_mask_8: 0.2053  loss_dice_8: 0.1476  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:54] d2.utils.events INFO:  eta: 0:14:35  iter: 43099  total_loss: 3.995  loss_ce: 0.0007347  loss_mask: 0.1346  loss_dice: 0.1845  loss_ce_0: 0.05818  loss_mask_0: 0.1332  loss_dice_0: 0.1821  loss_ce_1: 0.0006357  loss_mask_1: 0.1381  loss_dice_1: 0.1945  loss_ce_2: 0.0005912  loss_mask_2: 0.1477  loss_dice_2: 0.1824  loss_ce_3: 0.0006356  loss_mask_3: 0.1464  loss_dice_3: 0.1991  loss_ce_4: 0.0008041  loss_mask_4: 0.1403  loss_dice_4: 0.1856  loss_ce_5: 0.0007348  loss_mask_5: 0.1338  loss_dice_5: 0.1864  loss_ce_6: 0.0007409  loss_mask_6: 0.1449  loss_dice_6: 0.1921  loss_ce_7: 0.0008479  loss_mask_7: 0.1336  loss_dice_7: 0.1943  loss_ce_8: 0.0008236  loss_mask_8: 0.1421  loss_dice_8: 0.1866  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:57] d2.utils.events INFO:  eta: 0:14:33  iter: 43119  total_loss: 3.645  loss_ce: 0.0006354  loss_mask: 0.2159  loss_dice: 0.1334  loss_ce_0: 0.06187  loss_mask_0: 0.2239  loss_dice_0: 0.1369  loss_ce_1: 0.0005605  loss_mask_1: 0.2199  loss_dice_1: 0.1376  loss_ce_2: 0.0004623  loss_mask_2: 0.2185  loss_dice_2: 0.1315  loss_ce_3: 0.0005219  loss_mask_3: 0.2209  loss_dice_3: 0.1378  loss_ce_4: 0.0006204  loss_mask_4: 0.2187  loss_dice_4: 0.1302  loss_ce_5: 0.000505  loss_mask_5: 0.212  loss_dice_5: 0.1354  loss_ce_6: 0.0006833  loss_mask_6: 0.2227  loss_dice_6: 0.1358  loss_ce_7: 0.0007292  loss_mask_7: 0.2068  loss_dice_7: 0.1314  loss_ce_8: 0.0007246  loss_mask_8: 0.2203  loss_dice_8: 0.1331  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:37:59] d2.utils.events INFO:  eta: 0:14:30  iter: 43139  total_loss: 4.344  loss_ce: 0.0007793  loss_mask: 0.1852  loss_dice: 0.2436  loss_ce_0: 0.05833  loss_mask_0: 0.1819  loss_dice_0: 0.2311  loss_ce_1: 0.0009115  loss_mask_1: 0.183  loss_dice_1: 0.2522  loss_ce_2: 0.001026  loss_mask_2: 0.1839  loss_dice_2: 0.2375  loss_ce_3: 0.0006893  loss_mask_3: 0.187  loss_dice_3: 0.2509  loss_ce_4: 0.0009574  loss_mask_4: 0.1782  loss_dice_4: 0.2352  loss_ce_5: 0.001059  loss_mask_5: 0.1937  loss_dice_5: 0.246  loss_ce_6: 0.0008143  loss_mask_6: 0.1832  loss_dice_6: 0.2503  loss_ce_7: 0.0008593  loss_mask_7: 0.1868  loss_dice_7: 0.2418  loss_ce_8: 0.001115  loss_mask_8: 0.1943  loss_dice_8: 0.2487  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:02] d2.utils.events INFO:  eta: 0:14:28  iter: 43159  total_loss: 3.785  loss_ce: 0.0007431  loss_mask: 0.1796  loss_dice: 0.1757  loss_ce_0: 0.06537  loss_mask_0: 0.1788  loss_dice_0: 0.1728  loss_ce_1: 0.0006608  loss_mask_1: 0.1845  loss_dice_1: 0.1773  loss_ce_2: 0.0005349  loss_mask_2: 0.176  loss_dice_2: 0.1772  loss_ce_3: 0.0006195  loss_mask_3: 0.183  loss_dice_3: 0.1761  loss_ce_4: 0.0006984  loss_mask_4: 0.1868  loss_dice_4: 0.1803  loss_ce_5: 0.0006211  loss_mask_5: 0.1865  loss_dice_5: 0.183  loss_ce_6: 0.0007524  loss_mask_6: 0.1824  loss_dice_6: 0.1671  loss_ce_7: 0.0008351  loss_mask_7: 0.1863  loss_dice_7: 0.1703  loss_ce_8: 0.0008597  loss_mask_8: 0.1848  loss_dice_8: 0.1721  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:04] d2.utils.events INFO:  eta: 0:14:25  iter: 43179  total_loss: 3.61  loss_ce: 0.000518  loss_mask: 0.1869  loss_dice: 0.1552  loss_ce_0: 0.06484  loss_mask_0: 0.2012  loss_dice_0: 0.1566  loss_ce_1: 0.0004864  loss_mask_1: 0.2076  loss_dice_1: 0.1532  loss_ce_2: 0.0003425  loss_mask_2: 0.2011  loss_dice_2: 0.1521  loss_ce_3: 0.0004404  loss_mask_3: 0.2023  loss_dice_3: 0.1557  loss_ce_4: 0.000505  loss_mask_4: 0.2006  loss_dice_4: 0.154  loss_ce_5: 0.0004616  loss_mask_5: 0.2004  loss_dice_5: 0.1555  loss_ce_6: 0.0005688  loss_mask_6: 0.2001  loss_dice_6: 0.1544  loss_ce_7: 0.0005218  loss_mask_7: 0.202  loss_dice_7: 0.1608  loss_ce_8: 0.0006728  loss_mask_8: 0.2091  loss_dice_8: 0.1509  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:07] d2.utils.events INFO:  eta: 0:14:22  iter: 43199  total_loss: 4.347  loss_ce: 0.0007356  loss_mask: 0.1679  loss_dice: 0.1689  loss_ce_0: 0.06181  loss_mask_0: 0.1722  loss_dice_0: 0.1623  loss_ce_1: 0.001072  loss_mask_1: 0.1689  loss_dice_1: 0.1643  loss_ce_2: 0.0006869  loss_mask_2: 0.1727  loss_dice_2: 0.156  loss_ce_3: 0.0006168  loss_mask_3: 0.1681  loss_dice_3: 0.1667  loss_ce_4: 0.0009437  loss_mask_4: 0.1721  loss_dice_4: 0.1609  loss_ce_5: 0.0007618  loss_mask_5: 0.1712  loss_dice_5: 0.1557  loss_ce_6: 0.0008193  loss_mask_6: 0.1665  loss_dice_6: 0.1623  loss_ce_7: 0.0009606  loss_mask_7: 0.1657  loss_dice_7: 0.1665  loss_ce_8: 0.0009709  loss_mask_8: 0.1694  loss_dice_8: 0.1667  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:09] d2.utils.events INFO:  eta: 0:14:20  iter: 43219  total_loss: 4.239  loss_ce: 0.0005716  loss_mask: 0.1945  loss_dice: 0.1875  loss_ce_0: 0.06182  loss_mask_0: 0.1968  loss_dice_0: 0.1926  loss_ce_1: 0.0005509  loss_mask_1: 0.1857  loss_dice_1: 0.1823  loss_ce_2: 0.0003275  loss_mask_2: 0.1884  loss_dice_2: 0.1817  loss_ce_3: 0.0004145  loss_mask_3: 0.1827  loss_dice_3: 0.1833  loss_ce_4: 0.0005738  loss_mask_4: 0.1743  loss_dice_4: 0.1883  loss_ce_5: 0.0004075  loss_mask_5: 0.1934  loss_dice_5: 0.182  loss_ce_6: 0.0005761  loss_mask_6: 0.1958  loss_dice_6: 0.186  loss_ce_7: 0.0006439  loss_mask_7: 0.1932  loss_dice_7: 0.1863  loss_ce_8: 0.0006692  loss_mask_8: 0.1904  loss_dice_8: 0.1892  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:12] d2.utils.events INFO:  eta: 0:14:17  iter: 43239  total_loss: 4.551  loss_ce: 0.0006999  loss_mask: 0.1528  loss_dice: 0.2004  loss_ce_0: 0.06178  loss_mask_0: 0.1501  loss_dice_0: 0.207  loss_ce_1: 0.0004778  loss_mask_1: 0.1474  loss_dice_1: 0.2038  loss_ce_2: 0.0003688  loss_mask_2: 0.1636  loss_dice_2: 0.2022  loss_ce_3: 0.0005214  loss_mask_3: 0.1651  loss_dice_3: 0.1967  loss_ce_4: 0.0005514  loss_mask_4: 0.1551  loss_dice_4: 0.2022  loss_ce_5: 0.0004864  loss_mask_5: 0.1597  loss_dice_5: 0.2001  loss_ce_6: 0.0007149  loss_mask_6: 0.159  loss_dice_6: 0.2016  loss_ce_7: 0.0005755  loss_mask_7: 0.1641  loss_dice_7: 0.2009  loss_ce_8: 0.000737  loss_mask_8: 0.1466  loss_dice_8: 0.2003  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:14] d2.utils.events INFO:  eta: 0:14:14  iter: 43259  total_loss: 3.849  loss_ce: 0.0004681  loss_mask: 0.2021  loss_dice: 0.1429  loss_ce_0: 0.06011  loss_mask_0: 0.2028  loss_dice_0: 0.1364  loss_ce_1: 0.0005447  loss_mask_1: 0.208  loss_dice_1: 0.1382  loss_ce_2: 0.0003478  loss_mask_2: 0.212  loss_dice_2: 0.1415  loss_ce_3: 0.0005382  loss_mask_3: 0.2049  loss_dice_3: 0.1342  loss_ce_4: 0.0005544  loss_mask_4: 0.2037  loss_dice_4: 0.1409  loss_ce_5: 0.0004076  loss_mask_5: 0.2085  loss_dice_5: 0.1374  loss_ce_6: 0.000642  loss_mask_6: 0.225  loss_dice_6: 0.1403  loss_ce_7: 0.0004792  loss_mask_7: 0.208  loss_dice_7: 0.1398  loss_ce_8: 0.0006055  loss_mask_8: 0.2098  loss_dice_8: 0.1412  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:17] d2.utils.events INFO:  eta: 0:14:11  iter: 43279  total_loss: 3.875  loss_ce: 0.0006271  loss_mask: 0.1726  loss_dice: 0.1777  loss_ce_0: 0.05981  loss_mask_0: 0.1671  loss_dice_0: 0.1771  loss_ce_1: 0.0007025  loss_mask_1: 0.1793  loss_dice_1: 0.184  loss_ce_2: 0.000621  loss_mask_2: 0.1731  loss_dice_2: 0.1788  loss_ce_3: 0.0005713  loss_mask_3: 0.1669  loss_dice_3: 0.1915  loss_ce_4: 0.0006546  loss_mask_4: 0.1681  loss_dice_4: 0.1909  loss_ce_5: 0.000569  loss_mask_5: 0.1695  loss_dice_5: 0.1912  loss_ce_6: 0.0007089  loss_mask_6: 0.1736  loss_dice_6: 0.205  loss_ce_7: 0.0007356  loss_mask_7: 0.156  loss_dice_7: 0.187  loss_ce_8: 0.000777  loss_mask_8: 0.1674  loss_dice_8: 0.1886  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:20] d2.utils.events INFO:  eta: 0:14:09  iter: 43299  total_loss: 4.245  loss_ce: 0.0008575  loss_mask: 0.2052  loss_dice: 0.1642  loss_ce_0: 0.05935  loss_mask_0: 0.2114  loss_dice_0: 0.1716  loss_ce_1: 0.0006504  loss_mask_1: 0.2029  loss_dice_1: 0.1738  loss_ce_2: 0.0004849  loss_mask_2: 0.2071  loss_dice_2: 0.1735  loss_ce_3: 0.0006555  loss_mask_3: 0.2054  loss_dice_3: 0.1679  loss_ce_4: 0.0006791  loss_mask_4: 0.2078  loss_dice_4: 0.1685  loss_ce_5: 0.0004644  loss_mask_5: 0.2068  loss_dice_5: 0.168  loss_ce_6: 0.0006613  loss_mask_6: 0.2052  loss_dice_6: 0.1763  loss_ce_7: 0.0005691  loss_mask_7: 0.2083  loss_dice_7: 0.1682  loss_ce_8: 0.0006251  loss_mask_8: 0.2157  loss_dice_8: 0.1716  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:22] d2.utils.events INFO:  eta: 0:14:05  iter: 43319  total_loss: 4.427  loss_ce: 0.0005429  loss_mask: 0.2245  loss_dice: 0.1861  loss_ce_0: 0.05887  loss_mask_0: 0.2221  loss_dice_0: 0.1802  loss_ce_1: 0.0005169  loss_mask_1: 0.2238  loss_dice_1: 0.1845  loss_ce_2: 0.0005062  loss_mask_2: 0.2216  loss_dice_2: 0.1756  loss_ce_3: 0.0005569  loss_mask_3: 0.2294  loss_dice_3: 0.1816  loss_ce_4: 0.000477  loss_mask_4: 0.2301  loss_dice_4: 0.18  loss_ce_5: 0.0004598  loss_mask_5: 0.2358  loss_dice_5: 0.1805  loss_ce_6: 0.0006637  loss_mask_6: 0.2173  loss_dice_6: 0.1856  loss_ce_7: 0.0005276  loss_mask_7: 0.2113  loss_dice_7: 0.1884  loss_ce_8: 0.0006491  loss_mask_8: 0.221  loss_dice_8: 0.1892  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:25] d2.utils.events INFO:  eta: 0:14:03  iter: 43339  total_loss: 4.182  loss_ce: 0.0005634  loss_mask: 0.2213  loss_dice: 0.1634  loss_ce_0: 0.05817  loss_mask_0: 0.2203  loss_dice_0: 0.1577  loss_ce_1: 0.0005999  loss_mask_1: 0.2377  loss_dice_1: 0.1554  loss_ce_2: 0.0003951  loss_mask_2: 0.2115  loss_dice_2: 0.1529  loss_ce_3: 0.0005734  loss_mask_3: 0.2238  loss_dice_3: 0.1517  loss_ce_4: 0.0005471  loss_mask_4: 0.2253  loss_dice_4: 0.1615  loss_ce_5: 0.0005159  loss_mask_5: 0.2175  loss_dice_5: 0.1546  loss_ce_6: 0.0006716  loss_mask_6: 0.2219  loss_dice_6: 0.1565  loss_ce_7: 0.0006671  loss_mask_7: 0.2246  loss_dice_7: 0.1516  loss_ce_8: 0.0007368  loss_mask_8: 0.2107  loss_dice_8: 0.1624  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:27] d2.utils.events INFO:  eta: 0:14:01  iter: 43359  total_loss: 3.68  loss_ce: 0.0006973  loss_mask: 0.1372  loss_dice: 0.1906  loss_ce_0: 0.06186  loss_mask_0: 0.1378  loss_dice_0: 0.1912  loss_ce_1: 0.0005896  loss_mask_1: 0.148  loss_dice_1: 0.1925  loss_ce_2: 0.0003543  loss_mask_2: 0.1394  loss_dice_2: 0.1995  loss_ce_3: 0.0005958  loss_mask_3: 0.1484  loss_dice_3: 0.1803  loss_ce_4: 0.0005772  loss_mask_4: 0.1361  loss_dice_4: 0.1851  loss_ce_5: 0.000491  loss_mask_5: 0.1442  loss_dice_5: 0.1969  loss_ce_6: 0.0007525  loss_mask_6: 0.1418  loss_dice_6: 0.1949  loss_ce_7: 0.0007181  loss_mask_7: 0.1492  loss_dice_7: 0.1977  loss_ce_8: 0.0007049  loss_mask_8: 0.148  loss_dice_8: 0.1809  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:30] d2.utils.events INFO:  eta: 0:13:59  iter: 43379  total_loss: 4.109  loss_ce: 0.0007458  loss_mask: 0.1745  loss_dice: 0.1549  loss_ce_0: 0.06488  loss_mask_0: 0.1765  loss_dice_0: 0.1456  loss_ce_1: 0.001087  loss_mask_1: 0.1749  loss_dice_1: 0.1473  loss_ce_2: 0.0007009  loss_mask_2: 0.1772  loss_dice_2: 0.1459  loss_ce_3: 0.0007647  loss_mask_3: 0.1707  loss_dice_3: 0.1475  loss_ce_4: 0.0008887  loss_mask_4: 0.1806  loss_dice_4: 0.1468  loss_ce_5: 0.0008172  loss_mask_5: 0.1743  loss_dice_5: 0.1447  loss_ce_6: 0.0009323  loss_mask_6: 0.1801  loss_dice_6: 0.1454  loss_ce_7: 0.0009342  loss_mask_7: 0.1764  loss_dice_7: 0.1421  loss_ce_8: 0.0009335  loss_mask_8: 0.18  loss_dice_8: 0.1416  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:32] d2.utils.events INFO:  eta: 0:13:57  iter: 43399  total_loss: 4.132  loss_ce: 0.000577  loss_mask: 0.2063  loss_dice: 0.1848  loss_ce_0: 0.05901  loss_mask_0: 0.2026  loss_dice_0: 0.1892  loss_ce_1: 0.0006754  loss_mask_1: 0.2088  loss_dice_1: 0.1868  loss_ce_2: 0.0004828  loss_mask_2: 0.2059  loss_dice_2: 0.1897  loss_ce_3: 0.0005983  loss_mask_3: 0.2076  loss_dice_3: 0.1842  loss_ce_4: 0.0006854  loss_mask_4: 0.2066  loss_dice_4: 0.185  loss_ce_5: 0.0006059  loss_mask_5: 0.2079  loss_dice_5: 0.1844  loss_ce_6: 0.0007748  loss_mask_6: 0.2127  loss_dice_6: 0.1886  loss_ce_7: 0.000714  loss_mask_7: 0.2134  loss_dice_7: 0.1898  loss_ce_8: 0.0007509  loss_mask_8: 0.2109  loss_dice_8: 0.1891  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:38:35] d2.utils.events INFO:  eta: 0:13:54  iter: 43419  total_loss: 4.402  loss_ce: 0.001015  loss_mask: 0.1706  loss_dice: 0.1645  loss_ce_0: 0.06436  loss_mask_0: 0.1704  loss_dice_0: 0.1589  loss_ce_1: 0.001035  loss_mask_1: 0.1681  loss_dice_1: 0.1573  loss_ce_2: 0.0007622  loss_mask_2: 0.169  loss_dice_2: 0.1636  loss_ce_3: 0.001147  loss_mask_3: 0.1743  loss_dice_3: 0.1607  loss_ce_4: 0.0008293  loss_mask_4: 0.1704  loss_dice_4: 0.1604  loss_ce_5: 0.0008345  loss_mask_5: 0.1708  loss_dice_5: 0.162  loss_ce_6: 0.001191  loss_mask_6: 0.1696  loss_dice_6: 0.166  loss_ce_7: 0.0008799  loss_mask_7: 0.1685  loss_dice_7: 0.1642  loss_ce_8: 0.001061  loss_mask_8: 0.1653  loss_dice_8: 0.1613  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:38:37] d2.utils.events INFO:  eta: 0:13:51  iter: 43439  total_loss: 4.103  loss_ce: 0.0006526  loss_mask: 0.1926  loss_dice: 0.1775  loss_ce_0: 0.06397  loss_mask_0: 0.1851  loss_dice_0: 0.1868  loss_ce_1: 0.000582  loss_mask_1: 0.1947  loss_dice_1: 0.1866  loss_ce_2: 0.0004584  loss_mask_2: 0.1902  loss_dice_2: 0.1878  loss_ce_3: 0.0005681  loss_mask_3: 0.1851  loss_dice_3: 0.178  loss_ce_4: 0.0005979  loss_mask_4: 0.186  loss_dice_4: 0.1812  loss_ce_5: 0.0005852  loss_mask_5: 0.1879  loss_dice_5: 0.1802  loss_ce_6: 0.0006939  loss_mask_6: 0.1857  loss_dice_6: 0.1848  loss_ce_7: 0.0007985  loss_mask_7: 0.178  loss_dice_7: 0.1765  loss_ce_8: 0.0008002  loss_mask_8: 0.1858  loss_dice_8: 0.1863  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:38:40] d2.utils.events INFO:  eta: 0:13:48  iter: 43459  total_loss: 4.309  loss_ce: 0.0006776  loss_mask: 0.2107  loss_dice: 0.185  loss_ce_0: 0.05976  loss_mask_0: 0.2119  loss_dice_0: 0.1949  loss_ce_1: 0.0006644  loss_mask_1: 0.2045  loss_dice_1: 0.1912  loss_ce_2: 0.0003852  loss_mask_2: 0.2133  loss_dice_2: 0.1938  loss_ce_3: 0.0005898  loss_mask_3: 0.2131  loss_dice_3: 0.1891  loss_ce_4: 0.0005856  loss_mask_4: 0.2093  loss_dice_4: 0.1962  loss_ce_5: 0.0004937  loss_mask_5: 0.2087  loss_dice_5: 0.1957  loss_ce_6: 0.0006949  loss_mask_6: 0.2069  loss_dice_6: 0.1944  loss_ce_7: 0.000629  loss_mask_7: 0.2033  loss_dice_7: 0.1926  loss_ce_8: 0.000683  loss_mask_8: 0.2103  loss_dice_8: 0.1848  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:38:42] d2.utils.events INFO:  eta: 0:13:45  iter: 43479  total_loss: 4.064  loss_ce: 0.0005462  loss_mask: 0.2139  loss_dice: 0.155  loss_ce_0: 0.06344  loss_mask_0: 0.2122  loss_dice_0: 0.162  loss_ce_1: 0.0005626  loss_mask_1: 0.2174  loss_dice_1: 0.162  loss_ce_2: 0.0003262  loss_mask_2: 0.2258  loss_dice_2: 0.1603  loss_ce_3: 0.0005044  loss_mask_3: 0.219  loss_dice_3: 0.1583  loss_ce_4: 0.0004992  loss_mask_4: 0.2225  loss_dice_4: 0.1594  loss_ce_5: 0.0004985  loss_mask_5: 0.2186  loss_dice_5: 0.1589  loss_ce_6: 0.0006274  loss_mask_6: 0.2267  loss_dice_6: 0.1583  loss_ce_7: 0.0006081  loss_mask_7: 0.2152  loss_dice_7: 0.1615  loss_ce_8: 0.0007093  loss_mask_8: 0.221  loss_dice_8: 0.1623  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:38:45] d2.utils.events INFO:  eta: 0:13:42  iter: 43499  total_loss: 4.471  loss_ce: 0.0007352  loss_mask: 0.1753  loss_dice: 0.2071  loss_ce_0: 0.0601  loss_mask_0: 0.1857  loss_dice_0: 0.1996  loss_ce_1: 0.0007384  loss_mask_1: 0.1754  loss_dice_1: 0.2093  loss_ce_2: 0.0004859  loss_mask_2: 0.182  loss_dice_2: 0.2076  loss_ce_3: 0.0007437  loss_mask_3: 0.1775  loss_dice_3: 0.2136  loss_ce_4: 0.0006426  loss_mask_4: 0.1864  loss_dice_4: 0.2065  loss_ce_5: 0.0006118  loss_mask_5: 0.1823  loss_dice_5: 0.2023  loss_ce_6: 0.000781  loss_mask_6: 0.1861  loss_dice_6: 0.2043  loss_ce_7: 0.0007098  loss_mask_7: 0.1776  loss_dice_7: 0.2042  loss_ce_8: 0.000751  loss_mask_8: 0.1701  loss_dice_8: 0.2074  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:47] d2.utils.events INFO:  eta: 0:13:40  iter: 43519  total_loss: 4.04  loss_ce: 0.00107  loss_mask: 0.1861  loss_dice: 0.1604  loss_ce_0: 0.0603  loss_mask_0: 0.1882  loss_dice_0: 0.1643  loss_ce_1: 0.00112  loss_mask_1: 0.1725  loss_dice_1: 0.1583  loss_ce_2: 0.0008864  loss_mask_2: 0.1944  loss_dice_2: 0.1621  loss_ce_3: 0.001068  loss_mask_3: 0.2061  loss_dice_3: 0.1655  loss_ce_4: 0.001243  loss_mask_4: 0.1848  loss_dice_4: 0.1694  loss_ce_5: 0.0008553  loss_mask_5: 0.1893  loss_dice_5: 0.1642  loss_ce_6: 0.0009763  loss_mask_6: 0.1978  loss_dice_6: 0.1646  loss_ce_7: 0.001271  loss_mask_7: 0.1973  loss_dice_7: 0.1623  loss_ce_8: 0.0008583  loss_mask_8: 0.1881  loss_dice_8: 0.1617  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:50] d2.utils.events INFO:  eta: 0:13:38  iter: 43539  total_loss: 4.013  loss_ce: 0.000708  loss_mask: 0.2197  loss_dice: 0.1628  loss_ce_0: 0.06273  loss_mask_0: 0.2096  loss_dice_0: 0.1696  loss_ce_1: 0.000741  loss_mask_1: 0.2045  loss_dice_1: 0.1629  loss_ce_2: 0.0004744  loss_mask_2: 0.2079  loss_dice_2: 0.1668  loss_ce_3: 0.0005407  loss_mask_3: 0.1974  loss_dice_3: 0.1624  loss_ce_4: 0.0006852  loss_mask_4: 0.2055  loss_dice_4: 0.162  loss_ce_5: 0.0006468  loss_mask_5: 0.2053  loss_dice_5: 0.1604  loss_ce_6: 0.0007227  loss_mask_6: 0.2002  loss_dice_6: 0.1601  loss_ce_7: 0.0007303  loss_mask_7: 0.2069  loss_dice_7: 0.1604  loss_ce_8: 0.0008102  loss_mask_8: 0.2098  loss_dice_8: 0.1632  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:52] d2.utils.events INFO:  eta: 0:13:36  iter: 43559  total_loss: 4.081  loss_ce: 0.0006857  loss_mask: 0.2112  loss_dice: 0.1548  loss_ce_0: 0.06209  loss_mask_0: 0.2193  loss_dice_0: 0.1488  loss_ce_1: 0.001267  loss_mask_1: 0.2101  loss_dice_1: 0.1493  loss_ce_2: 0.000753  loss_mask_2: 0.2098  loss_dice_2: 0.1488  loss_ce_3: 0.0007603  loss_mask_3: 0.2148  loss_dice_3: 0.1518  loss_ce_4: 0.00105  loss_mask_4: 0.2116  loss_dice_4: 0.1468  loss_ce_5: 0.0007644  loss_mask_5: 0.218  loss_dice_5: 0.1538  loss_ce_6: 0.0008262  loss_mask_6: 0.2167  loss_dice_6: 0.1424  loss_ce_7: 0.0009618  loss_mask_7: 0.2103  loss_dice_7: 0.1504  loss_ce_8: 0.0009021  loss_mask_8: 0.2148  loss_dice_8: 0.1518  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:55] d2.utils.events INFO:  eta: 0:13:34  iter: 43579  total_loss: 4.014  loss_ce: 0.0007472  loss_mask: 0.2042  loss_dice: 0.1499  loss_ce_0: 0.06177  loss_mask_0: 0.2125  loss_dice_0: 0.1501  loss_ce_1: 0.0007886  loss_mask_1: 0.2164  loss_dice_1: 0.1471  loss_ce_2: 0.0005708  loss_mask_2: 0.2109  loss_dice_2: 0.1542  loss_ce_3: 0.0006564  loss_mask_3: 0.2093  loss_dice_3: 0.1542  loss_ce_4: 0.0007717  loss_mask_4: 0.22  loss_dice_4: 0.1507  loss_ce_5: 0.0007938  loss_mask_5: 0.2064  loss_dice_5: 0.1453  loss_ce_6: 0.0007599  loss_mask_6: 0.2187  loss_dice_6: 0.1526  loss_ce_7: 0.0008371  loss_mask_7: 0.2073  loss_dice_7: 0.1477  loss_ce_8: 0.0008559  loss_mask_8: 0.205  loss_dice_8: 0.1528  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:38:58] d2.utils.events INFO:  eta: 0:13:31  iter: 43599  total_loss: 3.675  loss_ce: 0.0007881  loss_mask: 0.185  loss_dice: 0.1415  loss_ce_0: 0.06181  loss_mask_0: 0.1921  loss_dice_0: 0.1408  loss_ce_1: 0.0006458  loss_mask_1: 0.1853  loss_dice_1: 0.1438  loss_ce_2: 0.0004007  loss_mask_2: 0.1848  loss_dice_2: 0.1367  loss_ce_3: 0.0005585  loss_mask_3: 0.188  loss_dice_3: 0.1367  loss_ce_4: 0.0005885  loss_mask_4: 0.1845  loss_dice_4: 0.1401  loss_ce_5: 0.0005534  loss_mask_5: 0.1861  loss_dice_5: 0.1436  loss_ce_6: 0.0007206  loss_mask_6: 0.1732  loss_dice_6: 0.1426  loss_ce_7: 0.0008332  loss_mask_7: 0.1778  loss_dice_7: 0.1455  loss_ce_8: 0.000748  loss_mask_8: 0.1789  loss_dice_8: 0.1391  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:00] d2.utils.events INFO:  eta: 0:13:29  iter: 43619  total_loss: 3.914  loss_ce: 0.0009164  loss_mask: 0.1716  loss_dice: 0.1497  loss_ce_0: 0.06161  loss_mask_0: 0.1644  loss_dice_0: 0.1535  loss_ce_1: 0.001298  loss_mask_1: 0.1795  loss_dice_1: 0.154  loss_ce_2: 0.000885  loss_mask_2: 0.1819  loss_dice_2: 0.1515  loss_ce_3: 0.0008036  loss_mask_3: 0.1753  loss_dice_3: 0.1527  loss_ce_4: 0.000983  loss_mask_4: 0.1818  loss_dice_4: 0.1517  loss_ce_5: 0.0007446  loss_mask_5: 0.1797  loss_dice_5: 0.1527  loss_ce_6: 0.0008541  loss_mask_6: 0.1787  loss_dice_6: 0.153  loss_ce_7: 0.0008967  loss_mask_7: 0.1716  loss_dice_7: 0.1518  loss_ce_8: 0.0008441  loss_mask_8: 0.1788  loss_dice_8: 0.1523  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:03] d2.utils.events INFO:  eta: 0:13:26  iter: 43639  total_loss: 3.819  loss_ce: 0.000505  loss_mask: 0.1908  loss_dice: 0.1602  loss_ce_0: 0.06075  loss_mask_0: 0.195  loss_dice_0: 0.1515  loss_ce_1: 0.0006521  loss_mask_1: 0.1968  loss_dice_1: 0.155  loss_ce_2: 0.0004308  loss_mask_2: 0.1866  loss_dice_2: 0.1576  loss_ce_3: 0.000471  loss_mask_3: 0.1843  loss_dice_3: 0.1514  loss_ce_4: 0.0005848  loss_mask_4: 0.1957  loss_dice_4: 0.153  loss_ce_5: 0.0005287  loss_mask_5: 0.1866  loss_dice_5: 0.1499  loss_ce_6: 0.000599  loss_mask_6: 0.1856  loss_dice_6: 0.1538  loss_ce_7: 0.0008097  loss_mask_7: 0.1974  loss_dice_7: 0.1516  loss_ce_8: 0.0006479  loss_mask_8: 0.1953  loss_dice_8: 0.144  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:05] d2.utils.events INFO:  eta: 0:13:23  iter: 43659  total_loss: 4.471  loss_ce: 0.0005194  loss_mask: 0.1729  loss_dice: 0.1889  loss_ce_0: 0.06295  loss_mask_0: 0.1673  loss_dice_0: 0.1897  loss_ce_1: 0.0005488  loss_mask_1: 0.1804  loss_dice_1: 0.195  loss_ce_2: 0.0003606  loss_mask_2: 0.169  loss_dice_2: 0.1897  loss_ce_3: 0.0005136  loss_mask_3: 0.1707  loss_dice_3: 0.1931  loss_ce_4: 0.0005107  loss_mask_4: 0.175  loss_dice_4: 0.192  loss_ce_5: 0.0004486  loss_mask_5: 0.1753  loss_dice_5: 0.19  loss_ce_6: 0.0006549  loss_mask_6: 0.1773  loss_dice_6: 0.1906  loss_ce_7: 0.0005539  loss_mask_7: 0.1699  loss_dice_7: 0.1886  loss_ce_8: 0.0006316  loss_mask_8: 0.1725  loss_dice_8: 0.192  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:08] d2.utils.events INFO:  eta: 0:13:20  iter: 43679  total_loss: 3.529  loss_ce: 0.0004696  loss_mask: 0.191  loss_dice: 0.16  loss_ce_0: 0.06085  loss_mask_0: 0.1954  loss_dice_0: 0.1595  loss_ce_1: 0.0004828  loss_mask_1: 0.1903  loss_dice_1: 0.1569  loss_ce_2: 0.0002881  loss_mask_2: 0.1934  loss_dice_2: 0.1644  loss_ce_3: 0.0004033  loss_mask_3: 0.1989  loss_dice_3: 0.1598  loss_ce_4: 0.0004257  loss_mask_4: 0.1892  loss_dice_4: 0.1636  loss_ce_5: 0.0004237  loss_mask_5: 0.1949  loss_dice_5: 0.1565  loss_ce_6: 0.0005638  loss_mask_6: 0.195  loss_dice_6: 0.1611  loss_ce_7: 0.0004603  loss_mask_7: 0.1935  loss_dice_7: 0.1606  loss_ce_8: 0.0005864  loss_mask_8: 0.195  loss_dice_8: 0.16  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:39:10] d2.utils.events INFO:  eta: 0:13:17  iter: 43699  total_loss: 4.366  loss_ce: 0.0005762  loss_mask: 0.193  loss_dice: 0.176  loss_ce_0: 0.06285  loss_mask_0: 0.195  loss_dice_0: 0.1812  loss_ce_1: 0.0007619  loss_mask_1: 0.1902  loss_dice_1: 0.1779  loss_ce_2: 0.0004589  loss_mask_2: 0.1919  loss_dice_2: 0.1802  loss_ce_3: 0.0005389  loss_mask_3: 0.1884  loss_dice_3: 0.1839  loss_ce_4: 0.0007252  loss_mask_4: 0.1984  loss_dice_4: 0.1774  loss_ce_5: 0.0005251  loss_mask_5: 0.1974  loss_dice_5: 0.1803  loss_ce_6: 0.000639  loss_mask_6: 0.1938  loss_dice_6: 0.1838  loss_ce_7: 0.0007636  loss_mask_7: 0.1948  loss_dice_7: 0.1771  loss_ce_8: 0.0006843  loss_mask_8: 0.1802  loss_dice_8: 0.1754  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:39:13] d2.utils.events INFO:  eta: 0:13:14  iter: 43719  total_loss: 4.052  loss_ce: 0.0004929  loss_mask: 0.1779  loss_dice: 0.1669  loss_ce_0: 0.06178  loss_mask_0: 0.1865  loss_dice_0: 0.1646  loss_ce_1: 0.0004817  loss_mask_1: 0.1991  loss_dice_1: 0.1707  loss_ce_2: 0.0002862  loss_mask_2: 0.1894  loss_dice_2: 0.1586  loss_ce_3: 0.0004379  loss_mask_3: 0.1823  loss_dice_3: 0.1635  loss_ce_4: 0.0004304  loss_mask_4: 0.1826  loss_dice_4: 0.1614  loss_ce_5: 0.0004255  loss_mask_5: 0.1874  loss_dice_5: 0.1631  loss_ce_6: 0.0005785  loss_mask_6: 0.1848  loss_dice_6: 0.1641  loss_ce_7: 0.0005141  loss_mask_7: 0.1887  loss_dice_7: 0.1626  loss_ce_8: 0.0006035  loss_mask_8: 0.191  loss_dice_8: 0.1632  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:15] d2.utils.events INFO:  eta: 0:13:12  iter: 43739  total_loss: 4.322  loss_ce: 0.0004575  loss_mask: 0.182  loss_dice: 0.1955  loss_ce_0: 0.06362  loss_mask_0: 0.1959  loss_dice_0: 0.1956  loss_ce_1: 0.0004865  loss_mask_1: 0.1893  loss_dice_1: 0.1874  loss_ce_2: 0.0002975  loss_mask_2: 0.1868  loss_dice_2: 0.1848  loss_ce_3: 0.0004644  loss_mask_3: 0.1918  loss_dice_3: 0.1861  loss_ce_4: 0.0004369  loss_mask_4: 0.191  loss_dice_4: 0.1898  loss_ce_5: 0.000395  loss_mask_5: 0.1737  loss_dice_5: 0.1905  loss_ce_6: 0.000566  loss_mask_6: 0.1878  loss_dice_6: 0.1918  loss_ce_7: 0.0004655  loss_mask_7: 0.19  loss_dice_7: 0.1892  loss_ce_8: 0.000554  loss_mask_8: 0.1942  loss_dice_8: 0.1839  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:18] d2.utils.events INFO:  eta: 0:13:09  iter: 43759  total_loss: 3.907  loss_ce: 0.0004553  loss_mask: 0.2228  loss_dice: 0.1721  loss_ce_0: 0.06316  loss_mask_0: 0.2183  loss_dice_0: 0.1659  loss_ce_1: 0.0004567  loss_mask_1: 0.2155  loss_dice_1: 0.175  loss_ce_2: 0.0002301  loss_mask_2: 0.216  loss_dice_2: 0.1752  loss_ce_3: 0.0003734  loss_mask_3: 0.2109  loss_dice_3: 0.1747  loss_ce_4: 0.0003634  loss_mask_4: 0.2168  loss_dice_4: 0.1835  loss_ce_5: 0.0003515  loss_mask_5: 0.224  loss_dice_5: 0.1784  loss_ce_6: 0.0005274  loss_mask_6: 0.2088  loss_dice_6: 0.1723  loss_ce_7: 0.0004406  loss_mask_7: 0.2126  loss_dice_7: 0.1714  loss_ce_8: 0.0005294  loss_mask_8: 0.2209  loss_dice_8: 0.1742  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:20] d2.utils.events INFO:  eta: 0:13:06  iter: 43779  total_loss: 3.889  loss_ce: 0.0004931  loss_mask: 0.2074  loss_dice: 0.1716  loss_ce_0: 0.06271  loss_mask_0: 0.202  loss_dice_0: 0.1805  loss_ce_1: 0.0004715  loss_mask_1: 0.207  loss_dice_1: 0.1792  loss_ce_2: 0.0003091  loss_mask_2: 0.2026  loss_dice_2: 0.1698  loss_ce_3: 0.0005425  loss_mask_3: 0.1984  loss_dice_3: 0.1742  loss_ce_4: 0.0004508  loss_mask_4: 0.2128  loss_dice_4: 0.1821  loss_ce_5: 0.0004066  loss_mask_5: 0.2013  loss_dice_5: 0.1786  loss_ce_6: 0.0006255  loss_mask_6: 0.2066  loss_dice_6: 0.1818  loss_ce_7: 0.0004998  loss_mask_7: 0.204  loss_dice_7: 0.1785  loss_ce_8: 0.000582  loss_mask_8: 0.2044  loss_dice_8: 0.1776  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:23] d2.utils.events INFO:  eta: 0:13:04  iter: 43799  total_loss: 3.717  loss_ce: 0.0004081  loss_mask: 0.1902  loss_dice: 0.1581  loss_ce_0: 0.06212  loss_mask_0: 0.1903  loss_dice_0: 0.1583  loss_ce_1: 0.000461  loss_mask_1: 0.1979  loss_dice_1: 0.1657  loss_ce_2: 0.0002199  loss_mask_2: 0.1915  loss_dice_2: 0.1561  loss_ce_3: 0.0004026  loss_mask_3: 0.1988  loss_dice_3: 0.1619  loss_ce_4: 0.0003818  loss_mask_4: 0.19  loss_dice_4: 0.1597  loss_ce_5: 0.00033  loss_mask_5: 0.1908  loss_dice_5: 0.1585  loss_ce_6: 0.0005416  loss_mask_6: 0.1871  loss_dice_6: 0.1601  loss_ce_7: 0.0004046  loss_mask_7: 0.1942  loss_dice_7: 0.1593  loss_ce_8: 0.0004727  loss_mask_8: 0.1966  loss_dice_8: 0.159  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:25] d2.utils.events INFO:  eta: 0:13:01  iter: 43819  total_loss: 3.805  loss_ce: 0.0005395  loss_mask: 0.1777  loss_dice: 0.1524  loss_ce_0: 0.0618  loss_mask_0: 0.1776  loss_dice_0: 0.1556  loss_ce_1: 0.0005292  loss_mask_1: 0.1863  loss_dice_1: 0.1556  loss_ce_2: 0.0004506  loss_mask_2: 0.1828  loss_dice_2: 0.1561  loss_ce_3: 0.0004994  loss_mask_3: 0.179  loss_dice_3: 0.1527  loss_ce_4: 0.0005345  loss_mask_4: 0.173  loss_dice_4: 0.1611  loss_ce_5: 0.0005916  loss_mask_5: 0.1792  loss_dice_5: 0.1522  loss_ce_6: 0.0006274  loss_mask_6: 0.179  loss_dice_6: 0.1566  loss_ce_7: 0.0006429  loss_mask_7: 0.1803  loss_dice_7: 0.1605  loss_ce_8: 0.0005725  loss_mask_8: 0.1802  loss_dice_8: 0.1524  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:39:28] d2.utils.events INFO:  eta: 0:12:58  iter: 43839  total_loss: 4.65  loss_ce: 0.0005061  loss_mask: 0.2348  loss_dice: 0.1644  loss_ce_0: 0.06146  loss_mask_0: 0.2307  loss_dice_0: 0.1591  loss_ce_1: 0.0004805  loss_mask_1: 0.2417  loss_dice_1: 0.1631  loss_ce_2: 0.0002719  loss_mask_2: 0.2289  loss_dice_2: 0.1607  loss_ce_3: 0.0004227  loss_mask_3: 0.2446  loss_dice_3: 0.167  loss_ce_4: 0.0004195  loss_mask_4: 0.2496  loss_dice_4: 0.1668  loss_ce_5: 0.0004206  loss_mask_5: 0.2372  loss_dice_5: 0.1648  loss_ce_6: 0.000536  loss_mask_6: 0.2416  loss_dice_6: 0.1675  loss_ce_7: 0.0004337  loss_mask_7: 0.2322  loss_dice_7: 0.1596  loss_ce_8: 0.0004839  loss_mask_8: 0.232  loss_dice_8: 0.168  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:39:30] d2.utils.events INFO:  eta: 0:12:55  iter: 43859  total_loss: 3.84  loss_ce: 0.0004506  loss_mask: 0.2133  loss_dice: 0.1497  loss_ce_0: 0.06084  loss_mask_0: 0.2198  loss_dice_0: 0.1454  loss_ce_1: 0.0005293  loss_mask_1: 0.2106  loss_dice_1: 0.142  loss_ce_2: 0.0002863  loss_mask_2: 0.2164  loss_dice_2: 0.1391  loss_ce_3: 0.0004183  loss_mask_3: 0.2209  loss_dice_3: 0.1484  loss_ce_4: 0.0004523  loss_mask_4: 0.2208  loss_dice_4: 0.1473  loss_ce_5: 0.0004412  loss_mask_5: 0.2181  loss_dice_5: 0.1438  loss_ce_6: 0.0005186  loss_mask_6: 0.2123  loss_dice_6: 0.1515  loss_ce_7: 0.0005096  loss_mask_7: 0.2189  loss_dice_7: 0.1439  loss_ce_8: 0.0006377  loss_mask_8: 0.2195  loss_dice_8: 0.1482  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:33] d2.utils.events INFO:  eta: 0:12:52  iter: 43879  total_loss: 3.889  loss_ce: 0.0004706  loss_mask: 0.2235  loss_dice: 0.1491  loss_ce_0: 0.06064  loss_mask_0: 0.2224  loss_dice_0: 0.1506  loss_ce_1: 0.0005288  loss_mask_1: 0.2213  loss_dice_1: 0.1493  loss_ce_2: 0.0003509  loss_mask_2: 0.2184  loss_dice_2: 0.1473  loss_ce_3: 0.0004857  loss_mask_3: 0.2309  loss_dice_3: 0.1517  loss_ce_4: 0.0005094  loss_mask_4: 0.2249  loss_dice_4: 0.149  loss_ce_5: 0.0004761  loss_mask_5: 0.2137  loss_dice_5: 0.1523  loss_ce_6: 0.0006375  loss_mask_6: 0.2221  loss_dice_6: 0.1493  loss_ce_7: 0.0004751  loss_mask_7: 0.2214  loss_dice_7: 0.1479  loss_ce_8: 0.0005975  loss_mask_8: 0.2218  loss_dice_8: 0.1489  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:35] d2.utils.events INFO:  eta: 0:12:50  iter: 43899  total_loss: 3.891  loss_ce: 0.0004077  loss_mask: 0.1928  loss_dice: 0.1605  loss_ce_0: 0.06041  loss_mask_0: 0.1957  loss_dice_0: 0.1609  loss_ce_1: 0.0004338  loss_mask_1: 0.1966  loss_dice_1: 0.1592  loss_ce_2: 0.0002582  loss_mask_2: 0.1895  loss_dice_2: 0.1607  loss_ce_3: 0.00043  loss_mask_3: 0.1872  loss_dice_3: 0.1618  loss_ce_4: 0.0003856  loss_mask_4: 0.1916  loss_dice_4: 0.1668  loss_ce_5: 0.000369  loss_mask_5: 0.1984  loss_dice_5: 0.1594  loss_ce_6: 0.000502  loss_mask_6: 0.1963  loss_dice_6: 0.1641  loss_ce_7: 0.0004031  loss_mask_7: 0.1887  loss_dice_7: 0.1662  loss_ce_8: 0.0005282  loss_mask_8: 0.1918  loss_dice_8: 0.1642  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:39:38] d2.utils.events INFO:  eta: 0:12:48  iter: 43919  total_loss: 3.813  loss_ce: 0.0006332  loss_mask: 0.1922  loss_dice: 0.1654  loss_ce_0: 0.06021  loss_mask_0: 0.1873  loss_dice_0: 0.154  loss_ce_1: 0.0005124  loss_mask_1: 0.2001  loss_dice_1: 0.1604  loss_ce_2: 0.0003373  loss_mask_2: 0.1921  loss_dice_2: 0.1621  loss_ce_3: 0.0006149  loss_mask_3: 0.1847  loss_dice_3: 0.1684  loss_ce_4: 0.0005314  loss_mask_4: 0.2004  loss_dice_4: 0.1506  loss_ce_5: 0.0004763  loss_mask_5: 0.1976  loss_dice_5: 0.1592  loss_ce_6: 0.0005659  loss_mask_6: 0.1938  loss_dice_6: 0.161  loss_ce_7: 0.0004709  loss_mask_7: 0.188  loss_dice_7: 0.1593  loss_ce_8: 0.0006251  loss_mask_8: 0.1988  loss_dice_8: 0.1547  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:40] d2.utils.events INFO:  eta: 0:12:45  iter: 43939  total_loss: 4.622  loss_ce: 0.0005483  loss_mask: 0.166  loss_dice: 0.1966  loss_ce_0: 0.0602  loss_mask_0: 0.1662  loss_dice_0: 0.1985  loss_ce_1: 0.0005099  loss_mask_1: 0.1665  loss_dice_1: 0.1967  loss_ce_2: 0.0002805  loss_mask_2: 0.1629  loss_dice_2: 0.1995  loss_ce_3: 0.0004734  loss_mask_3: 0.1735  loss_dice_3: 0.209  loss_ce_4: 0.0005626  loss_mask_4: 0.1585  loss_dice_4: 0.1959  loss_ce_5: 0.0004  loss_mask_5: 0.1663  loss_dice_5: 0.1993  loss_ce_6: 0.0005297  loss_mask_6: 0.1672  loss_dice_6: 0.1993  loss_ce_7: 0.0005082  loss_mask_7: 0.1631  loss_dice_7: 0.1949  loss_ce_8: 0.0005756  loss_mask_8: 0.1677  loss_dice_8: 0.207  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:43] d2.utils.events INFO:  eta: 0:12:42  iter: 43959  total_loss: 4.434  loss_ce: 0.0004305  loss_mask: 0.2237  loss_dice: 0.1698  loss_ce_0: 0.06038  loss_mask_0: 0.23  loss_dice_0: 0.1706  loss_ce_1: 0.0004327  loss_mask_1: 0.217  loss_dice_1: 0.1651  loss_ce_2: 0.0002403  loss_mask_2: 0.2232  loss_dice_2: 0.1694  loss_ce_3: 0.0004114  loss_mask_3: 0.2171  loss_dice_3: 0.1671  loss_ce_4: 0.0004082  loss_mask_4: 0.2182  loss_dice_4: 0.1674  loss_ce_5: 0.0004341  loss_mask_5: 0.2189  loss_dice_5: 0.1662  loss_ce_6: 0.0004821  loss_mask_6: 0.217  loss_dice_6: 0.1661  loss_ce_7: 0.0004021  loss_mask_7: 0.2212  loss_dice_7: 0.1709  loss_ce_8: 0.000552  loss_mask_8: 0.2163  loss_dice_8: 0.1632  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:45] d2.utils.events INFO:  eta: 0:12:40  iter: 43979  total_loss: 4.088  loss_ce: 0.0003895  loss_mask: 0.2261  loss_dice: 0.1662  loss_ce_0: 0.06311  loss_mask_0: 0.2171  loss_dice_0: 0.1659  loss_ce_1: 0.0003856  loss_mask_1: 0.2189  loss_dice_1: 0.1613  loss_ce_2: 0.0001808  loss_mask_2: 0.2158  loss_dice_2: 0.1622  loss_ce_3: 0.0003457  loss_mask_3: 0.2214  loss_dice_3: 0.1671  loss_ce_4: 0.000398  loss_mask_4: 0.2197  loss_dice_4: 0.1672  loss_ce_5: 0.0003808  loss_mask_5: 0.2241  loss_dice_5: 0.1583  loss_ce_6: 0.0004664  loss_mask_6: 0.2161  loss_dice_6: 0.1674  loss_ce_7: 0.0004367  loss_mask_7: 0.2248  loss_dice_7: 0.162  loss_ce_8: 0.0005561  loss_mask_8: 0.2207  loss_dice_8: 0.1638  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:48] d2.utils.events INFO:  eta: 0:12:37  iter: 43999  total_loss: 4.322  loss_ce: 0.0007793  loss_mask: 0.1998  loss_dice: 0.1785  loss_ce_0: 0.06072  loss_mask_0: 0.1976  loss_dice_0: 0.1882  loss_ce_1: 0.0005984  loss_mask_1: 0.1932  loss_dice_1: 0.1863  loss_ce_2: 0.0005655  loss_mask_2: 0.2021  loss_dice_2: 0.1852  loss_ce_3: 0.0006742  loss_mask_3: 0.2044  loss_dice_3: 0.1853  loss_ce_4: 0.0006659  loss_mask_4: 0.1974  loss_dice_4: 0.1802  loss_ce_5: 0.0008371  loss_mask_5: 0.1916  loss_dice_5: 0.1808  loss_ce_6: 0.0006313  loss_mask_6: 0.1982  loss_dice_6: 0.1862  loss_ce_7: 0.0006064  loss_mask_7: 0.1939  loss_dice_7: 0.1901  loss_ce_8: 0.0009232  loss_mask_8: 0.1929  loss_dice_8: 0.1854  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:51] d2.utils.events INFO:  eta: 0:12:34  iter: 44019  total_loss: 4.405  loss_ce: 0.0007394  loss_mask: 0.1751  loss_dice: 0.1714  loss_ce_0: 0.06048  loss_mask_0: 0.1718  loss_dice_0: 0.1686  loss_ce_1: 0.0005253  loss_mask_1: 0.1803  loss_dice_1: 0.1686  loss_ce_2: 0.0004813  loss_mask_2: 0.1747  loss_dice_2: 0.1734  loss_ce_3: 0.0005713  loss_mask_3: 0.1722  loss_dice_3: 0.1727  loss_ce_4: 0.0005736  loss_mask_4: 0.1691  loss_dice_4: 0.1728  loss_ce_5: 0.0005449  loss_mask_5: 0.1762  loss_dice_5: 0.1765  loss_ce_6: 0.0006207  loss_mask_6: 0.178  loss_dice_6: 0.17  loss_ce_7: 0.0005373  loss_mask_7: 0.1691  loss_dice_7: 0.1741  loss_ce_8: 0.0006054  loss_mask_8: 0.172  loss_dice_8: 0.1731  time: 0.1260  data_time: 0.0014  lr: 1e-05  max_mem: 3105M
[04/13 16:39:53] d2.utils.events INFO:  eta: 0:12:32  iter: 44039  total_loss: 3.753  loss_ce: 0.0004406  loss_mask: 0.2034  loss_dice: 0.1788  loss_ce_0: 0.06347  loss_mask_0: 0.1964  loss_dice_0: 0.1677  loss_ce_1: 0.0004032  loss_mask_1: 0.1907  loss_dice_1: 0.1633  loss_ce_2: 0.0002485  loss_mask_2: 0.1884  loss_dice_2: 0.1622  loss_ce_3: 0.0004093  loss_mask_3: 0.1878  loss_dice_3: 0.1697  loss_ce_4: 0.0004037  loss_mask_4: 0.1953  loss_dice_4: 0.17  loss_ce_5: 0.0003725  loss_mask_5: 0.1888  loss_dice_5: 0.1812  loss_ce_6: 0.0004828  loss_mask_6: 0.194  loss_dice_6: 0.1644  loss_ce_7: 0.0004275  loss_mask_7: 0.1927  loss_dice_7: 0.1697  loss_ce_8: 0.0005314  loss_mask_8: 0.1923  loss_dice_8: 0.1665  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:39:56] d2.utils.events INFO:  eta: 0:12:29  iter: 44059  total_loss: 4.159  loss_ce: 0.000403  loss_mask: 0.1657  loss_dice: 0.1623  loss_ce_0: 0.06302  loss_mask_0: 0.1723  loss_dice_0: 0.1654  loss_ce_1: 0.000381  loss_mask_1: 0.1702  loss_dice_1: 0.1599  loss_ce_2: 0.0001831  loss_mask_2: 0.1797  loss_dice_2: 0.168  loss_ce_3: 0.000387  loss_mask_3: 0.1705  loss_dice_3: 0.1616  loss_ce_4: 0.0003778  loss_mask_4: 0.1736  loss_dice_4: 0.169  loss_ce_5: 0.0003864  loss_mask_5: 0.169  loss_dice_5: 0.1636  loss_ce_6: 0.0004919  loss_mask_6: 0.1777  loss_dice_6: 0.1623  loss_ce_7: 0.0004824  loss_mask_7: 0.1792  loss_dice_7: 0.164  loss_ce_8: 0.0005716  loss_mask_8: 0.1798  loss_dice_8: 0.165  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:39:58] d2.utils.events INFO:  eta: 0:12:27  iter: 44079  total_loss: 3.788  loss_ce: 0.0006294  loss_mask: 0.19  loss_dice: 0.1622  loss_ce_0: 0.06237  loss_mask_0: 0.1898  loss_dice_0: 0.159  loss_ce_1: 0.0005777  loss_mask_1: 0.191  loss_dice_1: 0.1608  loss_ce_2: 0.0004021  loss_mask_2: 0.192  loss_dice_2: 0.1642  loss_ce_3: 0.0004758  loss_mask_3: 0.19  loss_dice_3: 0.1684  loss_ce_4: 0.0006315  loss_mask_4: 0.1937  loss_dice_4: 0.1628  loss_ce_5: 0.0004723  loss_mask_5: 0.1956  loss_dice_5: 0.1662  loss_ce_6: 0.0005497  loss_mask_6: 0.1939  loss_dice_6: 0.1709  loss_ce_7: 0.0005598  loss_mask_7: 0.1917  loss_dice_7: 0.1626  loss_ce_8: 0.0006204  loss_mask_8: 0.1861  loss_dice_8: 0.1662  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:40:01] d2.utils.events INFO:  eta: 0:12:26  iter: 44099  total_loss: 3.921  loss_ce: 0.0004302  loss_mask: 0.1985  loss_dice: 0.1935  loss_ce_0: 0.06177  loss_mask_0: 0.1967  loss_dice_0: 0.1999  loss_ce_1: 0.0003834  loss_mask_1: 0.1926  loss_dice_1: 0.2054  loss_ce_2: 0.0002135  loss_mask_2: 0.1926  loss_dice_2: 0.1903  loss_ce_3: 0.0003821  loss_mask_3: 0.2064  loss_dice_3: 0.2082  loss_ce_4: 0.0004327  loss_mask_4: 0.2025  loss_dice_4: 0.2009  loss_ce_5: 0.0004385  loss_mask_5: 0.205  loss_dice_5: 0.201  loss_ce_6: 0.000493  loss_mask_6: 0.2043  loss_dice_6: 0.2125  loss_ce_7: 0.0005152  loss_mask_7: 0.2024  loss_dice_7: 0.2066  loss_ce_8: 0.000578  loss_mask_8: 0.2001  loss_dice_8: 0.1934  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:03] d2.utils.events INFO:  eta: 0:12:23  iter: 44119  total_loss: 4.28  loss_ce: 0.0005113  loss_mask: 0.2221  loss_dice: 0.1744  loss_ce_0: 0.06122  loss_mask_0: 0.2213  loss_dice_0: 0.1724  loss_ce_1: 0.0004859  loss_mask_1: 0.2244  loss_dice_1: 0.1743  loss_ce_2: 0.000354  loss_mask_2: 0.2094  loss_dice_2: 0.1777  loss_ce_3: 0.0004474  loss_mask_3: 0.2189  loss_dice_3: 0.178  loss_ce_4: 0.0005248  loss_mask_4: 0.2173  loss_dice_4: 0.1735  loss_ce_5: 0.0004666  loss_mask_5: 0.2183  loss_dice_5: 0.1697  loss_ce_6: 0.0005274  loss_mask_6: 0.2153  loss_dice_6: 0.1775  loss_ce_7: 0.0006752  loss_mask_7: 0.2108  loss_dice_7: 0.1763  loss_ce_8: 0.0006011  loss_mask_8: 0.2173  loss_dice_8: 0.1803  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:06] d2.utils.events INFO:  eta: 0:12:21  iter: 44139  total_loss: 3.777  loss_ce: 0.0005474  loss_mask: 0.1854  loss_dice: 0.1761  loss_ce_0: 0.06115  loss_mask_0: 0.1857  loss_dice_0: 0.1764  loss_ce_1: 0.0004595  loss_mask_1: 0.1919  loss_dice_1: 0.1816  loss_ce_2: 0.0003335  loss_mask_2: 0.1861  loss_dice_2: 0.1779  loss_ce_3: 0.0005053  loss_mask_3: 0.1861  loss_dice_3: 0.1686  loss_ce_4: 0.0004231  loss_mask_4: 0.1963  loss_dice_4: 0.1779  loss_ce_5: 0.0004224  loss_mask_5: 0.1916  loss_dice_5: 0.1778  loss_ce_6: 0.0005878  loss_mask_6: 0.1927  loss_dice_6: 0.1703  loss_ce_7: 0.000533  loss_mask_7: 0.195  loss_dice_7: 0.1743  loss_ce_8: 0.0005987  loss_mask_8: 0.1837  loss_dice_8: 0.1743  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:08] d2.utils.events INFO:  eta: 0:12:18  iter: 44159  total_loss: 4.632  loss_ce: 0.0003895  loss_mask: 0.2461  loss_dice: 0.1651  loss_ce_0: 0.06103  loss_mask_0: 0.2469  loss_dice_0: 0.1618  loss_ce_1: 0.0003911  loss_mask_1: 0.237  loss_dice_1: 0.1668  loss_ce_2: 0.0002063  loss_mask_2: 0.2404  loss_dice_2: 0.164  loss_ce_3: 0.0003959  loss_mask_3: 0.2426  loss_dice_3: 0.1686  loss_ce_4: 0.0003443  loss_mask_4: 0.2508  loss_dice_4: 0.173  loss_ce_5: 0.0002837  loss_mask_5: 0.2515  loss_dice_5: 0.1622  loss_ce_6: 0.0004727  loss_mask_6: 0.2446  loss_dice_6: 0.1605  loss_ce_7: 0.0003988  loss_mask_7: 0.2475  loss_dice_7: 0.1645  loss_ce_8: 0.0004441  loss_mask_8: 0.2464  loss_dice_8: 0.1687  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:11] d2.utils.events INFO:  eta: 0:12:16  iter: 44179  total_loss: 3.949  loss_ce: 0.0004073  loss_mask: 0.1776  loss_dice: 0.1896  loss_ce_0: 0.06182  loss_mask_0: 0.1784  loss_dice_0: 0.1906  loss_ce_1: 0.0004024  loss_mask_1: 0.1772  loss_dice_1: 0.1857  loss_ce_2: 0.0002682  loss_mask_2: 0.1812  loss_dice_2: 0.1952  loss_ce_3: 0.0004673  loss_mask_3: 0.181  loss_dice_3: 0.1907  loss_ce_4: 0.0004262  loss_mask_4: 0.1787  loss_dice_4: 0.1818  loss_ce_5: 0.0004099  loss_mask_5: 0.184  loss_dice_5: 0.1899  loss_ce_6: 0.0005276  loss_mask_6: 0.1774  loss_dice_6: 0.1942  loss_ce_7: 0.0004618  loss_mask_7: 0.177  loss_dice_7: 0.1934  loss_ce_8: 0.0005334  loss_mask_8: 0.1791  loss_dice_8: 0.1875  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:14] d2.utils.events INFO:  eta: 0:12:13  iter: 44199  total_loss: 3.746  loss_ce: 0.000431  loss_mask: 0.1828  loss_dice: 0.14  loss_ce_0: 0.06338  loss_mask_0: 0.1849  loss_dice_0: 0.1305  loss_ce_1: 0.0005038  loss_mask_1: 0.1803  loss_dice_1: 0.1289  loss_ce_2: 0.00032  loss_mask_2: 0.1925  loss_dice_2: 0.1281  loss_ce_3: 0.0004027  loss_mask_3: 0.1785  loss_dice_3: 0.1284  loss_ce_4: 0.0004339  loss_mask_4: 0.1927  loss_dice_4: 0.1273  loss_ce_5: 0.0003879  loss_mask_5: 0.1847  loss_dice_5: 0.1351  loss_ce_6: 0.0005254  loss_mask_6: 0.1889  loss_dice_6: 0.1344  loss_ce_7: 0.0005962  loss_mask_7: 0.1934  loss_dice_7: 0.1292  loss_ce_8: 0.000548  loss_mask_8: 0.181  loss_dice_8: 0.1299  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:40:16] d2.utils.events INFO:  eta: 0:12:11  iter: 44219  total_loss: 4.252  loss_ce: 0.0004056  loss_mask: 0.2043  loss_dice: 0.1644  loss_ce_0: 0.06312  loss_mask_0: 0.2155  loss_dice_0: 0.1645  loss_ce_1: 0.0004915  loss_mask_1: 0.2089  loss_dice_1: 0.1634  loss_ce_2: 0.0003183  loss_mask_2: 0.2129  loss_dice_2: 0.1686  loss_ce_3: 0.0004366  loss_mask_3: 0.2178  loss_dice_3: 0.1619  loss_ce_4: 0.0003585  loss_mask_4: 0.2205  loss_dice_4: 0.1716  loss_ce_5: 0.0003734  loss_mask_5: 0.2091  loss_dice_5: 0.1703  loss_ce_6: 0.0005108  loss_mask_6: 0.2095  loss_dice_6: 0.1693  loss_ce_7: 0.0004604  loss_mask_7: 0.2098  loss_dice_7: 0.1686  loss_ce_8: 0.0005344  loss_mask_8: 0.2045  loss_dice_8: 0.1699  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:19] d2.utils.events INFO:  eta: 0:12:08  iter: 44239  total_loss: 4.167  loss_ce: 0.0007214  loss_mask: 0.1338  loss_dice: 0.2409  loss_ce_0: 0.0618  loss_mask_0: 0.1315  loss_dice_0: 0.2231  loss_ce_1: 0.0006509  loss_mask_1: 0.1384  loss_dice_1: 0.2198  loss_ce_2: 0.000555  loss_mask_2: 0.1331  loss_dice_2: 0.2309  loss_ce_3: 0.0007059  loss_mask_3: 0.1505  loss_dice_3: 0.2337  loss_ce_4: 0.0005427  loss_mask_4: 0.1421  loss_dice_4: 0.223  loss_ce_5: 0.0005124  loss_mask_5: 0.1404  loss_dice_5: 0.2199  loss_ce_6: 0.0006891  loss_mask_6: 0.1383  loss_dice_6: 0.2351  loss_ce_7: 0.0008386  loss_mask_7: 0.1412  loss_dice_7: 0.2287  loss_ce_8: 0.0007842  loss_mask_8: 0.1317  loss_dice_8: 0.2249  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:21] d2.utils.events INFO:  eta: 0:12:06  iter: 44259  total_loss: 3.78  loss_ce: 0.0005072  loss_mask: 0.2018  loss_dice: 0.1665  loss_ce_0: 0.06169  loss_mask_0: 0.2046  loss_dice_0: 0.1635  loss_ce_1: 0.0004864  loss_mask_1: 0.2114  loss_dice_1: 0.1646  loss_ce_2: 0.000322  loss_mask_2: 0.2166  loss_dice_2: 0.1721  loss_ce_3: 0.0004705  loss_mask_3: 0.206  loss_dice_3: 0.1674  loss_ce_4: 0.0003875  loss_mask_4: 0.1981  loss_dice_4: 0.1656  loss_ce_5: 0.0004088  loss_mask_5: 0.2162  loss_dice_5: 0.1602  loss_ce_6: 0.0005487  loss_mask_6: 0.2084  loss_dice_6: 0.1657  loss_ce_7: 0.0005369  loss_mask_7: 0.2078  loss_dice_7: 0.1655  loss_ce_8: 0.0005568  loss_mask_8: 0.2094  loss_dice_8: 0.1729  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:24] d2.utils.events INFO:  eta: 0:12:04  iter: 44279  total_loss: 4.666  loss_ce: 0.000687  loss_mask: 0.1898  loss_dice: 0.1986  loss_ce_0: 0.06182  loss_mask_0: 0.1849  loss_dice_0: 0.1908  loss_ce_1: 0.0006281  loss_mask_1: 0.1914  loss_dice_1: 0.1973  loss_ce_2: 0.0003572  loss_mask_2: 0.1826  loss_dice_2: 0.2051  loss_ce_3: 0.0005835  loss_mask_3: 0.2012  loss_dice_3: 0.1963  loss_ce_4: 0.0005582  loss_mask_4: 0.1872  loss_dice_4: 0.1967  loss_ce_5: 0.0005036  loss_mask_5: 0.1868  loss_dice_5: 0.198  loss_ce_6: 0.0006724  loss_mask_6: 0.2003  loss_dice_6: 0.1991  loss_ce_7: 0.0008333  loss_mask_7: 0.1875  loss_dice_7: 0.2037  loss_ce_8: 0.0006161  loss_mask_8: 0.1792  loss_dice_8: 0.1929  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:26] d2.utils.events INFO:  eta: 0:12:01  iter: 44299  total_loss: 4.146  loss_ce: 0.0007663  loss_mask: 0.2069  loss_dice: 0.1815  loss_ce_0: 0.06175  loss_mask_0: 0.2056  loss_dice_0: 0.1807  loss_ce_1: 0.0009742  loss_mask_1: 0.2057  loss_dice_1: 0.1839  loss_ce_2: 0.0007694  loss_mask_2: 0.209  loss_dice_2: 0.1876  loss_ce_3: 0.000901  loss_mask_3: 0.1877  loss_dice_3: 0.1847  loss_ce_4: 0.0008601  loss_mask_4: 0.2101  loss_dice_4: 0.1806  loss_ce_5: 0.0006562  loss_mask_5: 0.1945  loss_dice_5: 0.1813  loss_ce_6: 0.0008324  loss_mask_6: 0.2019  loss_dice_6: 0.1903  loss_ce_7: 0.0009911  loss_mask_7: 0.2004  loss_dice_7: 0.1867  loss_ce_8: 0.000773  loss_mask_8: 0.2058  loss_dice_8: 0.1817  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:29] d2.utils.events INFO:  eta: 0:11:59  iter: 44319  total_loss: 4.491  loss_ce: 0.0003654  loss_mask: 0.2208  loss_dice: 0.1853  loss_ce_0: 0.06223  loss_mask_0: 0.2095  loss_dice_0: 0.1784  loss_ce_1: 0.0004671  loss_mask_1: 0.2111  loss_dice_1: 0.1828  loss_ce_2: 0.000284  loss_mask_2: 0.2166  loss_dice_2: 0.1829  loss_ce_3: 0.0004425  loss_mask_3: 0.2106  loss_dice_3: 0.1841  loss_ce_4: 0.0003375  loss_mask_4: 0.2093  loss_dice_4: 0.18  loss_ce_5: 0.00036  loss_mask_5: 0.2083  loss_dice_5: 0.1847  loss_ce_6: 0.0005359  loss_mask_6: 0.2074  loss_dice_6: 0.1893  loss_ce_7: 0.0004545  loss_mask_7: 0.1994  loss_dice_7: 0.1881  loss_ce_8: 0.0004946  loss_mask_8: 0.2159  loss_dice_8: 0.1848  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:40:32] d2.utils.events INFO:  eta: 0:11:57  iter: 44339  total_loss: 3.968  loss_ce: 0.0005575  loss_mask: 0.2024  loss_dice: 0.1809  loss_ce_0: 0.0611  loss_mask_0: 0.201  loss_dice_0: 0.1871  loss_ce_1: 0.000499  loss_mask_1: 0.2062  loss_dice_1: 0.1797  loss_ce_2: 0.0003676  loss_mask_2: 0.2052  loss_dice_2: 0.184  loss_ce_3: 0.0005164  loss_mask_3: 0.2087  loss_dice_3: 0.1862  loss_ce_4: 0.0006117  loss_mask_4: 0.2082  loss_dice_4: 0.1759  loss_ce_5: 0.000472  loss_mask_5: 0.2012  loss_dice_5: 0.1832  loss_ce_6: 0.0006545  loss_mask_6: 0.2071  loss_dice_6: 0.1784  loss_ce_7: 0.0007144  loss_mask_7: 0.2021  loss_dice_7: 0.1793  loss_ce_8: 0.0006528  loss_mask_8: 0.2069  loss_dice_8: 0.1779  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:34] d2.utils.events INFO:  eta: 0:11:55  iter: 44359  total_loss: 3.98  loss_ce: 0.0004431  loss_mask: 0.192  loss_dice: 0.173  loss_ce_0: 0.06072  loss_mask_0: 0.1942  loss_dice_0: 0.1787  loss_ce_1: 0.0005113  loss_mask_1: 0.2053  loss_dice_1: 0.1749  loss_ce_2: 0.0003273  loss_mask_2: 0.1993  loss_dice_2: 0.1734  loss_ce_3: 0.0004295  loss_mask_3: 0.2052  loss_dice_3: 0.1699  loss_ce_4: 0.0003446  loss_mask_4: 0.1965  loss_dice_4: 0.1702  loss_ce_5: 0.0003686  loss_mask_5: 0.2026  loss_dice_5: 0.1812  loss_ce_6: 0.0005013  loss_mask_6: 0.2078  loss_dice_6: 0.1648  loss_ce_7: 0.0004714  loss_mask_7: 0.2043  loss_dice_7: 0.1671  loss_ce_8: 0.0004925  loss_mask_8: 0.1977  loss_dice_8: 0.1782  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:40:37] d2.utils.events INFO:  eta: 0:11:52  iter: 44379  total_loss: 3.799  loss_ce: 0.0003975  loss_mask: 0.1808  loss_dice: 0.1898  loss_ce_0: 0.06381  loss_mask_0: 0.1794  loss_dice_0: 0.1775  loss_ce_1: 0.0004721  loss_mask_1: 0.1926  loss_dice_1: 0.1767  loss_ce_2: 0.0003283  loss_mask_2: 0.182  loss_dice_2: 0.1781  loss_ce_3: 0.0004311  loss_mask_3: 0.193  loss_dice_3: 0.1755  loss_ce_4: 0.0003389  loss_mask_4: 0.1777  loss_dice_4: 0.189  loss_ce_5: 0.0003421  loss_mask_5: 0.182  loss_dice_5: 0.1843  loss_ce_6: 0.0005068  loss_mask_6: 0.1842  loss_dice_6: 0.1878  loss_ce_7: 0.0004782  loss_mask_7: 0.1798  loss_dice_7: 0.1814  loss_ce_8: 0.0004941  loss_mask_8: 0.1849  loss_dice_8: 0.1801  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:39] d2.utils.events INFO:  eta: 0:11:49  iter: 44399  total_loss: 3.944  loss_ce: 0.0004619  loss_mask: 0.223  loss_dice: 0.1923  loss_ce_0: 0.06342  loss_mask_0: 0.22  loss_dice_0: 0.1977  loss_ce_1: 0.000509  loss_mask_1: 0.2255  loss_dice_1: 0.2006  loss_ce_2: 0.0003512  loss_mask_2: 0.2233  loss_dice_2: 0.1997  loss_ce_3: 0.0004982  loss_mask_3: 0.2218  loss_dice_3: 0.2052  loss_ce_4: 0.0003972  loss_mask_4: 0.2193  loss_dice_4: 0.1905  loss_ce_5: 0.0003858  loss_mask_5: 0.2205  loss_dice_5: 0.1892  loss_ce_6: 0.0005199  loss_mask_6: 0.2184  loss_dice_6: 0.1955  loss_ce_7: 0.0005199  loss_mask_7: 0.2215  loss_dice_7: 0.2097  loss_ce_8: 0.000525  loss_mask_8: 0.2334  loss_dice_8: 0.2049  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:42] d2.utils.events INFO:  eta: 0:11:47  iter: 44419  total_loss: 3.712  loss_ce: 0.0004123  loss_mask: 0.178  loss_dice: 0.1872  loss_ce_0: 0.06178  loss_mask_0: 0.1703  loss_dice_0: 0.187  loss_ce_1: 0.0004433  loss_mask_1: 0.1752  loss_dice_1: 0.1892  loss_ce_2: 0.0002864  loss_mask_2: 0.1841  loss_dice_2: 0.1964  loss_ce_3: 0.0004924  loss_mask_3: 0.1672  loss_dice_3: 0.1859  loss_ce_4: 0.0003766  loss_mask_4: 0.176  loss_dice_4: 0.186  loss_ce_5: 0.0003518  loss_mask_5: 0.1725  loss_dice_5: 0.1902  loss_ce_6: 0.0005287  loss_mask_6: 0.1694  loss_dice_6: 0.1811  loss_ce_7: 0.0004095  loss_mask_7: 0.1736  loss_dice_7: 0.191  loss_ce_8: 0.000477  loss_mask_8: 0.1747  loss_dice_8: 0.1892  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:44] d2.utils.events INFO:  eta: 0:11:45  iter: 44439  total_loss: 5.213  loss_ce: 0.0006628  loss_mask: 0.1627  loss_dice: 0.2103  loss_ce_0: 0.0625  loss_mask_0: 0.172  loss_dice_0: 0.199  loss_ce_1: 0.0005149  loss_mask_1: 0.1659  loss_dice_1: 0.2042  loss_ce_2: 0.000368  loss_mask_2: 0.1665  loss_dice_2: 0.2008  loss_ce_3: 0.0006812  loss_mask_3: 0.1634  loss_dice_3: 0.1996  loss_ce_4: 0.0004701  loss_mask_4: 0.164  loss_dice_4: 0.2047  loss_ce_5: 0.0004672  loss_mask_5: 0.1608  loss_dice_5: 0.1996  loss_ce_6: 0.000609  loss_mask_6: 0.1662  loss_dice_6: 0.2069  loss_ce_7: 0.0006117  loss_mask_7: 0.1624  loss_dice_7: 0.2117  loss_ce_8: 0.000547  loss_mask_8: 0.1665  loss_dice_8: 0.2032  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:47] d2.utils.events INFO:  eta: 0:11:42  iter: 44459  total_loss: 3.92  loss_ce: 0.0003809  loss_mask: 0.1978  loss_dice: 0.1664  loss_ce_0: 0.06122  loss_mask_0: 0.1904  loss_dice_0: 0.161  loss_ce_1: 0.0004159  loss_mask_1: 0.1981  loss_dice_1: 0.1661  loss_ce_2: 0.0002457  loss_mask_2: 0.2024  loss_dice_2: 0.1618  loss_ce_3: 0.0003729  loss_mask_3: 0.1881  loss_dice_3: 0.16  loss_ce_4: 0.0003247  loss_mask_4: 0.187  loss_dice_4: 0.1606  loss_ce_5: 0.0003205  loss_mask_5: 0.1944  loss_dice_5: 0.1653  loss_ce_6: 0.0004596  loss_mask_6: 0.1925  loss_dice_6: 0.1657  loss_ce_7: 0.0003734  loss_mask_7: 0.1873  loss_dice_7: 0.1609  loss_ce_8: 0.0004562  loss_mask_8: 0.1918  loss_dice_8: 0.1599  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:49] d2.utils.events INFO:  eta: 0:11:40  iter: 44479  total_loss: 3.899  loss_ce: 0.0003458  loss_mask: 0.2015  loss_dice: 0.1678  loss_ce_0: 0.06175  loss_mask_0: 0.213  loss_dice_0: 0.1601  loss_ce_1: 0.0004359  loss_mask_1: 0.2163  loss_dice_1: 0.1656  loss_ce_2: 0.000297  loss_mask_2: 0.2121  loss_dice_2: 0.1603  loss_ce_3: 0.0003864  loss_mask_3: 0.2178  loss_dice_3: 0.1639  loss_ce_4: 0.0003456  loss_mask_4: 0.2036  loss_dice_4: 0.16  loss_ce_5: 0.0003374  loss_mask_5: 0.2102  loss_dice_5: 0.164  loss_ce_6: 0.0004666  loss_mask_6: 0.2015  loss_dice_6: 0.1684  loss_ce_7: 0.000378  loss_mask_7: 0.2147  loss_dice_7: 0.1704  loss_ce_8: 0.0004687  loss_mask_8: 0.2085  loss_dice_8: 0.1647  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:52] d2.utils.events INFO:  eta: 0:11:37  iter: 44499  total_loss: 4.645  loss_ce: 0.0005946  loss_mask: 0.2001  loss_dice: 0.1791  loss_ce_0: 0.06245  loss_mask_0: 0.205  loss_dice_0: 0.1854  loss_ce_1: 0.0005275  loss_mask_1: 0.2115  loss_dice_1: 0.1875  loss_ce_2: 0.0004822  loss_mask_2: 0.2054  loss_dice_2: 0.1909  loss_ce_3: 0.0005118  loss_mask_3: 0.2042  loss_dice_3: 0.1815  loss_ce_4: 0.0004236  loss_mask_4: 0.2028  loss_dice_4: 0.178  loss_ce_5: 0.0005119  loss_mask_5: 0.1996  loss_dice_5: 0.1815  loss_ce_6: 0.0005296  loss_mask_6: 0.2174  loss_dice_6: 0.1881  loss_ce_7: 0.0005163  loss_mask_7: 0.2089  loss_dice_7: 0.184  loss_ce_8: 0.0006614  loss_mask_8: 0.2084  loss_dice_8: 0.1863  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:40:54] d2.utils.events INFO:  eta: 0:11:34  iter: 44519  total_loss: 4.462  loss_ce: 0.000512  loss_mask: 0.156  loss_dice: 0.1858  loss_ce_0: 0.06227  loss_mask_0: 0.1435  loss_dice_0: 0.1822  loss_ce_1: 0.0005559  loss_mask_1: 0.1466  loss_dice_1: 0.1808  loss_ce_2: 0.0004483  loss_mask_2: 0.158  loss_dice_2: 0.1813  loss_ce_3: 0.000501  loss_mask_3: 0.1571  loss_dice_3: 0.1932  loss_ce_4: 0.0004479  loss_mask_4: 0.1526  loss_dice_4: 0.1836  loss_ce_5: 0.0004655  loss_mask_5: 0.1516  loss_dice_5: 0.1848  loss_ce_6: 0.0005622  loss_mask_6: 0.1599  loss_dice_6: 0.1831  loss_ce_7: 0.0005245  loss_mask_7: 0.1597  loss_dice_7: 0.1879  loss_ce_8: 0.0005866  loss_mask_8: 0.1575  loss_dice_8: 0.1762  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:57] d2.utils.events INFO:  eta: 0:11:31  iter: 44539  total_loss: 3.951  loss_ce: 0.0003886  loss_mask: 0.1914  loss_dice: 0.1991  loss_ce_0: 0.06127  loss_mask_0: 0.1838  loss_dice_0: 0.1898  loss_ce_1: 0.0004584  loss_mask_1: 0.1843  loss_dice_1: 0.2066  loss_ce_2: 0.0003321  loss_mask_2: 0.1847  loss_dice_2: 0.195  loss_ce_3: 0.0004497  loss_mask_3: 0.1885  loss_dice_3: 0.1771  loss_ce_4: 0.0004241  loss_mask_4: 0.1864  loss_dice_4: 0.1833  loss_ce_5: 0.0003661  loss_mask_5: 0.1783  loss_dice_5: 0.1994  loss_ce_6: 0.0004926  loss_mask_6: 0.1895  loss_dice_6: 0.1971  loss_ce_7: 0.0004431  loss_mask_7: 0.1836  loss_dice_7: 0.1819  loss_ce_8: 0.0005102  loss_mask_8: 0.1863  loss_dice_8: 0.1917  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:40:59] d2.utils.events INFO:  eta: 0:11:29  iter: 44559  total_loss: 4.621  loss_ce: 0.0005273  loss_mask: 0.1943  loss_dice: 0.188  loss_ce_0: 0.06291  loss_mask_0: 0.1957  loss_dice_0: 0.1811  loss_ce_1: 0.0005226  loss_mask_1: 0.1961  loss_dice_1: 0.1833  loss_ce_2: 0.000362  loss_mask_2: 0.2006  loss_dice_2: 0.1767  loss_ce_3: 0.0005461  loss_mask_3: 0.1999  loss_dice_3: 0.1887  loss_ce_4: 0.0003726  loss_mask_4: 0.2001  loss_dice_4: 0.1858  loss_ce_5: 0.0003862  loss_mask_5: 0.1955  loss_dice_5: 0.185  loss_ce_6: 0.0005549  loss_mask_6: 0.2011  loss_dice_6: 0.1852  loss_ce_7: 0.0004606  loss_mask_7: 0.1903  loss_dice_7: 0.1777  loss_ce_8: 0.0005372  loss_mask_8: 0.1927  loss_dice_8: 0.1814  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:02] d2.utils.events INFO:  eta: 0:11:26  iter: 44579  total_loss: 3.795  loss_ce: 0.0004694  loss_mask: 0.1897  loss_dice: 0.1514  loss_ce_0: 0.06078  loss_mask_0: 0.1963  loss_dice_0: 0.1457  loss_ce_1: 0.0004949  loss_mask_1: 0.1862  loss_dice_1: 0.1473  loss_ce_2: 0.0003442  loss_mask_2: 0.1926  loss_dice_2: 0.1529  loss_ce_3: 0.0004595  loss_mask_3: 0.1895  loss_dice_3: 0.1503  loss_ce_4: 0.000328  loss_mask_4: 0.1809  loss_dice_4: 0.1497  loss_ce_5: 0.0003586  loss_mask_5: 0.1851  loss_dice_5: 0.1497  loss_ce_6: 0.0005157  loss_mask_6: 0.1958  loss_dice_6: 0.1573  loss_ce_7: 0.0004359  loss_mask_7: 0.1939  loss_dice_7: 0.1501  loss_ce_8: 0.0005111  loss_mask_8: 0.1895  loss_dice_8: 0.1496  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:04] d2.utils.events INFO:  eta: 0:11:24  iter: 44599  total_loss: 4.355  loss_ce: 0.0004541  loss_mask: 0.1914  loss_dice: 0.2005  loss_ce_0: 0.06253  loss_mask_0: 0.1957  loss_dice_0: 0.1952  loss_ce_1: 0.0006501  loss_mask_1: 0.2034  loss_dice_1: 0.1917  loss_ce_2: 0.0003998  loss_mask_2: 0.1985  loss_dice_2: 0.2023  loss_ce_3: 0.0004912  loss_mask_3: 0.1942  loss_dice_3: 0.2004  loss_ce_4: 0.0004427  loss_mask_4: 0.1871  loss_dice_4: 0.1936  loss_ce_5: 0.0003901  loss_mask_5: 0.1963  loss_dice_5: 0.1952  loss_ce_6: 0.0005413  loss_mask_6: 0.1945  loss_dice_6: 0.2043  loss_ce_7: 0.0005284  loss_mask_7: 0.1999  loss_dice_7: 0.1967  loss_ce_8: 0.000582  loss_mask_8: 0.1929  loss_dice_8: 0.1931  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:07] d2.utils.events INFO:  eta: 0:11:21  iter: 44619  total_loss: 4.206  loss_ce: 0.0004842  loss_mask: 0.1818  loss_dice: 0.1901  loss_ce_0: 0.06132  loss_mask_0: 0.1737  loss_dice_0: 0.1792  loss_ce_1: 0.000483  loss_mask_1: 0.179  loss_dice_1: 0.1916  loss_ce_2: 0.0002904  loss_mask_2: 0.1795  loss_dice_2: 0.1837  loss_ce_3: 0.0004902  loss_mask_3: 0.1814  loss_dice_3: 0.1957  loss_ce_4: 0.0003285  loss_mask_4: 0.1753  loss_dice_4: 0.1915  loss_ce_5: 0.00038  loss_mask_5: 0.1745  loss_dice_5: 0.1812  loss_ce_6: 0.0005189  loss_mask_6: 0.1835  loss_dice_6: 0.1944  loss_ce_7: 0.0004359  loss_mask_7: 0.1861  loss_dice_7: 0.1914  loss_ce_8: 0.0005452  loss_mask_8: 0.1827  loss_dice_8: 0.1837  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:10] d2.utils.events INFO:  eta: 0:11:19  iter: 44639  total_loss: 4.694  loss_ce: 0.0005993  loss_mask: 0.2312  loss_dice: 0.1916  loss_ce_0: 0.06217  loss_mask_0: 0.2376  loss_dice_0: 0.176  loss_ce_1: 0.0005248  loss_mask_1: 0.2428  loss_dice_1: 0.1855  loss_ce_2: 0.0003169  loss_mask_2: 0.2474  loss_dice_2: 0.1898  loss_ce_3: 0.000557  loss_mask_3: 0.2379  loss_dice_3: 0.1836  loss_ce_4: 0.0004136  loss_mask_4: 0.242  loss_dice_4: 0.1916  loss_ce_5: 0.0003631  loss_mask_5: 0.2413  loss_dice_5: 0.1888  loss_ce_6: 0.0005594  loss_mask_6: 0.2391  loss_dice_6: 0.1864  loss_ce_7: 0.0005182  loss_mask_7: 0.241  loss_dice_7: 0.1862  loss_ce_8: 0.0005402  loss_mask_8: 0.2462  loss_dice_8: 0.1893  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:12] d2.utils.events INFO:  eta: 0:11:17  iter: 44659  total_loss: 4.046  loss_ce: 0.0004901  loss_mask: 0.247  loss_dice: 0.1735  loss_ce_0: 0.06148  loss_mask_0: 0.2527  loss_dice_0: 0.1662  loss_ce_1: 0.0004682  loss_mask_1: 0.2507  loss_dice_1: 0.1699  loss_ce_2: 0.0003657  loss_mask_2: 0.2503  loss_dice_2: 0.1686  loss_ce_3: 0.0004678  loss_mask_3: 0.2613  loss_dice_3: 0.1696  loss_ce_4: 0.0004496  loss_mask_4: 0.2504  loss_dice_4: 0.1716  loss_ce_5: 0.0005174  loss_mask_5: 0.244  loss_dice_5: 0.1667  loss_ce_6: 0.0006035  loss_mask_6: 0.2426  loss_dice_6: 0.1692  loss_ce_7: 0.0005341  loss_mask_7: 0.2608  loss_dice_7: 0.1637  loss_ce_8: 0.0005913  loss_mask_8: 0.2496  loss_dice_8: 0.1645  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:15] d2.utils.events INFO:  eta: 0:11:14  iter: 44679  total_loss: 4.119  loss_ce: 0.000488  loss_mask: 0.2074  loss_dice: 0.148  loss_ce_0: 0.06111  loss_mask_0: 0.2064  loss_dice_0: 0.1399  loss_ce_1: 0.0004915  loss_mask_1: 0.2098  loss_dice_1: 0.1451  loss_ce_2: 0.0003987  loss_mask_2: 0.2162  loss_dice_2: 0.1483  loss_ce_3: 0.0004792  loss_mask_3: 0.2141  loss_dice_3: 0.1515  loss_ce_4: 0.000374  loss_mask_4: 0.2102  loss_dice_4: 0.1508  loss_ce_5: 0.000461  loss_mask_5: 0.215  loss_dice_5: 0.1478  loss_ce_6: 0.00052  loss_mask_6: 0.2043  loss_dice_6: 0.1462  loss_ce_7: 0.0005744  loss_mask_7: 0.2211  loss_dice_7: 0.1508  loss_ce_8: 0.0006109  loss_mask_8: 0.2046  loss_dice_8: 0.1522  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:17] d2.utils.events INFO:  eta: 0:11:12  iter: 44699  total_loss: 3.967  loss_ce: 0.000606  loss_mask: 0.1595  loss_dice: 0.1278  loss_ce_0: 0.06311  loss_mask_0: 0.1619  loss_dice_0: 0.1225  loss_ce_1: 0.0005997  loss_mask_1: 0.1596  loss_dice_1: 0.1254  loss_ce_2: 0.0005177  loss_mask_2: 0.1578  loss_dice_2: 0.1188  loss_ce_3: 0.0005747  loss_mask_3: 0.1555  loss_dice_3: 0.1237  loss_ce_4: 0.0004828  loss_mask_4: 0.1672  loss_dice_4: 0.1252  loss_ce_5: 0.0005251  loss_mask_5: 0.158  loss_dice_5: 0.125  loss_ce_6: 0.0005815  loss_mask_6: 0.1553  loss_dice_6: 0.121  loss_ce_7: 0.0005395  loss_mask_7: 0.1619  loss_dice_7: 0.1227  loss_ce_8: 0.0006472  loss_mask_8: 0.1618  loss_dice_8: 0.1184  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:41:20] d2.utils.events INFO:  eta: 0:11:09  iter: 44719  total_loss: 3.937  loss_ce: 0.0004398  loss_mask: 0.1972  loss_dice: 0.1634  loss_ce_0: 0.06177  loss_mask_0: 0.1892  loss_dice_0: 0.1618  loss_ce_1: 0.0005874  loss_mask_1: 0.1981  loss_dice_1: 0.1654  loss_ce_2: 0.000368  loss_mask_2: 0.1967  loss_dice_2: 0.162  loss_ce_3: 0.0004491  loss_mask_3: 0.1905  loss_dice_3: 0.1663  loss_ce_4: 0.000478  loss_mask_4: 0.1977  loss_dice_4: 0.1617  loss_ce_5: 0.0003702  loss_mask_5: 0.1898  loss_dice_5: 0.165  loss_ce_6: 0.0005095  loss_mask_6: 0.1915  loss_dice_6: 0.1607  loss_ce_7: 0.0004305  loss_mask_7: 0.1936  loss_dice_7: 0.164  loss_ce_8: 0.0005238  loss_mask_8: 0.1985  loss_dice_8: 0.1654  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:22] d2.utils.events INFO:  eta: 0:11:07  iter: 44739  total_loss: 3.836  loss_ce: 0.0004797  loss_mask: 0.2112  loss_dice: 0.1508  loss_ce_0: 0.05995  loss_mask_0: 0.2113  loss_dice_0: 0.1527  loss_ce_1: 0.0004434  loss_mask_1: 0.2086  loss_dice_1: 0.1558  loss_ce_2: 0.0003029  loss_mask_2: 0.216  loss_dice_2: 0.1575  loss_ce_3: 0.0005024  loss_mask_3: 0.2067  loss_dice_3: 0.1502  loss_ce_4: 0.0004258  loss_mask_4: 0.213  loss_dice_4: 0.1526  loss_ce_5: 0.0004547  loss_mask_5: 0.2114  loss_dice_5: 0.158  loss_ce_6: 0.0005513  loss_mask_6: 0.206  loss_dice_6: 0.1523  loss_ce_7: 0.0004707  loss_mask_7: 0.2017  loss_dice_7: 0.1527  loss_ce_8: 0.0005546  loss_mask_8: 0.2074  loss_dice_8: 0.1506  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:41:25] d2.utils.events INFO:  eta: 0:11:04  iter: 44759  total_loss: 4.039  loss_ce: 0.0005538  loss_mask: 0.1727  loss_dice: 0.1759  loss_ce_0: 0.06397  loss_mask_0: 0.1711  loss_dice_0: 0.1739  loss_ce_1: 0.0006214  loss_mask_1: 0.1776  loss_dice_1: 0.1767  loss_ce_2: 0.0005059  loss_mask_2: 0.1743  loss_dice_2: 0.1778  loss_ce_3: 0.00052  loss_mask_3: 0.1749  loss_dice_3: 0.1866  loss_ce_4: 0.000448  loss_mask_4: 0.1791  loss_dice_4: 0.1832  loss_ce_5: 0.0004959  loss_mask_5: 0.176  loss_dice_5: 0.1802  loss_ce_6: 0.0006276  loss_mask_6: 0.179  loss_dice_6: 0.1849  loss_ce_7: 0.0005928  loss_mask_7: 0.1691  loss_dice_7: 0.1787  loss_ce_8: 0.0006957  loss_mask_8: 0.1699  loss_dice_8: 0.1785  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:27] d2.utils.events INFO:  eta: 0:11:02  iter: 44779  total_loss: 4.705  loss_ce: 0.002692  loss_mask: 0.1918  loss_dice: 0.1715  loss_ce_0: 0.06376  loss_mask_0: 0.1857  loss_dice_0: 0.1781  loss_ce_1: 0.008611  loss_mask_1: 0.1855  loss_dice_1: 0.1766  loss_ce_2: 0.002033  loss_mask_2: 0.1852  loss_dice_2: 0.1751  loss_ce_3: 0.002483  loss_mask_3: 0.1899  loss_dice_3: 0.1758  loss_ce_4: 0.005434  loss_mask_4: 0.1927  loss_dice_4: 0.1766  loss_ce_5: 0.002002  loss_mask_5: 0.1856  loss_dice_5: 0.1796  loss_ce_6: 0.002376  loss_mask_6: 0.1861  loss_dice_6: 0.1726  loss_ce_7: 0.003577  loss_mask_7: 0.1878  loss_dice_7: 0.1711  loss_ce_8: 0.001648  loss_mask_8: 0.1946  loss_dice_8: 0.1763  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:30] d2.utils.events INFO:  eta: 0:10:59  iter: 44799  total_loss: 4.184  loss_ce: 0.0006214  loss_mask: 0.1554  loss_dice: 0.2102  loss_ce_0: 0.05995  loss_mask_0: 0.1595  loss_dice_0: 0.2139  loss_ce_1: 0.0006531  loss_mask_1: 0.1564  loss_dice_1: 0.2162  loss_ce_2: 0.0005033  loss_mask_2: 0.1645  loss_dice_2: 0.2197  loss_ce_3: 0.0006185  loss_mask_3: 0.1578  loss_dice_3: 0.2148  loss_ce_4: 0.0007565  loss_mask_4: 0.1609  loss_dice_4: 0.2108  loss_ce_5: 0.0005687  loss_mask_5: 0.1544  loss_dice_5: 0.216  loss_ce_6: 0.0006899  loss_mask_6: 0.1507  loss_dice_6: 0.2157  loss_ce_7: 0.0009205  loss_mask_7: 0.1616  loss_dice_7: 0.2182  loss_ce_8: 0.0007621  loss_mask_8: 0.163  loss_dice_8: 0.2034  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:32] d2.utils.events INFO:  eta: 0:10:57  iter: 44819  total_loss: 3.692  loss_ce: 0.001827  loss_mask: 0.1572  loss_dice: 0.16  loss_ce_0: 0.06177  loss_mask_0: 0.1532  loss_dice_0: 0.16  loss_ce_1: 0.0007787  loss_mask_1: 0.1648  loss_dice_1: 0.1613  loss_ce_2: 0.000976  loss_mask_2: 0.1628  loss_dice_2: 0.1617  loss_ce_3: 0.002185  loss_mask_3: 0.1629  loss_dice_3: 0.161  loss_ce_4: 0.001068  loss_mask_4: 0.1654  loss_dice_4: 0.1541  loss_ce_5: 0.001047  loss_mask_5: 0.1535  loss_dice_5: 0.1618  loss_ce_6: 0.001581  loss_mask_6: 0.1588  loss_dice_6: 0.16  loss_ce_7: 0.001222  loss_mask_7: 0.1649  loss_dice_7: 0.163  loss_ce_8: 0.001094  loss_mask_8: 0.1632  loss_dice_8: 0.1631  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:35] d2.utils.events INFO:  eta: 0:10:55  iter: 44839  total_loss: 4.154  loss_ce: 0.0005566  loss_mask: 0.1972  loss_dice: 0.1839  loss_ce_0: 0.06326  loss_mask_0: 0.1939  loss_dice_0: 0.1815  loss_ce_1: 0.0005162  loss_mask_1: 0.2034  loss_dice_1: 0.1869  loss_ce_2: 0.0004857  loss_mask_2: 0.195  loss_dice_2: 0.1756  loss_ce_3: 0.0005853  loss_mask_3: 0.2001  loss_dice_3: 0.1796  loss_ce_4: 0.0004722  loss_mask_4: 0.1813  loss_dice_4: 0.1779  loss_ce_5: 0.0005668  loss_mask_5: 0.1858  loss_dice_5: 0.181  loss_ce_6: 0.0006375  loss_mask_6: 0.1946  loss_dice_6: 0.1854  loss_ce_7: 0.0005333  loss_mask_7: 0.1991  loss_dice_7: 0.184  loss_ce_8: 0.0007614  loss_mask_8: 0.1893  loss_dice_8: 0.1795  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:41:37] d2.utils.events INFO:  eta: 0:10:53  iter: 44859  total_loss: 3.856  loss_ce: 0.0003986  loss_mask: 0.2305  loss_dice: 0.1449  loss_ce_0: 0.06293  loss_mask_0: 0.2305  loss_dice_0: 0.1511  loss_ce_1: 0.0004796  loss_mask_1: 0.2271  loss_dice_1: 0.1525  loss_ce_2: 0.0003505  loss_mask_2: 0.2324  loss_dice_2: 0.1471  loss_ce_3: 0.0004444  loss_mask_3: 0.2305  loss_dice_3: 0.1436  loss_ce_4: 0.0003771  loss_mask_4: 0.2205  loss_dice_4: 0.1464  loss_ce_5: 0.000375  loss_mask_5: 0.2226  loss_dice_5: 0.1444  loss_ce_6: 0.0005271  loss_mask_6: 0.2199  loss_dice_6: 0.1457  loss_ce_7: 0.0004147  loss_mask_7: 0.2305  loss_dice_7: 0.1533  loss_ce_8: 0.0005056  loss_mask_8: 0.2286  loss_dice_8: 0.1422  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:40] d2.utils.events INFO:  eta: 0:10:50  iter: 44879  total_loss: 3.952  loss_ce: 0.0004642  loss_mask: 0.1831  loss_dice: 0.1736  loss_ce_0: 0.06177  loss_mask_0: 0.1847  loss_dice_0: 0.1791  loss_ce_1: 0.0005272  loss_mask_1: 0.198  loss_dice_1: 0.1808  loss_ce_2: 0.000454  loss_mask_2: 0.2013  loss_dice_2: 0.1761  loss_ce_3: 0.0004489  loss_mask_3: 0.1945  loss_dice_3: 0.18  loss_ce_4: 0.0004292  loss_mask_4: 0.1978  loss_dice_4: 0.174  loss_ce_5: 0.0005797  loss_mask_5: 0.1978  loss_dice_5: 0.184  loss_ce_6: 0.0005363  loss_mask_6: 0.1956  loss_dice_6: 0.1815  loss_ce_7: 0.0005481  loss_mask_7: 0.189  loss_dice_7: 0.1837  loss_ce_8: 0.000597  loss_mask_8: 0.1906  loss_dice_8: 0.1791  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:43] d2.utils.events INFO:  eta: 0:10:47  iter: 44899  total_loss: 4.308  loss_ce: 0.0003723  loss_mask: 0.2392  loss_dice: 0.1699  loss_ce_0: 0.06281  loss_mask_0: 0.2421  loss_dice_0: 0.1726  loss_ce_1: 0.0004079  loss_mask_1: 0.2398  loss_dice_1: 0.1761  loss_ce_2: 0.0002629  loss_mask_2: 0.2389  loss_dice_2: 0.1671  loss_ce_3: 0.000414  loss_mask_3: 0.2357  loss_dice_3: 0.1698  loss_ce_4: 0.0003598  loss_mask_4: 0.2395  loss_dice_4: 0.1722  loss_ce_5: 0.0003318  loss_mask_5: 0.2355  loss_dice_5: 0.1703  loss_ce_6: 0.000486  loss_mask_6: 0.2349  loss_dice_6: 0.1699  loss_ce_7: 0.0003833  loss_mask_7: 0.2444  loss_dice_7: 0.1673  loss_ce_8: 0.0004522  loss_mask_8: 0.2382  loss_dice_8: 0.1676  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:45] d2.utils.events INFO:  eta: 0:10:45  iter: 44919  total_loss: 3.854  loss_ce: 0.0004108  loss_mask: 0.205  loss_dice: 0.1326  loss_ce_0: 0.06176  loss_mask_0: 0.2143  loss_dice_0: 0.1388  loss_ce_1: 0.0003921  loss_mask_1: 0.2109  loss_dice_1: 0.1356  loss_ce_2: 0.0002882  loss_mask_2: 0.2113  loss_dice_2: 0.1341  loss_ce_3: 0.0003528  loss_mask_3: 0.21  loss_dice_3: 0.1325  loss_ce_4: 0.0003562  loss_mask_4: 0.2107  loss_dice_4: 0.1326  loss_ce_5: 0.0003657  loss_mask_5: 0.2139  loss_dice_5: 0.1342  loss_ce_6: 0.0004461  loss_mask_6: 0.2103  loss_dice_6: 0.1383  loss_ce_7: 0.0004255  loss_mask_7: 0.2115  loss_dice_7: 0.1311  loss_ce_8: 0.0004786  loss_mask_8: 0.2175  loss_dice_8: 0.1355  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:48] d2.utils.events INFO:  eta: 0:10:42  iter: 44939  total_loss: 3.886  loss_ce: 0.001049  loss_mask: 0.1199  loss_dice: 0.2283  loss_ce_0: 0.06175  loss_mask_0: 0.1135  loss_dice_0: 0.2245  loss_ce_1: 0.0009942  loss_mask_1: 0.1197  loss_dice_1: 0.2241  loss_ce_2: 0.001176  loss_mask_2: 0.1216  loss_dice_2: 0.2317  loss_ce_3: 0.001059  loss_mask_3: 0.1199  loss_dice_3: 0.2293  loss_ce_4: 0.001046  loss_mask_4: 0.119  loss_dice_4: 0.2316  loss_ce_5: 0.00121  loss_mask_5: 0.1215  loss_dice_5: 0.2196  loss_ce_6: 0.0009226  loss_mask_6: 0.117  loss_dice_6: 0.2211  loss_ce_7: 0.001283  loss_mask_7: 0.1132  loss_dice_7: 0.2316  loss_ce_8: 0.001017  loss_mask_8: 0.1247  loss_dice_8: 0.2326  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:41:50] d2.utils.events INFO:  eta: 0:10:39  iter: 44959  total_loss: 4.264  loss_ce: 0.0005803  loss_mask: 0.2119  loss_dice: 0.1692  loss_ce_0: 0.06209  loss_mask_0: 0.1983  loss_dice_0: 0.1652  loss_ce_1: 0.0004926  loss_mask_1: 0.2108  loss_dice_1: 0.1678  loss_ce_2: 0.0004125  loss_mask_2: 0.2194  loss_dice_2: 0.1727  loss_ce_3: 0.0005065  loss_mask_3: 0.2132  loss_dice_3: 0.172  loss_ce_4: 0.0004127  loss_mask_4: 0.2095  loss_dice_4: 0.1702  loss_ce_5: 0.0004214  loss_mask_5: 0.1986  loss_dice_5: 0.1594  loss_ce_6: 0.0006179  loss_mask_6: 0.205  loss_dice_6: 0.163  loss_ce_7: 0.0005227  loss_mask_7: 0.2034  loss_dice_7: 0.1683  loss_ce_8: 0.0005766  loss_mask_8: 0.2089  loss_dice_8: 0.1678  time: 0.1260  data_time: 0.0012  lr: 1e-05  max_mem: 3105M
[04/13 16:41:53] d2.utils.events INFO:  eta: 0:10:37  iter: 44979  total_loss: 3.632  loss_ce: 0.0004282  loss_mask: 0.1847  loss_dice: 0.1494  loss_ce_0: 0.06174  loss_mask_0: 0.1846  loss_dice_0: 0.1399  loss_ce_1: 0.0004371  loss_mask_1: 0.1848  loss_dice_1: 0.1369  loss_ce_2: 0.0003639  loss_mask_2: 0.1846  loss_dice_2: 0.1364  loss_ce_3: 0.0004514  loss_mask_3: 0.1785  loss_dice_3: 0.1446  loss_ce_4: 0.0004274  loss_mask_4: 0.1809  loss_dice_4: 0.1436  loss_ce_5: 0.0004129  loss_mask_5: 0.1893  loss_dice_5: 0.1437  loss_ce_6: 0.0005278  loss_mask_6: 0.1841  loss_dice_6: 0.1461  loss_ce_7: 0.000493  loss_mask_7: 0.1783  loss_dice_7: 0.1395  loss_ce_8: 0.0005115  loss_mask_8: 0.1802  loss_dice_8: 0.1396  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:41:55] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0044999.pth
[04/13 16:41:55] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 16:41:55] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 16:41:55] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 16:41:55] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 16:41:55] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 16:41:59] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0005 s/iter. Inference: 0.0526 s/iter. Eval: 0.2293 s/iter. Total: 0.2825 s/iter. ETA=0:03:58
[04/13 16:42:04] d2.evaluation.evaluator INFO: Inference done 29/856. Dataloading: 0.0007 s/iter. Inference: 0.0528 s/iter. Eval: 0.2287 s/iter. Total: 0.2823 s/iter. ETA=0:03:53
[04/13 16:42:09] d2.evaluation.evaluator INFO: Inference done 47/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2286 s/iter. Total: 0.2824 s/iter. ETA=0:03:48
[04/13 16:42:14] d2.evaluation.evaluator INFO: Inference done 65/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2288 s/iter. Total: 0.2826 s/iter. ETA=0:03:43
[04/13 16:42:19] d2.evaluation.evaluator INFO: Inference done 83/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2304 s/iter. Total: 0.2841 s/iter. ETA=0:03:39
[04/13 16:42:24] d2.evaluation.evaluator INFO: Inference done 101/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2303 s/iter. Total: 0.2840 s/iter. ETA=0:03:34
[04/13 16:42:29] d2.evaluation.evaluator INFO: Inference done 119/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2305 s/iter. Total: 0.2843 s/iter. ETA=0:03:29
[04/13 16:42:35] d2.evaluation.evaluator INFO: Inference done 137/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2308 s/iter. Total: 0.2845 s/iter. ETA=0:03:24
[04/13 16:42:40] d2.evaluation.evaluator INFO: Inference done 155/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2310 s/iter. Total: 0.2848 s/iter. ETA=0:03:19
[04/13 16:42:45] d2.evaluation.evaluator INFO: Inference done 173/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2310 s/iter. Total: 0.2848 s/iter. ETA=0:03:14
[04/13 16:42:50] d2.evaluation.evaluator INFO: Inference done 191/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2311 s/iter. Total: 0.2850 s/iter. ETA=0:03:09
[04/13 16:42:55] d2.evaluation.evaluator INFO: Inference done 209/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2311 s/iter. Total: 0.2850 s/iter. ETA=0:03:04
[04/13 16:43:00] d2.evaluation.evaluator INFO: Inference done 227/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2313 s/iter. Total: 0.2852 s/iter. ETA=0:02:59
[04/13 16:43:06] d2.evaluation.evaluator INFO: Inference done 245/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2315 s/iter. Total: 0.2854 s/iter. ETA=0:02:54
[04/13 16:43:11] d2.evaluation.evaluator INFO: Inference done 263/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2316 s/iter. Total: 0.2854 s/iter. ETA=0:02:49
[04/13 16:43:16] d2.evaluation.evaluator INFO: Inference done 281/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2316 s/iter. Total: 0.2855 s/iter. ETA=0:02:44
[04/13 16:43:21] d2.evaluation.evaluator INFO: Inference done 299/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2320 s/iter. Total: 0.2859 s/iter. ETA=0:02:39
[04/13 16:43:26] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2320 s/iter. Total: 0.2860 s/iter. ETA=0:02:34
[04/13 16:43:31] d2.evaluation.evaluator INFO: Inference done 335/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2320 s/iter. Total: 0.2860 s/iter. ETA=0:02:29
[04/13 16:43:37] d2.evaluation.evaluator INFO: Inference done 353/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2323 s/iter. Total: 0.2863 s/iter. ETA=0:02:24
[04/13 16:43:42] d2.evaluation.evaluator INFO: Inference done 371/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2324 s/iter. Total: 0.2864 s/iter. ETA=0:02:18
[04/13 16:43:47] d2.evaluation.evaluator INFO: Inference done 389/856. Dataloading: 0.0008 s/iter. Inference: 0.0532 s/iter. Eval: 0.2325 s/iter. Total: 0.2866 s/iter. ETA=0:02:13
[04/13 16:43:52] d2.evaluation.evaluator INFO: Inference done 407/856. Dataloading: 0.0008 s/iter. Inference: 0.0532 s/iter. Eval: 0.2326 s/iter. Total: 0.2866 s/iter. ETA=0:02:08
[04/13 16:43:57] d2.evaluation.evaluator INFO: Inference done 425/856. Dataloading: 0.0008 s/iter. Inference: 0.0532 s/iter. Eval: 0.2325 s/iter. Total: 0.2866 s/iter. ETA=0:02:03
[04/13 16:44:03] d2.evaluation.evaluator INFO: Inference done 443/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2325 s/iter. Total: 0.2865 s/iter. ETA=0:01:58
[04/13 16:44:08] d2.evaluation.evaluator INFO: Inference done 461/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2325 s/iter. Total: 0.2865 s/iter. ETA=0:01:53
[04/13 16:44:13] d2.evaluation.evaluator INFO: Inference done 479/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2325 s/iter. Total: 0.2865 s/iter. ETA=0:01:48
[04/13 16:44:18] d2.evaluation.evaluator INFO: Inference done 497/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2325 s/iter. Total: 0.2865 s/iter. ETA=0:01:42
[04/13 16:44:23] d2.evaluation.evaluator INFO: Inference done 515/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2325 s/iter. Total: 0.2865 s/iter. ETA=0:01:37
[04/13 16:44:28] d2.evaluation.evaluator INFO: Inference done 533/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2327 s/iter. Total: 0.2867 s/iter. ETA=0:01:32
[04/13 16:44:34] d2.evaluation.evaluator INFO: Inference done 551/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2327 s/iter. Total: 0.2867 s/iter. ETA=0:01:27
[04/13 16:44:39] d2.evaluation.evaluator INFO: Inference done 569/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2326 s/iter. Total: 0.2865 s/iter. ETA=0:01:22
[04/13 16:44:44] d2.evaluation.evaluator INFO: Inference done 587/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2326 s/iter. Total: 0.2865 s/iter. ETA=0:01:17
[04/13 16:44:49] d2.evaluation.evaluator INFO: Inference done 605/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2325 s/iter. Total: 0.2864 s/iter. ETA=0:01:11
[04/13 16:44:54] d2.evaluation.evaluator INFO: Inference done 623/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2325 s/iter. Total: 0.2864 s/iter. ETA=0:01:06
[04/13 16:44:59] d2.evaluation.evaluator INFO: Inference done 641/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2325 s/iter. Total: 0.2864 s/iter. ETA=0:01:01
[04/13 16:45:04] d2.evaluation.evaluator INFO: Inference done 659/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2325 s/iter. Total: 0.2864 s/iter. ETA=0:00:56
[04/13 16:45:10] d2.evaluation.evaluator INFO: Inference done 677/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2325 s/iter. Total: 0.2864 s/iter. ETA=0:00:51
[04/13 16:45:15] d2.evaluation.evaluator INFO: Inference done 695/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2326 s/iter. Total: 0.2865 s/iter. ETA=0:00:46
[04/13 16:45:20] d2.evaluation.evaluator INFO: Inference done 713/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2326 s/iter. Total: 0.2865 s/iter. ETA=0:00:40
[04/13 16:45:25] d2.evaluation.evaluator INFO: Inference done 731/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2326 s/iter. Total: 0.2865 s/iter. ETA=0:00:35
[04/13 16:45:30] d2.evaluation.evaluator INFO: Inference done 749/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2327 s/iter. Total: 0.2866 s/iter. ETA=0:00:30
[04/13 16:45:35] d2.evaluation.evaluator INFO: Inference done 766/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2868 s/iter. ETA=0:00:25
[04/13 16:45:40] d2.evaluation.evaluator INFO: Inference done 784/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2867 s/iter. ETA=0:00:20
[04/13 16:45:46] d2.evaluation.evaluator INFO: Inference done 802/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2328 s/iter. Total: 0.2867 s/iter. ETA=0:00:15
[04/13 16:45:51] d2.evaluation.evaluator INFO: Inference done 820/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2867 s/iter. ETA=0:00:10
[04/13 16:45:56] d2.evaluation.evaluator INFO: Inference done 838/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2867 s/iter. ETA=0:00:05
[04/13 16:46:01] d2.evaluation.evaluator INFO: Inference done 856/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2329 s/iter. Total: 0.2867 s/iter. ETA=0:00:00
[04/13 16:46:01] d2.evaluation.evaluator INFO: Total inference time: 0:04:04.088212 (0.286825 s / iter per device, on 1 devices)
[04/13 16:46:01] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:45 (0.052978 s / iter per device, on 1 devices)
[04/13 16:46:02] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 16:46:02] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 16:46:04] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 16:46:04] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 16:46:04] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.27 seconds.
[04/13 16:46:04] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 16:46:04] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.04 seconds.
[04/13 16:46:04] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 16:46:04] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 16:46:04] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 16:46:08] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 16:46:09] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 1.41 seconds.
[04/13 16:46:09] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 16:46:09] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 16:46:09] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 85.378 | 85.378 | 85.378 |  nan  |  nan  | 85.378 |
[04/13 16:46:09] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 16:46:09] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 87.970 | defect     | 82.786 |
[04/13 16:46:09] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 16:46:09] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 16:46:09] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 16:46:09] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 16:46:09] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 16:46:09] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 16:46:09] d2.evaluation.testing INFO: copypaste: 85.3780,85.3780,85.3780,nan,nan,85.3780
[04/13 16:46:10] d2.utils.events INFO:  eta: 0:10:35  iter: 44999  total_loss: 3.707  loss_ce: 0.0003363  loss_mask: 0.1877  loss_dice: 0.1614  loss_ce_0: 0.06191  loss_mask_0: 0.1902  loss_dice_0: 0.1599  loss_ce_1: 0.0003727  loss_mask_1: 0.1829  loss_dice_1: 0.1511  loss_ce_2: 0.0002361  loss_mask_2: 0.1793  loss_dice_2: 0.1638  loss_ce_3: 0.0003967  loss_mask_3: 0.1812  loss_dice_3: 0.1583  loss_ce_4: 0.0003391  loss_mask_4: 0.1796  loss_dice_4: 0.1584  loss_ce_5: 0.0003918  loss_mask_5: 0.1867  loss_dice_5: 0.1586  loss_ce_6: 0.0004549  loss_mask_6: 0.1872  loss_dice_6: 0.1569  loss_ce_7: 0.0003641  loss_mask_7: 0.1875  loss_dice_7: 0.16  loss_ce_8: 0.0004897  loss_mask_8: 0.1834  loss_dice_8: 0.1604  time: 0.1260  data_time: 0.0013  lr: 1e-05  max_mem: 3105M
[04/13 16:46:12] d2.utils.events INFO:  eta: 0:10:32  iter: 45019  total_loss: 4.066  loss_ce: 0.0006755  loss_mask: 0.1696  loss_dice: 0.2049  loss_ce_0: 0.06175  loss_mask_0: 0.1712  loss_dice_0: 0.2078  loss_ce_1: 0.000495  loss_mask_1: 0.1697  loss_dice_1: 0.2056  loss_ce_2: 0.0005509  loss_mask_2: 0.1754  loss_dice_2: 0.2035  loss_ce_3: 0.0006488  loss_mask_3: 0.1771  loss_dice_3: 0.198  loss_ce_4: 0.0006424  loss_mask_4: 0.1731  loss_dice_4: 0.2076  loss_ce_5: 0.0005897  loss_mask_5: 0.1643  loss_dice_5: 0.2034  loss_ce_6: 0.0006504  loss_mask_6: 0.166  loss_dice_6: 0.2102  loss_ce_7: 0.0007661  loss_mask_7: 0.1679  loss_dice_7: 0.2129  loss_ce_8: 0.000649  loss_mask_8: 0.1753  loss_dice_8: 0.1972  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:15] d2.utils.events INFO:  eta: 0:10:29  iter: 45039  total_loss: 4.364  loss_ce: 0.0005915  loss_mask: 0.218  loss_dice: 0.1585  loss_ce_0: 0.06135  loss_mask_0: 0.2279  loss_dice_0: 0.1666  loss_ce_1: 0.0006638  loss_mask_1: 0.219  loss_dice_1: 0.1674  loss_ce_2: 0.0005479  loss_mask_2: 0.2166  loss_dice_2: 0.1659  loss_ce_3: 0.0005203  loss_mask_3: 0.2125  loss_dice_3: 0.1598  loss_ce_4: 0.0006116  loss_mask_4: 0.2108  loss_dice_4: 0.1542  loss_ce_5: 0.0005283  loss_mask_5: 0.2046  loss_dice_5: 0.1547  loss_ce_6: 0.0005655  loss_mask_6: 0.2092  loss_dice_6: 0.1653  loss_ce_7: 0.0006274  loss_mask_7: 0.2085  loss_dice_7: 0.1643  loss_ce_8: 0.0006558  loss_mask_8: 0.2139  loss_dice_8: 0.163  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:17] d2.utils.events INFO:  eta: 0:10:27  iter: 45059  total_loss: 4.27  loss_ce: 0.0003934  loss_mask: 0.2002  loss_dice: 0.1597  loss_ce_0: 0.06133  loss_mask_0: 0.2181  loss_dice_0: 0.1602  loss_ce_1: 0.000382  loss_mask_1: 0.2119  loss_dice_1: 0.1537  loss_ce_2: 0.0002843  loss_mask_2: 0.2049  loss_dice_2: 0.1542  loss_ce_3: 0.0004034  loss_mask_3: 0.1979  loss_dice_3: 0.1575  loss_ce_4: 0.0003003  loss_mask_4: 0.2105  loss_dice_4: 0.1508  loss_ce_5: 0.0003431  loss_mask_5: 0.2043  loss_dice_5: 0.154  loss_ce_6: 0.0004687  loss_mask_6: 0.2097  loss_dice_6: 0.1553  loss_ce_7: 0.0003828  loss_mask_7: 0.2101  loss_dice_7: 0.147  loss_ce_8: 0.0004457  loss_mask_8: 0.2124  loss_dice_8: 0.1563  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:20] d2.utils.events INFO:  eta: 0:10:24  iter: 45079  total_loss: 4.014  loss_ce: 0.0007503  loss_mask: 0.1945  loss_dice: 0.1762  loss_ce_0: 0.06176  loss_mask_0: 0.1926  loss_dice_0: 0.1924  loss_ce_1: 0.0007469  loss_mask_1: 0.1959  loss_dice_1: 0.187  loss_ce_2: 0.0009903  loss_mask_2: 0.1869  loss_dice_2: 0.1864  loss_ce_3: 0.0006137  loss_mask_3: 0.1955  loss_dice_3: 0.1845  loss_ce_4: 0.0006602  loss_mask_4: 0.202  loss_dice_4: 0.1933  loss_ce_5: 0.0007962  loss_mask_5: 0.1874  loss_dice_5: 0.1816  loss_ce_6: 0.0006564  loss_mask_6: 0.196  loss_dice_6: 0.1903  loss_ce_7: 0.0006221  loss_mask_7: 0.1884  loss_dice_7: 0.1904  loss_ce_8: 0.0007898  loss_mask_8: 0.1947  loss_dice_8: 0.188  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:22] d2.utils.events INFO:  eta: 0:10:21  iter: 45099  total_loss: 4.192  loss_ce: 0.0003846  loss_mask: 0.2176  loss_dice: 0.1828  loss_ce_0: 0.06135  loss_mask_0: 0.2326  loss_dice_0: 0.1764  loss_ce_1: 0.0004666  loss_mask_1: 0.2245  loss_dice_1: 0.1773  loss_ce_2: 0.0003224  loss_mask_2: 0.2315  loss_dice_2: 0.1722  loss_ce_3: 0.0004145  loss_mask_3: 0.2225  loss_dice_3: 0.1707  loss_ce_4: 0.0004893  loss_mask_4: 0.2146  loss_dice_4: 0.1724  loss_ce_5: 0.0004103  loss_mask_5: 0.22  loss_dice_5: 0.1817  loss_ce_6: 0.0005304  loss_mask_6: 0.2268  loss_dice_6: 0.17  loss_ce_7: 0.0004108  loss_mask_7: 0.2317  loss_dice_7: 0.1733  loss_ce_8: 0.0004963  loss_mask_8: 0.2281  loss_dice_8: 0.1769  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:46:25] d2.utils.events INFO:  eta: 0:10:18  iter: 45119  total_loss: 4.26  loss_ce: 0.0005452  loss_mask: 0.1861  loss_dice: 0.1994  loss_ce_0: 0.06132  loss_mask_0: 0.1887  loss_dice_0: 0.1954  loss_ce_1: 0.0004203  loss_mask_1: 0.1929  loss_dice_1: 0.1975  loss_ce_2: 0.0003541  loss_mask_2: 0.1797  loss_dice_2: 0.1881  loss_ce_3: 0.0004798  loss_mask_3: 0.1898  loss_dice_3: 0.2126  loss_ce_4: 0.0003961  loss_mask_4: 0.1867  loss_dice_4: 0.1988  loss_ce_5: 0.0003713  loss_mask_5: 0.1913  loss_dice_5: 0.1948  loss_ce_6: 0.0005076  loss_mask_6: 0.1932  loss_dice_6: 0.1968  loss_ce_7: 0.0004164  loss_mask_7: 0.1956  loss_dice_7: 0.1977  loss_ce_8: 0.0004604  loss_mask_8: 0.1967  loss_dice_8: 0.1899  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:27] d2.utils.events INFO:  eta: 0:10:16  iter: 45139  total_loss: 3.949  loss_ce: 0.000777  loss_mask: 0.2017  loss_dice: 0.1815  loss_ce_0: 0.0613  loss_mask_0: 0.2014  loss_dice_0: 0.1807  loss_ce_1: 0.0004858  loss_mask_1: 0.1933  loss_dice_1: 0.1888  loss_ce_2: 0.0005031  loss_mask_2: 0.1975  loss_dice_2: 0.1884  loss_ce_3: 0.0007521  loss_mask_3: 0.2002  loss_dice_3: 0.1852  loss_ce_4: 0.0005305  loss_mask_4: 0.1946  loss_dice_4: 0.1732  loss_ce_5: 0.0005779  loss_mask_5: 0.1979  loss_dice_5: 0.1781  loss_ce_6: 0.000736  loss_mask_6: 0.2017  loss_dice_6: 0.1858  loss_ce_7: 0.0005036  loss_mask_7: 0.1981  loss_dice_7: 0.1782  loss_ce_8: 0.0006073  loss_mask_8: 0.199  loss_dice_8: 0.1825  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:30] d2.utils.events INFO:  eta: 0:10:13  iter: 45159  total_loss: 3.92  loss_ce: 0.000365  loss_mask: 0.2022  loss_dice: 0.198  loss_ce_0: 0.06174  loss_mask_0: 0.2024  loss_dice_0: 0.1914  loss_ce_1: 0.0004354  loss_mask_1: 0.1848  loss_dice_1: 0.1915  loss_ce_2: 0.0003017  loss_mask_2: 0.1913  loss_dice_2: 0.1981  loss_ce_3: 0.0004904  loss_mask_3: 0.189  loss_dice_3: 0.1923  loss_ce_4: 0.0003935  loss_mask_4: 0.1958  loss_dice_4: 0.1937  loss_ce_5: 0.0003469  loss_mask_5: 0.1849  loss_dice_5: 0.1871  loss_ce_6: 0.0005129  loss_mask_6: 0.192  loss_dice_6: 0.1973  loss_ce_7: 0.0003641  loss_mask_7: 0.1982  loss_dice_7: 0.1976  loss_ce_8: 0.0004549  loss_mask_8: 0.1944  loss_dice_8: 0.1925  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:32] d2.utils.events INFO:  eta: 0:10:10  iter: 45179  total_loss: 3.832  loss_ce: 0.0003769  loss_mask: 0.1824  loss_dice: 0.192  loss_ce_0: 0.06178  loss_mask_0: 0.1768  loss_dice_0: 0.1833  loss_ce_1: 0.0004047  loss_mask_1: 0.1854  loss_dice_1: 0.1771  loss_ce_2: 0.000314  loss_mask_2: 0.1804  loss_dice_2: 0.1816  loss_ce_3: 0.000336  loss_mask_3: 0.1726  loss_dice_3: 0.1866  loss_ce_4: 0.0003338  loss_mask_4: 0.1767  loss_dice_4: 0.1802  loss_ce_5: 0.0003645  loss_mask_5: 0.1806  loss_dice_5: 0.177  loss_ce_6: 0.0004349  loss_mask_6: 0.1714  loss_dice_6: 0.1785  loss_ce_7: 0.0003707  loss_mask_7: 0.1783  loss_dice_7: 0.1897  loss_ce_8: 0.0004906  loss_mask_8: 0.1667  loss_dice_8: 0.1825  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:35] d2.utils.events INFO:  eta: 0:10:07  iter: 45199  total_loss: 4.472  loss_ce: 0.0003732  loss_mask: 0.2045  loss_dice: 0.1958  loss_ce_0: 0.06141  loss_mask_0: 0.1939  loss_dice_0: 0.1983  loss_ce_1: 0.0004409  loss_mask_1: 0.202  loss_dice_1: 0.2077  loss_ce_2: 0.0003512  loss_mask_2: 0.1955  loss_dice_2: 0.2179  loss_ce_3: 0.0004353  loss_mask_3: 0.1994  loss_dice_3: 0.2046  loss_ce_4: 0.0004351  loss_mask_4: 0.1988  loss_dice_4: 0.2075  loss_ce_5: 0.0003755  loss_mask_5: 0.2037  loss_dice_5: 0.2045  loss_ce_6: 0.000519  loss_mask_6: 0.198  loss_dice_6: 0.1991  loss_ce_7: 0.0004377  loss_mask_7: 0.1921  loss_dice_7: 0.2085  loss_ce_8: 0.0004995  loss_mask_8: 0.2027  loss_dice_8: 0.2094  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:37] d2.utils.events INFO:  eta: 0:10:04  iter: 45219  total_loss: 4.158  loss_ce: 0.0004821  loss_mask: 0.1882  loss_dice: 0.1984  loss_ce_0: 0.06133  loss_mask_0: 0.193  loss_dice_0: 0.1918  loss_ce_1: 0.0004707  loss_mask_1: 0.1953  loss_dice_1: 0.1896  loss_ce_2: 0.000529  loss_mask_2: 0.1931  loss_dice_2: 0.2001  loss_ce_3: 0.0005132  loss_mask_3: 0.1911  loss_dice_3: 0.1838  loss_ce_4: 0.0005156  loss_mask_4: 0.183  loss_dice_4: 0.1927  loss_ce_5: 0.0006253  loss_mask_5: 0.1892  loss_dice_5: 0.1936  loss_ce_6: 0.0005794  loss_mask_6: 0.1871  loss_dice_6: 0.1881  loss_ce_7: 0.0004529  loss_mask_7: 0.1888  loss_dice_7: 0.1927  loss_ce_8: 0.0006454  loss_mask_8: 0.1948  loss_dice_8: 0.1901  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:40] d2.utils.events INFO:  eta: 0:10:02  iter: 45239  total_loss: 4.485  loss_ce: 0.0004039  loss_mask: 0.192  loss_dice: 0.1903  loss_ce_0: 0.06132  loss_mask_0: 0.1994  loss_dice_0: 0.1947  loss_ce_1: 0.0003928  loss_mask_1: 0.1995  loss_dice_1: 0.1916  loss_ce_2: 0.0003195  loss_mask_2: 0.2047  loss_dice_2: 0.1969  loss_ce_3: 0.0004603  loss_mask_3: 0.2066  loss_dice_3: 0.1939  loss_ce_4: 0.0003894  loss_mask_4: 0.199  loss_dice_4: 0.2001  loss_ce_5: 0.0004064  loss_mask_5: 0.1971  loss_dice_5: 0.1956  loss_ce_6: 0.0005301  loss_mask_6: 0.2031  loss_dice_6: 0.1958  loss_ce_7: 0.000378  loss_mask_7: 0.2033  loss_dice_7: 0.1902  loss_ce_8: 0.0004914  loss_mask_8: 0.1965  loss_dice_8: 0.1896  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:43] d2.utils.events INFO:  eta: 0:09:59  iter: 45259  total_loss: 4.285  loss_ce: 0.0004203  loss_mask: 0.1677  loss_dice: 0.1769  loss_ce_0: 0.0613  loss_mask_0: 0.1627  loss_dice_0: 0.1776  loss_ce_1: 0.0004278  loss_mask_1: 0.1701  loss_dice_1: 0.1896  loss_ce_2: 0.0003281  loss_mask_2: 0.1657  loss_dice_2: 0.1783  loss_ce_3: 0.000419  loss_mask_3: 0.1635  loss_dice_3: 0.1792  loss_ce_4: 0.0004512  loss_mask_4: 0.1627  loss_dice_4: 0.1841  loss_ce_5: 0.0004126  loss_mask_5: 0.1681  loss_dice_5: 0.182  loss_ce_6: 0.0005089  loss_mask_6: 0.1653  loss_dice_6: 0.1767  loss_ce_7: 0.0005034  loss_mask_7: 0.1664  loss_dice_7: 0.181  loss_ce_8: 0.0004951  loss_mask_8: 0.1588  loss_dice_8: 0.177  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:45] d2.utils.events INFO:  eta: 0:09:58  iter: 45279  total_loss: 4.042  loss_ce: 0.0004232  loss_mask: 0.2287  loss_dice: 0.1554  loss_ce_0: 0.06128  loss_mask_0: 0.2201  loss_dice_0: 0.1603  loss_ce_1: 0.0004251  loss_mask_1: 0.2248  loss_dice_1: 0.1543  loss_ce_2: 0.0002796  loss_mask_2: 0.2239  loss_dice_2: 0.1542  loss_ce_3: 0.0004378  loss_mask_3: 0.2256  loss_dice_3: 0.1617  loss_ce_4: 0.000407  loss_mask_4: 0.2234  loss_dice_4: 0.1652  loss_ce_5: 0.0003853  loss_mask_5: 0.2225  loss_dice_5: 0.1564  loss_ce_6: 0.0004883  loss_mask_6: 0.2237  loss_dice_6: 0.1549  loss_ce_7: 0.0004615  loss_mask_7: 0.2217  loss_dice_7: 0.1611  loss_ce_8: 0.0004799  loss_mask_8: 0.2252  loss_dice_8: 0.1647  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:48] d2.utils.events INFO:  eta: 0:09:55  iter: 45299  total_loss: 3.654  loss_ce: 0.0005161  loss_mask: 0.1431  loss_dice: 0.2147  loss_ce_0: 0.0622  loss_mask_0: 0.1312  loss_dice_0: 0.2096  loss_ce_1: 0.0004948  loss_mask_1: 0.13  loss_dice_1: 0.2226  loss_ce_2: 0.0003788  loss_mask_2: 0.14  loss_dice_2: 0.2235  loss_ce_3: 0.0004536  loss_mask_3: 0.1316  loss_dice_3: 0.2197  loss_ce_4: 0.000505  loss_mask_4: 0.1392  loss_dice_4: 0.2128  loss_ce_5: 0.000455  loss_mask_5: 0.1345  loss_dice_5: 0.2243  loss_ce_6: 0.0005332  loss_mask_6: 0.1337  loss_dice_6: 0.2155  loss_ce_7: 0.0004622  loss_mask_7: 0.1439  loss_dice_7: 0.2049  loss_ce_8: 0.0005494  loss_mask_8: 0.1487  loss_dice_8: 0.2339  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:50] d2.utils.events INFO:  eta: 0:09:52  iter: 45319  total_loss: 4.207  loss_ce: 0.0003826  loss_mask: 0.1904  loss_dice: 0.1871  loss_ce_0: 0.06174  loss_mask_0: 0.1851  loss_dice_0: 0.1839  loss_ce_1: 0.000411  loss_mask_1: 0.1874  loss_dice_1: 0.1819  loss_ce_2: 0.0002718  loss_mask_2: 0.1897  loss_dice_2: 0.1895  loss_ce_3: 0.0004537  loss_mask_3: 0.2002  loss_dice_3: 0.1856  loss_ce_4: 0.0003883  loss_mask_4: 0.1874  loss_dice_4: 0.187  loss_ce_5: 0.0003365  loss_mask_5: 0.1809  loss_dice_5: 0.1804  loss_ce_6: 0.0004728  loss_mask_6: 0.1892  loss_dice_6: 0.18  loss_ce_7: 0.0003818  loss_mask_7: 0.1931  loss_dice_7: 0.1771  loss_ce_8: 0.000443  loss_mask_8: 0.1921  loss_dice_8: 0.1842  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:53] d2.utils.events INFO:  eta: 0:09:50  iter: 45339  total_loss: 4.327  loss_ce: 0.0004562  loss_mask: 0.2179  loss_dice: 0.1544  loss_ce_0: 0.06174  loss_mask_0: 0.2158  loss_dice_0: 0.151  loss_ce_1: 0.0005719  loss_mask_1: 0.2067  loss_dice_1: 0.1553  loss_ce_2: 0.0004088  loss_mask_2: 0.2161  loss_dice_2: 0.157  loss_ce_3: 0.0004502  loss_mask_3: 0.208  loss_dice_3: 0.1587  loss_ce_4: 0.00036  loss_mask_4: 0.2161  loss_dice_4: 0.1624  loss_ce_5: 0.000387  loss_mask_5: 0.2134  loss_dice_5: 0.1506  loss_ce_6: 0.0005342  loss_mask_6: 0.2079  loss_dice_6: 0.1534  loss_ce_7: 0.0003996  loss_mask_7: 0.2122  loss_dice_7: 0.1697  loss_ce_8: 0.0005076  loss_mask_8: 0.2118  loss_dice_8: 0.1536  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:55] d2.utils.events INFO:  eta: 0:09:47  iter: 45359  total_loss: 4.396  loss_ce: 0.00142  loss_mask: 0.1618  loss_dice: 0.2102  loss_ce_0: 0.06132  loss_mask_0: 0.1613  loss_dice_0: 0.2181  loss_ce_1: 0.0009637  loss_mask_1: 0.1614  loss_dice_1: 0.2006  loss_ce_2: 0.00068  loss_mask_2: 0.1636  loss_dice_2: 0.2064  loss_ce_3: 0.0008423  loss_mask_3: 0.1657  loss_dice_3: 0.2059  loss_ce_4: 0.0009437  loss_mask_4: 0.1716  loss_dice_4: 0.2168  loss_ce_5: 0.0007407  loss_mask_5: 0.162  loss_dice_5: 0.1995  loss_ce_6: 0.0008615  loss_mask_6: 0.1699  loss_dice_6: 0.2038  loss_ce_7: 0.001488  loss_mask_7: 0.1621  loss_dice_7: 0.2095  loss_ce_8: 0.0007489  loss_mask_8: 0.1619  loss_dice_8: 0.2088  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:46:58] d2.utils.events INFO:  eta: 0:09:44  iter: 45379  total_loss: 3.913  loss_ce: 0.0004451  loss_mask: 0.2079  loss_dice: 0.1575  loss_ce_0: 0.06174  loss_mask_0: 0.1981  loss_dice_0: 0.1561  loss_ce_1: 0.000384  loss_mask_1: 0.1996  loss_dice_1: 0.1554  loss_ce_2: 0.0002615  loss_mask_2: 0.2058  loss_dice_2: 0.1594  loss_ce_3: 0.0004155  loss_mask_3: 0.2064  loss_dice_3: 0.1577  loss_ce_4: 0.0003708  loss_mask_4: 0.2051  loss_dice_4: 0.1599  loss_ce_5: 0.0003474  loss_mask_5: 0.1969  loss_dice_5: 0.1634  loss_ce_6: 0.0005253  loss_mask_6: 0.2104  loss_dice_6: 0.1617  loss_ce_7: 0.0003953  loss_mask_7: 0.2126  loss_dice_7: 0.1613  loss_ce_8: 0.000447  loss_mask_8: 0.2107  loss_dice_8: 0.1646  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:00] d2.utils.events INFO:  eta: 0:09:41  iter: 45399  total_loss: 3.913  loss_ce: 0.000387  loss_mask: 0.2001  loss_dice: 0.1626  loss_ce_0: 0.06212  loss_mask_0: 0.2086  loss_dice_0: 0.1651  loss_ce_1: 0.0003575  loss_mask_1: 0.2039  loss_dice_1: 0.1611  loss_ce_2: 0.0002905  loss_mask_2: 0.2079  loss_dice_2: 0.1586  loss_ce_3: 0.0004039  loss_mask_3: 0.1929  loss_dice_3: 0.1555  loss_ce_4: 0.0003424  loss_mask_4: 0.2035  loss_dice_4: 0.1609  loss_ce_5: 0.000393  loss_mask_5: 0.2124  loss_dice_5: 0.157  loss_ce_6: 0.0004878  loss_mask_6: 0.2013  loss_dice_6: 0.1626  loss_ce_7: 0.0004219  loss_mask_7: 0.198  loss_dice_7: 0.1593  loss_ce_8: 0.0004891  loss_mask_8: 0.2046  loss_dice_8: 0.1548  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:03] d2.utils.events INFO:  eta: 0:09:38  iter: 45419  total_loss: 3.798  loss_ce: 0.0004697  loss_mask: 0.2027  loss_dice: 0.1611  loss_ce_0: 0.06137  loss_mask_0: 0.1992  loss_dice_0: 0.1558  loss_ce_1: 0.0005307  loss_mask_1: 0.1964  loss_dice_1: 0.1574  loss_ce_2: 0.0004473  loss_mask_2: 0.2002  loss_dice_2: 0.1516  loss_ce_3: 0.0005229  loss_mask_3: 0.1946  loss_dice_3: 0.1542  loss_ce_4: 0.0005403  loss_mask_4: 0.1995  loss_dice_4: 0.1581  loss_ce_5: 0.0004883  loss_mask_5: 0.2011  loss_dice_5: 0.1566  loss_ce_6: 0.0005186  loss_mask_6: 0.2069  loss_dice_6: 0.1625  loss_ce_7: 0.0004601  loss_mask_7: 0.2001  loss_dice_7: 0.1627  loss_ce_8: 0.0005241  loss_mask_8: 0.2087  loss_dice_8: 0.1589  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:47:05] d2.utils.events INFO:  eta: 0:09:36  iter: 45439  total_loss: 4.463  loss_ce: 0.0003528  loss_mask: 0.2091  loss_dice: 0.2135  loss_ce_0: 0.06139  loss_mask_0: 0.1976  loss_dice_0: 0.2088  loss_ce_1: 0.00037  loss_mask_1: 0.1992  loss_dice_1: 0.2091  loss_ce_2: 0.0003051  loss_mask_2: 0.2058  loss_dice_2: 0.2154  loss_ce_3: 0.0003909  loss_mask_3: 0.2069  loss_dice_3: 0.2114  loss_ce_4: 0.0003654  loss_mask_4: 0.2049  loss_dice_4: 0.2246  loss_ce_5: 0.0003854  loss_mask_5: 0.1928  loss_dice_5: 0.2137  loss_ce_6: 0.0004711  loss_mask_6: 0.2004  loss_dice_6: 0.2208  loss_ce_7: 0.0003838  loss_mask_7: 0.1953  loss_dice_7: 0.2148  loss_ce_8: 0.0004676  loss_mask_8: 0.1912  loss_dice_8: 0.2124  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:47:08] d2.utils.events INFO:  eta: 0:09:34  iter: 45459  total_loss: 4.109  loss_ce: 0.000482  loss_mask: 0.166  loss_dice: 0.177  loss_ce_0: 0.06208  loss_mask_0: 0.1799  loss_dice_0: 0.1773  loss_ce_1: 0.0003997  loss_mask_1: 0.1722  loss_dice_1: 0.1793  loss_ce_2: 0.0002959  loss_mask_2: 0.1701  loss_dice_2: 0.1719  loss_ce_3: 0.0004312  loss_mask_3: 0.1724  loss_dice_3: 0.1738  loss_ce_4: 0.0003551  loss_mask_4: 0.1658  loss_dice_4: 0.1803  loss_ce_5: 0.0004097  loss_mask_5: 0.1665  loss_dice_5: 0.1744  loss_ce_6: 0.0004615  loss_mask_6: 0.1733  loss_dice_6: 0.1745  loss_ce_7: 0.0003668  loss_mask_7: 0.1717  loss_dice_7: 0.1746  loss_ce_8: 0.0004856  loss_mask_8: 0.1664  loss_dice_8: 0.1657  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:11] d2.utils.events INFO:  eta: 0:09:32  iter: 45479  total_loss: 4.403  loss_ce: 0.0004871  loss_mask: 0.1923  loss_dice: 0.1754  loss_ce_0: 0.0614  loss_mask_0: 0.1871  loss_dice_0: 0.1743  loss_ce_1: 0.0005178  loss_mask_1: 0.1866  loss_dice_1: 0.1751  loss_ce_2: 0.0005291  loss_mask_2: 0.1784  loss_dice_2: 0.1727  loss_ce_3: 0.0004544  loss_mask_3: 0.1897  loss_dice_3: 0.1743  loss_ce_4: 0.000645  loss_mask_4: 0.1793  loss_dice_4: 0.1653  loss_ce_5: 0.0005221  loss_mask_5: 0.1878  loss_dice_5: 0.1705  loss_ce_6: 0.0005219  loss_mask_6: 0.1849  loss_dice_6: 0.1673  loss_ce_7: 0.0005296  loss_mask_7: 0.1951  loss_dice_7: 0.1787  loss_ce_8: 0.0005655  loss_mask_8: 0.1924  loss_dice_8: 0.1793  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:13] d2.utils.events INFO:  eta: 0:09:29  iter: 45499  total_loss: 3.855  loss_ce: 0.0005034  loss_mask: 0.15  loss_dice: 0.1999  loss_ce_0: 0.06139  loss_mask_0: 0.1562  loss_dice_0: 0.2027  loss_ce_1: 0.0003726  loss_mask_1: 0.1538  loss_dice_1: 0.1928  loss_ce_2: 0.0002716  loss_mask_2: 0.146  loss_dice_2: 0.1978  loss_ce_3: 0.0004422  loss_mask_3: 0.1525  loss_dice_3: 0.2167  loss_ce_4: 0.0004638  loss_mask_4: 0.1461  loss_dice_4: 0.2027  loss_ce_5: 0.0004206  loss_mask_5: 0.1419  loss_dice_5: 0.2027  loss_ce_6: 0.000565  loss_mask_6: 0.1443  loss_dice_6: 0.208  loss_ce_7: 0.0005063  loss_mask_7: 0.1439  loss_dice_7: 0.1906  loss_ce_8: 0.0005201  loss_mask_8: 0.1574  loss_dice_8: 0.2038  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:47:16] d2.utils.events INFO:  eta: 0:09:27  iter: 45519  total_loss: 4.128  loss_ce: 0.000556  loss_mask: 0.1831  loss_dice: 0.198  loss_ce_0: 0.06175  loss_mask_0: 0.1833  loss_dice_0: 0.1948  loss_ce_1: 0.000688  loss_mask_1: 0.1818  loss_dice_1: 0.2013  loss_ce_2: 0.0006685  loss_mask_2: 0.1779  loss_dice_2: 0.2034  loss_ce_3: 0.0005697  loss_mask_3: 0.183  loss_dice_3: 0.1982  loss_ce_4: 0.0007055  loss_mask_4: 0.1788  loss_dice_4: 0.1985  loss_ce_5: 0.000661  loss_mask_5: 0.1755  loss_dice_5: 0.1897  loss_ce_6: 0.0005508  loss_mask_6: 0.1753  loss_dice_6: 0.1979  loss_ce_7: 0.0008104  loss_mask_7: 0.1845  loss_dice_7: 0.1882  loss_ce_8: 0.0006522  loss_mask_8: 0.1824  loss_dice_8: 0.1984  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:18] d2.utils.events INFO:  eta: 0:09:24  iter: 45539  total_loss: 3.838  loss_ce: 0.000535  loss_mask: 0.1692  loss_dice: 0.1962  loss_ce_0: 0.06212  loss_mask_0: 0.1656  loss_dice_0: 0.1902  loss_ce_1: 0.000526  loss_mask_1: 0.174  loss_dice_1: 0.1907  loss_ce_2: 0.0004152  loss_mask_2: 0.1637  loss_dice_2: 0.182  loss_ce_3: 0.0003786  loss_mask_3: 0.1623  loss_dice_3: 0.1931  loss_ce_4: 0.0004726  loss_mask_4: 0.1629  loss_dice_4: 0.1872  loss_ce_5: 0.0004371  loss_mask_5: 0.1632  loss_dice_5: 0.1904  loss_ce_6: 0.0005022  loss_mask_6: 0.1635  loss_dice_6: 0.186  loss_ce_7: 0.0004217  loss_mask_7: 0.166  loss_dice_7: 0.1945  loss_ce_8: 0.0005475  loss_mask_8: 0.1721  loss_dice_8: 0.1944  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:21] d2.utils.events INFO:  eta: 0:09:22  iter: 45559  total_loss: 4.411  loss_ce: 0.0005456  loss_mask: 0.1762  loss_dice: 0.227  loss_ce_0: 0.06174  loss_mask_0: 0.1724  loss_dice_0: 0.2253  loss_ce_1: 0.0006364  loss_mask_1: 0.1791  loss_dice_1: 0.2197  loss_ce_2: 0.0005359  loss_mask_2: 0.1886  loss_dice_2: 0.224  loss_ce_3: 0.0005756  loss_mask_3: 0.1814  loss_dice_3: 0.223  loss_ce_4: 0.000568  loss_mask_4: 0.1794  loss_dice_4: 0.223  loss_ce_5: 0.0006705  loss_mask_5: 0.1831  loss_dice_5: 0.2215  loss_ce_6: 0.0006834  loss_mask_6: 0.1868  loss_dice_6: 0.2245  loss_ce_7: 0.0006015  loss_mask_7: 0.1888  loss_dice_7: 0.2228  loss_ce_8: 0.0006838  loss_mask_8: 0.1881  loss_dice_8: 0.2219  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:23] d2.utils.events INFO:  eta: 0:09:19  iter: 45579  total_loss: 3.8  loss_ce: 0.0003934  loss_mask: 0.1956  loss_dice: 0.1487  loss_ce_0: 0.06174  loss_mask_0: 0.1929  loss_dice_0: 0.1458  loss_ce_1: 0.000387  loss_mask_1: 0.2061  loss_dice_1: 0.145  loss_ce_2: 0.0002853  loss_mask_2: 0.21  loss_dice_2: 0.1457  loss_ce_3: 0.0003971  loss_mask_3: 0.2014  loss_dice_3: 0.1508  loss_ce_4: 0.0003388  loss_mask_4: 0.1986  loss_dice_4: 0.1479  loss_ce_5: 0.0003407  loss_mask_5: 0.1963  loss_dice_5: 0.1438  loss_ce_6: 0.0004721  loss_mask_6: 0.1895  loss_dice_6: 0.1525  loss_ce_7: 0.0003835  loss_mask_7: 0.1981  loss_dice_7: 0.1427  loss_ce_8: 0.0004458  loss_mask_8: 0.2001  loss_dice_8: 0.1488  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:26] d2.utils.events INFO:  eta: 0:09:17  iter: 45599  total_loss: 4.064  loss_ce: 0.0007627  loss_mask: 0.1951  loss_dice: 0.1826  loss_ce_0: 0.06204  loss_mask_0: 0.1959  loss_dice_0: 0.1858  loss_ce_1: 0.0006686  loss_mask_1: 0.2011  loss_dice_1: 0.1901  loss_ce_2: 0.0007299  loss_mask_2: 0.2014  loss_dice_2: 0.1826  loss_ce_3: 0.0008204  loss_mask_3: 0.1988  loss_dice_3: 0.1912  loss_ce_4: 0.0006416  loss_mask_4: 0.1947  loss_dice_4: 0.1906  loss_ce_5: 0.0007308  loss_mask_5: 0.201  loss_dice_5: 0.1854  loss_ce_6: 0.0008322  loss_mask_6: 0.1985  loss_dice_6: 0.1828  loss_ce_7: 0.0007074  loss_mask_7: 0.2056  loss_dice_7: 0.1887  loss_ce_8: 0.0007521  loss_mask_8: 0.1972  loss_dice_8: 0.1857  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:28] d2.utils.events INFO:  eta: 0:09:14  iter: 45619  total_loss: 4.182  loss_ce: 0.0003784  loss_mask: 0.2264  loss_dice: 0.1559  loss_ce_0: 0.06144  loss_mask_0: 0.228  loss_dice_0: 0.1525  loss_ce_1: 0.0004025  loss_mask_1: 0.2242  loss_dice_1: 0.1528  loss_ce_2: 0.0003383  loss_mask_2: 0.2289  loss_dice_2: 0.1492  loss_ce_3: 0.0004504  loss_mask_3: 0.2236  loss_dice_3: 0.1522  loss_ce_4: 0.0003329  loss_mask_4: 0.227  loss_dice_4: 0.1517  loss_ce_5: 0.0004318  loss_mask_5: 0.2256  loss_dice_5: 0.1508  loss_ce_6: 0.0005409  loss_mask_6: 0.2245  loss_dice_6: 0.1508  loss_ce_7: 0.0004179  loss_mask_7: 0.2264  loss_dice_7: 0.1518  loss_ce_8: 0.000496  loss_mask_8: 0.2258  loss_dice_8: 0.1587  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:47:31] d2.utils.events INFO:  eta: 0:09:12  iter: 45639  total_loss: 4.305  loss_ce: 0.000371  loss_mask: 0.2145  loss_dice: 0.1892  loss_ce_0: 0.06203  loss_mask_0: 0.2261  loss_dice_0: 0.1877  loss_ce_1: 0.0003471  loss_mask_1: 0.206  loss_dice_1: 0.1883  loss_ce_2: 0.0003009  loss_mask_2: 0.2198  loss_dice_2: 0.1917  loss_ce_3: 0.00036  loss_mask_3: 0.2185  loss_dice_3: 0.188  loss_ce_4: 0.000327  loss_mask_4: 0.214  loss_dice_4: 0.1969  loss_ce_5: 0.0003513  loss_mask_5: 0.2087  loss_dice_5: 0.2107  loss_ce_6: 0.000491  loss_mask_6: 0.2196  loss_dice_6: 0.1984  loss_ce_7: 0.0003925  loss_mask_7: 0.2206  loss_dice_7: 0.1888  loss_ce_8: 0.0004581  loss_mask_8: 0.2131  loss_dice_8: 0.1958  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:33] d2.utils.events INFO:  eta: 0:09:09  iter: 45659  total_loss: 3.768  loss_ce: 0.0005876  loss_mask: 0.2086  loss_dice: 0.1721  loss_ce_0: 0.06202  loss_mask_0: 0.2135  loss_dice_0: 0.1751  loss_ce_1: 0.0005665  loss_mask_1: 0.2103  loss_dice_1: 0.1757  loss_ce_2: 0.000589  loss_mask_2: 0.2048  loss_dice_2: 0.1682  loss_ce_3: 0.0006584  loss_mask_3: 0.206  loss_dice_3: 0.1685  loss_ce_4: 0.0006772  loss_mask_4: 0.2163  loss_dice_4: 0.1765  loss_ce_5: 0.0005743  loss_mask_5: 0.2095  loss_dice_5: 0.1717  loss_ce_6: 0.0005987  loss_mask_6: 0.2054  loss_dice_6: 0.1732  loss_ce_7: 0.0005987  loss_mask_7: 0.2058  loss_dice_7: 0.1764  loss_ce_8: 0.0006458  loss_mask_8: 0.2139  loss_dice_8: 0.1744  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:47:36] d2.utils.events INFO:  eta: 0:09:06  iter: 45679  total_loss: 4.03  loss_ce: 0.0006  loss_mask: 0.1571  loss_dice: 0.1934  loss_ce_0: 0.06193  loss_mask_0: 0.1535  loss_dice_0: 0.1908  loss_ce_1: 0.0004615  loss_mask_1: 0.1639  loss_dice_1: 0.1866  loss_ce_2: 0.0003656  loss_mask_2: 0.1569  loss_dice_2: 0.1901  loss_ce_3: 0.0005696  loss_mask_3: 0.1504  loss_dice_3: 0.1862  loss_ce_4: 0.0003933  loss_mask_4: 0.1592  loss_dice_4: 0.1837  loss_ce_5: 0.0004348  loss_mask_5: 0.1581  loss_dice_5: 0.1836  loss_ce_6: 0.0005698  loss_mask_6: 0.1525  loss_dice_6: 0.1837  loss_ce_7: 0.0004164  loss_mask_7: 0.1527  loss_dice_7: 0.1919  loss_ce_8: 0.00054  loss_mask_8: 0.1632  loss_dice_8: 0.1798  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:47:38] d2.utils.events INFO:  eta: 0:09:04  iter: 45699  total_loss: 3.898  loss_ce: 0.0003148  loss_mask: 0.1977  loss_dice: 0.1755  loss_ce_0: 0.06158  loss_mask_0: 0.1935  loss_dice_0: 0.179  loss_ce_1: 0.0003722  loss_mask_1: 0.2049  loss_dice_1: 0.174  loss_ce_2: 0.0002475  loss_mask_2: 0.2082  loss_dice_2: 0.1746  loss_ce_3: 0.0003487  loss_mask_3: 0.1974  loss_dice_3: 0.1769  loss_ce_4: 0.000288  loss_mask_4: 0.1969  loss_dice_4: 0.1797  loss_ce_5: 0.0002948  loss_mask_5: 0.2151  loss_dice_5: 0.171  loss_ce_6: 0.0004387  loss_mask_6: 0.2103  loss_dice_6: 0.1734  loss_ce_7: 0.0002856  loss_mask_7: 0.1984  loss_dice_7: 0.1647  loss_ce_8: 0.0004066  loss_mask_8: 0.2022  loss_dice_8: 0.1643  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:41] d2.utils.events INFO:  eta: 0:09:01  iter: 45719  total_loss: 4.022  loss_ce: 0.0005041  loss_mask: 0.226  loss_dice: 0.1483  loss_ce_0: 0.06153  loss_mask_0: 0.2235  loss_dice_0: 0.1478  loss_ce_1: 0.0004457  loss_mask_1: 0.2301  loss_dice_1: 0.1449  loss_ce_2: 0.0003885  loss_mask_2: 0.2286  loss_dice_2: 0.1472  loss_ce_3: 0.0004942  loss_mask_3: 0.2285  loss_dice_3: 0.1486  loss_ce_4: 0.0004165  loss_mask_4: 0.2243  loss_dice_4: 0.1511  loss_ce_5: 0.000392  loss_mask_5: 0.2207  loss_dice_5: 0.1472  loss_ce_6: 0.0005305  loss_mask_6: 0.2276  loss_dice_6: 0.1497  loss_ce_7: 0.0004191  loss_mask_7: 0.2295  loss_dice_7: 0.1477  loss_ce_8: 0.0004965  loss_mask_8: 0.2258  loss_dice_8: 0.1488  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:43] d2.utils.events INFO:  eta: 0:08:59  iter: 45739  total_loss: 3.812  loss_ce: 0.0004222  loss_mask: 0.229  loss_dice: 0.1585  loss_ce_0: 0.06201  loss_mask_0: 0.2335  loss_dice_0: 0.1581  loss_ce_1: 0.0004099  loss_mask_1: 0.2198  loss_dice_1: 0.1643  loss_ce_2: 0.0003327  loss_mask_2: 0.2327  loss_dice_2: 0.1614  loss_ce_3: 0.000446  loss_mask_3: 0.2283  loss_dice_3: 0.1658  loss_ce_4: 0.00035  loss_mask_4: 0.2375  loss_dice_4: 0.1609  loss_ce_5: 0.0004035  loss_mask_5: 0.2317  loss_dice_5: 0.1644  loss_ce_6: 0.0004926  loss_mask_6: 0.2341  loss_dice_6: 0.1634  loss_ce_7: 0.0004329  loss_mask_7: 0.2367  loss_dice_7: 0.1656  loss_ce_8: 0.0004779  loss_mask_8: 0.2308  loss_dice_8: 0.1615  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:47:46] d2.utils.events INFO:  eta: 0:08:57  iter: 45759  total_loss: 3.79  loss_ce: 0.0004318  loss_mask: 0.1734  loss_dice: 0.1673  loss_ce_0: 0.06153  loss_mask_0: 0.1764  loss_dice_0: 0.1698  loss_ce_1: 0.0004393  loss_mask_1: 0.179  loss_dice_1: 0.1684  loss_ce_2: 0.0003687  loss_mask_2: 0.1715  loss_dice_2: 0.1649  loss_ce_3: 0.0004369  loss_mask_3: 0.1693  loss_dice_3: 0.1626  loss_ce_4: 0.0003906  loss_mask_4: 0.1803  loss_dice_4: 0.1655  loss_ce_5: 0.0004419  loss_mask_5: 0.1725  loss_dice_5: 0.1622  loss_ce_6: 0.0004925  loss_mask_6: 0.167  loss_dice_6: 0.1605  loss_ce_7: 0.0004548  loss_mask_7: 0.1755  loss_dice_7: 0.1644  loss_ce_8: 0.0004861  loss_mask_8: 0.1803  loss_dice_8: 0.1623  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:49] d2.utils.events INFO:  eta: 0:08:54  iter: 45779  total_loss: 4.199  loss_ce: 0.0004938  loss_mask: 0.1703  loss_dice: 0.1818  loss_ce_0: 0.06153  loss_mask_0: 0.1791  loss_dice_0: 0.1808  loss_ce_1: 0.0005246  loss_mask_1: 0.1716  loss_dice_1: 0.1856  loss_ce_2: 0.000552  loss_mask_2: 0.1804  loss_dice_2: 0.1864  loss_ce_3: 0.0004553  loss_mask_3: 0.1733  loss_dice_3: 0.178  loss_ce_4: 0.0006011  loss_mask_4: 0.1692  loss_dice_4: 0.1847  loss_ce_5: 0.0005745  loss_mask_5: 0.1804  loss_dice_5: 0.183  loss_ce_6: 0.0005342  loss_mask_6: 0.1733  loss_dice_6: 0.187  loss_ce_7: 0.0004878  loss_mask_7: 0.1723  loss_dice_7: 0.1905  loss_ce_8: 0.0006085  loss_mask_8: 0.1734  loss_dice_8: 0.1808  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:51] d2.utils.events INFO:  eta: 0:08:52  iter: 45799  total_loss: 4.229  loss_ce: 0.0003569  loss_mask: 0.2253  loss_dice: 0.1512  loss_ce_0: 0.06174  loss_mask_0: 0.2103  loss_dice_0: 0.1536  loss_ce_1: 0.0003982  loss_mask_1: 0.2152  loss_dice_1: 0.1513  loss_ce_2: 0.0003121  loss_mask_2: 0.225  loss_dice_2: 0.1533  loss_ce_3: 0.0003832  loss_mask_3: 0.2123  loss_dice_3: 0.1522  loss_ce_4: 0.0003424  loss_mask_4: 0.2094  loss_dice_4: 0.1494  loss_ce_5: 0.0003717  loss_mask_5: 0.2188  loss_dice_5: 0.1491  loss_ce_6: 0.0004749  loss_mask_6: 0.2186  loss_dice_6: 0.1567  loss_ce_7: 0.000369  loss_mask_7: 0.2164  loss_dice_7: 0.1562  loss_ce_8: 0.0004706  loss_mask_8: 0.2053  loss_dice_8: 0.1532  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:47:54] d2.utils.events INFO:  eta: 0:08:49  iter: 45819  total_loss: 4.663  loss_ce: 0.0005658  loss_mask: 0.1979  loss_dice: 0.1788  loss_ce_0: 0.06154  loss_mask_0: 0.1845  loss_dice_0: 0.1818  loss_ce_1: 0.0004588  loss_mask_1: 0.1924  loss_dice_1: 0.1806  loss_ce_2: 0.000372  loss_mask_2: 0.1958  loss_dice_2: 0.1728  loss_ce_3: 0.0004954  loss_mask_3: 0.1945  loss_dice_3: 0.1814  loss_ce_4: 0.0003995  loss_mask_4: 0.1875  loss_dice_4: 0.1795  loss_ce_5: 0.000432  loss_mask_5: 0.1994  loss_dice_5: 0.1787  loss_ce_6: 0.0005809  loss_mask_6: 0.1992  loss_dice_6: 0.1799  loss_ce_7: 0.0004433  loss_mask_7: 0.192  loss_dice_7: 0.1759  loss_ce_8: 0.0005229  loss_mask_8: 0.1938  loss_dice_8: 0.1787  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:47:56] d2.utils.events INFO:  eta: 0:08:47  iter: 45839  total_loss: 4.132  loss_ce: 0.0004041  loss_mask: 0.2193  loss_dice: 0.1538  loss_ce_0: 0.06148  loss_mask_0: 0.2187  loss_dice_0: 0.15  loss_ce_1: 0.0004607  loss_mask_1: 0.2147  loss_dice_1: 0.1504  loss_ce_2: 0.0003855  loss_mask_2: 0.2233  loss_dice_2: 0.1542  loss_ce_3: 0.0004558  loss_mask_3: 0.2225  loss_dice_3: 0.1536  loss_ce_4: 0.0004999  loss_mask_4: 0.2126  loss_dice_4: 0.1443  loss_ce_5: 0.0003857  loss_mask_5: 0.2259  loss_dice_5: 0.1497  loss_ce_6: 0.0005355  loss_mask_6: 0.2231  loss_dice_6: 0.1522  loss_ce_7: 0.0004475  loss_mask_7: 0.2166  loss_dice_7: 0.1496  loss_ce_8: 0.0004896  loss_mask_8: 0.2208  loss_dice_8: 0.1521  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:47:59] d2.utils.events INFO:  eta: 0:08:44  iter: 45859  total_loss: 4.114  loss_ce: 0.0004689  loss_mask: 0.1889  loss_dice: 0.1736  loss_ce_0: 0.06201  loss_mask_0: 0.1848  loss_dice_0: 0.163  loss_ce_1: 0.0005328  loss_mask_1: 0.1807  loss_dice_1: 0.1696  loss_ce_2: 0.000343  loss_mask_2: 0.1741  loss_dice_2: 0.1694  loss_ce_3: 0.0004576  loss_mask_3: 0.1773  loss_dice_3: 0.1658  loss_ce_4: 0.0004878  loss_mask_4: 0.19  loss_dice_4: 0.1743  loss_ce_5: 0.0003881  loss_mask_5: 0.1796  loss_dice_5: 0.1703  loss_ce_6: 0.0005063  loss_mask_6: 0.1941  loss_dice_6: 0.1709  loss_ce_7: 0.000556  loss_mask_7: 0.1906  loss_dice_7: 0.1676  loss_ce_8: 0.00051  loss_mask_8: 0.1864  loss_dice_8: 0.1689  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:01] d2.utils.events INFO:  eta: 0:08:42  iter: 45879  total_loss: 4.702  loss_ce: 0.0004318  loss_mask: 0.2061  loss_dice: 0.2123  loss_ce_0: 0.06147  loss_mask_0: 0.1979  loss_dice_0: 0.2087  loss_ce_1: 0.0004017  loss_mask_1: 0.2086  loss_dice_1: 0.2047  loss_ce_2: 0.0003058  loss_mask_2: 0.1998  loss_dice_2: 0.2055  loss_ce_3: 0.0004541  loss_mask_3: 0.1968  loss_dice_3: 0.2077  loss_ce_4: 0.0003377  loss_mask_4: 0.2098  loss_dice_4: 0.2098  loss_ce_5: 0.0003559  loss_mask_5: 0.2058  loss_dice_5: 0.2113  loss_ce_6: 0.0005184  loss_mask_6: 0.1986  loss_dice_6: 0.2106  loss_ce_7: 0.0003887  loss_mask_7: 0.2038  loss_dice_7: 0.2065  loss_ce_8: 0.0004635  loss_mask_8: 0.2037  loss_dice_8: 0.2101  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:48:04] d2.utils.events INFO:  eta: 0:08:39  iter: 45899  total_loss: 3.937  loss_ce: 0.0003667  loss_mask: 0.1988  loss_dice: 0.1541  loss_ce_0: 0.06204  loss_mask_0: 0.2006  loss_dice_0: 0.15  loss_ce_1: 0.0004061  loss_mask_1: 0.2028  loss_dice_1: 0.151  loss_ce_2: 0.0004041  loss_mask_2: 0.1968  loss_dice_2: 0.1506  loss_ce_3: 0.0003423  loss_mask_3: 0.1982  loss_dice_3: 0.1519  loss_ce_4: 0.0004388  loss_mask_4: 0.1988  loss_dice_4: 0.1498  loss_ce_5: 0.0004114  loss_mask_5: 0.1941  loss_dice_5: 0.1522  loss_ce_6: 0.0004182  loss_mask_6: 0.2065  loss_dice_6: 0.1454  loss_ce_7: 0.0004217  loss_mask_7: 0.1968  loss_dice_7: 0.1499  loss_ce_8: 0.0005086  loss_mask_8: 0.2002  loss_dice_8: 0.1506  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:06] d2.utils.events INFO:  eta: 0:08:36  iter: 45919  total_loss: 4.539  loss_ce: 0.0006153  loss_mask: 0.1561  loss_dice: 0.2284  loss_ce_0: 0.06148  loss_mask_0: 0.1595  loss_dice_0: 0.2356  loss_ce_1: 0.0006823  loss_mask_1: 0.164  loss_dice_1: 0.2212  loss_ce_2: 0.0005986  loss_mask_2: 0.1731  loss_dice_2: 0.2357  loss_ce_3: 0.0006049  loss_mask_3: 0.1794  loss_dice_3: 0.2377  loss_ce_4: 0.0005977  loss_mask_4: 0.1642  loss_dice_4: 0.2258  loss_ce_5: 0.0006373  loss_mask_5: 0.1701  loss_dice_5: 0.2266  loss_ce_6: 0.0008015  loss_mask_6: 0.1776  loss_dice_6: 0.2227  loss_ce_7: 0.0006877  loss_mask_7: 0.1647  loss_dice_7: 0.2423  loss_ce_8: 0.0006644  loss_mask_8: 0.167  loss_dice_8: 0.2352  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:09] d2.utils.events INFO:  eta: 0:08:34  iter: 45939  total_loss: 4.284  loss_ce: 0.0003706  loss_mask: 0.1994  loss_dice: 0.1976  loss_ce_0: 0.06147  loss_mask_0: 0.1903  loss_dice_0: 0.1979  loss_ce_1: 0.0003817  loss_mask_1: 0.1999  loss_dice_1: 0.2013  loss_ce_2: 0.0002663  loss_mask_2: 0.1955  loss_dice_2: 0.2101  loss_ce_3: 0.000393  loss_mask_3: 0.1932  loss_dice_3: 0.2019  loss_ce_4: 0.000284  loss_mask_4: 0.2028  loss_dice_4: 0.1959  loss_ce_5: 0.0003255  loss_mask_5: 0.1935  loss_dice_5: 0.1998  loss_ce_6: 0.0004617  loss_mask_6: 0.2113  loss_dice_6: 0.2019  loss_ce_7: 0.0003384  loss_mask_7: 0.1973  loss_dice_7: 0.2102  loss_ce_8: 0.0004335  loss_mask_8: 0.1967  loss_dice_8: 0.2038  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:11] d2.utils.events INFO:  eta: 0:08:32  iter: 45959  total_loss: 3.995  loss_ce: 0.0004492  loss_mask: 0.1624  loss_dice: 0.1983  loss_ce_0: 0.06144  loss_mask_0: 0.1501  loss_dice_0: 0.1984  loss_ce_1: 0.0004918  loss_mask_1: 0.1584  loss_dice_1: 0.1947  loss_ce_2: 0.0003897  loss_mask_2: 0.1577  loss_dice_2: 0.1955  loss_ce_3: 0.0004556  loss_mask_3: 0.1637  loss_dice_3: 0.1928  loss_ce_4: 0.000455  loss_mask_4: 0.1456  loss_dice_4: 0.194  loss_ce_5: 0.000381  loss_mask_5: 0.1628  loss_dice_5: 0.2028  loss_ce_6: 0.0005139  loss_mask_6: 0.1627  loss_dice_6: 0.1985  loss_ce_7: 0.0004389  loss_mask_7: 0.1563  loss_dice_7: 0.1998  loss_ce_8: 0.0004916  loss_mask_8: 0.1619  loss_dice_8: 0.2075  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:48:14] d2.utils.events INFO:  eta: 0:08:29  iter: 45979  total_loss: 3.718  loss_ce: 0.0006151  loss_mask: 0.1759  loss_dice: 0.1429  loss_ce_0: 0.06144  loss_mask_0: 0.1784  loss_dice_0: 0.1429  loss_ce_1: 0.0004154  loss_mask_1: 0.1768  loss_dice_1: 0.1474  loss_ce_2: 0.000413  loss_mask_2: 0.1811  loss_dice_2: 0.1459  loss_ce_3: 0.0005576  loss_mask_3: 0.1778  loss_dice_3: 0.1531  loss_ce_4: 0.000495  loss_mask_4: 0.1794  loss_dice_4: 0.1495  loss_ce_5: 0.0005205  loss_mask_5: 0.1777  loss_dice_5: 0.1465  loss_ce_6: 0.0005812  loss_mask_6: 0.1804  loss_dice_6: 0.151  loss_ce_7: 0.0005608  loss_mask_7: 0.173  loss_dice_7: 0.1503  loss_ce_8: 0.0005454  loss_mask_8: 0.1889  loss_dice_8: 0.1491  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:48:16] d2.utils.events INFO:  eta: 0:08:26  iter: 45999  total_loss: 4.007  loss_ce: 0.0004145  loss_mask: 0.2183  loss_dice: 0.1507  loss_ce_0: 0.06144  loss_mask_0: 0.2209  loss_dice_0: 0.1506  loss_ce_1: 0.000393  loss_mask_1: 0.2253  loss_dice_1: 0.1507  loss_ce_2: 0.0003531  loss_mask_2: 0.2226  loss_dice_2: 0.1505  loss_ce_3: 0.0004651  loss_mask_3: 0.2213  loss_dice_3: 0.1477  loss_ce_4: 0.0003491  loss_mask_4: 0.2241  loss_dice_4: 0.1546  loss_ce_5: 0.0004052  loss_mask_5: 0.222  loss_dice_5: 0.1517  loss_ce_6: 0.0004952  loss_mask_6: 0.2256  loss_dice_6: 0.1503  loss_ce_7: 0.0003983  loss_mask_7: 0.2198  loss_dice_7: 0.1532  loss_ce_8: 0.0004906  loss_mask_8: 0.2294  loss_dice_8: 0.1506  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:48:19] d2.utils.events INFO:  eta: 0:08:24  iter: 46019  total_loss: 4.087  loss_ce: 0.0003977  loss_mask: 0.2234  loss_dice: 0.2066  loss_ce_0: 0.06201  loss_mask_0: 0.2102  loss_dice_0: 0.205  loss_ce_1: 0.000398  loss_mask_1: 0.2056  loss_dice_1: 0.206  loss_ce_2: 0.0002922  loss_mask_2: 0.2127  loss_dice_2: 0.2015  loss_ce_3: 0.0003542  loss_mask_3: 0.2009  loss_dice_3: 0.2126  loss_ce_4: 0.0003443  loss_mask_4: 0.2061  loss_dice_4: 0.2027  loss_ce_5: 0.0003262  loss_mask_5: 0.2124  loss_dice_5: 0.2131  loss_ce_6: 0.000489  loss_mask_6: 0.216  loss_dice_6: 0.2087  loss_ce_7: 0.0003992  loss_mask_7: 0.212  loss_dice_7: 0.2097  loss_ce_8: 0.0004505  loss_mask_8: 0.204  loss_dice_8: 0.209  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:48:22] d2.utils.events INFO:  eta: 0:08:21  iter: 46039  total_loss: 4.144  loss_ce: 0.0003922  loss_mask: 0.2104  loss_dice: 0.1583  loss_ce_0: 0.06201  loss_mask_0: 0.1927  loss_dice_0: 0.1546  loss_ce_1: 0.0003935  loss_mask_1: 0.1984  loss_dice_1: 0.1548  loss_ce_2: 0.0002811  loss_mask_2: 0.2057  loss_dice_2: 0.1578  loss_ce_3: 0.000425  loss_mask_3: 0.1979  loss_dice_3: 0.162  loss_ce_4: 0.0003338  loss_mask_4: 0.1958  loss_dice_4: 0.1613  loss_ce_5: 0.0003391  loss_mask_5: 0.2152  loss_dice_5: 0.1613  loss_ce_6: 0.0004679  loss_mask_6: 0.2011  loss_dice_6: 0.1573  loss_ce_7: 0.0003239  loss_mask_7: 0.2009  loss_dice_7: 0.1555  loss_ce_8: 0.0004409  loss_mask_8: 0.2011  loss_dice_8: 0.161  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:24] d2.utils.events INFO:  eta: 0:08:19  iter: 46059  total_loss: 4.082  loss_ce: 0.0004072  loss_mask: 0.1861  loss_dice: 0.1711  loss_ce_0: 0.06151  loss_mask_0: 0.1808  loss_dice_0: 0.1731  loss_ce_1: 0.0003996  loss_mask_1: 0.1821  loss_dice_1: 0.1744  loss_ce_2: 0.0002961  loss_mask_2: 0.1826  loss_dice_2: 0.1752  loss_ce_3: 0.0004267  loss_mask_3: 0.1915  loss_dice_3: 0.1731  loss_ce_4: 0.0003124  loss_mask_4: 0.1773  loss_dice_4: 0.1699  loss_ce_5: 0.000324  loss_mask_5: 0.1729  loss_dice_5: 0.1737  loss_ce_6: 0.0004835  loss_mask_6: 0.1843  loss_dice_6: 0.1701  loss_ce_7: 0.0003416  loss_mask_7: 0.1857  loss_dice_7: 0.1774  loss_ce_8: 0.0004387  loss_mask_8: 0.1805  loss_dice_8: 0.1704  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:27] d2.utils.events INFO:  eta: 0:08:17  iter: 46079  total_loss: 4.658  loss_ce: 0.0006015  loss_mask: 0.1859  loss_dice: 0.163  loss_ce_0: 0.06144  loss_mask_0: 0.1885  loss_dice_0: 0.1613  loss_ce_1: 0.0004794  loss_mask_1: 0.1886  loss_dice_1: 0.1592  loss_ce_2: 0.0003385  loss_mask_2: 0.1959  loss_dice_2: 0.1629  loss_ce_3: 0.0005325  loss_mask_3: 0.1974  loss_dice_3: 0.1666  loss_ce_4: 0.0003899  loss_mask_4: 0.1792  loss_dice_4: 0.166  loss_ce_5: 0.0003805  loss_mask_5: 0.192  loss_dice_5: 0.1675  loss_ce_6: 0.0005375  loss_mask_6: 0.1975  loss_dice_6: 0.1568  loss_ce_7: 0.0005824  loss_mask_7: 0.1987  loss_dice_7: 0.1587  loss_ce_8: 0.0004856  loss_mask_8: 0.195  loss_dice_8: 0.1589  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:29] d2.utils.events INFO:  eta: 0:08:14  iter: 46099  total_loss: 4.087  loss_ce: 0.0003789  loss_mask: 0.2081  loss_dice: 0.1894  loss_ce_0: 0.06174  loss_mask_0: 0.2047  loss_dice_0: 0.1864  loss_ce_1: 0.0004414  loss_mask_1: 0.2102  loss_dice_1: 0.1935  loss_ce_2: 0.0003552  loss_mask_2: 0.1992  loss_dice_2: 0.1916  loss_ce_3: 0.0004022  loss_mask_3: 0.2035  loss_dice_3: 0.1895  loss_ce_4: 0.0003844  loss_mask_4: 0.2063  loss_dice_4: 0.1948  loss_ce_5: 0.0003629  loss_mask_5: 0.2051  loss_dice_5: 0.198  loss_ce_6: 0.0004625  loss_mask_6: 0.1931  loss_dice_6: 0.1917  loss_ce_7: 0.000399  loss_mask_7: 0.2116  loss_dice_7: 0.1953  loss_ce_8: 0.0004642  loss_mask_8: 0.208  loss_dice_8: 0.1954  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:32] d2.utils.events INFO:  eta: 0:08:11  iter: 46119  total_loss: 4.144  loss_ce: 0.000311  loss_mask: 0.2029  loss_dice: 0.1897  loss_ce_0: 0.06139  loss_mask_0: 0.209  loss_dice_0: 0.1951  loss_ce_1: 0.0003704  loss_mask_1: 0.1926  loss_dice_1: 0.1895  loss_ce_2: 0.0002306  loss_mask_2: 0.1968  loss_dice_2: 0.1861  loss_ce_3: 0.0003482  loss_mask_3: 0.1976  loss_dice_3: 0.1907  loss_ce_4: 0.0002773  loss_mask_4: 0.2047  loss_dice_4: 0.1848  loss_ce_5: 0.0002848  loss_mask_5: 0.2016  loss_dice_5: 0.1955  loss_ce_6: 0.0004407  loss_mask_6: 0.2022  loss_dice_6: 0.1888  loss_ce_7: 0.000267  loss_mask_7: 0.2074  loss_dice_7: 0.1879  loss_ce_8: 0.0003873  loss_mask_8: 0.2003  loss_dice_8: 0.1826  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:34] d2.utils.events INFO:  eta: 0:08:09  iter: 46139  total_loss: 3.83  loss_ce: 0.0005171  loss_mask: 0.1692  loss_dice: 0.1466  loss_ce_0: 0.06134  loss_mask_0: 0.1715  loss_dice_0: 0.1484  loss_ce_1: 0.0004679  loss_mask_1: 0.1618  loss_dice_1: 0.1506  loss_ce_2: 0.000415  loss_mask_2: 0.1696  loss_dice_2: 0.1502  loss_ce_3: 0.0004918  loss_mask_3: 0.1624  loss_dice_3: 0.1482  loss_ce_4: 0.000448  loss_mask_4: 0.1661  loss_dice_4: 0.1517  loss_ce_5: 0.0004413  loss_mask_5: 0.1652  loss_dice_5: 0.1456  loss_ce_6: 0.0005181  loss_mask_6: 0.1708  loss_dice_6: 0.1497  loss_ce_7: 0.0004856  loss_mask_7: 0.175  loss_dice_7: 0.1465  loss_ce_8: 0.0005164  loss_mask_8: 0.176  loss_dice_8: 0.1462  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:37] d2.utils.events INFO:  eta: 0:08:06  iter: 46159  total_loss: 3.861  loss_ce: 0.000492  loss_mask: 0.1683  loss_dice: 0.1865  loss_ce_0: 0.06174  loss_mask_0: 0.1751  loss_dice_0: 0.1824  loss_ce_1: 0.000444  loss_mask_1: 0.1667  loss_dice_1: 0.1821  loss_ce_2: 0.0005085  loss_mask_2: 0.1719  loss_dice_2: 0.1916  loss_ce_3: 0.0005164  loss_mask_3: 0.1754  loss_dice_3: 0.1889  loss_ce_4: 0.0005768  loss_mask_4: 0.1712  loss_dice_4: 0.1841  loss_ce_5: 0.000537  loss_mask_5: 0.1746  loss_dice_5: 0.1857  loss_ce_6: 0.0005285  loss_mask_6: 0.1734  loss_dice_6: 0.189  loss_ce_7: 0.0005513  loss_mask_7: 0.1733  loss_dice_7: 0.1882  loss_ce_8: 0.0005774  loss_mask_8: 0.1768  loss_dice_8: 0.1842  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:48:39] d2.utils.events INFO:  eta: 0:08:04  iter: 46179  total_loss: 4.193  loss_ce: 0.0006182  loss_mask: 0.1897  loss_dice: 0.2174  loss_ce_0: 0.06135  loss_mask_0: 0.1995  loss_dice_0: 0.2167  loss_ce_1: 0.000484  loss_mask_1: 0.2009  loss_dice_1: 0.2259  loss_ce_2: 0.0006425  loss_mask_2: 0.1868  loss_dice_2: 0.2096  loss_ce_3: 0.0006475  loss_mask_3: 0.2075  loss_dice_3: 0.2099  loss_ce_4: 0.0005307  loss_mask_4: 0.1917  loss_dice_4: 0.2161  loss_ce_5: 0.0006384  loss_mask_5: 0.1955  loss_dice_5: 0.2096  loss_ce_6: 0.000671  loss_mask_6: 0.1998  loss_dice_6: 0.2077  loss_ce_7: 0.0005286  loss_mask_7: 0.206  loss_dice_7: 0.2111  loss_ce_8: 0.0007158  loss_mask_8: 0.1997  loss_dice_8: 0.2176  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:48:42] d2.utils.events INFO:  eta: 0:08:01  iter: 46199  total_loss: 4.146  loss_ce: 0.0005426  loss_mask: 0.2356  loss_dice: 0.1451  loss_ce_0: 0.06133  loss_mask_0: 0.2305  loss_dice_0: 0.1447  loss_ce_1: 0.0007965  loss_mask_1: 0.2226  loss_dice_1: 0.1446  loss_ce_2: 0.0007784  loss_mask_2: 0.2289  loss_dice_2: 0.15  loss_ce_3: 0.0005692  loss_mask_3: 0.2184  loss_dice_3: 0.1461  loss_ce_4: 0.0005721  loss_mask_4: 0.2149  loss_dice_4: 0.1518  loss_ce_5: 0.0008483  loss_mask_5: 0.2234  loss_dice_5: 0.1459  loss_ce_6: 0.0006315  loss_mask_6: 0.2252  loss_dice_6: 0.1448  loss_ce_7: 0.00056  loss_mask_7: 0.2225  loss_dice_7: 0.1495  loss_ce_8: 0.0007826  loss_mask_8: 0.2217  loss_dice_8: 0.1448  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:48:44] d2.utils.events INFO:  eta: 0:07:59  iter: 46219  total_loss: 4.009  loss_ce: 0.000524  loss_mask: 0.1649  loss_dice: 0.2266  loss_ce_0: 0.06211  loss_mask_0: 0.1657  loss_dice_0: 0.2204  loss_ce_1: 0.0005652  loss_mask_1: 0.1624  loss_dice_1: 0.2284  loss_ce_2: 0.0004347  loss_mask_2: 0.1552  loss_dice_2: 0.229  loss_ce_3: 0.0005139  loss_mask_3: 0.1543  loss_dice_3: 0.2232  loss_ce_4: 0.000651  loss_mask_4: 0.1631  loss_dice_4: 0.2177  loss_ce_5: 0.0004444  loss_mask_5: 0.1604  loss_dice_5: 0.2163  loss_ce_6: 0.0005771  loss_mask_6: 0.1559  loss_dice_6: 0.2233  loss_ce_7: 0.0005988  loss_mask_7: 0.1608  loss_dice_7: 0.22  loss_ce_8: 0.0006299  loss_mask_8: 0.1589  loss_dice_8: 0.2163  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:47] d2.utils.events INFO:  eta: 0:07:56  iter: 46239  total_loss: 3.763  loss_ce: 0.0004065  loss_mask: 0.2109  loss_dice: 0.1417  loss_ce_0: 0.06139  loss_mask_0: 0.2126  loss_dice_0: 0.1377  loss_ce_1: 0.0004183  loss_mask_1: 0.2176  loss_dice_1: 0.1417  loss_ce_2: 0.0003418  loss_mask_2: 0.2173  loss_dice_2: 0.1395  loss_ce_3: 0.0004571  loss_mask_3: 0.2052  loss_dice_3: 0.1417  loss_ce_4: 0.0003934  loss_mask_4: 0.2159  loss_dice_4: 0.1408  loss_ce_5: 0.0004076  loss_mask_5: 0.2155  loss_dice_5: 0.1414  loss_ce_6: 0.0004884  loss_mask_6: 0.2087  loss_dice_6: 0.1406  loss_ce_7: 0.0004723  loss_mask_7: 0.2139  loss_dice_7: 0.1412  loss_ce_8: 0.000476  loss_mask_8: 0.2139  loss_dice_8: 0.1443  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:50] d2.utils.events INFO:  eta: 0:07:54  iter: 46259  total_loss: 5.218  loss_ce: 0.03839  loss_mask: 0.1556  loss_dice: 0.2493  loss_ce_0: 0.06139  loss_mask_0: 0.1717  loss_dice_0: 0.2591  loss_ce_1: 0.04765  loss_mask_1: 0.1544  loss_dice_1: 0.2788  loss_ce_2: 0.04556  loss_mask_2: 0.1591  loss_dice_2: 0.2517  loss_ce_3: 0.04832  loss_mask_3: 0.1577  loss_dice_3: 0.2548  loss_ce_4: 0.04094  loss_mask_4: 0.1665  loss_dice_4: 0.2438  loss_ce_5: 0.05043  loss_mask_5: 0.1583  loss_dice_5: 0.2308  loss_ce_6: 0.04459  loss_mask_6: 0.1598  loss_dice_6: 0.2572  loss_ce_7: 0.04847  loss_mask_7: 0.162  loss_dice_7: 0.2495  loss_ce_8: 0.05024  loss_mask_8: 0.1596  loss_dice_8: 0.2471  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:52] d2.utils.events INFO:  eta: 0:07:51  iter: 46279  total_loss: 4.533  loss_ce: 0.0005728  loss_mask: 0.1935  loss_dice: 0.1694  loss_ce_0: 0.06137  loss_mask_0: 0.1998  loss_dice_0: 0.1667  loss_ce_1: 0.0004421  loss_mask_1: 0.1946  loss_dice_1: 0.1658  loss_ce_2: 0.0003625  loss_mask_2: 0.197  loss_dice_2: 0.1693  loss_ce_3: 0.0005271  loss_mask_3: 0.1968  loss_dice_3: 0.1644  loss_ce_4: 0.0004121  loss_mask_4: 0.1879  loss_dice_4: 0.1667  loss_ce_5: 0.000419  loss_mask_5: 0.1844  loss_dice_5: 0.1686  loss_ce_6: 0.0005643  loss_mask_6: 0.1994  loss_dice_6: 0.1679  loss_ce_7: 0.0005339  loss_mask_7: 0.1977  loss_dice_7: 0.166  loss_ce_8: 0.0005148  loss_mask_8: 0.1984  loss_dice_8: 0.1681  time: 0.1260  data_time: 0.0014  lr: 1e-06  max_mem: 3105M
[04/13 16:48:55] d2.utils.events INFO:  eta: 0:07:49  iter: 46299  total_loss: 4.137  loss_ce: 0.0004668  loss_mask: 0.2149  loss_dice: 0.1779  loss_ce_0: 0.06131  loss_mask_0: 0.2035  loss_dice_0: 0.1745  loss_ce_1: 0.0004149  loss_mask_1: 0.2072  loss_dice_1: 0.1763  loss_ce_2: 0.0003681  loss_mask_2: 0.2055  loss_dice_2: 0.1718  loss_ce_3: 0.0004305  loss_mask_3: 0.218  loss_dice_3: 0.1781  loss_ce_4: 0.0003867  loss_mask_4: 0.2098  loss_dice_4: 0.1766  loss_ce_5: 0.0003818  loss_mask_5: 0.2038  loss_dice_5: 0.1708  loss_ce_6: 0.0004614  loss_mask_6: 0.2022  loss_dice_6: 0.1758  loss_ce_7: 0.0003891  loss_mask_7: 0.204  loss_dice_7: 0.1693  loss_ce_8: 0.0004699  loss_mask_8: 0.201  loss_dice_8: 0.1709  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:48:57] d2.utils.events INFO:  eta: 0:07:46  iter: 46319  total_loss: 4.353  loss_ce: 0.0004408  loss_mask: 0.1717  loss_dice: 0.1672  loss_ce_0: 0.06124  loss_mask_0: 0.1704  loss_dice_0: 0.1737  loss_ce_1: 0.0004108  loss_mask_1: 0.1747  loss_dice_1: 0.1711  loss_ce_2: 0.0003418  loss_mask_2: 0.1738  loss_dice_2: 0.1688  loss_ce_3: 0.0004614  loss_mask_3: 0.1708  loss_dice_3: 0.1716  loss_ce_4: 0.0005277  loss_mask_4: 0.1798  loss_dice_4: 0.1717  loss_ce_5: 0.0003963  loss_mask_5: 0.1823  loss_dice_5: 0.1689  loss_ce_6: 0.0005474  loss_mask_6: 0.1677  loss_dice_6: 0.1658  loss_ce_7: 0.0005714  loss_mask_7: 0.1709  loss_dice_7: 0.1734  loss_ce_8: 0.0005098  loss_mask_8: 0.1712  loss_dice_8: 0.1814  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:49:00] d2.utils.events INFO:  eta: 0:07:44  iter: 46339  total_loss: 4.403  loss_ce: 0.0004558  loss_mask: 0.2109  loss_dice: 0.1875  loss_ce_0: 0.06116  loss_mask_0: 0.2051  loss_dice_0: 0.1943  loss_ce_1: 0.0004093  loss_mask_1: 0.2114  loss_dice_1: 0.1963  loss_ce_2: 0.000363  loss_mask_2: 0.1996  loss_dice_2: 0.1944  loss_ce_3: 0.0004444  loss_mask_3: 0.2026  loss_dice_3: 0.1891  loss_ce_4: 0.0003712  loss_mask_4: 0.2103  loss_dice_4: 0.1956  loss_ce_5: 0.0004189  loss_mask_5: 0.1974  loss_dice_5: 0.1901  loss_ce_6: 0.0004901  loss_mask_6: 0.2038  loss_dice_6: 0.1935  loss_ce_7: 0.0003688  loss_mask_7: 0.2088  loss_dice_7: 0.1946  loss_ce_8: 0.0004633  loss_mask_8: 0.2138  loss_dice_8: 0.1948  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:49:02] d2.utils.events INFO:  eta: 0:07:41  iter: 46359  total_loss: 4.172  loss_ce: 0.0004205  loss_mask: 0.2113  loss_dice: 0.18  loss_ce_0: 0.06243  loss_mask_0: 0.2074  loss_dice_0: 0.1851  loss_ce_1: 0.0005196  loss_mask_1: 0.2137  loss_dice_1: 0.1875  loss_ce_2: 0.0003456  loss_mask_2: 0.2073  loss_dice_2: 0.1724  loss_ce_3: 0.0004286  loss_mask_3: 0.205  loss_dice_3: 0.1859  loss_ce_4: 0.0004157  loss_mask_4: 0.212  loss_dice_4: 0.1931  loss_ce_5: 0.0003313  loss_mask_5: 0.2015  loss_dice_5: 0.1768  loss_ce_6: 0.0005204  loss_mask_6: 0.1999  loss_dice_6: 0.1835  loss_ce_7: 0.0005125  loss_mask_7: 0.2082  loss_dice_7: 0.1819  loss_ce_8: 0.0004767  loss_mask_8: 0.2042  loss_dice_8: 0.1779  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:05] d2.utils.events INFO:  eta: 0:07:39  iter: 46379  total_loss: 4.102  loss_ce: 0.0004463  loss_mask: 0.1616  loss_dice: 0.2042  loss_ce_0: 0.06239  loss_mask_0: 0.1705  loss_dice_0: 0.2095  loss_ce_1: 0.0004367  loss_mask_1: 0.1653  loss_dice_1: 0.2055  loss_ce_2: 0.0004501  loss_mask_2: 0.1667  loss_dice_2: 0.202  loss_ce_3: 0.0003357  loss_mask_3: 0.1689  loss_dice_3: 0.2207  loss_ce_4: 0.0003847  loss_mask_4: 0.1668  loss_dice_4: 0.2113  loss_ce_5: 0.0005789  loss_mask_5: 0.173  loss_dice_5: 0.2068  loss_ce_6: 0.0004299  loss_mask_6: 0.1658  loss_dice_6: 0.207  loss_ce_7: 0.000437  loss_mask_7: 0.1644  loss_dice_7: 0.2141  loss_ce_8: 0.0005871  loss_mask_8: 0.167  loss_dice_8: 0.2094  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:07] d2.utils.events INFO:  eta: 0:07:37  iter: 46399  total_loss: 3.544  loss_ce: 0.000429  loss_mask: 0.1918  loss_dice: 0.1408  loss_ce_0: 0.0623  loss_mask_0: 0.2053  loss_dice_0: 0.1342  loss_ce_1: 0.000435  loss_mask_1: 0.2125  loss_dice_1: 0.1407  loss_ce_2: 0.0003624  loss_mask_2: 0.2073  loss_dice_2: 0.1386  loss_ce_3: 0.000394  loss_mask_3: 0.2026  loss_dice_3: 0.1473  loss_ce_4: 0.0003719  loss_mask_4: 0.1966  loss_dice_4: 0.1314  loss_ce_5: 0.0003922  loss_mask_5: 0.1958  loss_dice_5: 0.1355  loss_ce_6: 0.0004804  loss_mask_6: 0.2045  loss_dice_6: 0.1339  loss_ce_7: 0.0004321  loss_mask_7: 0.1975  loss_dice_7: 0.1369  loss_ce_8: 0.0004973  loss_mask_8: 0.2034  loss_dice_8: 0.137  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:10] d2.utils.events INFO:  eta: 0:07:34  iter: 46419  total_loss: 4.349  loss_ce: 0.0004862  loss_mask: 0.1802  loss_dice: 0.1981  loss_ce_0: 0.06174  loss_mask_0: 0.168  loss_dice_0: 0.1975  loss_ce_1: 0.0004542  loss_mask_1: 0.1823  loss_dice_1: 0.2025  loss_ce_2: 0.0003496  loss_mask_2: 0.1711  loss_dice_2: 0.1942  loss_ce_3: 0.0004265  loss_mask_3: 0.1713  loss_dice_3: 0.2007  loss_ce_4: 0.0003966  loss_mask_4: 0.178  loss_dice_4: 0.2021  loss_ce_5: 0.0003959  loss_mask_5: 0.1764  loss_dice_5: 0.2026  loss_ce_6: 0.0004761  loss_mask_6: 0.1749  loss_dice_6: 0.1916  loss_ce_7: 0.0005321  loss_mask_7: 0.1786  loss_dice_7: 0.1957  loss_ce_8: 0.0004918  loss_mask_8: 0.1768  loss_dice_8: 0.1975  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:13] d2.utils.events INFO:  eta: 0:07:32  iter: 46439  total_loss: 4.238  loss_ce: 0.0004844  loss_mask: 0.213  loss_dice: 0.1939  loss_ce_0: 0.06119  loss_mask_0: 0.2087  loss_dice_0: 0.1948  loss_ce_1: 0.0004083  loss_mask_1: 0.2134  loss_dice_1: 0.2066  loss_ce_2: 0.0003245  loss_mask_2: 0.2062  loss_dice_2: 0.1861  loss_ce_3: 0.00045  loss_mask_3: 0.2139  loss_dice_3: 0.2008  loss_ce_4: 0.0003506  loss_mask_4: 0.2107  loss_dice_4: 0.1959  loss_ce_5: 0.0003591  loss_mask_5: 0.2174  loss_dice_5: 0.1884  loss_ce_6: 0.0005001  loss_mask_6: 0.2059  loss_dice_6: 0.1939  loss_ce_7: 0.0004032  loss_mask_7: 0.2105  loss_dice_7: 0.1875  loss_ce_8: 0.0004595  loss_mask_8: 0.2101  loss_dice_8: 0.1901  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:15] d2.utils.events INFO:  eta: 0:07:29  iter: 46459  total_loss: 4.053  loss_ce: 0.0006866  loss_mask: 0.1654  loss_dice: 0.1791  loss_ce_0: 0.06231  loss_mask_0: 0.1611  loss_dice_0: 0.1734  loss_ce_1: 0.0004987  loss_mask_1: 0.1674  loss_dice_1: 0.1723  loss_ce_2: 0.0004066  loss_mask_2: 0.1743  loss_dice_2: 0.174  loss_ce_3: 0.0006204  loss_mask_3: 0.1654  loss_dice_3: 0.1726  loss_ce_4: 0.0006079  loss_mask_4: 0.1648  loss_dice_4: 0.1756  loss_ce_5: 0.0004793  loss_mask_5: 0.1664  loss_dice_5: 0.1749  loss_ce_6: 0.000651  loss_mask_6: 0.1621  loss_dice_6: 0.1774  loss_ce_7: 0.0006087  loss_mask_7: 0.1632  loss_dice_7: 0.1746  loss_ce_8: 0.0005996  loss_mask_8: 0.1692  loss_dice_8: 0.1753  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:18] d2.utils.events INFO:  eta: 0:07:27  iter: 46479  total_loss: 4.441  loss_ce: 0.0006194  loss_mask: 0.1962  loss_dice: 0.1651  loss_ce_0: 0.06113  loss_mask_0: 0.2123  loss_dice_0: 0.1739  loss_ce_1: 0.0005793  loss_mask_1: 0.2114  loss_dice_1: 0.1731  loss_ce_2: 0.0005005  loss_mask_2: 0.209  loss_dice_2: 0.169  loss_ce_3: 0.0006456  loss_mask_3: 0.2011  loss_dice_3: 0.1781  loss_ce_4: 0.0005688  loss_mask_4: 0.2116  loss_dice_4: 0.1721  loss_ce_5: 0.0005621  loss_mask_5: 0.2038  loss_dice_5: 0.1699  loss_ce_6: 0.0005942  loss_mask_6: 0.2043  loss_dice_6: 0.1733  loss_ce_7: 0.0005763  loss_mask_7: 0.212  loss_dice_7: 0.1819  loss_ce_8: 0.0006262  loss_mask_8: 0.2121  loss_dice_8: 0.1717  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:20] d2.utils.events INFO:  eta: 0:07:24  iter: 46499  total_loss: 4.384  loss_ce: 0.0004662  loss_mask: 0.2361  loss_dice: 0.1382  loss_ce_0: 0.0611  loss_mask_0: 0.2326  loss_dice_0: 0.1399  loss_ce_1: 0.0004188  loss_mask_1: 0.2222  loss_dice_1: 0.1361  loss_ce_2: 0.0003223  loss_mask_2: 0.2386  loss_dice_2: 0.1419  loss_ce_3: 0.0004601  loss_mask_3: 0.2353  loss_dice_3: 0.1399  loss_ce_4: 0.0003788  loss_mask_4: 0.2438  loss_dice_4: 0.1415  loss_ce_5: 0.0004349  loss_mask_5: 0.2373  loss_dice_5: 0.1392  loss_ce_6: 0.0004884  loss_mask_6: 0.2341  loss_dice_6: 0.1383  loss_ce_7: 0.0004403  loss_mask_7: 0.2411  loss_dice_7: 0.1408  loss_ce_8: 0.000507  loss_mask_8: 0.2285  loss_dice_8: 0.1392  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:23] d2.utils.events INFO:  eta: 0:07:22  iter: 46519  total_loss: 4.395  loss_ce: 0.005157  loss_mask: 0.1449  loss_dice: 0.2504  loss_ce_0: 0.06242  loss_mask_0: 0.1444  loss_dice_0: 0.2498  loss_ce_1: 0.003274  loss_mask_1: 0.1503  loss_dice_1: 0.2251  loss_ce_2: 0.003218  loss_mask_2: 0.1462  loss_dice_2: 0.2485  loss_ce_3: 0.004064  loss_mask_3: 0.1491  loss_dice_3: 0.2305  loss_ce_4: 0.003177  loss_mask_4: 0.1528  loss_dice_4: 0.2431  loss_ce_5: 0.00361  loss_mask_5: 0.1468  loss_dice_5: 0.2493  loss_ce_6: 0.004111  loss_mask_6: 0.1627  loss_dice_6: 0.244  loss_ce_7: 0.0032  loss_mask_7: 0.1438  loss_dice_7: 0.2466  loss_ce_8: 0.002319  loss_mask_8: 0.1519  loss_dice_8: 0.2361  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:25] d2.utils.events INFO:  eta: 0:07:19  iter: 46539  total_loss: 4.753  loss_ce: 0.0005625  loss_mask: 0.1808  loss_dice: 0.2171  loss_ce_0: 0.06103  loss_mask_0: 0.181  loss_dice_0: 0.2213  loss_ce_1: 0.0005094  loss_mask_1: 0.1772  loss_dice_1: 0.2189  loss_ce_2: 0.0003809  loss_mask_2: 0.1779  loss_dice_2: 0.2236  loss_ce_3: 0.0004191  loss_mask_3: 0.1799  loss_dice_3: 0.2135  loss_ce_4: 0.0004433  loss_mask_4: 0.1911  loss_dice_4: 0.2116  loss_ce_5: 0.0006073  loss_mask_5: 0.1862  loss_dice_5: 0.2269  loss_ce_6: 0.0004877  loss_mask_6: 0.202  loss_dice_6: 0.2156  loss_ce_7: 0.0006066  loss_mask_7: 0.1845  loss_dice_7: 0.2242  loss_ce_8: 0.0006992  loss_mask_8: 0.1881  loss_dice_8: 0.2216  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:28] d2.utils.events INFO:  eta: 0:07:16  iter: 46559  total_loss: 3.657  loss_ce: 0.0004253  loss_mask: 0.1647  loss_dice: 0.1509  loss_ce_0: 0.06246  loss_mask_0: 0.1661  loss_dice_0: 0.1469  loss_ce_1: 0.0004211  loss_mask_1: 0.1669  loss_dice_1: 0.1496  loss_ce_2: 0.0003762  loss_mask_2: 0.1615  loss_dice_2: 0.1494  loss_ce_3: 0.0004488  loss_mask_3: 0.1656  loss_dice_3: 0.1522  loss_ce_4: 0.0004165  loss_mask_4: 0.1701  loss_dice_4: 0.1541  loss_ce_5: 0.000427  loss_mask_5: 0.1599  loss_dice_5: 0.1547  loss_ce_6: 0.000509  loss_mask_6: 0.1678  loss_dice_6: 0.1486  loss_ce_7: 0.0004177  loss_mask_7: 0.1605  loss_dice_7: 0.1491  loss_ce_8: 0.0005217  loss_mask_8: 0.1717  loss_dice_8: 0.1565  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:30] d2.utils.events INFO:  eta: 0:07:14  iter: 46579  total_loss: 3.924  loss_ce: 0.0003823  loss_mask: 0.2133  loss_dice: 0.1506  loss_ce_0: 0.06174  loss_mask_0: 0.2106  loss_dice_0: 0.1481  loss_ce_1: 0.0003778  loss_mask_1: 0.1965  loss_dice_1: 0.1528  loss_ce_2: 0.0002824  loss_mask_2: 0.2073  loss_dice_2: 0.1494  loss_ce_3: 0.0004163  loss_mask_3: 0.2077  loss_dice_3: 0.1532  loss_ce_4: 0.0003305  loss_mask_4: 0.2098  loss_dice_4: 0.1529  loss_ce_5: 0.0003358  loss_mask_5: 0.2101  loss_dice_5: 0.1458  loss_ce_6: 0.0004767  loss_mask_6: 0.2118  loss_dice_6: 0.1476  loss_ce_7: 0.0004552  loss_mask_7: 0.207  loss_dice_7: 0.1493  loss_ce_8: 0.0004581  loss_mask_8: 0.2062  loss_dice_8: 0.1518  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:33] d2.utils.events INFO:  eta: 0:07:11  iter: 46599  total_loss: 4.39  loss_ce: 0.0005571  loss_mask: 0.2326  loss_dice: 0.1597  loss_ce_0: 0.0611  loss_mask_0: 0.2262  loss_dice_0: 0.1623  loss_ce_1: 0.0004633  loss_mask_1: 0.2345  loss_dice_1: 0.156  loss_ce_2: 0.0003509  loss_mask_2: 0.2189  loss_dice_2: 0.1499  loss_ce_3: 0.00049  loss_mask_3: 0.2322  loss_dice_3: 0.1619  loss_ce_4: 0.0004563  loss_mask_4: 0.2287  loss_dice_4: 0.1586  loss_ce_5: 0.0004977  loss_mask_5: 0.2333  loss_dice_5: 0.1576  loss_ce_6: 0.0005336  loss_mask_6: 0.2219  loss_dice_6: 0.1592  loss_ce_7: 0.0004338  loss_mask_7: 0.2359  loss_dice_7: 0.1631  loss_ce_8: 0.0006052  loss_mask_8: 0.2322  loss_dice_8: 0.1573  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:35] d2.utils.events INFO:  eta: 0:07:09  iter: 46619  total_loss: 3.882  loss_ce: 0.0003478  loss_mask: 0.219  loss_dice: 0.1537  loss_ce_0: 0.06109  loss_mask_0: 0.216  loss_dice_0: 0.1576  loss_ce_1: 0.0003862  loss_mask_1: 0.2159  loss_dice_1: 0.1496  loss_ce_2: 0.0002966  loss_mask_2: 0.22  loss_dice_2: 0.1568  loss_ce_3: 0.0003729  loss_mask_3: 0.2191  loss_dice_3: 0.161  loss_ce_4: 0.000346  loss_mask_4: 0.2139  loss_dice_4: 0.1537  loss_ce_5: 0.0003331  loss_mask_5: 0.2218  loss_dice_5: 0.1559  loss_ce_6: 0.0004485  loss_mask_6: 0.2153  loss_dice_6: 0.1553  loss_ce_7: 0.0003542  loss_mask_7: 0.2131  loss_dice_7: 0.1501  loss_ce_8: 0.0004425  loss_mask_8: 0.2131  loss_dice_8: 0.1551  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:38] d2.utils.events INFO:  eta: 0:07:06  iter: 46639  total_loss: 4.158  loss_ce: 0.0004099  loss_mask: 0.2376  loss_dice: 0.1909  loss_ce_0: 0.06108  loss_mask_0: 0.2373  loss_dice_0: 0.1932  loss_ce_1: 0.0003883  loss_mask_1: 0.2351  loss_dice_1: 0.1989  loss_ce_2: 0.000308  loss_mask_2: 0.2295  loss_dice_2: 0.1834  loss_ce_3: 0.0004458  loss_mask_3: 0.2393  loss_dice_3: 0.1854  loss_ce_4: 0.0003185  loss_mask_4: 0.24  loss_dice_4: 0.1884  loss_ce_5: 0.0003641  loss_mask_5: 0.2399  loss_dice_5: 0.1854  loss_ce_6: 0.0004878  loss_mask_6: 0.2376  loss_dice_6: 0.1856  loss_ce_7: 0.0003756  loss_mask_7: 0.2332  loss_dice_7: 0.188  loss_ce_8: 0.0004468  loss_mask_8: 0.2347  loss_dice_8: 0.1905  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:49:40] d2.utils.events INFO:  eta: 0:07:04  iter: 46659  total_loss: 4.429  loss_ce: 0.0003777  loss_mask: 0.2366  loss_dice: 0.1647  loss_ce_0: 0.06106  loss_mask_0: 0.234  loss_dice_0: 0.1617  loss_ce_1: 0.0003823  loss_mask_1: 0.2358  loss_dice_1: 0.1645  loss_ce_2: 0.0003048  loss_mask_2: 0.2303  loss_dice_2: 0.1626  loss_ce_3: 0.000366  loss_mask_3: 0.2323  loss_dice_3: 0.1635  loss_ce_4: 0.0003418  loss_mask_4: 0.2312  loss_dice_4: 0.1592  loss_ce_5: 0.0003589  loss_mask_5: 0.2347  loss_dice_5: 0.1578  loss_ce_6: 0.0004723  loss_mask_6: 0.2315  loss_dice_6: 0.1672  loss_ce_7: 0.0003813  loss_mask_7: 0.2351  loss_dice_7: 0.1628  loss_ce_8: 0.0004436  loss_mask_8: 0.2318  loss_dice_8: 0.1641  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:43] d2.utils.events INFO:  eta: 0:07:01  iter: 46679  total_loss: 3.793  loss_ce: 0.000412  loss_mask: 0.18  loss_dice: 0.1917  loss_ce_0: 0.06251  loss_mask_0: 0.1806  loss_dice_0: 0.1811  loss_ce_1: 0.0003955  loss_mask_1: 0.1838  loss_dice_1: 0.1823  loss_ce_2: 0.000302  loss_mask_2: 0.1778  loss_dice_2: 0.1852  loss_ce_3: 0.0004272  loss_mask_3: 0.1723  loss_dice_3: 0.1781  loss_ce_4: 0.0003458  loss_mask_4: 0.18  loss_dice_4: 0.1889  loss_ce_5: 0.0003681  loss_mask_5: 0.1822  loss_dice_5: 0.1887  loss_ce_6: 0.0004768  loss_mask_6: 0.1895  loss_dice_6: 0.1745  loss_ce_7: 0.0004156  loss_mask_7: 0.1841  loss_dice_7: 0.1861  loss_ce_8: 0.0004559  loss_mask_8: 0.19  loss_dice_8: 0.1872  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:46] d2.utils.events INFO:  eta: 0:06:59  iter: 46699  total_loss: 4.207  loss_ce: 0.0004507  loss_mask: 0.1877  loss_dice: 0.2435  loss_ce_0: 0.06097  loss_mask_0: 0.1891  loss_dice_0: 0.2408  loss_ce_1: 0.0004594  loss_mask_1: 0.1844  loss_dice_1: 0.2332  loss_ce_2: 0.000416  loss_mask_2: 0.1798  loss_dice_2: 0.2468  loss_ce_3: 0.000475  loss_mask_3: 0.1794  loss_dice_3: 0.2307  loss_ce_4: 0.0004128  loss_mask_4: 0.1876  loss_dice_4: 0.2263  loss_ce_5: 0.0004324  loss_mask_5: 0.1848  loss_dice_5: 0.2331  loss_ce_6: 0.0005432  loss_mask_6: 0.1832  loss_dice_6: 0.2381  loss_ce_7: 0.0004795  loss_mask_7: 0.1846  loss_dice_7: 0.2443  loss_ce_8: 0.0005333  loss_mask_8: 0.179  loss_dice_8: 0.2373  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:48] d2.utils.events INFO:  eta: 0:06:56  iter: 46719  total_loss: 3.856  loss_ce: 0.0004289  loss_mask: 0.1954  loss_dice: 0.1649  loss_ce_0: 0.06174  loss_mask_0: 0.1947  loss_dice_0: 0.164  loss_ce_1: 0.0004503  loss_mask_1: 0.1962  loss_dice_1: 0.1689  loss_ce_2: 0.0003513  loss_mask_2: 0.2001  loss_dice_2: 0.172  loss_ce_3: 0.0004012  loss_mask_3: 0.199  loss_dice_3: 0.1758  loss_ce_4: 0.0003798  loss_mask_4: 0.2024  loss_dice_4: 0.1776  loss_ce_5: 0.0003709  loss_mask_5: 0.1903  loss_dice_5: 0.1672  loss_ce_6: 0.0004808  loss_mask_6: 0.1987  loss_dice_6: 0.1757  loss_ce_7: 0.0004032  loss_mask_7: 0.2039  loss_dice_7: 0.1719  loss_ce_8: 0.0004711  loss_mask_8: 0.2118  loss_dice_8: 0.1695  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:51] d2.utils.events INFO:  eta: 0:06:54  iter: 46739  total_loss: 3.974  loss_ce: 0.0003556  loss_mask: 0.1758  loss_dice: 0.1901  loss_ce_0: 0.06254  loss_mask_0: 0.1679  loss_dice_0: 0.1813  loss_ce_1: 0.0003866  loss_mask_1: 0.1719  loss_dice_1: 0.1925  loss_ce_2: 0.0002795  loss_mask_2: 0.1701  loss_dice_2: 0.1958  loss_ce_3: 0.000341  loss_mask_3: 0.1671  loss_dice_3: 0.1934  loss_ce_4: 0.0003381  loss_mask_4: 0.1683  loss_dice_4: 0.1831  loss_ce_5: 0.0003606  loss_mask_5: 0.1724  loss_dice_5: 0.1834  loss_ce_6: 0.0004416  loss_mask_6: 0.166  loss_dice_6: 0.1922  loss_ce_7: 0.0003711  loss_mask_7: 0.1711  loss_dice_7: 0.1895  loss_ce_8: 0.0004626  loss_mask_8: 0.1793  loss_dice_8: 0.1903  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:53] d2.utils.events INFO:  eta: 0:06:51  iter: 46759  total_loss: 3.992  loss_ce: 0.0006379  loss_mask: 0.1815  loss_dice: 0.1735  loss_ce_0: 0.06098  loss_mask_0: 0.178  loss_dice_0: 0.1747  loss_ce_1: 0.0005796  loss_mask_1: 0.1723  loss_dice_1: 0.176  loss_ce_2: 0.0005444  loss_mask_2: 0.1675  loss_dice_2: 0.1731  loss_ce_3: 0.0006248  loss_mask_3: 0.1704  loss_dice_3: 0.1659  loss_ce_4: 0.0005007  loss_mask_4: 0.1747  loss_dice_4: 0.17  loss_ce_5: 0.0006604  loss_mask_5: 0.169  loss_dice_5: 0.1757  loss_ce_6: 0.0006226  loss_mask_6: 0.1827  loss_dice_6: 0.1839  loss_ce_7: 0.0006612  loss_mask_7: 0.168  loss_dice_7: 0.1809  loss_ce_8: 0.0006652  loss_mask_8: 0.1708  loss_dice_8: 0.1764  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:49:56] d2.utils.events INFO:  eta: 0:06:48  iter: 46779  total_loss: 4.662  loss_ce: 0.002036  loss_mask: 0.1351  loss_dice: 0.2816  loss_ce_0: 0.06176  loss_mask_0: 0.1359  loss_dice_0: 0.2775  loss_ce_1: 0.003224  loss_mask_1: 0.1357  loss_dice_1: 0.2644  loss_ce_2: 0.002839  loss_mask_2: 0.1356  loss_dice_2: 0.27  loss_ce_3: 0.003119  loss_mask_3: 0.1366  loss_dice_3: 0.2738  loss_ce_4: 0.002721  loss_mask_4: 0.1368  loss_dice_4: 0.2821  loss_ce_5: 0.003558  loss_mask_5: 0.1331  loss_dice_5: 0.2732  loss_ce_6: 0.001159  loss_mask_6: 0.1349  loss_dice_6: 0.2723  loss_ce_7: 0.002298  loss_mask_7: 0.1298  loss_dice_7: 0.2716  loss_ce_8: 0.002609  loss_mask_8: 0.1386  loss_dice_8: 0.2814  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:49:58] d2.utils.events INFO:  eta: 0:06:46  iter: 46799  total_loss: 4.248  loss_ce: 0.0004164  loss_mask: 0.1941  loss_dice: 0.1774  loss_ce_0: 0.06089  loss_mask_0: 0.1913  loss_dice_0: 0.1753  loss_ce_1: 0.0003953  loss_mask_1: 0.2022  loss_dice_1: 0.1731  loss_ce_2: 0.0003213  loss_mask_2: 0.2031  loss_dice_2: 0.1856  loss_ce_3: 0.0004239  loss_mask_3: 0.1941  loss_dice_3: 0.1818  loss_ce_4: 0.0003743  loss_mask_4: 0.1979  loss_dice_4: 0.1886  loss_ce_5: 0.0003685  loss_mask_5: 0.1973  loss_dice_5: 0.1826  loss_ce_6: 0.0004846  loss_mask_6: 0.2037  loss_dice_6: 0.1897  loss_ce_7: 0.0004572  loss_mask_7: 0.1968  loss_dice_7: 0.1714  loss_ce_8: 0.0004689  loss_mask_8: 0.1938  loss_dice_8: 0.1856  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:01] d2.utils.events INFO:  eta: 0:06:43  iter: 46819  total_loss: 4.287  loss_ce: 0.0005828  loss_mask: 0.1635  loss_dice: 0.2171  loss_ce_0: 0.06174  loss_mask_0: 0.1677  loss_dice_0: 0.2046  loss_ce_1: 0.0005938  loss_mask_1: 0.1675  loss_dice_1: 0.2033  loss_ce_2: 0.0006652  loss_mask_2: 0.174  loss_dice_2: 0.199  loss_ce_3: 0.0005504  loss_mask_3: 0.1622  loss_dice_3: 0.1994  loss_ce_4: 0.0006638  loss_mask_4: 0.1632  loss_dice_4: 0.2047  loss_ce_5: 0.0006155  loss_mask_5: 0.1621  loss_dice_5: 0.1985  loss_ce_6: 0.0006459  loss_mask_6: 0.1695  loss_dice_6: 0.2069  loss_ce_7: 0.0006601  loss_mask_7: 0.1581  loss_dice_7: 0.2041  loss_ce_8: 0.0007269  loss_mask_8: 0.1646  loss_dice_8: 0.2057  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:03] d2.utils.events INFO:  eta: 0:06:40  iter: 46839  total_loss: 4.286  loss_ce: 0.0005978  loss_mask: 0.1889  loss_dice: 0.1674  loss_ce_0: 0.06174  loss_mask_0: 0.1907  loss_dice_0: 0.1744  loss_ce_1: 0.0005852  loss_mask_1: 0.1935  loss_dice_1: 0.1681  loss_ce_2: 0.0004864  loss_mask_2: 0.1921  loss_dice_2: 0.1698  loss_ce_3: 0.0004875  loss_mask_3: 0.2002  loss_dice_3: 0.1631  loss_ce_4: 0.000501  loss_mask_4: 0.1923  loss_dice_4: 0.1666  loss_ce_5: 0.0005621  loss_mask_5: 0.1873  loss_dice_5: 0.1671  loss_ce_6: 0.0005346  loss_mask_6: 0.1972  loss_dice_6: 0.1653  loss_ce_7: 0.000592  loss_mask_7: 0.1924  loss_dice_7: 0.1644  loss_ce_8: 0.0006455  loss_mask_8: 0.1908  loss_dice_8: 0.1789  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:06] d2.utils.events INFO:  eta: 0:06:38  iter: 46859  total_loss: 4.433  loss_ce: 0.0004344  loss_mask: 0.2159  loss_dice: 0.1505  loss_ce_0: 0.06174  loss_mask_0: 0.215  loss_dice_0: 0.1549  loss_ce_1: 0.0003886  loss_mask_1: 0.217  loss_dice_1: 0.1471  loss_ce_2: 0.0003392  loss_mask_2: 0.2196  loss_dice_2: 0.1528  loss_ce_3: 0.0004574  loss_mask_3: 0.2191  loss_dice_3: 0.1492  loss_ce_4: 0.0003566  loss_mask_4: 0.22  loss_dice_4: 0.1547  loss_ce_5: 0.0004208  loss_mask_5: 0.2219  loss_dice_5: 0.1516  loss_ce_6: 0.0004898  loss_mask_6: 0.2272  loss_dice_6: 0.1555  loss_ce_7: 0.0003793  loss_mask_7: 0.2213  loss_dice_7: 0.151  loss_ce_8: 0.0004831  loss_mask_8: 0.2255  loss_dice_8: 0.1571  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:08] d2.utils.events INFO:  eta: 0:06:35  iter: 46879  total_loss: 4.066  loss_ce: 0.0009946  loss_mask: 0.1596  loss_dice: 0.1674  loss_ce_0: 0.06259  loss_mask_0: 0.1645  loss_dice_0: 0.1649  loss_ce_1: 0.0008337  loss_mask_1: 0.157  loss_dice_1: 0.1633  loss_ce_2: 0.0009871  loss_mask_2: 0.1659  loss_dice_2: 0.1655  loss_ce_3: 0.0009143  loss_mask_3: 0.1594  loss_dice_3: 0.1702  loss_ce_4: 0.0009517  loss_mask_4: 0.1612  loss_dice_4: 0.18  loss_ce_5: 0.0008064  loss_mask_5: 0.1553  loss_dice_5: 0.181  loss_ce_6: 0.000728  loss_mask_6: 0.1589  loss_dice_6: 0.1814  loss_ce_7: 0.0007532  loss_mask_7: 0.1612  loss_dice_7: 0.167  loss_ce_8: 0.0008897  loss_mask_8: 0.1623  loss_dice_8: 0.1713  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:11] d2.utils.events INFO:  eta: 0:06:33  iter: 46899  total_loss: 4.195  loss_ce: 0.000378  loss_mask: 0.2234  loss_dice: 0.1856  loss_ce_0: 0.06254  loss_mask_0: 0.2171  loss_dice_0: 0.1822  loss_ce_1: 0.0004114  loss_mask_1: 0.2315  loss_dice_1: 0.1741  loss_ce_2: 0.0002779  loss_mask_2: 0.2221  loss_dice_2: 0.178  loss_ce_3: 0.0004259  loss_mask_3: 0.2287  loss_dice_3: 0.1809  loss_ce_4: 0.0003527  loss_mask_4: 0.2285  loss_dice_4: 0.1776  loss_ce_5: 0.0003715  loss_mask_5: 0.2271  loss_dice_5: 0.1846  loss_ce_6: 0.0004978  loss_mask_6: 0.225  loss_dice_6: 0.1837  loss_ce_7: 0.0004082  loss_mask_7: 0.2269  loss_dice_7: 0.1805  loss_ce_8: 0.0004917  loss_mask_8: 0.2213  loss_dice_8: 0.1772  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:13] d2.utils.events INFO:  eta: 0:06:30  iter: 46919  total_loss: 3.788  loss_ce: 0.0006626  loss_mask: 0.1683  loss_dice: 0.1629  loss_ce_0: 0.06096  loss_mask_0: 0.1627  loss_dice_0: 0.1596  loss_ce_1: 0.0006157  loss_mask_1: 0.1622  loss_dice_1: 0.163  loss_ce_2: 0.0007249  loss_mask_2: 0.1592  loss_dice_2: 0.1703  loss_ce_3: 0.0006588  loss_mask_3: 0.178  loss_dice_3: 0.1642  loss_ce_4: 0.0007216  loss_mask_4: 0.1699  loss_dice_4: 0.1642  loss_ce_5: 0.000563  loss_mask_5: 0.1627  loss_dice_5: 0.1624  loss_ce_6: 0.000672  loss_mask_6: 0.1639  loss_dice_6: 0.1654  loss_ce_7: 0.0009148  loss_mask_7: 0.1645  loss_dice_7: 0.1602  loss_ce_8: 0.000642  loss_mask_8: 0.1636  loss_dice_8: 0.1655  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:16] d2.utils.events INFO:  eta: 0:06:28  iter: 46939  total_loss: 4.159  loss_ce: 0.0005446  loss_mask: 0.215  loss_dice: 0.1492  loss_ce_0: 0.06096  loss_mask_0: 0.2096  loss_dice_0: 0.1525  loss_ce_1: 0.0004512  loss_mask_1: 0.2165  loss_dice_1: 0.1498  loss_ce_2: 0.0003163  loss_mask_2: 0.2171  loss_dice_2: 0.1507  loss_ce_3: 0.0004617  loss_mask_3: 0.2037  loss_dice_3: 0.1526  loss_ce_4: 0.0004096  loss_mask_4: 0.2121  loss_dice_4: 0.1537  loss_ce_5: 0.0004095  loss_mask_5: 0.2082  loss_dice_5: 0.1525  loss_ce_6: 0.0005985  loss_mask_6: 0.2078  loss_dice_6: 0.1515  loss_ce_7: 0.000475  loss_mask_7: 0.2053  loss_dice_7: 0.1519  loss_ce_8: 0.0005056  loss_mask_8: 0.2068  loss_dice_8: 0.1524  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:19] d2.utils.events INFO:  eta: 0:06:25  iter: 46959  total_loss: 4.548  loss_ce: 0.000445  loss_mask: 0.1966  loss_dice: 0.1962  loss_ce_0: 0.06096  loss_mask_0: 0.199  loss_dice_0: 0.202  loss_ce_1: 0.0004835  loss_mask_1: 0.1889  loss_dice_1: 0.2032  loss_ce_2: 0.0003398  loss_mask_2: 0.2019  loss_dice_2: 0.2018  loss_ce_3: 0.0004735  loss_mask_3: 0.1915  loss_dice_3: 0.2037  loss_ce_4: 0.0004124  loss_mask_4: 0.1906  loss_dice_4: 0.2036  loss_ce_5: 0.0004014  loss_mask_5: 0.1955  loss_dice_5: 0.1962  loss_ce_6: 0.0005206  loss_mask_6: 0.1987  loss_dice_6: 0.2124  loss_ce_7: 0.0004598  loss_mask_7: 0.1933  loss_dice_7: 0.2047  loss_ce_8: 0.0004906  loss_mask_8: 0.2025  loss_dice_8: 0.2081  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:21] d2.utils.events INFO:  eta: 0:06:23  iter: 46979  total_loss: 3.963  loss_ce: 0.0003413  loss_mask: 0.2015  loss_dice: 0.1656  loss_ce_0: 0.06094  loss_mask_0: 0.2046  loss_dice_0: 0.1627  loss_ce_1: 0.0003699  loss_mask_1: 0.2062  loss_dice_1: 0.1646  loss_ce_2: 0.0002611  loss_mask_2: 0.2028  loss_dice_2: 0.1699  loss_ce_3: 0.0003732  loss_mask_3: 0.2111  loss_dice_3: 0.1629  loss_ce_4: 0.0002903  loss_mask_4: 0.2082  loss_dice_4: 0.1642  loss_ce_5: 0.0003367  loss_mask_5: 0.2056  loss_dice_5: 0.1707  loss_ce_6: 0.0004713  loss_mask_6: 0.2025  loss_dice_6: 0.1652  loss_ce_7: 0.0003916  loss_mask_7: 0.2111  loss_dice_7: 0.1653  loss_ce_8: 0.0004336  loss_mask_8: 0.2038  loss_dice_8: 0.1674  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:24] d2.utils.events INFO:  eta: 0:06:21  iter: 46999  total_loss: 3.809  loss_ce: 0.0005194  loss_mask: 0.1714  loss_dice: 0.159  loss_ce_0: 0.06256  loss_mask_0: 0.1677  loss_dice_0: 0.1543  loss_ce_1: 0.000385  loss_mask_1: 0.1702  loss_dice_1: 0.1569  loss_ce_2: 0.000428  loss_mask_2: 0.1757  loss_dice_2: 0.1672  loss_ce_3: 0.0005314  loss_mask_3: 0.1688  loss_dice_3: 0.1539  loss_ce_4: 0.0004373  loss_mask_4: 0.1709  loss_dice_4: 0.1551  loss_ce_5: 0.0005143  loss_mask_5: 0.1791  loss_dice_5: 0.1557  loss_ce_6: 0.0005847  loss_mask_6: 0.1702  loss_dice_6: 0.1651  loss_ce_7: 0.0005333  loss_mask_7: 0.1709  loss_dice_7: 0.1521  loss_ce_8: 0.0005323  loss_mask_8: 0.1791  loss_dice_8: 0.1573  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:26] d2.utils.events INFO:  eta: 0:06:18  iter: 47019  total_loss: 4.256  loss_ce: 0.0006628  loss_mask: 0.2158  loss_dice: 0.1757  loss_ce_0: 0.061  loss_mask_0: 0.2117  loss_dice_0: 0.1781  loss_ce_1: 0.0005495  loss_mask_1: 0.2076  loss_dice_1: 0.1768  loss_ce_2: 0.0006385  loss_mask_2: 0.2136  loss_dice_2: 0.1695  loss_ce_3: 0.0006705  loss_mask_3: 0.2158  loss_dice_3: 0.1785  loss_ce_4: 0.0005077  loss_mask_4: 0.2168  loss_dice_4: 0.1728  loss_ce_5: 0.000601  loss_mask_5: 0.2159  loss_dice_5: 0.1788  loss_ce_6: 0.0006462  loss_mask_6: 0.2095  loss_dice_6: 0.1748  loss_ce_7: 0.000548  loss_mask_7: 0.2148  loss_dice_7: 0.1651  loss_ce_8: 0.0006343  loss_mask_8: 0.2167  loss_dice_8: 0.1713  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:29] d2.utils.events INFO:  eta: 0:06:15  iter: 47039  total_loss: 3.773  loss_ce: 0.0004357  loss_mask: 0.1722  loss_dice: 0.1666  loss_ce_0: 0.06253  loss_mask_0: 0.1806  loss_dice_0: 0.1694  loss_ce_1: 0.0004665  loss_mask_1: 0.1766  loss_dice_1: 0.1708  loss_ce_2: 0.0003737  loss_mask_2: 0.1729  loss_dice_2: 0.1631  loss_ce_3: 0.0004546  loss_mask_3: 0.173  loss_dice_3: 0.1764  loss_ce_4: 0.0004282  loss_mask_4: 0.1716  loss_dice_4: 0.1671  loss_ce_5: 0.0004719  loss_mask_5: 0.1763  loss_dice_5: 0.1662  loss_ce_6: 0.0005044  loss_mask_6: 0.1722  loss_dice_6: 0.1661  loss_ce_7: 0.0004796  loss_mask_7: 0.1758  loss_dice_7: 0.1625  loss_ce_8: 0.0005592  loss_mask_8: 0.1717  loss_dice_8: 0.1748  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:31] d2.utils.events INFO:  eta: 0:06:13  iter: 47059  total_loss: 4.244  loss_ce: 0.0009602  loss_mask: 0.1645  loss_dice: 0.1932  loss_ce_0: 0.06097  loss_mask_0: 0.1777  loss_dice_0: 0.1909  loss_ce_1: 0.0005959  loss_mask_1: 0.1704  loss_dice_1: 0.1856  loss_ce_2: 0.0005083  loss_mask_2: 0.1674  loss_dice_2: 0.1872  loss_ce_3: 0.0008568  loss_mask_3: 0.1799  loss_dice_3: 0.1914  loss_ce_4: 0.0006839  loss_mask_4: 0.1692  loss_dice_4: 0.1909  loss_ce_5: 0.0006324  loss_mask_5: 0.1697  loss_dice_5: 0.1941  loss_ce_6: 0.0007086  loss_mask_6: 0.1596  loss_dice_6: 0.183  loss_ce_7: 0.0008197  loss_mask_7: 0.1704  loss_dice_7: 0.1913  loss_ce_8: 0.0006353  loss_mask_8: 0.1614  loss_dice_8: 0.1938  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:34] d2.utils.events INFO:  eta: 0:06:10  iter: 47079  total_loss: 4.104  loss_ce: 0.0005455  loss_mask: 0.1768  loss_dice: 0.2172  loss_ce_0: 0.06092  loss_mask_0: 0.1783  loss_dice_0: 0.2023  loss_ce_1: 0.0005058  loss_mask_1: 0.1743  loss_dice_1: 0.2083  loss_ce_2: 0.0005047  loss_mask_2: 0.1798  loss_dice_2: 0.2113  loss_ce_3: 0.0004864  loss_mask_3: 0.1809  loss_dice_3: 0.2151  loss_ce_4: 0.000516  loss_mask_4: 0.1698  loss_dice_4: 0.2044  loss_ce_5: 0.0005208  loss_mask_5: 0.1786  loss_dice_5: 0.2038  loss_ce_6: 0.0005951  loss_mask_6: 0.1817  loss_dice_6: 0.2115  loss_ce_7: 0.0006823  loss_mask_7: 0.1757  loss_dice_7: 0.205  loss_ce_8: 0.0006062  loss_mask_8: 0.1774  loss_dice_8: 0.2182  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:36] d2.utils.events INFO:  eta: 0:06:08  iter: 47099  total_loss: 4.058  loss_ce: 0.000304  loss_mask: 0.2146  loss_dice: 0.1568  loss_ce_0: 0.06174  loss_mask_0: 0.2141  loss_dice_0: 0.1599  loss_ce_1: 0.0003441  loss_mask_1: 0.2135  loss_dice_1: 0.1635  loss_ce_2: 0.0002085  loss_mask_2: 0.217  loss_dice_2: 0.1629  loss_ce_3: 0.0003203  loss_mask_3: 0.2107  loss_dice_3: 0.1596  loss_ce_4: 0.0002526  loss_mask_4: 0.2107  loss_dice_4: 0.16  loss_ce_5: 0.0002746  loss_mask_5: 0.2145  loss_dice_5: 0.1629  loss_ce_6: 0.0004107  loss_mask_6: 0.2086  loss_dice_6: 0.1638  loss_ce_7: 0.0002805  loss_mask_7: 0.2122  loss_dice_7: 0.1617  loss_ce_8: 0.0003833  loss_mask_8: 0.2089  loss_dice_8: 0.1604  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:39] d2.utils.events INFO:  eta: 0:06:05  iter: 47119  total_loss: 3.696  loss_ce: 0.0004979  loss_mask: 0.1881  loss_dice: 0.152  loss_ce_0: 0.06251  loss_mask_0: 0.1753  loss_dice_0: 0.1483  loss_ce_1: 0.0007071  loss_mask_1: 0.1801  loss_dice_1: 0.1488  loss_ce_2: 0.0005454  loss_mask_2: 0.1782  loss_dice_2: 0.1551  loss_ce_3: 0.0005611  loss_mask_3: 0.1743  loss_dice_3: 0.1498  loss_ce_4: 0.0005541  loss_mask_4: 0.1777  loss_dice_4: 0.1487  loss_ce_5: 0.0006254  loss_mask_5: 0.177  loss_dice_5: 0.147  loss_ce_6: 0.0006215  loss_mask_6: 0.1752  loss_dice_6: 0.1509  loss_ce_7: 0.0006459  loss_mask_7: 0.1827  loss_dice_7: 0.1491  loss_ce_8: 0.0006437  loss_mask_8: 0.1751  loss_dice_8: 0.1548  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:41] d2.utils.events INFO:  eta: 0:06:02  iter: 47139  total_loss: 4.517  loss_ce: 0.0005647  loss_mask: 0.2039  loss_dice: 0.1687  loss_ce_0: 0.06176  loss_mask_0: 0.1959  loss_dice_0: 0.1706  loss_ce_1: 0.0005663  loss_mask_1: 0.2018  loss_dice_1: 0.1681  loss_ce_2: 0.0005  loss_mask_2: 0.1917  loss_dice_2: 0.1775  loss_ce_3: 0.0005911  loss_mask_3: 0.2006  loss_dice_3: 0.1714  loss_ce_4: 0.0005163  loss_mask_4: 0.1971  loss_dice_4: 0.1715  loss_ce_5: 0.0006098  loss_mask_5: 0.1963  loss_dice_5: 0.174  loss_ce_6: 0.0006308  loss_mask_6: 0.1972  loss_dice_6: 0.1747  loss_ce_7: 0.0006677  loss_mask_7: 0.1983  loss_dice_7: 0.1721  loss_ce_8: 0.0005892  loss_mask_8: 0.1942  loss_dice_8: 0.1695  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:44] d2.utils.events INFO:  eta: 0:06:00  iter: 47159  total_loss: 3.943  loss_ce: 0.0004621  loss_mask: 0.1562  loss_dice: 0.2116  loss_ce_0: 0.06239  loss_mask_0: 0.1654  loss_dice_0: 0.2096  loss_ce_1: 0.0004716  loss_mask_1: 0.1661  loss_dice_1: 0.2115  loss_ce_2: 0.0003982  loss_mask_2: 0.1683  loss_dice_2: 0.2086  loss_ce_3: 0.0005066  loss_mask_3: 0.1566  loss_dice_3: 0.2176  loss_ce_4: 0.0004863  loss_mask_4: 0.1736  loss_dice_4: 0.2037  loss_ce_5: 0.0004483  loss_mask_5: 0.1519  loss_dice_5: 0.208  loss_ce_6: 0.0005506  loss_mask_6: 0.162  loss_dice_6: 0.1992  loss_ce_7: 0.0005274  loss_mask_7: 0.1624  loss_dice_7: 0.2071  loss_ce_8: 0.0005187  loss_mask_8: 0.1719  loss_dice_8: 0.2164  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:46] d2.utils.events INFO:  eta: 0:05:57  iter: 47179  total_loss: 4.084  loss_ce: 0.000445  loss_mask: 0.1997  loss_dice: 0.1643  loss_ce_0: 0.06115  loss_mask_0: 0.2038  loss_dice_0: 0.1659  loss_ce_1: 0.0004606  loss_mask_1: 0.1954  loss_dice_1: 0.1676  loss_ce_2: 0.0004833  loss_mask_2: 0.2118  loss_dice_2: 0.1676  loss_ce_3: 0.000461  loss_mask_3: 0.198  loss_dice_3: 0.1634  loss_ce_4: 0.0004486  loss_mask_4: 0.2024  loss_dice_4: 0.1649  loss_ce_5: 0.0004633  loss_mask_5: 0.1993  loss_dice_5: 0.1588  loss_ce_6: 0.0005302  loss_mask_6: 0.204  loss_dice_6: 0.1667  loss_ce_7: 0.0004529  loss_mask_7: 0.2065  loss_dice_7: 0.17  loss_ce_8: 0.0005493  loss_mask_8: 0.1967  loss_dice_8: 0.165  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:49] d2.utils.events INFO:  eta: 0:05:54  iter: 47199  total_loss: 3.946  loss_ce: 0.0005641  loss_mask: 0.1909  loss_dice: 0.1685  loss_ce_0: 0.06116  loss_mask_0: 0.1858  loss_dice_0: 0.1704  loss_ce_1: 0.0004725  loss_mask_1: 0.1896  loss_dice_1: 0.1627  loss_ce_2: 0.0003696  loss_mask_2: 0.1852  loss_dice_2: 0.1686  loss_ce_3: 0.0005539  loss_mask_3: 0.1909  loss_dice_3: 0.1631  loss_ce_4: 0.0004541  loss_mask_4: 0.1933  loss_dice_4: 0.1674  loss_ce_5: 0.0004259  loss_mask_5: 0.1767  loss_dice_5: 0.1678  loss_ce_6: 0.0005624  loss_mask_6: 0.1867  loss_dice_6: 0.166  loss_ce_7: 0.0005339  loss_mask_7: 0.1898  loss_dice_7: 0.1656  loss_ce_8: 0.0005672  loss_mask_8: 0.1952  loss_dice_8: 0.1663  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:50:51] d2.utils.events INFO:  eta: 0:05:52  iter: 47219  total_loss: 4.331  loss_ce: 0.0003595  loss_mask: 0.2411  loss_dice: 0.1698  loss_ce_0: 0.06115  loss_mask_0: 0.2451  loss_dice_0: 0.1733  loss_ce_1: 0.00039  loss_mask_1: 0.2488  loss_dice_1: 0.173  loss_ce_2: 0.0002626  loss_mask_2: 0.2489  loss_dice_2: 0.1692  loss_ce_3: 0.0004088  loss_mask_3: 0.2531  loss_dice_3: 0.1716  loss_ce_4: 0.0002963  loss_mask_4: 0.2425  loss_dice_4: 0.1679  loss_ce_5: 0.0003118  loss_mask_5: 0.2495  loss_dice_5: 0.1729  loss_ce_6: 0.0005098  loss_mask_6: 0.2532  loss_dice_6: 0.1813  loss_ce_7: 0.0003998  loss_mask_7: 0.2416  loss_dice_7: 0.17  loss_ce_8: 0.000442  loss_mask_8: 0.2527  loss_dice_8: 0.1734  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:54] d2.utils.events INFO:  eta: 0:05:49  iter: 47239  total_loss: 4.113  loss_ce: 0.0007352  loss_mask: 0.1466  loss_dice: 0.2129  loss_ce_0: 0.06109  loss_mask_0: 0.1522  loss_dice_0: 0.2234  loss_ce_1: 0.0008461  loss_mask_1: 0.1493  loss_dice_1: 0.223  loss_ce_2: 0.0006996  loss_mask_2: 0.1415  loss_dice_2: 0.2234  loss_ce_3: 0.0007783  loss_mask_3: 0.1548  loss_dice_3: 0.2297  loss_ce_4: 0.0008841  loss_mask_4: 0.1497  loss_dice_4: 0.2145  loss_ce_5: 0.0007498  loss_mask_5: 0.1489  loss_dice_5: 0.2331  loss_ce_6: 0.0007763  loss_mask_6: 0.1461  loss_dice_6: 0.2226  loss_ce_7: 0.0007269  loss_mask_7: 0.1532  loss_dice_7: 0.2233  loss_ce_8: 0.0007565  loss_mask_8: 0.1608  loss_dice_8: 0.2133  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:50:56] d2.utils.events INFO:  eta: 0:05:46  iter: 47259  total_loss: 3.918  loss_ce: 0.0004026  loss_mask: 0.1698  loss_dice: 0.1868  loss_ce_0: 0.06174  loss_mask_0: 0.1693  loss_dice_0: 0.1899  loss_ce_1: 0.0003701  loss_mask_1: 0.1715  loss_dice_1: 0.1956  loss_ce_2: 0.0003294  loss_mask_2: 0.1745  loss_dice_2: 0.1899  loss_ce_3: 0.0004285  loss_mask_3: 0.1691  loss_dice_3: 0.186  loss_ce_4: 0.0003422  loss_mask_4: 0.1693  loss_dice_4: 0.1882  loss_ce_5: 0.0003775  loss_mask_5: 0.1784  loss_dice_5: 0.1907  loss_ce_6: 0.0004844  loss_mask_6: 0.1668  loss_dice_6: 0.1935  loss_ce_7: 0.0004085  loss_mask_7: 0.1664  loss_dice_7: 0.1839  loss_ce_8: 0.0004793  loss_mask_8: 0.1722  loss_dice_8: 0.1893  time: 0.1260  data_time: 0.0014  lr: 1e-06  max_mem: 3105M
[04/13 16:50:59] d2.utils.events INFO:  eta: 0:05:44  iter: 47279  total_loss: 4.453  loss_ce: 0.0006947  loss_mask: 0.1787  loss_dice: 0.1703  loss_ce_0: 0.06109  loss_mask_0: 0.1765  loss_dice_0: 0.1687  loss_ce_1: 0.0005854  loss_mask_1: 0.1825  loss_dice_1: 0.1683  loss_ce_2: 0.0008513  loss_mask_2: 0.1802  loss_dice_2: 0.1611  loss_ce_3: 0.0006595  loss_mask_3: 0.1828  loss_dice_3: 0.1708  loss_ce_4: 0.0006152  loss_mask_4: 0.1783  loss_dice_4: 0.1725  loss_ce_5: 0.000771  loss_mask_5: 0.1861  loss_dice_5: 0.1687  loss_ce_6: 0.0006507  loss_mask_6: 0.171  loss_dice_6: 0.173  loss_ce_7: 0.0007344  loss_mask_7: 0.1757  loss_dice_7: 0.171  loss_ce_8: 0.0008951  loss_mask_8: 0.1853  loss_dice_8: 0.1661  time: 0.1260  data_time: 0.0014  lr: 1e-06  max_mem: 3105M
[04/13 16:51:02] d2.utils.events INFO:  eta: 0:05:41  iter: 47299  total_loss: 4.187  loss_ce: 0.0005701  loss_mask: 0.2192  loss_dice: 0.1588  loss_ce_0: 0.06174  loss_mask_0: 0.22  loss_dice_0: 0.1624  loss_ce_1: 0.000626  loss_mask_1: 0.2275  loss_dice_1: 0.162  loss_ce_2: 0.000684  loss_mask_2: 0.2198  loss_dice_2: 0.1603  loss_ce_3: 0.0005763  loss_mask_3: 0.2237  loss_dice_3: 0.1613  loss_ce_4: 0.001046  loss_mask_4: 0.2248  loss_dice_4: 0.159  loss_ce_5: 0.0006801  loss_mask_5: 0.2318  loss_dice_5: 0.1577  loss_ce_6: 0.0006511  loss_mask_6: 0.2224  loss_dice_6: 0.1597  loss_ce_7: 0.001437  loss_mask_7: 0.2168  loss_dice_7: 0.1578  loss_ce_8: 0.0006943  loss_mask_8: 0.2191  loss_dice_8: 0.159  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:04] d2.utils.events INFO:  eta: 0:05:38  iter: 47319  total_loss: 3.899  loss_ce: 0.0006029  loss_mask: 0.1594  loss_dice: 0.1837  loss_ce_0: 0.06174  loss_mask_0: 0.1596  loss_dice_0: 0.1867  loss_ce_1: 0.0007537  loss_mask_1: 0.159  loss_dice_1: 0.1894  loss_ce_2: 0.0006032  loss_mask_2: 0.1595  loss_dice_2: 0.185  loss_ce_3: 0.0006101  loss_mask_3: 0.1461  loss_dice_3: 0.1766  loss_ce_4: 0.0006918  loss_mask_4: 0.1637  loss_dice_4: 0.1766  loss_ce_5: 0.0005096  loss_mask_5: 0.1493  loss_dice_5: 0.1765  loss_ce_6: 0.0006295  loss_mask_6: 0.1603  loss_dice_6: 0.174  loss_ce_7: 0.0008219  loss_mask_7: 0.161  loss_dice_7: 0.178  loss_ce_8: 0.0005597  loss_mask_8: 0.1621  loss_dice_8: 0.1791  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:51:07] d2.utils.events INFO:  eta: 0:05:36  iter: 47339  total_loss: 4.133  loss_ce: 0.0004744  loss_mask: 0.2159  loss_dice: 0.1861  loss_ce_0: 0.06174  loss_mask_0: 0.2137  loss_dice_0: 0.1898  loss_ce_1: 0.0004143  loss_mask_1: 0.2142  loss_dice_1: 0.1938  loss_ce_2: 0.0003625  loss_mask_2: 0.2173  loss_dice_2: 0.1788  loss_ce_3: 0.0004309  loss_mask_3: 0.2185  loss_dice_3: 0.1919  loss_ce_4: 0.0003767  loss_mask_4: 0.2143  loss_dice_4: 0.1867  loss_ce_5: 0.0003832  loss_mask_5: 0.2165  loss_dice_5: 0.1869  loss_ce_6: 0.0005066  loss_mask_6: 0.2189  loss_dice_6: 0.1904  loss_ce_7: 0.0004463  loss_mask_7: 0.2185  loss_dice_7: 0.1812  loss_ce_8: 0.0004782  loss_mask_8: 0.2207  loss_dice_8: 0.1865  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:09] d2.utils.events INFO:  eta: 0:05:33  iter: 47359  total_loss: 3.9  loss_ce: 0.0004299  loss_mask: 0.2045  loss_dice: 0.1465  loss_ce_0: 0.06236  loss_mask_0: 0.2107  loss_dice_0: 0.1457  loss_ce_1: 0.0005423  loss_mask_1: 0.2225  loss_dice_1: 0.1509  loss_ce_2: 0.0005331  loss_mask_2: 0.212  loss_dice_2: 0.1476  loss_ce_3: 0.0004961  loss_mask_3: 0.202  loss_dice_3: 0.1404  loss_ce_4: 0.0005096  loss_mask_4: 0.2159  loss_dice_4: 0.1499  loss_ce_5: 0.0006709  loss_mask_5: 0.2245  loss_dice_5: 0.1454  loss_ce_6: 0.0006479  loss_mask_6: 0.2183  loss_dice_6: 0.1498  loss_ce_7: 0.0005665  loss_mask_7: 0.213  loss_dice_7: 0.1423  loss_ce_8: 0.0006086  loss_mask_8: 0.2187  loss_dice_8: 0.1452  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:51:12] d2.utils.events INFO:  eta: 0:05:31  iter: 47379  total_loss: 4.444  loss_ce: 0.0003479  loss_mask: 0.2018  loss_dice: 0.2289  loss_ce_0: 0.06118  loss_mask_0: 0.1937  loss_dice_0: 0.2319  loss_ce_1: 0.0004426  loss_mask_1: 0.2025  loss_dice_1: 0.2464  loss_ce_2: 0.0003327  loss_mask_2: 0.2019  loss_dice_2: 0.2359  loss_ce_3: 0.0003474  loss_mask_3: 0.1982  loss_dice_3: 0.2395  loss_ce_4: 0.0004326  loss_mask_4: 0.2032  loss_dice_4: 0.2424  loss_ce_5: 0.000369  loss_mask_5: 0.2008  loss_dice_5: 0.2347  loss_ce_6: 0.0004257  loss_mask_6: 0.2087  loss_dice_6: 0.236  loss_ce_7: 0.0004822  loss_mask_7: 0.2072  loss_dice_7: 0.2494  loss_ce_8: 0.0004786  loss_mask_8: 0.2054  loss_dice_8: 0.2354  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:51:14] d2.utils.events INFO:  eta: 0:05:28  iter: 47399  total_loss: 4.21  loss_ce: 0.0003706  loss_mask: 0.2005  loss_dice: 0.1845  loss_ce_0: 0.06227  loss_mask_0: 0.2067  loss_dice_0: 0.1884  loss_ce_1: 0.0003846  loss_mask_1: 0.2054  loss_dice_1: 0.1853  loss_ce_2: 0.000281  loss_mask_2: 0.1972  loss_dice_2: 0.179  loss_ce_3: 0.0003784  loss_mask_3: 0.2041  loss_dice_3: 0.1846  loss_ce_4: 0.0003627  loss_mask_4: 0.2027  loss_dice_4: 0.1803  loss_ce_5: 0.0003508  loss_mask_5: 0.1983  loss_dice_5: 0.1787  loss_ce_6: 0.000478  loss_mask_6: 0.1922  loss_dice_6: 0.1844  loss_ce_7: 0.0004349  loss_mask_7: 0.1934  loss_dice_7: 0.1807  loss_ce_8: 0.000484  loss_mask_8: 0.2039  loss_dice_8: 0.1888  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:17] d2.utils.events INFO:  eta: 0:05:26  iter: 47419  total_loss: 4.209  loss_ce: 0.0007155  loss_mask: 0.193  loss_dice: 0.1568  loss_ce_0: 0.06124  loss_mask_0: 0.1956  loss_dice_0: 0.1599  loss_ce_1: 0.0005122  loss_mask_1: 0.1934  loss_dice_1: 0.1564  loss_ce_2: 0.0006401  loss_mask_2: 0.1893  loss_dice_2: 0.1577  loss_ce_3: 0.0008049  loss_mask_3: 0.1916  loss_dice_3: 0.1555  loss_ce_4: 0.0005682  loss_mask_4: 0.1949  loss_dice_4: 0.1545  loss_ce_5: 0.0006462  loss_mask_5: 0.1945  loss_dice_5: 0.1575  loss_ce_6: 0.0007651  loss_mask_6: 0.1956  loss_dice_6: 0.159  loss_ce_7: 0.0006122  loss_mask_7: 0.1959  loss_dice_7: 0.1608  loss_ce_8: 0.0006515  loss_mask_8: 0.1959  loss_dice_8: 0.162  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:19] d2.utils.events INFO:  eta: 0:05:23  iter: 47439  total_loss: 4.284  loss_ce: 0.0007957  loss_mask: 0.2035  loss_dice: 0.1997  loss_ce_0: 0.06124  loss_mask_0: 0.207  loss_dice_0: 0.1995  loss_ce_1: 0.0007963  loss_mask_1: 0.2011  loss_dice_1: 0.1957  loss_ce_2: 0.0006122  loss_mask_2: 0.1996  loss_dice_2: 0.1974  loss_ce_3: 0.0007397  loss_mask_3: 0.2092  loss_dice_3: 0.2039  loss_ce_4: 0.0007076  loss_mask_4: 0.2026  loss_dice_4: 0.1902  loss_ce_5: 0.0006103  loss_mask_5: 0.1936  loss_dice_5: 0.1986  loss_ce_6: 0.000644  loss_mask_6: 0.2014  loss_dice_6: 0.2037  loss_ce_7: 0.0006124  loss_mask_7: 0.1981  loss_dice_7: 0.1933  loss_ce_8: 0.0006051  loss_mask_8: 0.2014  loss_dice_8: 0.2034  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:22] d2.utils.events INFO:  eta: 0:05:21  iter: 47459  total_loss: 3.815  loss_ce: 0.0005511  loss_mask: 0.1737  loss_dice: 0.1898  loss_ce_0: 0.06223  loss_mask_0: 0.1724  loss_dice_0: 0.1913  loss_ce_1: 0.0005033  loss_mask_1: 0.1698  loss_dice_1: 0.1797  loss_ce_2: 0.0005623  loss_mask_2: 0.1785  loss_dice_2: 0.1941  loss_ce_3: 0.0005429  loss_mask_3: 0.1723  loss_dice_3: 0.1959  loss_ce_4: 0.0005118  loss_mask_4: 0.1782  loss_dice_4: 0.1806  loss_ce_5: 0.0005995  loss_mask_5: 0.1751  loss_dice_5: 0.1832  loss_ce_6: 0.000582  loss_mask_6: 0.1766  loss_dice_6: 0.184  loss_ce_7: 0.000555  loss_mask_7: 0.1693  loss_dice_7: 0.1825  loss_ce_8: 0.000649  loss_mask_8: 0.1724  loss_dice_8: 0.192  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:24] d2.utils.events INFO:  eta: 0:05:18  iter: 47479  total_loss: 3.892  loss_ce: 0.0003977  loss_mask: 0.1933  loss_dice: 0.1583  loss_ce_0: 0.06217  loss_mask_0: 0.1882  loss_dice_0: 0.1515  loss_ce_1: 0.0004036  loss_mask_1: 0.1895  loss_dice_1: 0.1496  loss_ce_2: 0.0003357  loss_mask_2: 0.1971  loss_dice_2: 0.155  loss_ce_3: 0.0003972  loss_mask_3: 0.1915  loss_dice_3: 0.1528  loss_ce_4: 0.0004005  loss_mask_4: 0.1928  loss_dice_4: 0.1535  loss_ce_5: 0.0003412  loss_mask_5: 0.1904  loss_dice_5: 0.1526  loss_ce_6: 0.0004806  loss_mask_6: 0.1858  loss_dice_6: 0.1533  loss_ce_7: 0.0003924  loss_mask_7: 0.1895  loss_dice_7: 0.1489  loss_ce_8: 0.0004732  loss_mask_8: 0.1927  loss_dice_8: 0.1577  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:27] d2.utils.events INFO:  eta: 0:05:16  iter: 47499  total_loss: 4.079  loss_ce: 0.0003957  loss_mask: 0.1826  loss_dice: 0.1614  loss_ce_0: 0.06136  loss_mask_0: 0.1834  loss_dice_0: 0.1633  loss_ce_1: 0.0003853  loss_mask_1: 0.1953  loss_dice_1: 0.1645  loss_ce_2: 0.0003006  loss_mask_2: 0.1898  loss_dice_2: 0.1587  loss_ce_3: 0.0003878  loss_mask_3: 0.1979  loss_dice_3: 0.1647  loss_ce_4: 0.0003407  loss_mask_4: 0.1844  loss_dice_4: 0.164  loss_ce_5: 0.0003735  loss_mask_5: 0.1931  loss_dice_5: 0.1632  loss_ce_6: 0.0004683  loss_mask_6: 0.1897  loss_dice_6: 0.1624  loss_ce_7: 0.0003456  loss_mask_7: 0.1906  loss_dice_7: 0.1602  loss_ce_8: 0.0004303  loss_mask_8: 0.1832  loss_dice_8: 0.1649  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:29] d2.utils.events INFO:  eta: 0:05:13  iter: 47519  total_loss: 4.199  loss_ce: 0.0003672  loss_mask: 0.1991  loss_dice: 0.1952  loss_ce_0: 0.06131  loss_mask_0: 0.1952  loss_dice_0: 0.1872  loss_ce_1: 0.0003783  loss_mask_1: 0.2001  loss_dice_1: 0.1922  loss_ce_2: 0.0002218  loss_mask_2: 0.1967  loss_dice_2: 0.2002  loss_ce_3: 0.0004027  loss_mask_3: 0.1957  loss_dice_3: 0.196  loss_ce_4: 0.0002839  loss_mask_4: 0.2021  loss_dice_4: 0.1957  loss_ce_5: 0.0002967  loss_mask_5: 0.1988  loss_dice_5: 0.1986  loss_ce_6: 0.0004376  loss_mask_6: 0.1929  loss_dice_6: 0.1927  loss_ce_7: 0.0003429  loss_mask_7: 0.2021  loss_dice_7: 0.1939  loss_ce_8: 0.0004005  loss_mask_8: 0.1956  loss_dice_8: 0.1953  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:32] d2.utils.events INFO:  eta: 0:05:11  iter: 47539  total_loss: 4.162  loss_ce: 0.0005311  loss_mask: 0.1823  loss_dice: 0.2302  loss_ce_0: 0.06124  loss_mask_0: 0.179  loss_dice_0: 0.2191  loss_ce_1: 0.0004936  loss_mask_1: 0.1789  loss_dice_1: 0.2319  loss_ce_2: 0.0004498  loss_mask_2: 0.182  loss_dice_2: 0.2266  loss_ce_3: 0.0005148  loss_mask_3: 0.1771  loss_dice_3: 0.2286  loss_ce_4: 0.0004905  loss_mask_4: 0.1721  loss_dice_4: 0.2236  loss_ce_5: 0.0004302  loss_mask_5: 0.1766  loss_dice_5: 0.2407  loss_ce_6: 0.000515  loss_mask_6: 0.1762  loss_dice_6: 0.2253  loss_ce_7: 0.0004746  loss_mask_7: 0.1705  loss_dice_7: 0.225  loss_ce_8: 0.0005138  loss_mask_8: 0.1746  loss_dice_8: 0.2296  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:34] d2.utils.events INFO:  eta: 0:05:08  iter: 47559  total_loss: 3.652  loss_ce: 0.0004927  loss_mask: 0.1916  loss_dice: 0.1515  loss_ce_0: 0.06114  loss_mask_0: 0.1865  loss_dice_0: 0.1494  loss_ce_1: 0.0004296  loss_mask_1: 0.1891  loss_dice_1: 0.1516  loss_ce_2: 0.0004339  loss_mask_2: 0.1875  loss_dice_2: 0.1529  loss_ce_3: 0.0005376  loss_mask_3: 0.1905  loss_dice_3: 0.1515  loss_ce_4: 0.000477  loss_mask_4: 0.1963  loss_dice_4: 0.1515  loss_ce_5: 0.0004716  loss_mask_5: 0.1918  loss_dice_5: 0.1486  loss_ce_6: 0.000514  loss_mask_6: 0.1909  loss_dice_6: 0.1553  loss_ce_7: 0.0005649  loss_mask_7: 0.1918  loss_dice_7: 0.1491  loss_ce_8: 0.000556  loss_mask_8: 0.1886  loss_dice_8: 0.1527  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:37] d2.utils.events INFO:  eta: 0:05:06  iter: 47579  total_loss: 5.211  loss_ce: 0.0005084  loss_mask: 0.2054  loss_dice: 0.1841  loss_ce_0: 0.06114  loss_mask_0: 0.2018  loss_dice_0: 0.1752  loss_ce_1: 0.0004613  loss_mask_1: 0.202  loss_dice_1: 0.1808  loss_ce_2: 0.0004043  loss_mask_2: 0.1987  loss_dice_2: 0.1867  loss_ce_3: 0.0005059  loss_mask_3: 0.2039  loss_dice_3: 0.1868  loss_ce_4: 0.0004565  loss_mask_4: 0.2003  loss_dice_4: 0.1817  loss_ce_5: 0.0004534  loss_mask_5: 0.1952  loss_dice_5: 0.1831  loss_ce_6: 0.0005359  loss_mask_6: 0.2063  loss_dice_6: 0.1755  loss_ce_7: 0.0005028  loss_mask_7: 0.2046  loss_dice_7: 0.1818  loss_ce_8: 0.0005317  loss_mask_8: 0.2031  loss_dice_8: 0.1902  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:39] d2.utils.events INFO:  eta: 0:05:03  iter: 47599  total_loss: 3.929  loss_ce: 0.000305  loss_mask: 0.2119  loss_dice: 0.1502  loss_ce_0: 0.06114  loss_mask_0: 0.2072  loss_dice_0: 0.1475  loss_ce_1: 0.0003473  loss_mask_1: 0.2113  loss_dice_1: 0.1472  loss_ce_2: 0.0002036  loss_mask_2: 0.2036  loss_dice_2: 0.1531  loss_ce_3: 0.0003223  loss_mask_3: 0.2099  loss_dice_3: 0.1476  loss_ce_4: 0.0002712  loss_mask_4: 0.2139  loss_dice_4: 0.1444  loss_ce_5: 0.000286  loss_mask_5: 0.2112  loss_dice_5: 0.1459  loss_ce_6: 0.0004217  loss_mask_6: 0.21  loss_dice_6: 0.1496  loss_ce_7: 0.0003203  loss_mask_7: 0.2149  loss_dice_7: 0.1404  loss_ce_8: 0.0003969  loss_mask_8: 0.2032  loss_dice_8: 0.1469  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:42] d2.utils.events INFO:  eta: 0:05:01  iter: 47619  total_loss: 3.807  loss_ce: 0.0003201  loss_mask: 0.1886  loss_dice: 0.1674  loss_ce_0: 0.06238  loss_mask_0: 0.1862  loss_dice_0: 0.163  loss_ce_1: 0.000398  loss_mask_1: 0.188  loss_dice_1: 0.1671  loss_ce_2: 0.0002614  loss_mask_2: 0.181  loss_dice_2: 0.1666  loss_ce_3: 0.0003888  loss_mask_3: 0.1857  loss_dice_3: 0.1694  loss_ce_4: 0.0003283  loss_mask_4: 0.1878  loss_dice_4: 0.166  loss_ce_5: 0.0003456  loss_mask_5: 0.1833  loss_dice_5: 0.167  loss_ce_6: 0.0004586  loss_mask_6: 0.184  loss_dice_6: 0.1696  loss_ce_7: 0.000351  loss_mask_7: 0.1848  loss_dice_7: 0.171  loss_ce_8: 0.0004286  loss_mask_8: 0.1856  loss_dice_8: 0.1626  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:44] d2.utils.events INFO:  eta: 0:04:58  iter: 47639  total_loss: 3.635  loss_ce: 0.0003804  loss_mask: 0.2133  loss_dice: 0.1449  loss_ce_0: 0.06116  loss_mask_0: 0.2103  loss_dice_0: 0.1434  loss_ce_1: 0.0003709  loss_mask_1: 0.2158  loss_dice_1: 0.1434  loss_ce_2: 0.0003429  loss_mask_2: 0.2113  loss_dice_2: 0.1391  loss_ce_3: 0.0003795  loss_mask_3: 0.2176  loss_dice_3: 0.1366  loss_ce_4: 0.0003649  loss_mask_4: 0.2083  loss_dice_4: 0.1425  loss_ce_5: 0.0004207  loss_mask_5: 0.2063  loss_dice_5: 0.1452  loss_ce_6: 0.0004506  loss_mask_6: 0.206  loss_dice_6: 0.1493  loss_ce_7: 0.0003782  loss_mask_7: 0.2141  loss_dice_7: 0.1394  loss_ce_8: 0.0004919  loss_mask_8: 0.2139  loss_dice_8: 0.1416  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:51:47] d2.utils.events INFO:  eta: 0:04:55  iter: 47659  total_loss: 4.119  loss_ce: 0.000532  loss_mask: 0.2033  loss_dice: 0.1594  loss_ce_0: 0.06232  loss_mask_0: 0.2156  loss_dice_0: 0.1583  loss_ce_1: 0.0004198  loss_mask_1: 0.2047  loss_dice_1: 0.153  loss_ce_2: 0.0003983  loss_mask_2: 0.2047  loss_dice_2: 0.1669  loss_ce_3: 0.0004919  loss_mask_3: 0.2043  loss_dice_3: 0.1539  loss_ce_4: 0.0004786  loss_mask_4: 0.1984  loss_dice_4: 0.1567  loss_ce_5: 0.0004306  loss_mask_5: 0.2039  loss_dice_5: 0.1623  loss_ce_6: 0.0005781  loss_mask_6: 0.199  loss_dice_6: 0.1569  loss_ce_7: 0.0004767  loss_mask_7: 0.198  loss_dice_7: 0.1593  loss_ce_8: 0.0004937  loss_mask_8: 0.2124  loss_dice_8: 0.1578  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:51:49] d2.utils.events INFO:  eta: 0:04:53  iter: 47679  total_loss: 3.597  loss_ce: 0.0004839  loss_mask: 0.1655  loss_dice: 0.1536  loss_ce_0: 0.06174  loss_mask_0: 0.1721  loss_dice_0: 0.1581  loss_ce_1: 0.0003942  loss_mask_1: 0.1666  loss_dice_1: 0.1494  loss_ce_2: 0.0002949  loss_mask_2: 0.171  loss_dice_2: 0.161  loss_ce_3: 0.0004267  loss_mask_3: 0.1646  loss_dice_3: 0.1507  loss_ce_4: 0.0003535  loss_mask_4: 0.1715  loss_dice_4: 0.1597  loss_ce_5: 0.0003801  loss_mask_5: 0.1807  loss_dice_5: 0.158  loss_ce_6: 0.0005153  loss_mask_6: 0.1783  loss_dice_6: 0.1527  loss_ce_7: 0.0004563  loss_mask_7: 0.1702  loss_dice_7: 0.1571  loss_ce_8: 0.0005117  loss_mask_8: 0.1729  loss_dice_8: 0.1552  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:51:52] d2.utils.events INFO:  eta: 0:04:50  iter: 47699  total_loss: 3.864  loss_ce: 0.0004595  loss_mask: 0.2195  loss_dice: 0.1505  loss_ce_0: 0.06126  loss_mask_0: 0.2172  loss_dice_0: 0.156  loss_ce_1: 0.0004276  loss_mask_1: 0.2262  loss_dice_1: 0.1546  loss_ce_2: 0.0002637  loss_mask_2: 0.2187  loss_dice_2: 0.1534  loss_ce_3: 0.0004629  loss_mask_3: 0.2186  loss_dice_3: 0.1513  loss_ce_4: 0.0003951  loss_mask_4: 0.2147  loss_dice_4: 0.1521  loss_ce_5: 0.0003619  loss_mask_5: 0.2242  loss_dice_5: 0.1597  loss_ce_6: 0.0004905  loss_mask_6: 0.216  loss_dice_6: 0.1539  loss_ce_7: 0.0004162  loss_mask_7: 0.2232  loss_dice_7: 0.1542  loss_ce_8: 0.0004681  loss_mask_8: 0.2198  loss_dice_8: 0.1529  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:51:55] d2.utils.events INFO:  eta: 0:04:48  iter: 47719  total_loss: 3.966  loss_ce: 0.0006349  loss_mask: 0.2041  loss_dice: 0.1599  loss_ce_0: 0.06124  loss_mask_0: 0.203  loss_dice_0: 0.159  loss_ce_1: 0.0007836  loss_mask_1: 0.2105  loss_dice_1: 0.155  loss_ce_2: 0.0008463  loss_mask_2: 0.2132  loss_dice_2: 0.1605  loss_ce_3: 0.0005951  loss_mask_3: 0.1988  loss_dice_3: 0.1576  loss_ce_4: 0.0006405  loss_mask_4: 0.2027  loss_dice_4: 0.1564  loss_ce_5: 0.0008824  loss_mask_5: 0.2183  loss_dice_5: 0.1585  loss_ce_6: 0.0006016  loss_mask_6: 0.2161  loss_dice_6: 0.1578  loss_ce_7: 0.0007104  loss_mask_7: 0.1985  loss_dice_7: 0.156  loss_ce_8: 0.0008267  loss_mask_8: 0.2079  loss_dice_8: 0.1525  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:51:57] d2.utils.events INFO:  eta: 0:04:45  iter: 47739  total_loss: 4.366  loss_ce: 0.0004081  loss_mask: 0.2  loss_dice: 0.1916  loss_ce_0: 0.06116  loss_mask_0: 0.1992  loss_dice_0: 0.1951  loss_ce_1: 0.0004198  loss_mask_1: 0.2051  loss_dice_1: 0.1984  loss_ce_2: 0.0003154  loss_mask_2: 0.2092  loss_dice_2: 0.1976  loss_ce_3: 0.0004619  loss_mask_3: 0.2076  loss_dice_3: 0.198  loss_ce_4: 0.0003657  loss_mask_4: 0.2064  loss_dice_4: 0.2057  loss_ce_5: 0.0003669  loss_mask_5: 0.2019  loss_dice_5: 0.2036  loss_ce_6: 0.0004634  loss_mask_6: 0.2041  loss_dice_6: 0.1988  loss_ce_7: 0.0003837  loss_mask_7: 0.2086  loss_dice_7: 0.1952  loss_ce_8: 0.0004378  loss_mask_8: 0.2022  loss_dice_8: 0.1921  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:00] d2.utils.events INFO:  eta: 0:04:43  iter: 47759  total_loss: 4.165  loss_ce: 0.0004466  loss_mask: 0.1586  loss_dice: 0.2166  loss_ce_0: 0.06174  loss_mask_0: 0.1555  loss_dice_0: 0.202  loss_ce_1: 0.0004187  loss_mask_1: 0.1574  loss_dice_1: 0.2076  loss_ce_2: 0.0004059  loss_mask_2: 0.1591  loss_dice_2: 0.2003  loss_ce_3: 0.0004162  loss_mask_3: 0.1555  loss_dice_3: 0.2031  loss_ce_4: 0.0005168  loss_mask_4: 0.158  loss_dice_4: 0.2148  loss_ce_5: 0.0004238  loss_mask_5: 0.1571  loss_dice_5: 0.2156  loss_ce_6: 0.0004951  loss_mask_6: 0.1584  loss_dice_6: 0.2123  loss_ce_7: 0.0005784  loss_mask_7: 0.157  loss_dice_7: 0.2101  loss_ce_8: 0.0005274  loss_mask_8: 0.158  loss_dice_8: 0.2123  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:02] d2.utils.events INFO:  eta: 0:04:40  iter: 47779  total_loss: 4.15  loss_ce: 0.0003766  loss_mask: 0.2007  loss_dice: 0.1567  loss_ce_0: 0.06116  loss_mask_0: 0.1891  loss_dice_0: 0.1481  loss_ce_1: 0.0004201  loss_mask_1: 0.1984  loss_dice_1: 0.1541  loss_ce_2: 0.0002598  loss_mask_2: 0.1985  loss_dice_2: 0.1545  loss_ce_3: 0.0003691  loss_mask_3: 0.1961  loss_dice_3: 0.1485  loss_ce_4: 0.0003753  loss_mask_4: 0.1932  loss_dice_4: 0.1501  loss_ce_5: 0.0003616  loss_mask_5: 0.1913  loss_dice_5: 0.1565  loss_ce_6: 0.0004593  loss_mask_6: 0.1991  loss_dice_6: 0.1505  loss_ce_7: 0.0004038  loss_mask_7: 0.1897  loss_dice_7: 0.1512  loss_ce_8: 0.0004645  loss_mask_8: 0.1928  loss_dice_8: 0.1556  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:05] d2.utils.events INFO:  eta: 0:04:38  iter: 47799  total_loss: 4.007  loss_ce: 0.0003693  loss_mask: 0.2179  loss_dice: 0.166  loss_ce_0: 0.06116  loss_mask_0: 0.2143  loss_dice_0: 0.1652  loss_ce_1: 0.000373  loss_mask_1: 0.2177  loss_dice_1: 0.1697  loss_ce_2: 0.0002645  loss_mask_2: 0.2181  loss_dice_2: 0.1623  loss_ce_3: 0.0004034  loss_mask_3: 0.2166  loss_dice_3: 0.1653  loss_ce_4: 0.0003681  loss_mask_4: 0.2179  loss_dice_4: 0.1736  loss_ce_5: 0.0003177  loss_mask_5: 0.2211  loss_dice_5: 0.1606  loss_ce_6: 0.0004762  loss_mask_6: 0.2148  loss_dice_6: 0.166  loss_ce_7: 0.0003795  loss_mask_7: 0.2232  loss_dice_7: 0.1686  loss_ce_8: 0.0004259  loss_mask_8: 0.2167  loss_dice_8: 0.169  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:07] d2.utils.events INFO:  eta: 0:04:36  iter: 47819  total_loss: 3.955  loss_ce: 0.0006583  loss_mask: 0.1504  loss_dice: 0.2153  loss_ce_0: 0.06232  loss_mask_0: 0.1449  loss_dice_0: 0.2149  loss_ce_1: 0.0007251  loss_mask_1: 0.1542  loss_dice_1: 0.2148  loss_ce_2: 0.000783  loss_mask_2: 0.1498  loss_dice_2: 0.214  loss_ce_3: 0.0006119  loss_mask_3: 0.1477  loss_dice_3: 0.2031  loss_ce_4: 0.0005957  loss_mask_4: 0.155  loss_dice_4: 0.2102  loss_ce_5: 0.0006198  loss_mask_5: 0.1564  loss_dice_5: 0.213  loss_ce_6: 0.000687  loss_mask_6: 0.1517  loss_dice_6: 0.2076  loss_ce_7: 0.000635  loss_mask_7: 0.1438  loss_dice_7: 0.2051  loss_ce_8: 0.0006778  loss_mask_8: 0.151  loss_dice_8: 0.2105  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:10] d2.utils.events INFO:  eta: 0:04:33  iter: 47839  total_loss: 4.126  loss_ce: 0.0003885  loss_mask: 0.1814  loss_dice: 0.1885  loss_ce_0: 0.06174  loss_mask_0: 0.1774  loss_dice_0: 0.1769  loss_ce_1: 0.0004356  loss_mask_1: 0.1851  loss_dice_1: 0.1832  loss_ce_2: 0.0003193  loss_mask_2: 0.1854  loss_dice_2: 0.1789  loss_ce_3: 0.0003902  loss_mask_3: 0.183  loss_dice_3: 0.1835  loss_ce_4: 0.000348  loss_mask_4: 0.1864  loss_dice_4: 0.1869  loss_ce_5: 0.0003769  loss_mask_5: 0.1896  loss_dice_5: 0.1818  loss_ce_6: 0.0004667  loss_mask_6: 0.1826  loss_dice_6: 0.1769  loss_ce_7: 0.000406  loss_mask_7: 0.1855  loss_dice_7: 0.181  loss_ce_8: 0.00044  loss_mask_8: 0.1821  loss_dice_8: 0.1825  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:12] d2.utils.events INFO:  eta: 0:04:31  iter: 47859  total_loss: 4.313  loss_ce: 0.0004979  loss_mask: 0.1736  loss_dice: 0.1739  loss_ce_0: 0.06129  loss_mask_0: 0.1737  loss_dice_0: 0.1751  loss_ce_1: 0.0004706  loss_mask_1: 0.1745  loss_dice_1: 0.1812  loss_ce_2: 0.0004359  loss_mask_2: 0.1691  loss_dice_2: 0.1755  loss_ce_3: 0.0006393  loss_mask_3: 0.162  loss_dice_3: 0.1742  loss_ce_4: 0.0005287  loss_mask_4: 0.1762  loss_dice_4: 0.1822  loss_ce_5: 0.0005542  loss_mask_5: 0.1658  loss_dice_5: 0.1781  loss_ce_6: 0.0005906  loss_mask_6: 0.1721  loss_dice_6: 0.1735  loss_ce_7: 0.0005029  loss_mask_7: 0.1724  loss_dice_7: 0.1782  loss_ce_8: 0.0005709  loss_mask_8: 0.1688  loss_dice_8: 0.1822  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:15] d2.utils.events INFO:  eta: 0:04:28  iter: 47879  total_loss: 3.595  loss_ce: 0.0007847  loss_mask: 0.166  loss_dice: 0.1402  loss_ce_0: 0.06174  loss_mask_0: 0.1677  loss_dice_0: 0.1437  loss_ce_1: 0.0005499  loss_mask_1: 0.1599  loss_dice_1: 0.1428  loss_ce_2: 0.00073  loss_mask_2: 0.1646  loss_dice_2: 0.145  loss_ce_3: 0.0009013  loss_mask_3: 0.163  loss_dice_3: 0.1391  loss_ce_4: 0.00061  loss_mask_4: 0.1663  loss_dice_4: 0.1381  loss_ce_5: 0.0008297  loss_mask_5: 0.1623  loss_dice_5: 0.138  loss_ce_6: 0.0007032  loss_mask_6: 0.1637  loss_dice_6: 0.14  loss_ce_7: 0.0006682  loss_mask_7: 0.1646  loss_dice_7: 0.1398  loss_ce_8: 0.0008046  loss_mask_8: 0.1715  loss_dice_8: 0.1419  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:18] d2.utils.events INFO:  eta: 0:04:25  iter: 47899  total_loss: 3.941  loss_ce: 0.0004298  loss_mask: 0.2241  loss_dice: 0.1499  loss_ce_0: 0.06217  loss_mask_0: 0.2254  loss_dice_0: 0.1388  loss_ce_1: 0.0003965  loss_mask_1: 0.223  loss_dice_1: 0.1501  loss_ce_2: 0.0002841  loss_mask_2: 0.2289  loss_dice_2: 0.1453  loss_ce_3: 0.0004047  loss_mask_3: 0.2346  loss_dice_3: 0.1509  loss_ce_4: 0.0003414  loss_mask_4: 0.2283  loss_dice_4: 0.1464  loss_ce_5: 0.0003623  loss_mask_5: 0.2231  loss_dice_5: 0.1462  loss_ce_6: 0.0004538  loss_mask_6: 0.2263  loss_dice_6: 0.1475  loss_ce_7: 0.0003808  loss_mask_7: 0.2254  loss_dice_7: 0.1445  loss_ce_8: 0.0004603  loss_mask_8: 0.2289  loss_dice_8: 0.1533  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:52:20] d2.utils.events INFO:  eta: 0:04:23  iter: 47919  total_loss: 3.944  loss_ce: 0.0004953  loss_mask: 0.1938  loss_dice: 0.1671  loss_ce_0: 0.06174  loss_mask_0: 0.1948  loss_dice_0: 0.163  loss_ce_1: 0.000437  loss_mask_1: 0.1992  loss_dice_1: 0.1768  loss_ce_2: 0.0005202  loss_mask_2: 0.1973  loss_dice_2: 0.1737  loss_ce_3: 0.000446  loss_mask_3: 0.2008  loss_dice_3: 0.1749  loss_ce_4: 0.0005289  loss_mask_4: 0.1987  loss_dice_4: 0.1694  loss_ce_5: 0.0004483  loss_mask_5: 0.1958  loss_dice_5: 0.1727  loss_ce_6: 0.0005401  loss_mask_6: 0.1927  loss_dice_6: 0.1757  loss_ce_7: 0.0004696  loss_mask_7: 0.207  loss_dice_7: 0.174  loss_ce_8: 0.0005357  loss_mask_8: 0.19  loss_dice_8: 0.1709  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:23] d2.utils.events INFO:  eta: 0:04:20  iter: 47939  total_loss: 4.044  loss_ce: 0.0003934  loss_mask: 0.2135  loss_dice: 0.1611  loss_ce_0: 0.06139  loss_mask_0: 0.2144  loss_dice_0: 0.153  loss_ce_1: 0.0003801  loss_mask_1: 0.2159  loss_dice_1: 0.1539  loss_ce_2: 0.0002416  loss_mask_2: 0.2103  loss_dice_2: 0.1551  loss_ce_3: 0.0003574  loss_mask_3: 0.2079  loss_dice_3: 0.1552  loss_ce_4: 0.0003005  loss_mask_4: 0.2118  loss_dice_4: 0.1652  loss_ce_5: 0.0003203  loss_mask_5: 0.2113  loss_dice_5: 0.1565  loss_ce_6: 0.0004327  loss_mask_6: 0.2243  loss_dice_6: 0.1608  loss_ce_7: 0.0003433  loss_mask_7: 0.2135  loss_dice_7: 0.1589  loss_ce_8: 0.0004215  loss_mask_8: 0.2065  loss_dice_8: 0.158  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:25] d2.utils.events INFO:  eta: 0:04:18  iter: 47959  total_loss: 4.196  loss_ce: 0.0005107  loss_mask: 0.1398  loss_dice: 0.25  loss_ce_0: 0.06208  loss_mask_0: 0.1425  loss_dice_0: 0.2392  loss_ce_1: 0.0004064  loss_mask_1: 0.1539  loss_dice_1: 0.2452  loss_ce_2: 0.0004363  loss_mask_2: 0.1416  loss_dice_2: 0.2561  loss_ce_3: 0.0005556  loss_mask_3: 0.1482  loss_dice_3: 0.2541  loss_ce_4: 0.0004155  loss_mask_4: 0.1481  loss_dice_4: 0.2408  loss_ce_5: 0.0005152  loss_mask_5: 0.1428  loss_dice_5: 0.2437  loss_ce_6: 0.0006831  loss_mask_6: 0.1546  loss_dice_6: 0.2503  loss_ce_7: 0.000528  loss_mask_7: 0.1504  loss_dice_7: 0.2478  loss_ce_8: 0.0005542  loss_mask_8: 0.1432  loss_dice_8: 0.2369  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:28] d2.utils.events INFO:  eta: 0:04:15  iter: 47979  total_loss: 3.883  loss_ce: 0.0003131  loss_mask: 0.2126  loss_dice: 0.1537  loss_ce_0: 0.06143  loss_mask_0: 0.2158  loss_dice_0: 0.16  loss_ce_1: 0.000394  loss_mask_1: 0.2091  loss_dice_1: 0.1522  loss_ce_2: 0.0002938  loss_mask_2: 0.2056  loss_dice_2: 0.1548  loss_ce_3: 0.0003983  loss_mask_3: 0.2129  loss_dice_3: 0.1543  loss_ce_4: 0.0003255  loss_mask_4: 0.2043  loss_dice_4: 0.1566  loss_ce_5: 0.0003652  loss_mask_5: 0.2069  loss_dice_5: 0.1558  loss_ce_6: 0.0004889  loss_mask_6: 0.2112  loss_dice_6: 0.1646  loss_ce_7: 0.0003907  loss_mask_7: 0.2031  loss_dice_7: 0.1507  loss_ce_8: 0.000421  loss_mask_8: 0.2069  loss_dice_8: 0.1569  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:52:30] d2.utils.events INFO:  eta: 0:04:13  iter: 47999  total_loss: 4.222  loss_ce: 0.0004178  loss_mask: 0.2175  loss_dice: 0.1692  loss_ce_0: 0.06209  loss_mask_0: 0.206  loss_dice_0: 0.1706  loss_ce_1: 0.0004617  loss_mask_1: 0.2142  loss_dice_1: 0.1633  loss_ce_2: 0.0003891  loss_mask_2: 0.2045  loss_dice_2: 0.1642  loss_ce_3: 0.0003543  loss_mask_3: 0.2137  loss_dice_3: 0.162  loss_ce_4: 0.0004414  loss_mask_4: 0.211  loss_dice_4: 0.1673  loss_ce_5: 0.0003835  loss_mask_5: 0.21  loss_dice_5: 0.1602  loss_ce_6: 0.000493  loss_mask_6: 0.2106  loss_dice_6: 0.158  loss_ce_7: 0.0004193  loss_mask_7: 0.2069  loss_dice_7: 0.1605  loss_ce_8: 0.0004921  loss_mask_8: 0.2138  loss_dice_8: 0.1646  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:52:33] d2.utils.events INFO:  eta: 0:04:10  iter: 48019  total_loss: 4.154  loss_ce: 0.0004093  loss_mask: 0.2141  loss_dice: 0.1597  loss_ce_0: 0.06203  loss_mask_0: 0.2155  loss_dice_0: 0.1589  loss_ce_1: 0.0004532  loss_mask_1: 0.2309  loss_dice_1: 0.164  loss_ce_2: 0.0003815  loss_mask_2: 0.2255  loss_dice_2: 0.1615  loss_ce_3: 0.0003985  loss_mask_3: 0.2069  loss_dice_3: 0.1552  loss_ce_4: 0.0004284  loss_mask_4: 0.2164  loss_dice_4: 0.1584  loss_ce_5: 0.0004177  loss_mask_5: 0.2206  loss_dice_5: 0.1574  loss_ce_6: 0.0005012  loss_mask_6: 0.2209  loss_dice_6: 0.1569  loss_ce_7: 0.0004691  loss_mask_7: 0.223  loss_dice_7: 0.1631  loss_ce_8: 0.000498  loss_mask_8: 0.224  loss_dice_8: 0.1581  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:52:35] d2.utils.events INFO:  eta: 0:04:08  iter: 48039  total_loss: 3.907  loss_ce: 0.000347  loss_mask: 0.2238  loss_dice: 0.1541  loss_ce_0: 0.06145  loss_mask_0: 0.2223  loss_dice_0: 0.1524  loss_ce_1: 0.0003782  loss_mask_1: 0.2189  loss_dice_1: 0.1539  loss_ce_2: 0.0002753  loss_mask_2: 0.2152  loss_dice_2: 0.1488  loss_ce_3: 0.0003803  loss_mask_3: 0.2164  loss_dice_3: 0.1481  loss_ce_4: 0.0002982  loss_mask_4: 0.2147  loss_dice_4: 0.145  loss_ce_5: 0.0003205  loss_mask_5: 0.2072  loss_dice_5: 0.1459  loss_ce_6: 0.000447  loss_mask_6: 0.2182  loss_dice_6: 0.1497  loss_ce_7: 0.0003407  loss_mask_7: 0.227  loss_dice_7: 0.155  loss_ce_8: 0.0004325  loss_mask_8: 0.2217  loss_dice_8: 0.1495  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:38] d2.utils.events INFO:  eta: 0:04:05  iter: 48059  total_loss: 4.257  loss_ce: 0.0005074  loss_mask: 0.2271  loss_dice: 0.1577  loss_ce_0: 0.06145  loss_mask_0: 0.2257  loss_dice_0: 0.1572  loss_ce_1: 0.0004913  loss_mask_1: 0.2155  loss_dice_1: 0.157  loss_ce_2: 0.0003171  loss_mask_2: 0.2167  loss_dice_2: 0.1578  loss_ce_3: 0.0004317  loss_mask_3: 0.2186  loss_dice_3: 0.1568  loss_ce_4: 0.0006279  loss_mask_4: 0.2189  loss_dice_4: 0.1529  loss_ce_5: 0.0004013  loss_mask_5: 0.222  loss_dice_5: 0.1597  loss_ce_6: 0.0005341  loss_mask_6: 0.2191  loss_dice_6: 0.1614  loss_ce_7: 0.0007766  loss_mask_7: 0.2136  loss_dice_7: 0.1562  loss_ce_8: 0.0004951  loss_mask_8: 0.2076  loss_dice_8: 0.1487  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:52:41] d2.utils.events INFO:  eta: 0:04:03  iter: 48079  total_loss: 4.007  loss_ce: 0.0006522  loss_mask: 0.1939  loss_dice: 0.1534  loss_ce_0: 0.06145  loss_mask_0: 0.1944  loss_dice_0: 0.1535  loss_ce_1: 0.002058  loss_mask_1: 0.2003  loss_dice_1: 0.1492  loss_ce_2: 0.0008837  loss_mask_2: 0.2041  loss_dice_2: 0.1523  loss_ce_3: 0.0007314  loss_mask_3: 0.2028  loss_dice_3: 0.1544  loss_ce_4: 0.001215  loss_mask_4: 0.1908  loss_dice_4: 0.1529  loss_ce_5: 0.0008412  loss_mask_5: 0.1915  loss_dice_5: 0.1498  loss_ce_6: 0.0008157  loss_mask_6: 0.1976  loss_dice_6: 0.149  loss_ce_7: 0.0009318  loss_mask_7: 0.2061  loss_dice_7: 0.1555  loss_ce_8: 0.0008432  loss_mask_8: 0.1999  loss_dice_8: 0.1549  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:43] d2.utils.events INFO:  eta: 0:04:00  iter: 48099  total_loss: 4.138  loss_ce: 0.000475  loss_mask: 0.1993  loss_dice: 0.1671  loss_ce_0: 0.06143  loss_mask_0: 0.1929  loss_dice_0: 0.1743  loss_ce_1: 0.0004563  loss_mask_1: 0.1926  loss_dice_1: 0.1735  loss_ce_2: 0.000407  loss_mask_2: 0.1931  loss_dice_2: 0.1796  loss_ce_3: 0.0004802  loss_mask_3: 0.1916  loss_dice_3: 0.1685  loss_ce_4: 0.0004316  loss_mask_4: 0.2029  loss_dice_4: 0.175  loss_ce_5: 0.0004079  loss_mask_5: 0.1967  loss_dice_5: 0.1826  loss_ce_6: 0.000507  loss_mask_6: 0.2008  loss_dice_6: 0.1717  loss_ce_7: 0.0004352  loss_mask_7: 0.203  loss_dice_7: 0.1793  loss_ce_8: 0.0004648  loss_mask_8: 0.1929  loss_dice_8: 0.1732  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:52:46] d2.utils.events INFO:  eta: 0:03:58  iter: 48119  total_loss: 4.198  loss_ce: 0.0003912  loss_mask: 0.1887  loss_dice: 0.1855  loss_ce_0: 0.06142  loss_mask_0: 0.1924  loss_dice_0: 0.1939  loss_ce_1: 0.0004025  loss_mask_1: 0.1958  loss_dice_1: 0.2028  loss_ce_2: 0.0003208  loss_mask_2: 0.19  loss_dice_2: 0.2045  loss_ce_3: 0.0003974  loss_mask_3: 0.2012  loss_dice_3: 0.2001  loss_ce_4: 0.00035  loss_mask_4: 0.1894  loss_dice_4: 0.1929  loss_ce_5: 0.0003763  loss_mask_5: 0.1899  loss_dice_5: 0.1917  loss_ce_6: 0.0004697  loss_mask_6: 0.1886  loss_dice_6: 0.1883  loss_ce_7: 0.0004871  loss_mask_7: 0.196  loss_dice_7: 0.1906  loss_ce_8: 0.0005053  loss_mask_8: 0.1983  loss_dice_8: 0.1883  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:48] d2.utils.events INFO:  eta: 0:03:55  iter: 48139  total_loss: 4.087  loss_ce: 0.0003472  loss_mask: 0.2007  loss_dice: 0.1645  loss_ce_0: 0.06133  loss_mask_0: 0.2035  loss_dice_0: 0.1723  loss_ce_1: 0.0003666  loss_mask_1: 0.2091  loss_dice_1: 0.164  loss_ce_2: 0.0002263  loss_mask_2: 0.2034  loss_dice_2: 0.167  loss_ce_3: 0.0003545  loss_mask_3: 0.2006  loss_dice_3: 0.1677  loss_ce_4: 0.0003102  loss_mask_4: 0.2047  loss_dice_4: 0.1678  loss_ce_5: 0.0003137  loss_mask_5: 0.1992  loss_dice_5: 0.1684  loss_ce_6: 0.0004382  loss_mask_6: 0.2056  loss_dice_6: 0.1733  loss_ce_7: 0.0003618  loss_mask_7: 0.2054  loss_dice_7: 0.1663  loss_ce_8: 0.0004171  loss_mask_8: 0.203  loss_dice_8: 0.1719  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:51] d2.utils.events INFO:  eta: 0:03:53  iter: 48159  total_loss: 4.174  loss_ce: 0.0004808  loss_mask: 0.178  loss_dice: 0.204  loss_ce_0: 0.06132  loss_mask_0: 0.183  loss_dice_0: 0.1974  loss_ce_1: 0.000425  loss_mask_1: 0.1857  loss_dice_1: 0.2035  loss_ce_2: 0.0002957  loss_mask_2: 0.1927  loss_dice_2: 0.2091  loss_ce_3: 0.0004483  loss_mask_3: 0.1796  loss_dice_3: 0.2082  loss_ce_4: 0.0004257  loss_mask_4: 0.185  loss_dice_4: 0.2132  loss_ce_5: 0.0003716  loss_mask_5: 0.1787  loss_dice_5: 0.2073  loss_ce_6: 0.0005177  loss_mask_6: 0.1895  loss_dice_6: 0.2167  loss_ce_7: 0.0004742  loss_mask_7: 0.1878  loss_dice_7: 0.1943  loss_ce_8: 0.0004753  loss_mask_8: 0.1809  loss_dice_8: 0.213  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:52:53] d2.utils.events INFO:  eta: 0:03:50  iter: 48179  total_loss: 4.028  loss_ce: 0.0004122  loss_mask: 0.1819  loss_dice: 0.2271  loss_ce_0: 0.06126  loss_mask_0: 0.1636  loss_dice_0: 0.2186  loss_ce_1: 0.0004809  loss_mask_1: 0.1694  loss_dice_1: 0.2233  loss_ce_2: 0.0004034  loss_mask_2: 0.1742  loss_dice_2: 0.2222  loss_ce_3: 0.000436  loss_mask_3: 0.1796  loss_dice_3: 0.2194  loss_ce_4: 0.0004459  loss_mask_4: 0.1794  loss_dice_4: 0.227  loss_ce_5: 0.0004231  loss_mask_5: 0.172  loss_dice_5: 0.2218  loss_ce_6: 0.000509  loss_mask_6: 0.174  loss_dice_6: 0.2196  loss_ce_7: 0.0004617  loss_mask_7: 0.1649  loss_dice_7: 0.2251  loss_ce_8: 0.0005188  loss_mask_8: 0.161  loss_dice_8: 0.213  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:52:56] d2.utils.events INFO:  eta: 0:03:48  iter: 48199  total_loss: 3.899  loss_ce: 0.0005836  loss_mask: 0.1616  loss_dice: 0.1904  loss_ce_0: 0.06174  loss_mask_0: 0.1605  loss_dice_0: 0.1889  loss_ce_1: 0.0004236  loss_mask_1: 0.1677  loss_dice_1: 0.1933  loss_ce_2: 0.0004766  loss_mask_2: 0.1607  loss_dice_2: 0.1879  loss_ce_3: 0.000507  loss_mask_3: 0.1783  loss_dice_3: 0.1938  loss_ce_4: 0.0004397  loss_mask_4: 0.16  loss_dice_4: 0.1886  loss_ce_5: 0.0005023  loss_mask_5: 0.1706  loss_dice_5: 0.1942  loss_ce_6: 0.0005882  loss_mask_6: 0.1607  loss_dice_6: 0.1969  loss_ce_7: 0.0005816  loss_mask_7: 0.167  loss_dice_7: 0.1848  loss_ce_8: 0.000521  loss_mask_8: 0.1713  loss_dice_8: 0.1941  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:52:58] d2.utils.events INFO:  eta: 0:03:46  iter: 48219  total_loss: 3.742  loss_ce: 0.0004192  loss_mask: 0.1813  loss_dice: 0.1461  loss_ce_0: 0.06213  loss_mask_0: 0.185  loss_dice_0: 0.1442  loss_ce_1: 0.0003982  loss_mask_1: 0.1809  loss_dice_1: 0.1443  loss_ce_2: 0.0002902  loss_mask_2: 0.1854  loss_dice_2: 0.1467  loss_ce_3: 0.0003364  loss_mask_3: 0.1856  loss_dice_3: 0.1467  loss_ce_4: 0.000383  loss_mask_4: 0.187  loss_dice_4: 0.1479  loss_ce_5: 0.0003372  loss_mask_5: 0.1967  loss_dice_5: 0.1496  loss_ce_6: 0.0004153  loss_mask_6: 0.1895  loss_dice_6: 0.1474  loss_ce_7: 0.0003811  loss_mask_7: 0.1839  loss_dice_7: 0.1505  loss_ce_8: 0.0004633  loss_mask_8: 0.1804  loss_dice_8: 0.143  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:53:01] d2.utils.events INFO:  eta: 0:03:43  iter: 48239  total_loss: 4.168  loss_ce: 0.0004878  loss_mask: 0.1979  loss_dice: 0.1837  loss_ce_0: 0.06137  loss_mask_0: 0.2035  loss_dice_0: 0.1799  loss_ce_1: 0.000398  loss_mask_1: 0.2027  loss_dice_1: 0.1837  loss_ce_2: 0.0003377  loss_mask_2: 0.2021  loss_dice_2: 0.1871  loss_ce_3: 0.0005022  loss_mask_3: 0.2076  loss_dice_3: 0.1906  loss_ce_4: 0.0003564  loss_mask_4: 0.2013  loss_dice_4: 0.1849  loss_ce_5: 0.0004205  loss_mask_5: 0.2056  loss_dice_5: 0.181  loss_ce_6: 0.0004864  loss_mask_6: 0.2089  loss_dice_6: 0.1899  loss_ce_7: 0.0003712  loss_mask_7: 0.1983  loss_dice_7: 0.1865  loss_ce_8: 0.0004716  loss_mask_8: 0.2069  loss_dice_8: 0.179  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:03] d2.utils.events INFO:  eta: 0:03:41  iter: 48259  total_loss: 4.301  loss_ce: 0.0007351  loss_mask: 0.1431  loss_dice: 0.2367  loss_ce_0: 0.06172  loss_mask_0: 0.1439  loss_dice_0: 0.2294  loss_ce_1: 0.0005278  loss_mask_1: 0.1432  loss_dice_1: 0.2397  loss_ce_2: 0.0005085  loss_mask_2: 0.1566  loss_dice_2: 0.2163  loss_ce_3: 0.0006095  loss_mask_3: 0.1555  loss_dice_3: 0.2378  loss_ce_4: 0.0006455  loss_mask_4: 0.139  loss_dice_4: 0.2292  loss_ce_5: 0.0005854  loss_mask_5: 0.1401  loss_dice_5: 0.2273  loss_ce_6: 0.0006532  loss_mask_6: 0.1392  loss_dice_6: 0.2344  loss_ce_7: 0.0008348  loss_mask_7: 0.1407  loss_dice_7: 0.219  loss_ce_8: 0.0006241  loss_mask_8: 0.1402  loss_dice_8: 0.229  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:06] d2.utils.events INFO:  eta: 0:03:38  iter: 48279  total_loss: 3.969  loss_ce: 0.0004416  loss_mask: 0.2009  loss_dice: 0.1775  loss_ce_0: 0.06139  loss_mask_0: 0.2016  loss_dice_0: 0.1763  loss_ce_1: 0.0005599  loss_mask_1: 0.1973  loss_dice_1: 0.1803  loss_ce_2: 0.000463  loss_mask_2: 0.2034  loss_dice_2: 0.176  loss_ce_3: 0.0004546  loss_mask_3: 0.2011  loss_dice_3: 0.1813  loss_ce_4: 0.0004801  loss_mask_4: 0.2116  loss_dice_4: 0.1782  loss_ce_5: 0.0005541  loss_mask_5: 0.197  loss_dice_5: 0.1796  loss_ce_6: 0.0005235  loss_mask_6: 0.1984  loss_dice_6: 0.1759  loss_ce_7: 0.0005005  loss_mask_7: 0.2028  loss_dice_7: 0.1801  loss_ce_8: 0.0005732  loss_mask_8: 0.2074  loss_dice_8: 0.1823  time: 0.1260  data_time: 0.0014  lr: 1e-06  max_mem: 3105M
[04/13 16:53:09] d2.utils.events INFO:  eta: 0:03:36  iter: 48299  total_loss: 3.721  loss_ce: 0.0004718  loss_mask: 0.2138  loss_dice: 0.1629  loss_ce_0: 0.06135  loss_mask_0: 0.2169  loss_dice_0: 0.1632  loss_ce_1: 0.0004092  loss_mask_1: 0.2175  loss_dice_1: 0.1677  loss_ce_2: 0.0003633  loss_mask_2: 0.2153  loss_dice_2: 0.1633  loss_ce_3: 0.0004528  loss_mask_3: 0.2159  loss_dice_3: 0.1644  loss_ce_4: 0.0003876  loss_mask_4: 0.2119  loss_dice_4: 0.1683  loss_ce_5: 0.0003977  loss_mask_5: 0.2112  loss_dice_5: 0.1642  loss_ce_6: 0.0005141  loss_mask_6: 0.2193  loss_dice_6: 0.1674  loss_ce_7: 0.0004091  loss_mask_7: 0.2154  loss_dice_7: 0.1614  loss_ce_8: 0.000474  loss_mask_8: 0.2149  loss_dice_8: 0.1666  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:11] d2.utils.events INFO:  eta: 0:03:33  iter: 48319  total_loss: 4.163  loss_ce: 0.0004127  loss_mask: 0.1626  loss_dice: 0.171  loss_ce_0: 0.06216  loss_mask_0: 0.1658  loss_dice_0: 0.1712  loss_ce_1: 0.0003946  loss_mask_1: 0.1664  loss_dice_1: 0.1685  loss_ce_2: 0.0003005  loss_mask_2: 0.1675  loss_dice_2: 0.172  loss_ce_3: 0.0004115  loss_mask_3: 0.162  loss_dice_3: 0.1637  loss_ce_4: 0.0003491  loss_mask_4: 0.1679  loss_dice_4: 0.1794  loss_ce_5: 0.0003697  loss_mask_5: 0.1552  loss_dice_5: 0.1646  loss_ce_6: 0.0004738  loss_mask_6: 0.1653  loss_dice_6: 0.1701  loss_ce_7: 0.0004051  loss_mask_7: 0.1648  loss_dice_7: 0.1679  loss_ce_8: 0.0004786  loss_mask_8: 0.1592  loss_dice_8: 0.1667  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:14] d2.utils.events INFO:  eta: 0:03:31  iter: 48339  total_loss: 4.121  loss_ce: 0.0003456  loss_mask: 0.1831  loss_dice: 0.173  loss_ce_0: 0.06174  loss_mask_0: 0.1833  loss_dice_0: 0.1771  loss_ce_1: 0.000412  loss_mask_1: 0.1936  loss_dice_1: 0.1721  loss_ce_2: 0.000296  loss_mask_2: 0.2001  loss_dice_2: 0.1666  loss_ce_3: 0.0003508  loss_mask_3: 0.1943  loss_dice_3: 0.1755  loss_ce_4: 0.0003961  loss_mask_4: 0.1907  loss_dice_4: 0.1704  loss_ce_5: 0.0003475  loss_mask_5: 0.1982  loss_dice_5: 0.1708  loss_ce_6: 0.0004113  loss_mask_6: 0.1919  loss_dice_6: 0.1688  loss_ce_7: 0.0003726  loss_mask_7: 0.1834  loss_dice_7: 0.1644  loss_ce_8: 0.0004115  loss_mask_8: 0.1904  loss_dice_8: 0.1654  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:53:16] d2.utils.events INFO:  eta: 0:03:28  iter: 48359  total_loss: 4.348  loss_ce: 0.0004016  loss_mask: 0.2135  loss_dice: 0.1653  loss_ce_0: 0.06134  loss_mask_0: 0.2275  loss_dice_0: 0.1723  loss_ce_1: 0.0004747  loss_mask_1: 0.2175  loss_dice_1: 0.1718  loss_ce_2: 0.0003366  loss_mask_2: 0.2169  loss_dice_2: 0.1725  loss_ce_3: 0.0003755  loss_mask_3: 0.2229  loss_dice_3: 0.173  loss_ce_4: 0.0004571  loss_mask_4: 0.2121  loss_dice_4: 0.1674  loss_ce_5: 0.0003728  loss_mask_5: 0.2243  loss_dice_5: 0.1687  loss_ce_6: 0.0004887  loss_mask_6: 0.2113  loss_dice_6: 0.1642  loss_ce_7: 0.0004479  loss_mask_7: 0.2154  loss_dice_7: 0.1723  loss_ce_8: 0.0004596  loss_mask_8: 0.2082  loss_dice_8: 0.1709  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:53:19] d2.utils.events INFO:  eta: 0:03:26  iter: 48379  total_loss: 4.63  loss_ce: 0.0004823  loss_mask: 0.1611  loss_dice: 0.1815  loss_ce_0: 0.06214  loss_mask_0: 0.1669  loss_dice_0: 0.1806  loss_ce_1: 0.0004126  loss_mask_1: 0.1536  loss_dice_1: 0.1886  loss_ce_2: 0.0002912  loss_mask_2: 0.1655  loss_dice_2: 0.1878  loss_ce_3: 0.0004529  loss_mask_3: 0.1701  loss_dice_3: 0.1808  loss_ce_4: 0.0004321  loss_mask_4: 0.1692  loss_dice_4: 0.1886  loss_ce_5: 0.0003176  loss_mask_5: 0.1639  loss_dice_5: 0.1824  loss_ce_6: 0.000481  loss_mask_6: 0.1635  loss_dice_6: 0.1903  loss_ce_7: 0.0003721  loss_mask_7: 0.1628  loss_dice_7: 0.186  loss_ce_8: 0.0004339  loss_mask_8: 0.168  loss_dice_8: 0.1844  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:21] d2.utils.events INFO:  eta: 0:03:23  iter: 48399  total_loss: 4.038  loss_ce: 0.000333  loss_mask: 0.2005  loss_dice: 0.1806  loss_ce_0: 0.06139  loss_mask_0: 0.1933  loss_dice_0: 0.1698  loss_ce_1: 0.0003929  loss_mask_1: 0.2001  loss_dice_1: 0.1851  loss_ce_2: 0.0002884  loss_mask_2: 0.2001  loss_dice_2: 0.1709  loss_ce_3: 0.0003573  loss_mask_3: 0.1987  loss_dice_3: 0.1774  loss_ce_4: 0.0003561  loss_mask_4: 0.1944  loss_dice_4: 0.1713  loss_ce_5: 0.0003262  loss_mask_5: 0.2039  loss_dice_5: 0.1768  loss_ce_6: 0.0004459  loss_mask_6: 0.198  loss_dice_6: 0.1752  loss_ce_7: 0.0003534  loss_mask_7: 0.1992  loss_dice_7: 0.1741  loss_ce_8: 0.0004375  loss_mask_8: 0.1969  loss_dice_8: 0.1747  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:53:24] d2.utils.events INFO:  eta: 0:03:21  iter: 48419  total_loss: 4.085  loss_ce: 0.0003663  loss_mask: 0.2284  loss_dice: 0.1818  loss_ce_0: 0.06208  loss_mask_0: 0.2287  loss_dice_0: 0.1741  loss_ce_1: 0.0004264  loss_mask_1: 0.2273  loss_dice_1: 0.1817  loss_ce_2: 0.0003192  loss_mask_2: 0.2317  loss_dice_2: 0.1808  loss_ce_3: 0.000344  loss_mask_3: 0.2327  loss_dice_3: 0.1818  loss_ce_4: 0.0004113  loss_mask_4: 0.2364  loss_dice_4: 0.1776  loss_ce_5: 0.0004034  loss_mask_5: 0.2257  loss_dice_5: 0.1779  loss_ce_6: 0.0004547  loss_mask_6: 0.2416  loss_dice_6: 0.1688  loss_ce_7: 0.0003744  loss_mask_7: 0.2306  loss_dice_7: 0.1765  loss_ce_8: 0.0004662  loss_mask_8: 0.2326  loss_dice_8: 0.1729  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:27] d2.utils.events INFO:  eta: 0:03:18  iter: 48439  total_loss: 5.032  loss_ce: 0.0005385  loss_mask: 0.2259  loss_dice: 0.194  loss_ce_0: 0.06139  loss_mask_0: 0.2287  loss_dice_0: 0.19  loss_ce_1: 0.0004726  loss_mask_1: 0.2224  loss_dice_1: 0.194  loss_ce_2: 0.0003635  loss_mask_2: 0.2198  loss_dice_2: 0.1959  loss_ce_3: 0.0004598  loss_mask_3: 0.2265  loss_dice_3: 0.1943  loss_ce_4: 0.000472  loss_mask_4: 0.2253  loss_dice_4: 0.1879  loss_ce_5: 0.000414  loss_mask_5: 0.2239  loss_dice_5: 0.1867  loss_ce_6: 0.0005154  loss_mask_6: 0.216  loss_dice_6: 0.1917  loss_ce_7: 0.0005335  loss_mask_7: 0.225  loss_dice_7: 0.1925  loss_ce_8: 0.000477  loss_mask_8: 0.2235  loss_dice_8: 0.1924  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:29] d2.utils.events INFO:  eta: 0:03:16  iter: 48459  total_loss: 4.188  loss_ce: 0.0004082  loss_mask: 0.2065  loss_dice: 0.1746  loss_ce_0: 0.06139  loss_mask_0: 0.1973  loss_dice_0: 0.1631  loss_ce_1: 0.0003641  loss_mask_1: 0.2033  loss_dice_1: 0.1588  loss_ce_2: 0.0002946  loss_mask_2: 0.1919  loss_dice_2: 0.1599  loss_ce_3: 0.0003975  loss_mask_3: 0.207  loss_dice_3: 0.1728  loss_ce_4: 0.0003235  loss_mask_4: 0.2084  loss_dice_4: 0.1712  loss_ce_5: 0.0003536  loss_mask_5: 0.2079  loss_dice_5: 0.1699  loss_ce_6: 0.0004453  loss_mask_6: 0.207  loss_dice_6: 0.1623  loss_ce_7: 0.0003546  loss_mask_7: 0.207  loss_dice_7: 0.1621  loss_ce_8: 0.0004623  loss_mask_8: 0.2085  loss_dice_8: 0.1657  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:32] d2.utils.events INFO:  eta: 0:03:13  iter: 48479  total_loss: 4.428  loss_ce: 0.000585  loss_mask: 0.2202  loss_dice: 0.161  loss_ce_0: 0.06174  loss_mask_0: 0.2142  loss_dice_0: 0.154  loss_ce_1: 0.0004172  loss_mask_1: 0.2175  loss_dice_1: 0.1628  loss_ce_2: 0.0003476  loss_mask_2: 0.2166  loss_dice_2: 0.1578  loss_ce_3: 0.0004605  loss_mask_3: 0.211  loss_dice_3: 0.1621  loss_ce_4: 0.000408  loss_mask_4: 0.2234  loss_dice_4: 0.1595  loss_ce_5: 0.0004151  loss_mask_5: 0.2167  loss_dice_5: 0.1617  loss_ce_6: 0.0005642  loss_mask_6: 0.2216  loss_dice_6: 0.1683  loss_ce_7: 0.0004992  loss_mask_7: 0.2175  loss_dice_7: 0.1578  loss_ce_8: 0.0004839  loss_mask_8: 0.2107  loss_dice_8: 0.1598  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:34] d2.utils.events INFO:  eta: 0:03:10  iter: 48499  total_loss: 4.856  loss_ce: 0.0004186  loss_mask: 0.2077  loss_dice: 0.1778  loss_ce_0: 0.06139  loss_mask_0: 0.211  loss_dice_0: 0.1817  loss_ce_1: 0.0004027  loss_mask_1: 0.2184  loss_dice_1: 0.1727  loss_ce_2: 0.0003242  loss_mask_2: 0.2159  loss_dice_2: 0.1825  loss_ce_3: 0.0004232  loss_mask_3: 0.2214  loss_dice_3: 0.1863  loss_ce_4: 0.0003786  loss_mask_4: 0.2184  loss_dice_4: 0.18  loss_ce_5: 0.0003543  loss_mask_5: 0.2129  loss_dice_5: 0.1722  loss_ce_6: 0.0004915  loss_mask_6: 0.2018  loss_dice_6: 0.1855  loss_ce_7: 0.0004038  loss_mask_7: 0.2113  loss_dice_7: 0.1859  loss_ce_8: 0.0004309  loss_mask_8: 0.2177  loss_dice_8: 0.1814  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:53:37] d2.utils.events INFO:  eta: 0:03:08  iter: 48519  total_loss: 4.381  loss_ce: 0.000749  loss_mask: 0.2123  loss_dice: 0.1755  loss_ce_0: 0.06138  loss_mask_0: 0.2033  loss_dice_0: 0.1771  loss_ce_1: 0.001094  loss_mask_1: 0.214  loss_dice_1: 0.177  loss_ce_2: 0.0006984  loss_mask_2: 0.205  loss_dice_2: 0.1771  loss_ce_3: 0.0007366  loss_mask_3: 0.2111  loss_dice_3: 0.1777  loss_ce_4: 0.0011  loss_mask_4: 0.2072  loss_dice_4: 0.1761  loss_ce_5: 0.0006359  loss_mask_5: 0.2081  loss_dice_5: 0.176  loss_ce_6: 0.0007278  loss_mask_6: 0.2059  loss_dice_6: 0.1727  loss_ce_7: 0.001202  loss_mask_7: 0.2091  loss_dice_7: 0.1776  loss_ce_8: 0.0007581  loss_mask_8: 0.2099  loss_dice_8: 0.1784  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:39] d2.utils.events INFO:  eta: 0:03:05  iter: 48539  total_loss: 4.123  loss_ce: 0.0004561  loss_mask: 0.1884  loss_dice: 0.1609  loss_ce_0: 0.06174  loss_mask_0: 0.1797  loss_dice_0: 0.1683  loss_ce_1: 0.0003984  loss_mask_1: 0.1864  loss_dice_1: 0.1674  loss_ce_2: 0.0004115  loss_mask_2: 0.1877  loss_dice_2: 0.1629  loss_ce_3: 0.0004785  loss_mask_3: 0.1869  loss_dice_3: 0.1608  loss_ce_4: 0.0004879  loss_mask_4: 0.1916  loss_dice_4: 0.1597  loss_ce_5: 0.0005292  loss_mask_5: 0.1847  loss_dice_5: 0.1604  loss_ce_6: 0.0006025  loss_mask_6: 0.2015  loss_dice_6: 0.1607  loss_ce_7: 0.0005443  loss_mask_7: 0.1903  loss_dice_7: 0.1642  loss_ce_8: 0.000568  loss_mask_8: 0.1864  loss_dice_8: 0.159  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:42] d2.utils.events INFO:  eta: 0:03:03  iter: 48559  total_loss: 3.941  loss_ce: 0.0004698  loss_mask: 0.1736  loss_dice: 0.1895  loss_ce_0: 0.06175  loss_mask_0: 0.1745  loss_dice_0: 0.1881  loss_ce_1: 0.0003764  loss_mask_1: 0.1681  loss_dice_1: 0.1894  loss_ce_2: 0.0003099  loss_mask_2: 0.1751  loss_dice_2: 0.1824  loss_ce_3: 0.000418  loss_mask_3: 0.1656  loss_dice_3: 0.189  loss_ce_4: 0.0003724  loss_mask_4: 0.1669  loss_dice_4: 0.1891  loss_ce_5: 0.0004553  loss_mask_5: 0.1792  loss_dice_5: 0.1948  loss_ce_6: 0.0004845  loss_mask_6: 0.1655  loss_dice_6: 0.1854  loss_ce_7: 0.000427  loss_mask_7: 0.1662  loss_dice_7: 0.1872  loss_ce_8: 0.000508  loss_mask_8: 0.1752  loss_dice_8: 0.1881  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:44] d2.utils.events INFO:  eta: 0:03:00  iter: 48579  total_loss: 4.063  loss_ce: 0.0004174  loss_mask: 0.1773  loss_dice: 0.1978  loss_ce_0: 0.06138  loss_mask_0: 0.1827  loss_dice_0: 0.2137  loss_ce_1: 0.0003812  loss_mask_1: 0.1786  loss_dice_1: 0.2031  loss_ce_2: 0.0002749  loss_mask_2: 0.1791  loss_dice_2: 0.204  loss_ce_3: 0.0003983  loss_mask_3: 0.1875  loss_dice_3: 0.2109  loss_ce_4: 0.0003376  loss_mask_4: 0.1846  loss_dice_4: 0.2017  loss_ce_5: 0.000335  loss_mask_5: 0.1892  loss_dice_5: 0.2186  loss_ce_6: 0.000444  loss_mask_6: 0.1834  loss_dice_6: 0.2099  loss_ce_7: 0.0004296  loss_mask_7: 0.1811  loss_dice_7: 0.2047  loss_ce_8: 0.0004523  loss_mask_8: 0.1835  loss_dice_8: 0.2124  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:53:47] d2.utils.events INFO:  eta: 0:02:57  iter: 48599  total_loss: 4.094  loss_ce: 0.0003278  loss_mask: 0.2008  loss_dice: 0.1673  loss_ce_0: 0.06208  loss_mask_0: 0.2011  loss_dice_0: 0.1661  loss_ce_1: 0.0003789  loss_mask_1: 0.201  loss_dice_1: 0.1657  loss_ce_2: 0.0002674  loss_mask_2: 0.1938  loss_dice_2: 0.165  loss_ce_3: 0.0003237  loss_mask_3: 0.1987  loss_dice_3: 0.1598  loss_ce_4: 0.0003747  loss_mask_4: 0.2016  loss_dice_4: 0.168  loss_ce_5: 0.0003082  loss_mask_5: 0.1961  loss_dice_5: 0.1594  loss_ce_6: 0.0004179  loss_mask_6: 0.1929  loss_dice_6: 0.1638  loss_ce_7: 0.0004084  loss_mask_7: 0.193  loss_dice_7: 0.1661  loss_ce_8: 0.0004253  loss_mask_8: 0.2005  loss_dice_8: 0.1625  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:49] d2.utils.events INFO:  eta: 0:02:55  iter: 48619  total_loss: 4.323  loss_ce: 0.0009065  loss_mask: 0.1689  loss_dice: 0.2245  loss_ce_0: 0.06204  loss_mask_0: 0.1587  loss_dice_0: 0.2302  loss_ce_1: 0.0006115  loss_mask_1: 0.1513  loss_dice_1: 0.2205  loss_ce_2: 0.0007808  loss_mask_2: 0.1663  loss_dice_2: 0.2241  loss_ce_3: 0.0006574  loss_mask_3: 0.1596  loss_dice_3: 0.2293  loss_ce_4: 0.0007969  loss_mask_4: 0.1579  loss_dice_4: 0.2194  loss_ce_5: 0.000629  loss_mask_5: 0.1515  loss_dice_5: 0.2334  loss_ce_6: 0.0007071  loss_mask_6: 0.1516  loss_dice_6: 0.2198  loss_ce_7: 0.0006742  loss_mask_7: 0.1616  loss_dice_7: 0.2354  loss_ce_8: 0.0007618  loss_mask_8: 0.1636  loss_dice_8: 0.2177  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:53:52] d2.utils.events INFO:  eta: 0:02:52  iter: 48639  total_loss: 4.082  loss_ce: 0.0006592  loss_mask: 0.1666  loss_dice: 0.2236  loss_ce_0: 0.06143  loss_mask_0: 0.1738  loss_dice_0: 0.2192  loss_ce_1: 0.0008408  loss_mask_1: 0.1719  loss_dice_1: 0.2285  loss_ce_2: 0.0008655  loss_mask_2: 0.1765  loss_dice_2: 0.2256  loss_ce_3: 0.0007508  loss_mask_3: 0.1662  loss_dice_3: 0.2173  loss_ce_4: 0.0008193  loss_mask_4: 0.1707  loss_dice_4: 0.2172  loss_ce_5: 0.000627  loss_mask_5: 0.1716  loss_dice_5: 0.218  loss_ce_6: 0.0007791  loss_mask_6: 0.1721  loss_dice_6: 0.2278  loss_ce_7: 0.0007643  loss_mask_7: 0.168  loss_dice_7: 0.2142  loss_ce_8: 0.000646  loss_mask_8: 0.1632  loss_dice_8: 0.214  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:53:54] d2.utils.events INFO:  eta: 0:02:50  iter: 48659  total_loss: 4.082  loss_ce: 0.0006672  loss_mask: 0.1738  loss_dice: 0.1854  loss_ce_0: 0.06138  loss_mask_0: 0.1741  loss_dice_0: 0.184  loss_ce_1: 0.0007803  loss_mask_1: 0.173  loss_dice_1: 0.1799  loss_ce_2: 0.0005424  loss_mask_2: 0.1736  loss_dice_2: 0.1846  loss_ce_3: 0.0007213  loss_mask_3: 0.1735  loss_dice_3: 0.18  loss_ce_4: 0.0007012  loss_mask_4: 0.1774  loss_dice_4: 0.1731  loss_ce_5: 0.0005757  loss_mask_5: 0.1744  loss_dice_5: 0.1868  loss_ce_6: 0.0006242  loss_mask_6: 0.1744  loss_dice_6: 0.1824  loss_ce_7: 0.0005775  loss_mask_7: 0.1734  loss_dice_7: 0.1831  loss_ce_8: 0.0006205  loss_mask_8: 0.1722  loss_dice_8: 0.1794  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:53:57] d2.utils.events INFO:  eta: 0:02:47  iter: 48679  total_loss: 4.488  loss_ce: 0.0004342  loss_mask: 0.2255  loss_dice: 0.1917  loss_ce_0: 0.06137  loss_mask_0: 0.2143  loss_dice_0: 0.197  loss_ce_1: 0.0004358  loss_mask_1: 0.2133  loss_dice_1: 0.1846  loss_ce_2: 0.0003466  loss_mask_2: 0.2343  loss_dice_2: 0.1913  loss_ce_3: 0.0004189  loss_mask_3: 0.2115  loss_dice_3: 0.1972  loss_ce_4: 0.000365  loss_mask_4: 0.2189  loss_dice_4: 0.2001  loss_ce_5: 0.0003281  loss_mask_5: 0.2147  loss_dice_5: 0.194  loss_ce_6: 0.0004475  loss_mask_6: 0.2322  loss_dice_6: 0.193  loss_ce_7: 0.0003583  loss_mask_7: 0.2221  loss_dice_7: 0.1954  loss_ce_8: 0.0004311  loss_mask_8: 0.217  loss_dice_8: 0.1947  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:53:59] d2.utils.events INFO:  eta: 0:02:45  iter: 48699  total_loss: 4.083  loss_ce: 0.00063  loss_mask: 0.214  loss_dice: 0.1442  loss_ce_0: 0.06131  loss_mask_0: 0.2077  loss_dice_0: 0.1378  loss_ce_1: 0.0004655  loss_mask_1: 0.2123  loss_dice_1: 0.1404  loss_ce_2: 0.0005008  loss_mask_2: 0.2035  loss_dice_2: 0.1391  loss_ce_3: 0.000572  loss_mask_3: 0.2087  loss_dice_3: 0.1398  loss_ce_4: 0.0004543  loss_mask_4: 0.2122  loss_dice_4: 0.1445  loss_ce_5: 0.0005282  loss_mask_5: 0.2096  loss_dice_5: 0.1427  loss_ce_6: 0.0005639  loss_mask_6: 0.2097  loss_dice_6: 0.142  loss_ce_7: 0.0005004  loss_mask_7: 0.2124  loss_dice_7: 0.139  loss_ce_8: 0.0005608  loss_mask_8: 0.216  loss_dice_8: 0.1418  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:02] d2.utils.events INFO:  eta: 0:02:42  iter: 48719  total_loss: 4.491  loss_ce: 0.0009831  loss_mask: 0.2112  loss_dice: 0.1556  loss_ce_0: 0.06176  loss_mask_0: 0.2025  loss_dice_0: 0.158  loss_ce_1: 0.0008873  loss_mask_1: 0.2152  loss_dice_1: 0.1569  loss_ce_2: 0.0009478  loss_mask_2: 0.2066  loss_dice_2: 0.1524  loss_ce_3: 0.0005933  loss_mask_3: 0.2029  loss_dice_3: 0.1515  loss_ce_4: 0.0004799  loss_mask_4: 0.2082  loss_dice_4: 0.153  loss_ce_5: 0.0007951  loss_mask_5: 0.2202  loss_dice_5: 0.1506  loss_ce_6: 0.0006265  loss_mask_6: 0.2183  loss_dice_6: 0.1521  loss_ce_7: 0.000566  loss_mask_7: 0.2074  loss_dice_7: 0.1532  loss_ce_8: 0.0008097  loss_mask_8: 0.2137  loss_dice_8: 0.1582  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:04] d2.utils.events INFO:  eta: 0:02:40  iter: 48739  total_loss: 4.045  loss_ce: 0.0002964  loss_mask: 0.2114  loss_dice: 0.1732  loss_ce_0: 0.06217  loss_mask_0: 0.2077  loss_dice_0: 0.1714  loss_ce_1: 0.0003349  loss_mask_1: 0.2  loss_dice_1: 0.1725  loss_ce_2: 0.0001944  loss_mask_2: 0.2087  loss_dice_2: 0.1646  loss_ce_3: 0.000305  loss_mask_3: 0.2081  loss_dice_3: 0.1701  loss_ce_4: 0.0002796  loss_mask_4: 0.2021  loss_dice_4: 0.1729  loss_ce_5: 0.0002724  loss_mask_5: 0.2047  loss_dice_5: 0.1686  loss_ce_6: 0.0003944  loss_mask_6: 0.2012  loss_dice_6: 0.1732  loss_ce_7: 0.0002879  loss_mask_7: 0.2106  loss_dice_7: 0.173  loss_ce_8: 0.0003881  loss_mask_8: 0.2029  loss_dice_8: 0.1782  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:07] d2.utils.events INFO:  eta: 0:02:37  iter: 48759  total_loss: 4.114  loss_ce: 0.0004919  loss_mask: 0.1679  loss_dice: 0.2057  loss_ce_0: 0.06131  loss_mask_0: 0.1694  loss_dice_0: 0.1965  loss_ce_1: 0.0004477  loss_mask_1: 0.1706  loss_dice_1: 0.199  loss_ce_2: 0.0003282  loss_mask_2: 0.1683  loss_dice_2: 0.2036  loss_ce_3: 0.0004086  loss_mask_3: 0.1841  loss_dice_3: 0.1946  loss_ce_4: 0.000561  loss_mask_4: 0.1763  loss_dice_4: 0.2035  loss_ce_5: 0.0003952  loss_mask_5: 0.1805  loss_dice_5: 0.2064  loss_ce_6: 0.0005017  loss_mask_6: 0.1811  loss_dice_6: 0.1905  loss_ce_7: 0.0006449  loss_mask_7: 0.1781  loss_dice_7: 0.1978  loss_ce_8: 0.0004971  loss_mask_8: 0.1759  loss_dice_8: 0.1905  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:09] d2.utils.events INFO:  eta: 0:02:35  iter: 48779  total_loss: 4.692  loss_ce: 0.0009025  loss_mask: 0.2013  loss_dice: 0.175  loss_ce_0: 0.0613  loss_mask_0: 0.2048  loss_dice_0: 0.1805  loss_ce_1: 0.0005817  loss_mask_1: 0.2085  loss_dice_1: 0.1789  loss_ce_2: 0.0005961  loss_mask_2: 0.1998  loss_dice_2: 0.1746  loss_ce_3: 0.0007251  loss_mask_3: 0.2101  loss_dice_3: 0.1802  loss_ce_4: 0.0009036  loss_mask_4: 0.2086  loss_dice_4: 0.177  loss_ce_5: 0.0006525  loss_mask_5: 0.2023  loss_dice_5: 0.1754  loss_ce_6: 0.000751  loss_mask_6: 0.2123  loss_dice_6: 0.1783  loss_ce_7: 0.000805  loss_mask_7: 0.2022  loss_dice_7: 0.1808  loss_ce_8: 0.0006899  loss_mask_8: 0.2086  loss_dice_8: 0.182  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:12] d2.utils.events INFO:  eta: 0:02:32  iter: 48799  total_loss: 4.536  loss_ce: 0.001112  loss_mask: 0.1603  loss_dice: 0.2187  loss_ce_0: 0.0613  loss_mask_0: 0.1641  loss_dice_0: 0.2275  loss_ce_1: 0.0008419  loss_mask_1: 0.1695  loss_dice_1: 0.2294  loss_ce_2: 0.0008079  loss_mask_2: 0.1568  loss_dice_2: 0.2293  loss_ce_3: 0.0008084  loss_mask_3: 0.158  loss_dice_3: 0.2288  loss_ce_4: 0.0009862  loss_mask_4: 0.1663  loss_dice_4: 0.2454  loss_ce_5: 0.0008031  loss_mask_5: 0.1615  loss_dice_5: 0.2298  loss_ce_6: 0.0007602  loss_mask_6: 0.1645  loss_dice_6: 0.2408  loss_ce_7: 0.001234  loss_mask_7: 0.159  loss_dice_7: 0.235  loss_ce_8: 0.0007619  loss_mask_8: 0.1642  loss_dice_8: 0.235  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:15] d2.utils.events INFO:  eta: 0:02:29  iter: 48819  total_loss: 4.06  loss_ce: 0.0008077  loss_mask: 0.1657  loss_dice: 0.1588  loss_ce_0: 0.06219  loss_mask_0: 0.1707  loss_dice_0: 0.158  loss_ce_1: 0.0007269  loss_mask_1: 0.1573  loss_dice_1: 0.158  loss_ce_2: 0.000925  loss_mask_2: 0.1637  loss_dice_2: 0.1536  loss_ce_3: 0.0008735  loss_mask_3: 0.1667  loss_dice_3: 0.1591  loss_ce_4: 0.0006181  loss_mask_4: 0.1684  loss_dice_4: 0.1559  loss_ce_5: 0.0009265  loss_mask_5: 0.1635  loss_dice_5: 0.1523  loss_ce_6: 0.0007325  loss_mask_6: 0.1591  loss_dice_6: 0.1567  loss_ce_7: 0.00066  loss_mask_7: 0.1631  loss_dice_7: 0.1556  loss_ce_8: 0.0007607  loss_mask_8: 0.1558  loss_dice_8: 0.1579  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:17] d2.utils.events INFO:  eta: 0:02:27  iter: 48839  total_loss: 4.42  loss_ce: 0.0004389  loss_mask: 0.2037  loss_dice: 0.1626  loss_ce_0: 0.06128  loss_mask_0: 0.2093  loss_dice_0: 0.1613  loss_ce_1: 0.0003626  loss_mask_1: 0.2076  loss_dice_1: 0.1636  loss_ce_2: 0.0002513  loss_mask_2: 0.2094  loss_dice_2: 0.1605  loss_ce_3: 0.000438  loss_mask_3: 0.1983  loss_dice_3: 0.1634  loss_ce_4: 0.0003529  loss_mask_4: 0.2088  loss_dice_4: 0.1627  loss_ce_5: 0.0003172  loss_mask_5: 0.2108  loss_dice_5: 0.1605  loss_ce_6: 0.0004463  loss_mask_6: 0.2118  loss_dice_6: 0.16  loss_ce_7: 0.0004026  loss_mask_7: 0.2138  loss_dice_7: 0.1574  loss_ce_8: 0.0004171  loss_mask_8: 0.2123  loss_dice_8: 0.1611  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:54:20] d2.utils.events INFO:  eta: 0:02:24  iter: 48859  total_loss: 3.625  loss_ce: 0.0007363  loss_mask: 0.1796  loss_dice: 0.1742  loss_ce_0: 0.0612  loss_mask_0: 0.1996  loss_dice_0: 0.1721  loss_ce_1: 0.000524  loss_mask_1: 0.1894  loss_dice_1: 0.1746  loss_ce_2: 0.0005881  loss_mask_2: 0.19  loss_dice_2: 0.1745  loss_ce_3: 0.0008173  loss_mask_3: 0.1802  loss_dice_3: 0.174  loss_ce_4: 0.0005525  loss_mask_4: 0.1906  loss_dice_4: 0.1796  loss_ce_5: 0.0007001  loss_mask_5: 0.1905  loss_dice_5: 0.1728  loss_ce_6: 0.0008688  loss_mask_6: 0.1855  loss_dice_6: 0.1813  loss_ce_7: 0.0005988  loss_mask_7: 0.1963  loss_dice_7: 0.1821  loss_ce_8: 0.0006985  loss_mask_8: 0.1931  loss_dice_8: 0.1687  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:54:22] d2.utils.events INFO:  eta: 0:02:22  iter: 48879  total_loss: 4.084  loss_ce: 0.001396  loss_mask: 0.1663  loss_dice: 0.1892  loss_ce_0: 0.06226  loss_mask_0: 0.1549  loss_dice_0: 0.1801  loss_ce_1: 0.001107  loss_mask_1: 0.1573  loss_dice_1: 0.1824  loss_ce_2: 0.001441  loss_mask_2: 0.1613  loss_dice_2: 0.1864  loss_ce_3: 0.001999  loss_mask_3: 0.1619  loss_dice_3: 0.1898  loss_ce_4: 0.00125  loss_mask_4: 0.1539  loss_dice_4: 0.1832  loss_ce_5: 0.001415  loss_mask_5: 0.1517  loss_dice_5: 0.1907  loss_ce_6: 0.001241  loss_mask_6: 0.1596  loss_dice_6: 0.1897  loss_ce_7: 0.001241  loss_mask_7: 0.1535  loss_dice_7: 0.1814  loss_ce_8: 0.001192  loss_mask_8: 0.1463  loss_dice_8: 0.1817  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:54:25] d2.utils.events INFO:  eta: 0:02:19  iter: 48899  total_loss: 4.007  loss_ce: 0.0004559  loss_mask: 0.2324  loss_dice: 0.1552  loss_ce_0: 0.06221  loss_mask_0: 0.2296  loss_dice_0: 0.1537  loss_ce_1: 0.0004458  loss_mask_1: 0.2285  loss_dice_1: 0.1531  loss_ce_2: 0.0004116  loss_mask_2: 0.2267  loss_dice_2: 0.1558  loss_ce_3: 0.0004703  loss_mask_3: 0.2287  loss_dice_3: 0.1615  loss_ce_4: 0.0004415  loss_mask_4: 0.2249  loss_dice_4: 0.1592  loss_ce_5: 0.0004386  loss_mask_5: 0.2293  loss_dice_5: 0.1516  loss_ce_6: 0.0004943  loss_mask_6: 0.2208  loss_dice_6: 0.16  loss_ce_7: 0.0004231  loss_mask_7: 0.224  loss_dice_7: 0.1597  loss_ce_8: 0.0004567  loss_mask_8: 0.2238  loss_dice_8: 0.1577  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:54:27] d2.utils.events INFO:  eta: 0:02:17  iter: 48919  total_loss: 4.095  loss_ce: 0.0004054  loss_mask: 0.186  loss_dice: 0.1941  loss_ce_0: 0.06131  loss_mask_0: 0.1887  loss_dice_0: 0.1988  loss_ce_1: 0.0004436  loss_mask_1: 0.1794  loss_dice_1: 0.1841  loss_ce_2: 0.0003317  loss_mask_2: 0.1807  loss_dice_2: 0.1876  loss_ce_3: 0.0004063  loss_mask_3: 0.1733  loss_dice_3: 0.1955  loss_ce_4: 0.0003553  loss_mask_4: 0.1882  loss_dice_4: 0.2018  loss_ce_5: 0.0004102  loss_mask_5: 0.1839  loss_dice_5: 0.1866  loss_ce_6: 0.0004414  loss_mask_6: 0.1846  loss_dice_6: 0.1856  loss_ce_7: 0.0004495  loss_mask_7: 0.1778  loss_dice_7: 0.1914  loss_ce_8: 0.0005321  loss_mask_8: 0.182  loss_dice_8: 0.1911  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:54:30] d2.utils.events INFO:  eta: 0:02:14  iter: 48939  total_loss: 4.254  loss_ce: 0.0006475  loss_mask: 0.1729  loss_dice: 0.1825  loss_ce_0: 0.06125  loss_mask_0: 0.1647  loss_dice_0: 0.1798  loss_ce_1: 0.000483  loss_mask_1: 0.1663  loss_dice_1: 0.1793  loss_ce_2: 0.0004353  loss_mask_2: 0.1653  loss_dice_2: 0.1753  loss_ce_3: 0.0006201  loss_mask_3: 0.166  loss_dice_3: 0.1775  loss_ce_4: 0.0004459  loss_mask_4: 0.1661  loss_dice_4: 0.1838  loss_ce_5: 0.0004756  loss_mask_5: 0.1726  loss_dice_5: 0.1782  loss_ce_6: 0.0005555  loss_mask_6: 0.1677  loss_dice_6: 0.1747  loss_ce_7: 0.0005667  loss_mask_7: 0.1654  loss_dice_7: 0.18  loss_ce_8: 0.0005618  loss_mask_8: 0.1606  loss_dice_8: 0.1839  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:54:32] d2.utils.events INFO:  eta: 0:02:12  iter: 48959  total_loss: 3.642  loss_ce: 0.0003771  loss_mask: 0.1848  loss_dice: 0.1697  loss_ce_0: 0.06126  loss_mask_0: 0.1915  loss_dice_0: 0.1733  loss_ce_1: 0.0003954  loss_mask_1: 0.1898  loss_dice_1: 0.1729  loss_ce_2: 0.000294  loss_mask_2: 0.1808  loss_dice_2: 0.1744  loss_ce_3: 0.0003853  loss_mask_3: 0.1858  loss_dice_3: 0.1777  loss_ce_4: 0.0003366  loss_mask_4: 0.1843  loss_dice_4: 0.179  loss_ce_5: 0.0003503  loss_mask_5: 0.1875  loss_dice_5: 0.1863  loss_ce_6: 0.0004344  loss_mask_6: 0.1777  loss_dice_6: 0.1753  loss_ce_7: 0.0004276  loss_mask_7: 0.1836  loss_dice_7: 0.1809  loss_ce_8: 0.000443  loss_mask_8: 0.1801  loss_dice_8: 0.1869  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:54:35] d2.utils.events INFO:  eta: 0:02:09  iter: 48979  total_loss: 4.256  loss_ce: 0.0003969  loss_mask: 0.2172  loss_dice: 0.1777  loss_ce_0: 0.06217  loss_mask_0: 0.2136  loss_dice_0: 0.1686  loss_ce_1: 0.0003865  loss_mask_1: 0.2097  loss_dice_1: 0.1727  loss_ce_2: 0.0002404  loss_mask_2: 0.2055  loss_dice_2: 0.1763  loss_ce_3: 0.000312  loss_mask_3: 0.2108  loss_dice_3: 0.175  loss_ce_4: 0.0003734  loss_mask_4: 0.2012  loss_dice_4: 0.1707  loss_ce_5: 0.0003056  loss_mask_5: 0.2122  loss_dice_5: 0.1797  loss_ce_6: 0.0004044  loss_mask_6: 0.2154  loss_dice_6: 0.1773  loss_ce_7: 0.0003373  loss_mask_7: 0.2198  loss_dice_7: 0.1751  loss_ce_8: 0.0004271  loss_mask_8: 0.2128  loss_dice_8: 0.1777  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:37] d2.utils.events INFO:  eta: 0:02:07  iter: 48999  total_loss: 3.863  loss_ce: 0.0003256  loss_mask: 0.1957  loss_dice: 0.1486  loss_ce_0: 0.06212  loss_mask_0: 0.1903  loss_dice_0: 0.1494  loss_ce_1: 0.0003349  loss_mask_1: 0.1978  loss_dice_1: 0.1537  loss_ce_2: 0.0002401  loss_mask_2: 0.1887  loss_dice_2: 0.1475  loss_ce_3: 0.0002982  loss_mask_3: 0.198  loss_dice_3: 0.1446  loss_ce_4: 0.0003174  loss_mask_4: 0.2009  loss_dice_4: 0.1487  loss_ce_5: 0.0003598  loss_mask_5: 0.2065  loss_dice_5: 0.1474  loss_ce_6: 0.0004019  loss_mask_6: 0.198  loss_dice_6: 0.1494  loss_ce_7: 0.0003682  loss_mask_7: 0.2022  loss_dice_7: 0.1448  loss_ce_8: 0.0004581  loss_mask_8: 0.1927  loss_dice_8: 0.1463  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:40] d2.utils.events INFO:  eta: 0:02:04  iter: 49019  total_loss: 4.312  loss_ce: 0.0005277  loss_mask: 0.2029  loss_dice: 0.1601  loss_ce_0: 0.0621  loss_mask_0: 0.2086  loss_dice_0: 0.1557  loss_ce_1: 0.0004356  loss_mask_1: 0.2061  loss_dice_1: 0.1577  loss_ce_2: 0.0003829  loss_mask_2: 0.208  loss_dice_2: 0.1584  loss_ce_3: 0.0005131  loss_mask_3: 0.2183  loss_dice_3: 0.1588  loss_ce_4: 0.0004008  loss_mask_4: 0.2139  loss_dice_4: 0.1535  loss_ce_5: 0.0004104  loss_mask_5: 0.2047  loss_dice_5: 0.1588  loss_ce_6: 0.0005032  loss_mask_6: 0.2058  loss_dice_6: 0.1543  loss_ce_7: 0.0004012  loss_mask_7: 0.2032  loss_dice_7: 0.155  loss_ce_8: 0.0004627  loss_mask_8: 0.2029  loss_dice_8: 0.1579  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:42] d2.utils.events INFO:  eta: 0:02:01  iter: 49039  total_loss: 4.163  loss_ce: 0.0005964  loss_mask: 0.1767  loss_dice: 0.1757  loss_ce_0: 0.06139  loss_mask_0: 0.1771  loss_dice_0: 0.18  loss_ce_1: 0.0005925  loss_mask_1: 0.1831  loss_dice_1: 0.1752  loss_ce_2: 0.0004052  loss_mask_2: 0.1787  loss_dice_2: 0.177  loss_ce_3: 0.0005466  loss_mask_3: 0.18  loss_dice_3: 0.1834  loss_ce_4: 0.0006705  loss_mask_4: 0.1767  loss_dice_4: 0.1793  loss_ce_5: 0.00056  loss_mask_5: 0.1804  loss_dice_5: 0.1827  loss_ce_6: 0.0005178  loss_mask_6: 0.1864  loss_dice_6: 0.1757  loss_ce_7: 0.0006038  loss_mask_7: 0.1761  loss_dice_7: 0.1786  loss_ce_8: 0.0005944  loss_mask_8: 0.1854  loss_dice_8: 0.1798  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:45] d2.utils.events INFO:  eta: 0:01:59  iter: 49059  total_loss: 4.077  loss_ce: 0.0003317  loss_mask: 0.2408  loss_dice: 0.153  loss_ce_0: 0.06144  loss_mask_0: 0.2389  loss_dice_0: 0.1524  loss_ce_1: 0.0003893  loss_mask_1: 0.2387  loss_dice_1: 0.1439  loss_ce_2: 0.0002965  loss_mask_2: 0.2423  loss_dice_2: 0.1415  loss_ce_3: 0.0003368  loss_mask_3: 0.2349  loss_dice_3: 0.1455  loss_ce_4: 0.0003411  loss_mask_4: 0.2351  loss_dice_4: 0.1469  loss_ce_5: 0.0003684  loss_mask_5: 0.2523  loss_dice_5: 0.149  loss_ce_6: 0.0004347  loss_mask_6: 0.2428  loss_dice_6: 0.1481  loss_ce_7: 0.0003378  loss_mask_7: 0.2434  loss_dice_7: 0.1472  loss_ce_8: 0.0004471  loss_mask_8: 0.2416  loss_dice_8: 0.1548  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:47] d2.utils.events INFO:  eta: 0:01:56  iter: 49079  total_loss: 4.082  loss_ce: 0.0003101  loss_mask: 0.2354  loss_dice: 0.1523  loss_ce_0: 0.06144  loss_mask_0: 0.2184  loss_dice_0: 0.1471  loss_ce_1: 0.0003403  loss_mask_1: 0.2339  loss_dice_1: 0.1523  loss_ce_2: 0.0002297  loss_mask_2: 0.2258  loss_dice_2: 0.15  loss_ce_3: 0.0003224  loss_mask_3: 0.2218  loss_dice_3: 0.1577  loss_ce_4: 0.000299  loss_mask_4: 0.2283  loss_dice_4: 0.1564  loss_ce_5: 0.0002811  loss_mask_5: 0.2257  loss_dice_5: 0.1489  loss_ce_6: 0.000406  loss_mask_6: 0.2211  loss_dice_6: 0.1551  loss_ce_7: 0.0002982  loss_mask_7: 0.2317  loss_dice_7: 0.1498  loss_ce_8: 0.0004185  loss_mask_8: 0.231  loss_dice_8: 0.1512  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:50] d2.utils.events INFO:  eta: 0:01:54  iter: 49099  total_loss: 3.968  loss_ce: 0.0004099  loss_mask: 0.2057  loss_dice: 0.1754  loss_ce_0: 0.06142  loss_mask_0: 0.2042  loss_dice_0: 0.173  loss_ce_1: 0.0004663  loss_mask_1: 0.2021  loss_dice_1: 0.1756  loss_ce_2: 0.000339  loss_mask_2: 0.2019  loss_dice_2: 0.1817  loss_ce_3: 0.0003919  loss_mask_3: 0.2043  loss_dice_3: 0.1738  loss_ce_4: 0.0003744  loss_mask_4: 0.2084  loss_dice_4: 0.1746  loss_ce_5: 0.0003863  loss_mask_5: 0.2086  loss_dice_5: 0.1708  loss_ce_6: 0.000432  loss_mask_6: 0.2041  loss_dice_6: 0.1755  loss_ce_7: 0.0004232  loss_mask_7: 0.1999  loss_dice_7: 0.1768  loss_ce_8: 0.0004818  loss_mask_8: 0.2021  loss_dice_8: 0.1753  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:52] d2.utils.events INFO:  eta: 0:01:51  iter: 49119  total_loss: 4.126  loss_ce: 0.0003594  loss_mask: 0.2102  loss_dice: 0.1666  loss_ce_0: 0.06144  loss_mask_0: 0.2015  loss_dice_0: 0.1668  loss_ce_1: 0.0003579  loss_mask_1: 0.2063  loss_dice_1: 0.1732  loss_ce_2: 0.0002576  loss_mask_2: 0.2083  loss_dice_2: 0.1665  loss_ce_3: 0.0003219  loss_mask_3: 0.1963  loss_dice_3: 0.1667  loss_ce_4: 0.0003094  loss_mask_4: 0.203  loss_dice_4: 0.1658  loss_ce_5: 0.0003139  loss_mask_5: 0.2079  loss_dice_5: 0.165  loss_ce_6: 0.000419  loss_mask_6: 0.2046  loss_dice_6: 0.1685  loss_ce_7: 0.000324  loss_mask_7: 0.2119  loss_dice_7: 0.166  loss_ce_8: 0.0004094  loss_mask_8: 0.2012  loss_dice_8: 0.1722  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:55] d2.utils.events INFO:  eta: 0:01:49  iter: 49139  total_loss: 4.12  loss_ce: 0.0003984  loss_mask: 0.2162  loss_dice: 0.1666  loss_ce_0: 0.06143  loss_mask_0: 0.2209  loss_dice_0: 0.167  loss_ce_1: 0.0003805  loss_mask_1: 0.231  loss_dice_1: 0.1691  loss_ce_2: 0.0003483  loss_mask_2: 0.2258  loss_dice_2: 0.1623  loss_ce_3: 0.0003822  loss_mask_3: 0.2202  loss_dice_3: 0.1654  loss_ce_4: 0.0003547  loss_mask_4: 0.2227  loss_dice_4: 0.1661  loss_ce_5: 0.0004039  loss_mask_5: 0.2221  loss_dice_5: 0.1687  loss_ce_6: 0.0004486  loss_mask_6: 0.2179  loss_dice_6: 0.1645  loss_ce_7: 0.0004021  loss_mask_7: 0.2121  loss_dice_7: 0.1641  loss_ce_8: 0.0004747  loss_mask_8: 0.2052  loss_dice_8: 0.1639  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:54:58] d2.utils.events INFO:  eta: 0:01:46  iter: 49159  total_loss: 3.966  loss_ce: 0.0004871  loss_mask: 0.202  loss_dice: 0.1579  loss_ce_0: 0.06199  loss_mask_0: 0.2021  loss_dice_0: 0.1626  loss_ce_1: 0.0004069  loss_mask_1: 0.208  loss_dice_1: 0.164  loss_ce_2: 0.0004419  loss_mask_2: 0.2003  loss_dice_2: 0.1617  loss_ce_3: 0.0005202  loss_mask_3: 0.2035  loss_dice_3: 0.1573  loss_ce_4: 0.0004426  loss_mask_4: 0.2066  loss_dice_4: 0.1664  loss_ce_5: 0.000536  loss_mask_5: 0.2006  loss_dice_5: 0.1572  loss_ce_6: 0.0005285  loss_mask_6: 0.2048  loss_dice_6: 0.1697  loss_ce_7: 0.0004912  loss_mask_7: 0.2048  loss_dice_7: 0.1629  loss_ce_8: 0.0005231  loss_mask_8: 0.2027  loss_dice_8: 0.1574  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:00] d2.utils.events INFO:  eta: 0:01:43  iter: 49179  total_loss: 4.15  loss_ce: 0.0005248  loss_mask: 0.1913  loss_dice: 0.2109  loss_ce_0: 0.06151  loss_mask_0: 0.1895  loss_dice_0: 0.2054  loss_ce_1: 0.0004717  loss_mask_1: 0.1945  loss_dice_1: 0.215  loss_ce_2: 0.0004487  loss_mask_2: 0.1966  loss_dice_2: 0.2068  loss_ce_3: 0.0005268  loss_mask_3: 0.1853  loss_dice_3: 0.2059  loss_ce_4: 0.0004749  loss_mask_4: 0.187  loss_dice_4: 0.2132  loss_ce_5: 0.0004837  loss_mask_5: 0.1906  loss_dice_5: 0.2112  loss_ce_6: 0.0005783  loss_mask_6: 0.1955  loss_dice_6: 0.2093  loss_ce_7: 0.0005288  loss_mask_7: 0.1989  loss_dice_7: 0.2064  loss_ce_8: 0.0005288  loss_mask_8: 0.1929  loss_dice_8: 0.2077  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:03] d2.utils.events INFO:  eta: 0:01:41  iter: 49199  total_loss: 4.551  loss_ce: 0.0004353  loss_mask: 0.2289  loss_dice: 0.1673  loss_ce_0: 0.06145  loss_mask_0: 0.2344  loss_dice_0: 0.1664  loss_ce_1: 0.0003717  loss_mask_1: 0.2413  loss_dice_1: 0.1704  loss_ce_2: 0.0003251  loss_mask_2: 0.2422  loss_dice_2: 0.1651  loss_ce_3: 0.0003946  loss_mask_3: 0.2443  loss_dice_3: 0.1687  loss_ce_4: 0.0003748  loss_mask_4: 0.2323  loss_dice_4: 0.1638  loss_ce_5: 0.0003831  loss_mask_5: 0.2369  loss_dice_5: 0.1651  loss_ce_6: 0.0004596  loss_mask_6: 0.231  loss_dice_6: 0.1681  loss_ce_7: 0.000347  loss_mask_7: 0.2286  loss_dice_7: 0.1638  loss_ce_8: 0.0004629  loss_mask_8: 0.237  loss_dice_8: 0.1647  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:05] d2.utils.events INFO:  eta: 0:01:38  iter: 49219  total_loss: 4.44  loss_ce: 0.0004551  loss_mask: 0.1665  loss_dice: 0.2077  loss_ce_0: 0.06143  loss_mask_0: 0.1588  loss_dice_0: 0.2001  loss_ce_1: 0.0003872  loss_mask_1: 0.1754  loss_dice_1: 0.1964  loss_ce_2: 0.0002973  loss_mask_2: 0.1678  loss_dice_2: 0.2063  loss_ce_3: 0.0004288  loss_mask_3: 0.1603  loss_dice_3: 0.1991  loss_ce_4: 0.0003372  loss_mask_4: 0.1665  loss_dice_4: 0.2  loss_ce_5: 0.0003692  loss_mask_5: 0.17  loss_dice_5: 0.197  loss_ce_6: 0.0004735  loss_mask_6: 0.1608  loss_dice_6: 0.2026  loss_ce_7: 0.0003936  loss_mask_7: 0.1604  loss_dice_7: 0.1922  loss_ce_8: 0.0004651  loss_mask_8: 0.1664  loss_dice_8: 0.2015  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:08] d2.utils.events INFO:  eta: 0:01:36  iter: 49239  total_loss: 3.892  loss_ce: 0.0007514  loss_mask: 0.1973  loss_dice: 0.1584  loss_ce_0: 0.06142  loss_mask_0: 0.199  loss_dice_0: 0.1614  loss_ce_1: 0.0004728  loss_mask_1: 0.1915  loss_dice_1: 0.1532  loss_ce_2: 0.000522  loss_mask_2: 0.1904  loss_dice_2: 0.1585  loss_ce_3: 0.0006528  loss_mask_3: 0.2073  loss_dice_3: 0.1524  loss_ce_4: 0.0005495  loss_mask_4: 0.1969  loss_dice_4: 0.1559  loss_ce_5: 0.0005892  loss_mask_5: 0.1961  loss_dice_5: 0.1597  loss_ce_6: 0.000612  loss_mask_6: 0.1912  loss_dice_6: 0.1539  loss_ce_7: 0.0006123  loss_mask_7: 0.1911  loss_dice_7: 0.1546  loss_ce_8: 0.0006077  loss_mask_8: 0.2131  loss_dice_8: 0.1588  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:10] d2.utils.events INFO:  eta: 0:01:33  iter: 49259  total_loss: 3.935  loss_ce: 0.0003526  loss_mask: 0.201  loss_dice: 0.1576  loss_ce_0: 0.06174  loss_mask_0: 0.2086  loss_dice_0: 0.1434  loss_ce_1: 0.0003716  loss_mask_1: 0.2073  loss_dice_1: 0.1533  loss_ce_2: 0.0002481  loss_mask_2: 0.2076  loss_dice_2: 0.1568  loss_ce_3: 0.000399  loss_mask_3: 0.2035  loss_dice_3: 0.1509  loss_ce_4: 0.0003354  loss_mask_4: 0.2043  loss_dice_4: 0.1524  loss_ce_5: 0.0003006  loss_mask_5: 0.2052  loss_dice_5: 0.1548  loss_ce_6: 0.000468  loss_mask_6: 0.2062  loss_dice_6: 0.1509  loss_ce_7: 0.0003575  loss_mask_7: 0.2061  loss_dice_7: 0.1544  loss_ce_8: 0.0004181  loss_mask_8: 0.2085  loss_dice_8: 0.1572  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:13] d2.utils.events INFO:  eta: 0:01:31  iter: 49279  total_loss: 4.446  loss_ce: 0.001409  loss_mask: 0.1771  loss_dice: 0.1828  loss_ce_0: 0.06143  loss_mask_0: 0.1802  loss_dice_0: 0.1867  loss_ce_1: 0.0008325  loss_mask_1: 0.191  loss_dice_1: 0.1836  loss_ce_2: 0.0009079  loss_mask_2: 0.1702  loss_dice_2: 0.1821  loss_ce_3: 0.00118  loss_mask_3: 0.1888  loss_dice_3: 0.1851  loss_ce_4: 0.001269  loss_mask_4: 0.1806  loss_dice_4: 0.1902  loss_ce_5: 0.0008854  loss_mask_5: 0.1791  loss_dice_5: 0.1883  loss_ce_6: 0.0009279  loss_mask_6: 0.1791  loss_dice_6: 0.1891  loss_ce_7: 0.001586  loss_mask_7: 0.1878  loss_dice_7: 0.18  loss_ce_8: 0.0007184  loss_mask_8: 0.1873  loss_dice_8: 0.1849  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:15] d2.utils.events INFO:  eta: 0:01:28  iter: 49299  total_loss: 4.142  loss_ce: 0.0004688  loss_mask: 0.1872  loss_dice: 0.1942  loss_ce_0: 0.06143  loss_mask_0: 0.1865  loss_dice_0: 0.1846  loss_ce_1: 0.0004264  loss_mask_1: 0.1899  loss_dice_1: 0.1861  loss_ce_2: 0.0003918  loss_mask_2: 0.1806  loss_dice_2: 0.1942  loss_ce_3: 0.0004865  loss_mask_3: 0.1836  loss_dice_3: 0.1972  loss_ce_4: 0.0004438  loss_mask_4: 0.1988  loss_dice_4: 0.1864  loss_ce_5: 0.0004323  loss_mask_5: 0.1898  loss_dice_5: 0.1993  loss_ce_6: 0.0005451  loss_mask_6: 0.1857  loss_dice_6: 0.1884  loss_ce_7: 0.0004566  loss_mask_7: 0.1902  loss_dice_7: 0.2009  loss_ce_8: 0.0004865  loss_mask_8: 0.1828  loss_dice_8: 0.1856  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:18] d2.utils.events INFO:  eta: 0:01:25  iter: 49319  total_loss: 3.383  loss_ce: 0.0003311  loss_mask: 0.1953  loss_dice: 0.1422  loss_ce_0: 0.06174  loss_mask_0: 0.1943  loss_dice_0: 0.1403  loss_ce_1: 0.0003216  loss_mask_1: 0.192  loss_dice_1: 0.1419  loss_ce_2: 0.0001745  loss_mask_2: 0.1868  loss_dice_2: 0.1453  loss_ce_3: 0.0003079  loss_mask_3: 0.1998  loss_dice_3: 0.1429  loss_ce_4: 0.0002748  loss_mask_4: 0.192  loss_dice_4: 0.1368  loss_ce_5: 0.0002646  loss_mask_5: 0.1933  loss_dice_5: 0.1393  loss_ce_6: 0.0003931  loss_mask_6: 0.1927  loss_dice_6: 0.1397  loss_ce_7: 0.0002795  loss_mask_7: 0.2001  loss_dice_7: 0.1375  loss_ce_8: 0.0003773  loss_mask_8: 0.1832  loss_dice_8: 0.1399  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:20] d2.utils.events INFO:  eta: 0:01:23  iter: 49339  total_loss: 4.03  loss_ce: 0.0003843  loss_mask: 0.2071  loss_dice: 0.1758  loss_ce_0: 0.06174  loss_mask_0: 0.2182  loss_dice_0: 0.1798  loss_ce_1: 0.0004344  loss_mask_1: 0.2203  loss_dice_1: 0.1873  loss_ce_2: 0.0003467  loss_mask_2: 0.2169  loss_dice_2: 0.1737  loss_ce_3: 0.0003906  loss_mask_3: 0.2205  loss_dice_3: 0.1698  loss_ce_4: 0.0003864  loss_mask_4: 0.218  loss_dice_4: 0.1831  loss_ce_5: 0.0003489  loss_mask_5: 0.2113  loss_dice_5: 0.1731  loss_ce_6: 0.0004961  loss_mask_6: 0.2172  loss_dice_6: 0.1716  loss_ce_7: 0.0004677  loss_mask_7: 0.2253  loss_dice_7: 0.1713  loss_ce_8: 0.0004476  loss_mask_8: 0.2189  loss_dice_8: 0.17  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:23] d2.utils.events INFO:  eta: 0:01:20  iter: 49359  total_loss: 3.873  loss_ce: 0.0003903  loss_mask: 0.177  loss_dice: 0.1779  loss_ce_0: 0.06142  loss_mask_0: 0.1804  loss_dice_0: 0.1828  loss_ce_1: 0.0004137  loss_mask_1: 0.1778  loss_dice_1: 0.1787  loss_ce_2: 0.0002984  loss_mask_2: 0.1813  loss_dice_2: 0.1874  loss_ce_3: 0.0003534  loss_mask_3: 0.1822  loss_dice_3: 0.1795  loss_ce_4: 0.0004109  loss_mask_4: 0.1882  loss_dice_4: 0.178  loss_ce_5: 0.0003904  loss_mask_5: 0.1834  loss_dice_5: 0.1771  loss_ce_6: 0.0004466  loss_mask_6: 0.1791  loss_dice_6: 0.1813  loss_ce_7: 0.0004964  loss_mask_7: 0.179  loss_dice_7: 0.1775  loss_ce_8: 0.0004748  loss_mask_8: 0.1781  loss_dice_8: 0.1858  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:25] d2.utils.events INFO:  eta: 0:01:18  iter: 49379  total_loss: 3.979  loss_ce: 0.0004869  loss_mask: 0.1805  loss_dice: 0.2152  loss_ce_0: 0.06205  loss_mask_0: 0.184  loss_dice_0: 0.2127  loss_ce_1: 0.000629  loss_mask_1: 0.1854  loss_dice_1: 0.2155  loss_ce_2: 0.0005751  loss_mask_2: 0.1871  loss_dice_2: 0.2149  loss_ce_3: 0.000448  loss_mask_3: 0.1814  loss_dice_3: 0.2205  loss_ce_4: 0.0005274  loss_mask_4: 0.1894  loss_dice_4: 0.2281  loss_ce_5: 0.0005694  loss_mask_5: 0.1854  loss_dice_5: 0.2246  loss_ce_6: 0.0005002  loss_mask_6: 0.1806  loss_dice_6: 0.2161  loss_ce_7: 0.0007119  loss_mask_7: 0.1842  loss_dice_7: 0.2182  loss_ce_8: 0.0007005  loss_mask_8: 0.1844  loss_dice_8: 0.2306  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:28] d2.utils.events INFO:  eta: 0:01:15  iter: 49399  total_loss: 3.733  loss_ce: 0.0006717  loss_mask: 0.1637  loss_dice: 0.1498  loss_ce_0: 0.06174  loss_mask_0: 0.159  loss_dice_0: 0.1519  loss_ce_1: 0.000486  loss_mask_1: 0.1617  loss_dice_1: 0.1581  loss_ce_2: 0.0005644  loss_mask_2: 0.1657  loss_dice_2: 0.1552  loss_ce_3: 0.0005637  loss_mask_3: 0.1645  loss_dice_3: 0.1439  loss_ce_4: 0.0005183  loss_mask_4: 0.1708  loss_dice_4: 0.1473  loss_ce_5: 0.0007022  loss_mask_5: 0.1626  loss_dice_5: 0.1548  loss_ce_6: 0.0005974  loss_mask_6: 0.16  loss_dice_6: 0.1463  loss_ce_7: 0.0006428  loss_mask_7: 0.1692  loss_dice_7: 0.1495  loss_ce_8: 0.0007153  loss_mask_8: 0.1588  loss_dice_8: 0.1561  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:31] d2.utils.events INFO:  eta: 0:01:13  iter: 49419  total_loss: 3.812  loss_ce: 0.0003997  loss_mask: 0.169  loss_dice: 0.1719  loss_ce_0: 0.06194  loss_mask_0: 0.1719  loss_dice_0: 0.1811  loss_ce_1: 0.0003554  loss_mask_1: 0.1793  loss_dice_1: 0.173  loss_ce_2: 0.0002825  loss_mask_2: 0.1648  loss_dice_2: 0.1709  loss_ce_3: 0.0003774  loss_mask_3: 0.1648  loss_dice_3: 0.1725  loss_ce_4: 0.0003417  loss_mask_4: 0.1706  loss_dice_4: 0.1703  loss_ce_5: 0.0003615  loss_mask_5: 0.1629  loss_dice_5: 0.178  loss_ce_6: 0.0004461  loss_mask_6: 0.1749  loss_dice_6: 0.1733  loss_ce_7: 0.0003822  loss_mask_7: 0.1714  loss_dice_7: 0.1706  loss_ce_8: 0.0004544  loss_mask_8: 0.1655  loss_dice_8: 0.1805  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:33] d2.utils.events INFO:  eta: 0:01:10  iter: 49439  total_loss: 4.125  loss_ce: 0.0004409  loss_mask: 0.1971  loss_dice: 0.1825  loss_ce_0: 0.06174  loss_mask_0: 0.2044  loss_dice_0: 0.182  loss_ce_1: 0.0003983  loss_mask_1: 0.2012  loss_dice_1: 0.1876  loss_ce_2: 0.0002791  loss_mask_2: 0.1963  loss_dice_2: 0.1864  loss_ce_3: 0.0003946  loss_mask_3: 0.1928  loss_dice_3: 0.1837  loss_ce_4: 0.0003676  loss_mask_4: 0.1989  loss_dice_4: 0.1876  loss_ce_5: 0.0003377  loss_mask_5: 0.2058  loss_dice_5: 0.1726  loss_ce_6: 0.0004568  loss_mask_6: 0.2025  loss_dice_6: 0.1809  loss_ce_7: 0.0004151  loss_mask_7: 0.2052  loss_dice_7: 0.1827  loss_ce_8: 0.0004362  loss_mask_8: 0.1934  loss_dice_8: 0.1796  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:36] d2.utils.events INFO:  eta: 0:01:08  iter: 49459  total_loss: 3.988  loss_ce: 0.0005001  loss_mask: 0.1793  loss_dice: 0.1567  loss_ce_0: 0.06174  loss_mask_0: 0.1739  loss_dice_0: 0.1534  loss_ce_1: 0.0003916  loss_mask_1: 0.1839  loss_dice_1: 0.1565  loss_ce_2: 0.0003877  loss_mask_2: 0.1844  loss_dice_2: 0.1576  loss_ce_3: 0.0005642  loss_mask_3: 0.1698  loss_dice_3: 0.1571  loss_ce_4: 0.0004075  loss_mask_4: 0.1804  loss_dice_4: 0.1588  loss_ce_5: 0.0004371  loss_mask_5: 0.176  loss_dice_5: 0.1599  loss_ce_6: 0.0005665  loss_mask_6: 0.1713  loss_dice_6: 0.1497  loss_ce_7: 0.0004141  loss_mask_7: 0.1776  loss_dice_7: 0.1599  loss_ce_8: 0.0004759  loss_mask_8: 0.1804  loss_dice_8: 0.1572  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:38] d2.utils.events INFO:  eta: 0:01:05  iter: 49479  total_loss: 4.281  loss_ce: 0.0003164  loss_mask: 0.2068  loss_dice: 0.1755  loss_ce_0: 0.06164  loss_mask_0: 0.217  loss_dice_0: 0.1804  loss_ce_1: 0.0003366  loss_mask_1: 0.2089  loss_dice_1: 0.171  loss_ce_2: 0.0002101  loss_mask_2: 0.2147  loss_dice_2: 0.1773  loss_ce_3: 0.0003316  loss_mask_3: 0.2083  loss_dice_3: 0.1772  loss_ce_4: 0.0002772  loss_mask_4: 0.2217  loss_dice_4: 0.1735  loss_ce_5: 0.000284  loss_mask_5: 0.2045  loss_dice_5: 0.171  loss_ce_6: 0.0004109  loss_mask_6: 0.2143  loss_dice_6: 0.1712  loss_ce_7: 0.0003351  loss_mask_7: 0.2158  loss_dice_7: 0.1767  loss_ce_8: 0.0003865  loss_mask_8: 0.2077  loss_dice_8: 0.1731  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:55:41] d2.utils.events INFO:  eta: 0:01:03  iter: 49499  total_loss: 4.549  loss_ce: 0.0004156  loss_mask: 0.1982  loss_dice: 0.2131  loss_ce_0: 0.06162  loss_mask_0: 0.198  loss_dice_0: 0.2139  loss_ce_1: 0.0003642  loss_mask_1: 0.2061  loss_dice_1: 0.2174  loss_ce_2: 0.0002898  loss_mask_2: 0.2006  loss_dice_2: 0.2114  loss_ce_3: 0.0004122  loss_mask_3: 0.1978  loss_dice_3: 0.2046  loss_ce_4: 0.000393  loss_mask_4: 0.1977  loss_dice_4: 0.2087  loss_ce_5: 0.0003989  loss_mask_5: 0.2009  loss_dice_5: 0.2078  loss_ce_6: 0.0004704  loss_mask_6: 0.1985  loss_dice_6: 0.2064  loss_ce_7: 0.0004458  loss_mask_7: 0.1959  loss_dice_7: 0.2104  loss_ce_8: 0.0004714  loss_mask_8: 0.2026  loss_dice_8: 0.2117  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:55:43] d2.utils.events INFO:  eta: 0:01:00  iter: 49519  total_loss: 4.056  loss_ce: 0.0004477  loss_mask: 0.1784  loss_dice: 0.1457  loss_ce_0: 0.06174  loss_mask_0: 0.186  loss_dice_0: 0.1478  loss_ce_1: 0.0003741  loss_mask_1: 0.1842  loss_dice_1: 0.1509  loss_ce_2: 0.0003323  loss_mask_2: 0.1837  loss_dice_2: 0.1494  loss_ce_3: 0.0004604  loss_mask_3: 0.1905  loss_dice_3: 0.1421  loss_ce_4: 0.0003744  loss_mask_4: 0.1819  loss_dice_4: 0.1498  loss_ce_5: 0.0003805  loss_mask_5: 0.1851  loss_dice_5: 0.1507  loss_ce_6: 0.0004717  loss_mask_6: 0.1741  loss_dice_6: 0.1536  loss_ce_7: 0.0004532  loss_mask_7: 0.1904  loss_dice_7: 0.1532  loss_ce_8: 0.0004775  loss_mask_8: 0.1857  loss_dice_8: 0.1495  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:55:46] d2.utils.events INFO:  eta: 0:00:58  iter: 49539  total_loss: 4.09  loss_ce: 0.00039  loss_mask: 0.1981  loss_dice: 0.1636  loss_ce_0: 0.06188  loss_mask_0: 0.193  loss_dice_0: 0.1682  loss_ce_1: 0.0003779  loss_mask_1: 0.2038  loss_dice_1: 0.1576  loss_ce_2: 0.0003017  loss_mask_2: 0.1995  loss_dice_2: 0.1654  loss_ce_3: 0.0003167  loss_mask_3: 0.1982  loss_dice_3: 0.1692  loss_ce_4: 0.0004144  loss_mask_4: 0.2027  loss_dice_4: 0.1589  loss_ce_5: 0.0003391  loss_mask_5: 0.2011  loss_dice_5: 0.1691  loss_ce_6: 0.0004332  loss_mask_6: 0.191  loss_dice_6: 0.1665  loss_ce_7: 0.0004969  loss_mask_7: 0.1955  loss_dice_7: 0.1634  loss_ce_8: 0.0004243  loss_mask_8: 0.1978  loss_dice_8: 0.1617  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:55:48] d2.utils.events INFO:  eta: 0:00:55  iter: 49559  total_loss: 4.086  loss_ce: 0.0004408  loss_mask: 0.2154  loss_dice: 0.1572  loss_ce_0: 0.06174  loss_mask_0: 0.2062  loss_dice_0: 0.1775  loss_ce_1: 0.0004409  loss_mask_1: 0.2103  loss_dice_1: 0.1731  loss_ce_2: 0.0003957  loss_mask_2: 0.2007  loss_dice_2: 0.1604  loss_ce_3: 0.0004544  loss_mask_3: 0.1969  loss_dice_3: 0.1699  loss_ce_4: 0.000461  loss_mask_4: 0.1948  loss_dice_4: 0.1611  loss_ce_5: 0.0004127  loss_mask_5: 0.2064  loss_dice_5: 0.1717  loss_ce_6: 0.000509  loss_mask_6: 0.209  loss_dice_6: 0.1715  loss_ce_7: 0.0004462  loss_mask_7: 0.2096  loss_dice_7: 0.1691  loss_ce_8: 0.0004657  loss_mask_8: 0.1972  loss_dice_8: 0.1648  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:55:51] d2.utils.events INFO:  eta: 0:00:53  iter: 49579  total_loss: 4.176  loss_ce: 0.00127  loss_mask: 0.1953  loss_dice: 0.1756  loss_ce_0: 0.06174  loss_mask_0: 0.1934  loss_dice_0: 0.1854  loss_ce_1: 0.0007632  loss_mask_1: 0.1923  loss_dice_1: 0.1798  loss_ce_2: 0.0009437  loss_mask_2: 0.1921  loss_dice_2: 0.176  loss_ce_3: 0.001095  loss_mask_3: 0.1913  loss_dice_3: 0.1757  loss_ce_4: 0.0009291  loss_mask_4: 0.1896  loss_dice_4: 0.1757  loss_ce_5: 0.000943  loss_mask_5: 0.1881  loss_dice_5: 0.1754  loss_ce_6: 0.0008376  loss_mask_6: 0.1883  loss_dice_6: 0.1668  loss_ce_7: 0.0008533  loss_mask_7: 0.1898  loss_dice_7: 0.1814  loss_ce_8: 0.0007977  loss_mask_8: 0.1898  loss_dice_8: 0.1779  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:55:53] d2.utils.events INFO:  eta: 0:00:50  iter: 49599  total_loss: 4.542  loss_ce: 0.000407  loss_mask: 0.2486  loss_dice: 0.1817  loss_ce_0: 0.06165  loss_mask_0: 0.2631  loss_dice_0: 0.1916  loss_ce_1: 0.0003631  loss_mask_1: 0.2523  loss_dice_1: 0.1945  loss_ce_2: 0.0002507  loss_mask_2: 0.2436  loss_dice_2: 0.1902  loss_ce_3: 0.0003664  loss_mask_3: 0.2558  loss_dice_3: 0.1809  loss_ce_4: 0.0003269  loss_mask_4: 0.2472  loss_dice_4: 0.1838  loss_ce_5: 0.0003391  loss_mask_5: 0.2528  loss_dice_5: 0.1899  loss_ce_6: 0.0004649  loss_mask_6: 0.2554  loss_dice_6: 0.1886  loss_ce_7: 0.0003391  loss_mask_7: 0.2572  loss_dice_7: 0.1823  loss_ce_8: 0.0004291  loss_mask_8: 0.2459  loss_dice_8: 0.1783  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:56] d2.utils.events INFO:  eta: 0:00:48  iter: 49619  total_loss: 5.329  loss_ce: 0.000597  loss_mask: 0.2142  loss_dice: 0.1909  loss_ce_0: 0.06158  loss_mask_0: 0.2128  loss_dice_0: 0.1851  loss_ce_1: 0.0005097  loss_mask_1: 0.2108  loss_dice_1: 0.2015  loss_ce_2: 0.0004836  loss_mask_2: 0.2258  loss_dice_2: 0.1965  loss_ce_3: 0.0005069  loss_mask_3: 0.2077  loss_dice_3: 0.1966  loss_ce_4: 0.0006655  loss_mask_4: 0.2168  loss_dice_4: 0.1904  loss_ce_5: 0.0004932  loss_mask_5: 0.2134  loss_dice_5: 0.189  loss_ce_6: 0.0005376  loss_mask_6: 0.2101  loss_dice_6: 0.1997  loss_ce_7: 0.0006199  loss_mask_7: 0.2158  loss_dice_7: 0.1934  loss_ce_8: 0.0005101  loss_mask_8: 0.2054  loss_dice_8: 0.1974  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:55:59] d2.utils.events INFO:  eta: 0:00:45  iter: 49639  total_loss: 3.983  loss_ce: 0.0003825  loss_mask: 0.1953  loss_dice: 0.1487  loss_ce_0: 0.06175  loss_mask_0: 0.1838  loss_dice_0: 0.1518  loss_ce_1: 0.0003859  loss_mask_1: 0.1983  loss_dice_1: 0.1529  loss_ce_2: 0.0003629  loss_mask_2: 0.1932  loss_dice_2: 0.1543  loss_ce_3: 0.0003529  loss_mask_3: 0.1979  loss_dice_3: 0.1486  loss_ce_4: 0.0003766  loss_mask_4: 0.2025  loss_dice_4: 0.151  loss_ce_5: 0.0004016  loss_mask_5: 0.1984  loss_dice_5: 0.1476  loss_ce_6: 0.0004259  loss_mask_6: 0.2026  loss_dice_6: 0.1511  loss_ce_7: 0.0003762  loss_mask_7: 0.1959  loss_dice_7: 0.1553  loss_ce_8: 0.0004882  loss_mask_8: 0.1932  loss_dice_8: 0.1514  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:01] d2.utils.events INFO:  eta: 0:00:43  iter: 49659  total_loss: 4.173  loss_ce: 0.0006808  loss_mask: 0.2165  loss_dice: 0.1583  loss_ce_0: 0.06152  loss_mask_0: 0.2044  loss_dice_0: 0.1558  loss_ce_1: 0.0006749  loss_mask_1: 0.2197  loss_dice_1: 0.1672  loss_ce_2: 0.0004843  loss_mask_2: 0.2189  loss_dice_2: 0.1586  loss_ce_3: 0.0005449  loss_mask_3: 0.2197  loss_dice_3: 0.1587  loss_ce_4: 0.0008349  loss_mask_4: 0.2137  loss_dice_4: 0.1561  loss_ce_5: 0.0004724  loss_mask_5: 0.2135  loss_dice_5: 0.157  loss_ce_6: 0.0005185  loss_mask_6: 0.2126  loss_dice_6: 0.1561  loss_ce_7: 0.0007299  loss_mask_7: 0.2149  loss_dice_7: 0.1628  loss_ce_8: 0.0006325  loss_mask_8: 0.2183  loss_dice_8: 0.1557  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:04] d2.utils.events INFO:  eta: 0:00:40  iter: 49679  total_loss: 4.209  loss_ce: 0.0004552  loss_mask: 0.177  loss_dice: 0.1781  loss_ce_0: 0.06149  loss_mask_0: 0.1719  loss_dice_0: 0.1824  loss_ce_1: 0.0004039  loss_mask_1: 0.1743  loss_dice_1: 0.1742  loss_ce_2: 0.0002875  loss_mask_2: 0.1723  loss_dice_2: 0.1729  loss_ce_3: 0.0004101  loss_mask_3: 0.1723  loss_dice_3: 0.1749  loss_ce_4: 0.0003544  loss_mask_4: 0.1707  loss_dice_4: 0.176  loss_ce_5: 0.0003273  loss_mask_5: 0.1704  loss_dice_5: 0.176  loss_ce_6: 0.0004731  loss_mask_6: 0.1746  loss_dice_6: 0.1742  loss_ce_7: 0.0003734  loss_mask_7: 0.1759  loss_dice_7: 0.1784  loss_ce_8: 0.0004378  loss_mask_8: 0.1788  loss_dice_8: 0.1753  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:06] d2.utils.events INFO:  eta: 0:00:38  iter: 49699  total_loss: 4.061  loss_ce: 0.0003033  loss_mask: 0.1935  loss_dice: 0.2097  loss_ce_0: 0.06143  loss_mask_0: 0.1925  loss_dice_0: 0.2095  loss_ce_1: 0.0003437  loss_mask_1: 0.1832  loss_dice_1: 0.2067  loss_ce_2: 0.0001929  loss_mask_2: 0.1971  loss_dice_2: 0.2135  loss_ce_3: 0.000329  loss_mask_3: 0.1865  loss_dice_3: 0.212  loss_ce_4: 0.0002736  loss_mask_4: 0.1985  loss_dice_4: 0.2076  loss_ce_5: 0.0002685  loss_mask_5: 0.1974  loss_dice_5: 0.2088  loss_ce_6: 0.0004107  loss_mask_6: 0.1912  loss_dice_6: 0.2129  loss_ce_7: 0.0003411  loss_mask_7: 0.1904  loss_dice_7: 0.2071  loss_ce_8: 0.0003874  loss_mask_8: 0.1888  loss_dice_8: 0.2031  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:09] d2.utils.events INFO:  eta: 0:00:35  iter: 49719  total_loss: 4.007  loss_ce: 0.0004299  loss_mask: 0.2106  loss_dice: 0.1656  loss_ce_0: 0.06211  loss_mask_0: 0.1982  loss_dice_0: 0.1674  loss_ce_1: 0.0003935  loss_mask_1: 0.2096  loss_dice_1: 0.1668  loss_ce_2: 0.0003509  loss_mask_2: 0.206  loss_dice_2: 0.1666  loss_ce_3: 0.0004337  loss_mask_3: 0.1993  loss_dice_3: 0.1665  loss_ce_4: 0.0004712  loss_mask_4: 0.2066  loss_dice_4: 0.174  loss_ce_5: 0.0004197  loss_mask_5: 0.2144  loss_dice_5: 0.16  loss_ce_6: 0.0005397  loss_mask_6: 0.2062  loss_dice_6: 0.163  loss_ce_7: 0.0005624  loss_mask_7: 0.2109  loss_dice_7: 0.1692  loss_ce_8: 0.0004771  loss_mask_8: 0.2097  loss_dice_8: 0.1609  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:11] d2.utils.events INFO:  eta: 0:00:32  iter: 49739  total_loss: 4.078  loss_ce: 0.0003675  loss_mask: 0.2008  loss_dice: 0.1674  loss_ce_0: 0.06143  loss_mask_0: 0.2008  loss_dice_0: 0.1663  loss_ce_1: 0.0004101  loss_mask_1: 0.2115  loss_dice_1: 0.17  loss_ce_2: 0.0003157  loss_mask_2: 0.2042  loss_dice_2: 0.1624  loss_ce_3: 0.0003946  loss_mask_3: 0.2095  loss_dice_3: 0.1681  loss_ce_4: 0.0004275  loss_mask_4: 0.2012  loss_dice_4: 0.1651  loss_ce_5: 0.0003814  loss_mask_5: 0.1966  loss_dice_5: 0.1638  loss_ce_6: 0.0004397  loss_mask_6: 0.2028  loss_dice_6: 0.1595  loss_ce_7: 0.0004299  loss_mask_7: 0.2042  loss_dice_7: 0.1701  loss_ce_8: 0.0004477  loss_mask_8: 0.212  loss_dice_8: 0.1691  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:14] d2.utils.events INFO:  eta: 0:00:30  iter: 49759  total_loss: 4.284  loss_ce: 0.0003782  loss_mask: 0.1942  loss_dice: 0.1794  loss_ce_0: 0.06143  loss_mask_0: 0.1894  loss_dice_0: 0.1781  loss_ce_1: 0.0004125  loss_mask_1: 0.1945  loss_dice_1: 0.1757  loss_ce_2: 0.0003536  loss_mask_2: 0.1943  loss_dice_2: 0.1735  loss_ce_3: 0.0003408  loss_mask_3: 0.1907  loss_dice_3: 0.1702  loss_ce_4: 0.0003786  loss_mask_4: 0.1947  loss_dice_4: 0.18  loss_ce_5: 0.0003961  loss_mask_5: 0.198  loss_dice_5: 0.1813  loss_ce_6: 0.0004054  loss_mask_6: 0.1874  loss_dice_6: 0.1726  loss_ce_7: 0.0004057  loss_mask_7: 0.1906  loss_dice_7: 0.1844  loss_ce_8: 0.0004312  loss_mask_8: 0.1928  loss_dice_8: 0.1818  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:16] d2.utils.events INFO:  eta: 0:00:27  iter: 49779  total_loss: 4.246  loss_ce: 0.0007415  loss_mask: 0.2133  loss_dice: 0.1732  loss_ce_0: 0.06174  loss_mask_0: 0.2168  loss_dice_0: 0.1667  loss_ce_1: 0.0004892  loss_mask_1: 0.2136  loss_dice_1: 0.1637  loss_ce_2: 0.0005652  loss_mask_2: 0.2151  loss_dice_2: 0.1643  loss_ce_3: 0.0007472  loss_mask_3: 0.2209  loss_dice_3: 0.1729  loss_ce_4: 0.0005476  loss_mask_4: 0.2048  loss_dice_4: 0.1599  loss_ce_5: 0.0006647  loss_mask_5: 0.2083  loss_dice_5: 0.1703  loss_ce_6: 0.0007314  loss_mask_6: 0.2156  loss_dice_6: 0.1725  loss_ce_7: 0.0005624  loss_mask_7: 0.2139  loss_dice_7: 0.1695  loss_ce_8: 0.0006297  loss_mask_8: 0.2072  loss_dice_8: 0.1652  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:19] d2.utils.events INFO:  eta: 0:00:25  iter: 49799  total_loss: 4.404  loss_ce: 0.0006138  loss_mask: 0.2037  loss_dice: 0.186  loss_ce_0: 0.0614  loss_mask_0: 0.2157  loss_dice_0: 0.1892  loss_ce_1: 0.0004606  loss_mask_1: 0.203  loss_dice_1: 0.1932  loss_ce_2: 0.000599  loss_mask_2: 0.2067  loss_dice_2: 0.187  loss_ce_3: 0.00058  loss_mask_3: 0.2071  loss_dice_3: 0.1889  loss_ce_4: 0.0005078  loss_mask_4: 0.2037  loss_dice_4: 0.1834  loss_ce_5: 0.0006325  loss_mask_5: 0.2063  loss_dice_5: 0.1853  loss_ce_6: 0.000571  loss_mask_6: 0.2038  loss_dice_6: 0.1836  loss_ce_7: 0.0004667  loss_mask_7: 0.2019  loss_dice_7: 0.1857  loss_ce_8: 0.0006033  loss_mask_8: 0.1986  loss_dice_8: 0.1845  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:21] d2.utils.events INFO:  eta: 0:00:22  iter: 49819  total_loss: 4.196  loss_ce: 0.0003341  loss_mask: 0.2225  loss_dice: 0.1819  loss_ce_0: 0.06136  loss_mask_0: 0.2193  loss_dice_0: 0.178  loss_ce_1: 0.0003388  loss_mask_1: 0.2175  loss_dice_1: 0.1799  loss_ce_2: 0.0002253  loss_mask_2: 0.2261  loss_dice_2: 0.1763  loss_ce_3: 0.0003236  loss_mask_3: 0.2191  loss_dice_3: 0.177  loss_ce_4: 0.0003029  loss_mask_4: 0.2194  loss_dice_4: 0.1794  loss_ce_5: 0.0002963  loss_mask_5: 0.2236  loss_dice_5: 0.1768  loss_ce_6: 0.000387  loss_mask_6: 0.2152  loss_dice_6: 0.1787  loss_ce_7: 0.0002977  loss_mask_7: 0.2142  loss_dice_7: 0.1774  loss_ce_8: 0.0003977  loss_mask_8: 0.2212  loss_dice_8: 0.1756  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:24] d2.utils.events INFO:  eta: 0:00:20  iter: 49839  total_loss: 4.219  loss_ce: 0.0004598  loss_mask: 0.2051  loss_dice: 0.1734  loss_ce_0: 0.06207  loss_mask_0: 0.2082  loss_dice_0: 0.1611  loss_ce_1: 0.0003556  loss_mask_1: 0.206  loss_dice_1: 0.169  loss_ce_2: 0.0003441  loss_mask_2: 0.2026  loss_dice_2: 0.1649  loss_ce_3: 0.0004035  loss_mask_3: 0.2137  loss_dice_3: 0.163  loss_ce_4: 0.0003868  loss_mask_4: 0.2068  loss_dice_4: 0.1665  loss_ce_5: 0.0003992  loss_mask_5: 0.202  loss_dice_5: 0.1658  loss_ce_6: 0.0004646  loss_mask_6: 0.2059  loss_dice_6: 0.162  loss_ce_7: 0.0004168  loss_mask_7: 0.2014  loss_dice_7: 0.1659  loss_ce_8: 0.0004771  loss_mask_8: 0.2129  loss_dice_8: 0.1695  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:26] d2.utils.events INFO:  eta: 0:00:17  iter: 49859  total_loss: 4.481  loss_ce: 0.001554  loss_mask: 0.1499  loss_dice: 0.1936  loss_ce_0: 0.06143  loss_mask_0: 0.1496  loss_dice_0: 0.1935  loss_ce_1: 0.00122  loss_mask_1: 0.1544  loss_dice_1: 0.1949  loss_ce_2: 0.001365  loss_mask_2: 0.1421  loss_dice_2: 0.1994  loss_ce_3: 0.0017  loss_mask_3: 0.156  loss_dice_3: 0.1873  loss_ce_4: 0.001551  loss_mask_4: 0.1504  loss_dice_4: 0.1931  loss_ce_5: 0.001308  loss_mask_5: 0.1553  loss_dice_5: 0.1981  loss_ce_6: 0.001445  loss_mask_6: 0.1554  loss_dice_6: 0.1975  loss_ce_7: 0.001391  loss_mask_7: 0.1554  loss_dice_7: 0.1956  loss_ce_8: 0.001595  loss_mask_8: 0.1565  loss_dice_8: 0.1979  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:29] d2.utils.events INFO:  eta: 0:00:15  iter: 49879  total_loss: 4.186  loss_ce: 0.0004005  loss_mask: 0.2102  loss_dice: 0.1958  loss_ce_0: 0.06143  loss_mask_0: 0.2055  loss_dice_0: 0.1891  loss_ce_1: 0.0003796  loss_mask_1: 0.211  loss_dice_1: 0.178  loss_ce_2: 0.000276  loss_mask_2: 0.199  loss_dice_2: 0.1822  loss_ce_3: 0.0003425  loss_mask_3: 0.2056  loss_dice_3: 0.1818  loss_ce_4: 0.0003436  loss_mask_4: 0.2005  loss_dice_4: 0.182  loss_ce_5: 0.0003773  loss_mask_5: 0.2056  loss_dice_5: 0.1821  loss_ce_6: 0.0004376  loss_mask_6: 0.2046  loss_dice_6: 0.1878  loss_ce_7: 0.0003989  loss_mask_7: 0.2051  loss_dice_7: 0.1821  loss_ce_8: 0.0004831  loss_mask_8: 0.2032  loss_dice_8: 0.1941  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:31] d2.utils.events INFO:  eta: 0:00:12  iter: 49899  total_loss: 3.795  loss_ce: 0.0004189  loss_mask: 0.1965  loss_dice: 0.1695  loss_ce_0: 0.06143  loss_mask_0: 0.2005  loss_dice_0: 0.1596  loss_ce_1: 0.0004396  loss_mask_1: 0.2111  loss_dice_1: 0.1706  loss_ce_2: 0.0003752  loss_mask_2: 0.2074  loss_dice_2: 0.1593  loss_ce_3: 0.0004151  loss_mask_3: 0.2005  loss_dice_3: 0.1608  loss_ce_4: 0.0004082  loss_mask_4: 0.2048  loss_dice_4: 0.1554  loss_ce_5: 0.000419  loss_mask_5: 0.2036  loss_dice_5: 0.1637  loss_ce_6: 0.0004635  loss_mask_6: 0.2053  loss_dice_6: 0.1655  loss_ce_7: 0.0004284  loss_mask_7: 0.2028  loss_dice_7: 0.1547  loss_ce_8: 0.000492  loss_mask_8: 0.2021  loss_dice_8: 0.1602  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:34] d2.utils.events INFO:  eta: 0:00:10  iter: 49919  total_loss: 4.442  loss_ce: 0.0003751  loss_mask: 0.2231  loss_dice: 0.1811  loss_ce_0: 0.06141  loss_mask_0: 0.22  loss_dice_0: 0.176  loss_ce_1: 0.0003976  loss_mask_1: 0.2172  loss_dice_1: 0.1715  loss_ce_2: 0.0003144  loss_mask_2: 0.2207  loss_dice_2: 0.1813  loss_ce_3: 0.0003708  loss_mask_3: 0.2242  loss_dice_3: 0.1788  loss_ce_4: 0.0003431  loss_mask_4: 0.2238  loss_dice_4: 0.1811  loss_ce_5: 0.0003243  loss_mask_5: 0.2202  loss_dice_5: 0.1765  loss_ce_6: 0.0004608  loss_mask_6: 0.2124  loss_dice_6: 0.1778  loss_ce_7: 0.0003725  loss_mask_7: 0.2158  loss_dice_7: 0.1831  loss_ce_8: 0.0004127  loss_mask_8: 0.2116  loss_dice_8: 0.1792  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:36] d2.utils.events INFO:  eta: 0:00:07  iter: 49939  total_loss: 3.846  loss_ce: 0.0005602  loss_mask: 0.1739  loss_dice: 0.2076  loss_ce_0: 0.06175  loss_mask_0: 0.1676  loss_dice_0: 0.2123  loss_ce_1: 0.000745  loss_mask_1: 0.1585  loss_dice_1: 0.2005  loss_ce_2: 0.0005488  loss_mask_2: 0.168  loss_dice_2: 0.1959  loss_ce_3: 0.0006454  loss_mask_3: 0.1691  loss_dice_3: 0.2026  loss_ce_4: 0.00089  loss_mask_4: 0.1707  loss_dice_4: 0.2058  loss_ce_5: 0.000564  loss_mask_5: 0.1645  loss_dice_5: 0.2105  loss_ce_6: 0.0006482  loss_mask_6: 0.1694  loss_dice_6: 0.1977  loss_ce_7: 0.0007402  loss_mask_7: 0.1708  loss_dice_7: 0.2091  loss_ce_8: 0.0005989  loss_mask_8: 0.1771  loss_dice_8: 0.2086  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:39] d2.utils.events INFO:  eta: 0:00:05  iter: 49959  total_loss: 4.178  loss_ce: 0.0003956  loss_mask: 0.1839  loss_dice: 0.1898  loss_ce_0: 0.06149  loss_mask_0: 0.1824  loss_dice_0: 0.1848  loss_ce_1: 0.0004921  loss_mask_1: 0.1849  loss_dice_1: 0.1869  loss_ce_2: 0.0005009  loss_mask_2: 0.1711  loss_dice_2: 0.1822  loss_ce_3: 0.0004838  loss_mask_3: 0.1835  loss_dice_3: 0.1816  loss_ce_4: 0.0005235  loss_mask_4: 0.1648  loss_dice_4: 0.1847  loss_ce_5: 0.0004973  loss_mask_5: 0.1716  loss_dice_5: 0.1808  loss_ce_6: 0.0005144  loss_mask_6: 0.177  loss_dice_6: 0.1836  loss_ce_7: 0.0004537  loss_mask_7: 0.1711  loss_dice_7: 0.1823  loss_ce_8: 0.0005199  loss_mask_8: 0.1817  loss_dice_8: 0.1796  time: 0.1260  data_time: 0.0012  lr: 1e-06  max_mem: 3105M
[04/13 16:56:41] d2.utils.events INFO:  eta: 0:00:02  iter: 49979  total_loss: 4.336  loss_ce: 0.0006114  loss_mask: 0.1846  loss_dice: 0.1693  loss_ce_0: 0.06202  loss_mask_0: 0.1934  loss_dice_0: 0.1709  loss_ce_1: 0.0008084  loss_mask_1: 0.1932  loss_dice_1: 0.1715  loss_ce_2: 0.0006645  loss_mask_2: 0.194  loss_dice_2: 0.1758  loss_ce_3: 0.0005169  loss_mask_3: 0.195  loss_dice_3: 0.1852  loss_ce_4: 0.000654  loss_mask_4: 0.1923  loss_dice_4: 0.176  loss_ce_5: 0.0005496  loss_mask_5: 0.1972  loss_dice_5: 0.1695  loss_ce_6: 0.0005679  loss_mask_6: 0.1987  loss_dice_6: 0.1769  loss_ce_7: 0.000632  loss_mask_7: 0.1956  loss_dice_7: 0.1744  loss_ce_8: 0.000579  loss_mask_8: 0.1855  loss_dice_8: 0.1758  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:44] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_0049999.pth
[04/13 16:56:44] fvcore.common.checkpoint INFO: Saving checkpoint to ./output_coco_256_100/model_final.pth
[04/13 16:56:44] d2.utils.events INFO:  eta: 0:00:00  iter: 49999  total_loss: 4.042  loss_ce: 0.0004096  loss_mask: 0.2195  loss_dice: 0.1703  loss_ce_0: 0.06146  loss_mask_0: 0.217  loss_dice_0: 0.1757  loss_ce_1: 0.0003943  loss_mask_1: 0.2202  loss_dice_1: 0.1717  loss_ce_2: 0.0003258  loss_mask_2: 0.2061  loss_dice_2: 0.1729  loss_ce_3: 0.0004107  loss_mask_3: 0.2294  loss_dice_3: 0.173  loss_ce_4: 0.0003162  loss_mask_4: 0.217  loss_dice_4: 0.1688  loss_ce_5: 0.0003386  loss_mask_5: 0.2248  loss_dice_5: 0.1703  loss_ce_6: 0.0004496  loss_mask_6: 0.2201  loss_dice_6: 0.1775  loss_ce_7: 0.0003552  loss_mask_7: 0.2205  loss_dice_7: 0.1802  loss_ce_8: 0.0004322  loss_mask_8: 0.1969  loss_dice_8: 0.1714  time: 0.1260  data_time: 0.0013  lr: 1e-06  max_mem: 3105M
[04/13 16:56:44] d2.engine.hooks INFO: Overall training speed: 49998 iterations in 1:45:01 (0.1260 s / it)
[04/13 16:56:44] d2.engine.hooks INFO: Total training time: 2:24:19 (0:39:18 on hooks)
[04/13 16:56:44] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/13 16:56:44] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/13 16:56:44] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/13 16:56:44] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/13 16:56:44] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/13 16:56:48] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0005 s/iter. Inference: 0.0533 s/iter. Eval: 0.2231 s/iter. Total: 0.2769 s/iter. ETA=0:03:54
[04/13 16:56:53] d2.evaluation.evaluator INFO: Inference done 30/856. Dataloading: 0.0007 s/iter. Inference: 0.0531 s/iter. Eval: 0.2237 s/iter. Total: 0.2776 s/iter. ETA=0:03:49
[04/13 16:56:58] d2.evaluation.evaluator INFO: Inference done 48/856. Dataloading: 0.0008 s/iter. Inference: 0.0532 s/iter. Eval: 0.2243 s/iter. Total: 0.2783 s/iter. ETA=0:03:44
[04/13 16:57:03] d2.evaluation.evaluator INFO: Inference done 66/856. Dataloading: 0.0008 s/iter. Inference: 0.0533 s/iter. Eval: 0.2254 s/iter. Total: 0.2795 s/iter. ETA=0:03:40
[04/13 16:57:08] d2.evaluation.evaluator INFO: Inference done 84/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2257 s/iter. Total: 0.2797 s/iter. ETA=0:03:35
[04/13 16:57:13] d2.evaluation.evaluator INFO: Inference done 102/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2260 s/iter. Total: 0.2800 s/iter. ETA=0:03:31
[04/13 16:57:18] d2.evaluation.evaluator INFO: Inference done 120/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2280 s/iter. Total: 0.2819 s/iter. ETA=0:03:27
[04/13 16:57:24] d2.evaluation.evaluator INFO: Inference done 137/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2299 s/iter. Total: 0.2839 s/iter. ETA=0:03:24
[04/13 16:57:29] d2.evaluation.evaluator INFO: Inference done 154/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2315 s/iter. Total: 0.2854 s/iter. ETA=0:03:20
[04/13 16:57:34] d2.evaluation.evaluator INFO: Inference done 171/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2326 s/iter. Total: 0.2866 s/iter. ETA=0:03:16
[04/13 16:57:39] d2.evaluation.evaluator INFO: Inference done 188/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2338 s/iter. Total: 0.2877 s/iter. ETA=0:03:12
[04/13 16:57:44] d2.evaluation.evaluator INFO: Inference done 205/856. Dataloading: 0.0008 s/iter. Inference: 0.0531 s/iter. Eval: 0.2345 s/iter. Total: 0.2885 s/iter. ETA=0:03:07
[04/13 16:57:49] d2.evaluation.evaluator INFO: Inference done 222/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2351 s/iter. Total: 0.2890 s/iter. ETA=0:03:03
[04/13 16:57:54] d2.evaluation.evaluator INFO: Inference done 239/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2356 s/iter. Total: 0.2895 s/iter. ETA=0:02:58
[04/13 16:57:59] d2.evaluation.evaluator INFO: Inference done 256/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2360 s/iter. Total: 0.2900 s/iter. ETA=0:02:53
[04/13 16:58:04] d2.evaluation.evaluator INFO: Inference done 273/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2364 s/iter. Total: 0.2903 s/iter. ETA=0:02:49
[04/13 16:58:09] d2.evaluation.evaluator INFO: Inference done 290/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2368 s/iter. Total: 0.2907 s/iter. ETA=0:02:44
[04/13 16:58:14] d2.evaluation.evaluator INFO: Inference done 307/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2372 s/iter. Total: 0.2911 s/iter. ETA=0:02:39
[04/13 16:58:19] d2.evaluation.evaluator INFO: Inference done 324/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2375 s/iter. Total: 0.2913 s/iter. ETA=0:02:34
[04/13 16:58:24] d2.evaluation.evaluator INFO: Inference done 341/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2381 s/iter. Total: 0.2920 s/iter. ETA=0:02:30
[04/13 16:58:29] d2.evaluation.evaluator INFO: Inference done 358/856. Dataloading: 0.0008 s/iter. Inference: 0.0530 s/iter. Eval: 0.2384 s/iter. Total: 0.2922 s/iter. ETA=0:02:25
[04/13 16:58:34] d2.evaluation.evaluator INFO: Inference done 375/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2386 s/iter. Total: 0.2925 s/iter. ETA=0:02:20
[04/13 16:58:39] d2.evaluation.evaluator INFO: Inference done 392/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2389 s/iter. Total: 0.2927 s/iter. ETA=0:02:15
[04/13 16:58:44] d2.evaluation.evaluator INFO: Inference done 409/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2391 s/iter. Total: 0.2930 s/iter. ETA=0:02:10
[04/13 16:58:49] d2.evaluation.evaluator INFO: Inference done 426/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2393 s/iter. Total: 0.2931 s/iter. ETA=0:02:06
[04/13 16:58:55] d2.evaluation.evaluator INFO: Inference done 443/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2395 s/iter. Total: 0.2933 s/iter. ETA=0:02:01
[04/13 16:59:00] d2.evaluation.evaluator INFO: Inference done 460/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2396 s/iter. Total: 0.2935 s/iter. ETA=0:01:56
[04/13 16:59:05] d2.evaluation.evaluator INFO: Inference done 477/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2397 s/iter. Total: 0.2936 s/iter. ETA=0:01:51
[04/13 16:59:10] d2.evaluation.evaluator INFO: Inference done 494/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2399 s/iter. Total: 0.2937 s/iter. ETA=0:01:46
[04/13 16:59:15] d2.evaluation.evaluator INFO: Inference done 511/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2399 s/iter. Total: 0.2938 s/iter. ETA=0:01:41
[04/13 16:59:20] d2.evaluation.evaluator INFO: Inference done 528/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2401 s/iter. Total: 0.2939 s/iter. ETA=0:01:36
[04/13 16:59:25] d2.evaluation.evaluator INFO: Inference done 545/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2401 s/iter. Total: 0.2939 s/iter. ETA=0:01:31
[04/13 16:59:30] d2.evaluation.evaluator INFO: Inference done 562/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2404 s/iter. Total: 0.2942 s/iter. ETA=0:01:26
[04/13 16:59:35] d2.evaluation.evaluator INFO: Inference done 579/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2404 s/iter. Total: 0.2942 s/iter. ETA=0:01:21
[04/13 16:59:40] d2.evaluation.evaluator INFO: Inference done 596/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2405 s/iter. Total: 0.2943 s/iter. ETA=0:01:16
[04/13 16:59:45] d2.evaluation.evaluator INFO: Inference done 614/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2405 s/iter. Total: 0.2943 s/iter. ETA=0:01:11
[04/13 16:59:51] d2.evaluation.evaluator INFO: Inference done 632/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2405 s/iter. Total: 0.2943 s/iter. ETA=0:01:05
[04/13 16:59:56] d2.evaluation.evaluator INFO: Inference done 650/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2404 s/iter. Total: 0.2942 s/iter. ETA=0:01:00
[04/13 17:00:01] d2.evaluation.evaluator INFO: Inference done 668/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2404 s/iter. Total: 0.2942 s/iter. ETA=0:00:55
[04/13 17:00:06] d2.evaluation.evaluator INFO: Inference done 685/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2404 s/iter. Total: 0.2942 s/iter. ETA=0:00:50
[04/13 17:00:11] d2.evaluation.evaluator INFO: Inference done 702/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2405 s/iter. Total: 0.2942 s/iter. ETA=0:00:45
[04/13 17:00:16] d2.evaluation.evaluator INFO: Inference done 719/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2405 s/iter. Total: 0.2943 s/iter. ETA=0:00:40
[04/13 17:00:21] d2.evaluation.evaluator INFO: Inference done 736/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2405 s/iter. Total: 0.2943 s/iter. ETA=0:00:35
[04/13 17:00:26] d2.evaluation.evaluator INFO: Inference done 753/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2406 s/iter. Total: 0.2944 s/iter. ETA=0:00:30
[04/13 17:00:31] d2.evaluation.evaluator INFO: Inference done 770/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2406 s/iter. Total: 0.2944 s/iter. ETA=0:00:25
[04/13 17:00:36] d2.evaluation.evaluator INFO: Inference done 787/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2407 s/iter. Total: 0.2944 s/iter. ETA=0:00:20
[04/13 17:00:41] d2.evaluation.evaluator INFO: Inference done 804/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2408 s/iter. Total: 0.2946 s/iter. ETA=0:00:15
[04/13 17:00:46] d2.evaluation.evaluator INFO: Inference done 821/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2408 s/iter. Total: 0.2946 s/iter. ETA=0:00:10
[04/13 17:00:52] d2.evaluation.evaluator INFO: Inference done 838/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2410 s/iter. Total: 0.2948 s/iter. ETA=0:00:05
[04/13 17:00:57] d2.evaluation.evaluator INFO: Inference done 855/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2411 s/iter. Total: 0.2949 s/iter. ETA=0:00:00
[04/13 17:00:57] d2.evaluation.evaluator INFO: Total inference time: 0:04:10.996371 (0.294943 s / iter per device, on 1 devices)
[04/13 17:00:57] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:45 (0.052883 s / iter per device, on 1 devices)
[04/13 17:00:58] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/13 17:00:58] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/13 17:01:00] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/13 17:01:00] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/13 17:01:00] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.27 seconds.
[04/13 17:01:00] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 17:01:00] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 17:01:00] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/13 17:01:00] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 17:01:00] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/13 17:01:04] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/13 17:01:05] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 1.43 seconds.
[04/13 17:01:05] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/13 17:01:05] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.05 seconds.
[04/13 17:01:05] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 83.663 | 83.663 | 83.663 |  nan  |  nan  | 83.663 |
[04/13 17:01:05] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/13 17:01:05] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 86.716 | defect     | 80.611 |
[04/13 17:01:05] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/13 17:01:05] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/13 17:01:05] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 17:01:05] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/13 17:01:05] d2.evaluation.testing INFO: copypaste: Task: segm
[04/13 17:01:05] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/13 17:01:05] d2.evaluation.testing INFO: copypaste: 83.6635,83.6635,83.6635,nan,nan,83.6635
[04/21 16:46:18] detectron2 INFO: Rank of current process: 0. World size: 1
[04/21 16:46:20] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/21 16:46:20] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/21 16:46:20] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: ''
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/21 16:46:20] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/21 16:46:20] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/21 16:46:20] d2.utils.env INFO: Using a generated random seed 21489665
[04/21 16:47:37] detectron2 INFO: Rank of current process: 0. World size: 1
[04/21 16:47:39] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/21 16:47:39] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/21 16:47:39] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/21 16:47:39] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/21 16:47:39] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/21 16:47:39] d2.utils.env INFO: Using a generated random seed 40985040
[04/21 16:47:59] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/21 16:47:59] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/21 16:47:59] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/21 16:47:59] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/21 16:47:59] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/21 16:47:59] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/21 16:47:59] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/21 16:47:59] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/21 16:48:06] d2.evaluation.evaluator INFO: Inference done 1/856. Dataloading: 0.1334 s/iter. Inference: 6.1496 s/iter. Eval: 0.2256 s/iter. Total: 6.5095 s/iter. ETA=1:32:45
[04/21 16:48:11] d2.evaluation.evaluator INFO: Inference done 18/856. Dataloading: 0.0007 s/iter. Inference: 0.0522 s/iter. Eval: 0.2380 s/iter. Total: 0.2910 s/iter. ETA=0:04:03
[04/21 16:48:16] d2.evaluation.evaluator INFO: Inference done 36/856. Dataloading: 0.0008 s/iter. Inference: 0.0523 s/iter. Eval: 0.2390 s/iter. Total: 0.2921 s/iter. ETA=0:03:59
[04/21 16:48:21] d2.evaluation.evaluator INFO: Inference done 54/856. Dataloading: 0.0008 s/iter. Inference: 0.0523 s/iter. Eval: 0.2393 s/iter. Total: 0.2924 s/iter. ETA=0:03:54
[04/21 16:48:26] d2.evaluation.evaluator INFO: Inference done 72/856. Dataloading: 0.0008 s/iter. Inference: 0.0523 s/iter. Eval: 0.2393 s/iter. Total: 0.2925 s/iter. ETA=0:03:49
[04/21 16:48:32] d2.evaluation.evaluator INFO: Inference done 90/856. Dataloading: 0.0008 s/iter. Inference: 0.0523 s/iter. Eval: 0.2393 s/iter. Total: 0.2925 s/iter. ETA=0:03:44
[04/21 16:48:37] d2.evaluation.evaluator INFO: Inference done 108/856. Dataloading: 0.0008 s/iter. Inference: 0.0523 s/iter. Eval: 0.2393 s/iter. Total: 0.2925 s/iter. ETA=0:03:38
[04/21 16:48:42] d2.evaluation.evaluator INFO: Inference done 126/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2393 s/iter. Total: 0.2925 s/iter. ETA=0:03:33
[04/21 16:48:48] d2.evaluation.evaluator INFO: Inference done 144/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2392 s/iter. Total: 0.2925 s/iter. ETA=0:03:28
[04/21 16:48:53] d2.evaluation.evaluator INFO: Inference done 162/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2391 s/iter. Total: 0.2924 s/iter. ETA=0:03:22
[04/21 16:48:58] d2.evaluation.evaluator INFO: Inference done 180/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2393 s/iter. Total: 0.2926 s/iter. ETA=0:03:17
[04/21 16:49:03] d2.evaluation.evaluator INFO: Inference done 198/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2391 s/iter. Total: 0.2924 s/iter. ETA=0:03:12
[04/21 16:49:09] d2.evaluation.evaluator INFO: Inference done 216/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2391 s/iter. Total: 0.2924 s/iter. ETA=0:03:07
[04/21 16:49:14] d2.evaluation.evaluator INFO: Inference done 234/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2392 s/iter. Total: 0.2925 s/iter. ETA=0:03:01
[04/21 16:49:19] d2.evaluation.evaluator INFO: Inference done 252/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2392 s/iter. Total: 0.2925 s/iter. ETA=0:02:56
[04/21 16:49:24] d2.evaluation.evaluator INFO: Inference done 270/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2392 s/iter. Total: 0.2925 s/iter. ETA=0:02:51
[04/21 16:49:30] d2.evaluation.evaluator INFO: Inference done 288/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2391 s/iter. Total: 0.2925 s/iter. ETA=0:02:46
[04/21 16:49:35] d2.evaluation.evaluator INFO: Inference done 306/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2391 s/iter. Total: 0.2925 s/iter. ETA=0:02:40
[04/21 16:49:40] d2.evaluation.evaluator INFO: Inference done 324/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2392 s/iter. Total: 0.2925 s/iter. ETA=0:02:35
[04/21 16:49:45] d2.evaluation.evaluator INFO: Inference done 342/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2392 s/iter. Total: 0.2925 s/iter. ETA=0:02:30
[04/21 16:49:51] d2.evaluation.evaluator INFO: Inference done 360/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2392 s/iter. Total: 0.2926 s/iter. ETA=0:02:25
[04/21 16:49:56] d2.evaluation.evaluator INFO: Inference done 378/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2392 s/iter. Total: 0.2926 s/iter. ETA=0:02:19
[04/21 16:50:01] d2.evaluation.evaluator INFO: Inference done 395/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2394 s/iter. Total: 0.2927 s/iter. ETA=0:02:14
[04/21 16:50:06] d2.evaluation.evaluator INFO: Inference done 413/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2393 s/iter. Total: 0.2927 s/iter. ETA=0:02:09
[04/21 16:50:12] d2.evaluation.evaluator INFO: Inference done 431/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2393 s/iter. Total: 0.2927 s/iter. ETA=0:02:04
[04/21 16:50:17] d2.evaluation.evaluator INFO: Inference done 448/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2394 s/iter. Total: 0.2928 s/iter. ETA=0:01:59
[04/21 16:50:22] d2.evaluation.evaluator INFO: Inference done 466/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2394 s/iter. Total: 0.2928 s/iter. ETA=0:01:54
[04/21 16:50:27] d2.evaluation.evaluator INFO: Inference done 484/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2393 s/iter. Total: 0.2927 s/iter. ETA=0:01:48
[04/21 16:50:32] d2.evaluation.evaluator INFO: Inference done 502/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2392 s/iter. Total: 0.2926 s/iter. ETA=0:01:43
[04/21 16:50:38] d2.evaluation.evaluator INFO: Inference done 520/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2392 s/iter. Total: 0.2926 s/iter. ETA=0:01:38
[04/21 16:50:43] d2.evaluation.evaluator INFO: Inference done 538/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2392 s/iter. Total: 0.2926 s/iter. ETA=0:01:33
[04/21 16:50:48] d2.evaluation.evaluator INFO: Inference done 556/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2392 s/iter. Total: 0.2926 s/iter. ETA=0:01:27
[04/21 16:50:53] d2.evaluation.evaluator INFO: Inference done 574/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2392 s/iter. Total: 0.2926 s/iter. ETA=0:01:22
[04/21 16:50:59] d2.evaluation.evaluator INFO: Inference done 592/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2392 s/iter. Total: 0.2926 s/iter. ETA=0:01:17
[04/21 16:51:04] d2.evaluation.evaluator INFO: Inference done 609/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2393 s/iter. Total: 0.2927 s/iter. ETA=0:01:12
[04/21 16:51:09] d2.evaluation.evaluator INFO: Inference done 627/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2393 s/iter. Total: 0.2927 s/iter. ETA=0:01:07
[04/21 16:51:14] d2.evaluation.evaluator INFO: Inference done 645/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2393 s/iter. Total: 0.2927 s/iter. ETA=0:01:01
[04/21 16:51:19] d2.evaluation.evaluator INFO: Inference done 662/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2395 s/iter. Total: 0.2929 s/iter. ETA=0:00:56
[04/21 16:51:24] d2.evaluation.evaluator INFO: Inference done 679/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2397 s/iter. Total: 0.2931 s/iter. ETA=0:00:51
[04/21 16:51:30] d2.evaluation.evaluator INFO: Inference done 696/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2399 s/iter. Total: 0.2933 s/iter. ETA=0:00:46
[04/21 16:51:35] d2.evaluation.evaluator INFO: Inference done 713/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2401 s/iter. Total: 0.2935 s/iter. ETA=0:00:41
[04/21 16:51:40] d2.evaluation.evaluator INFO: Inference done 730/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2402 s/iter. Total: 0.2936 s/iter. ETA=0:00:36
[04/21 16:51:45] d2.evaluation.evaluator INFO: Inference done 747/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2404 s/iter. Total: 0.2938 s/iter. ETA=0:00:32
[04/21 16:51:50] d2.evaluation.evaluator INFO: Inference done 764/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2405 s/iter. Total: 0.2939 s/iter. ETA=0:00:27
[04/21 16:51:55] d2.evaluation.evaluator INFO: Inference done 781/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2407 s/iter. Total: 0.2941 s/iter. ETA=0:00:22
[04/21 16:52:00] d2.evaluation.evaluator INFO: Inference done 798/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2408 s/iter. Total: 0.2942 s/iter. ETA=0:00:17
[04/21 16:52:05] d2.evaluation.evaluator INFO: Inference done 815/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2410 s/iter. Total: 0.2944 s/iter. ETA=0:00:12
[04/21 16:52:10] d2.evaluation.evaluator INFO: Inference done 832/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2411 s/iter. Total: 0.2945 s/iter. ETA=0:00:07
[04/21 16:52:16] d2.evaluation.evaluator INFO: Inference done 849/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2412 s/iter. Total: 0.2947 s/iter. ETA=0:00:02
[04/21 16:52:18] d2.evaluation.evaluator INFO: Total inference time: 0:04:10.839521 (0.294759 s / iter per device, on 1 devices)
[04/21 16:52:18] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052517 s / iter per device, on 1 devices)
[04/21 16:55:04] detectron2 INFO: Rank of current process: 0. World size: 1
[04/21 16:55:06] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/21 16:55:06] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/21 16:55:06] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/21 16:55:06] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/21 16:55:06] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/21 16:55:06] d2.utils.env INFO: Using a generated random seed 8393461
[04/21 16:55:27] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/21 16:55:27] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/21 16:55:27] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/21 16:55:27] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/21 16:55:27] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/21 16:55:27] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/21 16:55:27] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/21 16:55:27] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/21 16:55:34] d2.evaluation.evaluator INFO: Inference done 1/856. Dataloading: 0.1673 s/iter. Inference: 6.1878 s/iter. Eval: 0.2218 s/iter. Total: 6.5778 s/iter. ETA=1:33:43
[04/21 16:55:39] d2.evaluation.evaluator INFO: Inference done 19/856. Dataloading: 0.0007 s/iter. Inference: 0.0526 s/iter. Eval: 0.2236 s/iter. Total: 0.2769 s/iter. ETA=0:03:51
[04/21 16:55:44] d2.evaluation.evaluator INFO: Inference done 38/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2239 s/iter. Total: 0.2774 s/iter. ETA=0:03:46
[04/21 16:55:49] d2.evaluation.evaluator INFO: Inference done 56/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2242 s/iter. Total: 0.2777 s/iter. ETA=0:03:42
[04/21 16:55:54] d2.evaluation.evaluator INFO: Inference done 74/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2271 s/iter. Total: 0.2807 s/iter. ETA=0:03:39
[04/21 16:55:59] d2.evaluation.evaluator INFO: Inference done 92/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2273 s/iter. Total: 0.2810 s/iter. ETA=0:03:34
[04/21 16:56:04] d2.evaluation.evaluator INFO: Inference done 110/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2274 s/iter. Total: 0.2811 s/iter. ETA=0:03:29
[04/21 16:56:09] d2.evaluation.evaluator INFO: Inference done 128/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2272 s/iter. Total: 0.2808 s/iter. ETA=0:03:24
[04/21 16:56:15] d2.evaluation.evaluator INFO: Inference done 146/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2272 s/iter. Total: 0.2809 s/iter. ETA=0:03:19
[04/21 16:56:20] d2.evaluation.evaluator INFO: Inference done 164/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2276 s/iter. Total: 0.2812 s/iter. ETA=0:03:14
[04/21 16:56:25] d2.evaluation.evaluator INFO: Inference done 182/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2274 s/iter. Total: 0.2810 s/iter. ETA=0:03:09
[04/21 16:56:30] d2.evaluation.evaluator INFO: Inference done 200/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2274 s/iter. Total: 0.2810 s/iter. ETA=0:03:04
[04/21 16:56:35] d2.evaluation.evaluator INFO: Inference done 218/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2275 s/iter. Total: 0.2810 s/iter. ETA=0:02:59
[04/21 16:56:40] d2.evaluation.evaluator INFO: Inference done 236/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2275 s/iter. Total: 0.2811 s/iter. ETA=0:02:54
[04/21 16:56:45] d2.evaluation.evaluator INFO: Inference done 254/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2276 s/iter. Total: 0.2811 s/iter. ETA=0:02:49
[04/21 16:56:50] d2.evaluation.evaluator INFO: Inference done 272/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2276 s/iter. Total: 0.2812 s/iter. ETA=0:02:44
[04/21 16:56:55] d2.evaluation.evaluator INFO: Inference done 290/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2277 s/iter. Total: 0.2812 s/iter. ETA=0:02:39
[04/21 16:57:00] d2.evaluation.evaluator INFO: Inference done 308/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2277 s/iter. Total: 0.2812 s/iter. ETA=0:02:34
[04/21 16:57:05] d2.evaluation.evaluator INFO: Inference done 326/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2278 s/iter. Total: 0.2813 s/iter. ETA=0:02:29
[04/21 16:57:10] d2.evaluation.evaluator INFO: Inference done 344/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2278 s/iter. Total: 0.2813 s/iter. ETA=0:02:24
[04/21 16:57:15] d2.evaluation.evaluator INFO: Inference done 362/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2278 s/iter. Total: 0.2813 s/iter. ETA=0:02:18
[04/21 16:57:20] d2.evaluation.evaluator INFO: Inference done 380/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2278 s/iter. Total: 0.2813 s/iter. ETA=0:02:13
[04/21 16:57:26] d2.evaluation.evaluator INFO: Inference done 398/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2280 s/iter. Total: 0.2815 s/iter. ETA=0:02:08
[04/21 16:57:31] d2.evaluation.evaluator INFO: Inference done 416/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2280 s/iter. Total: 0.2816 s/iter. ETA=0:02:03
[04/21 16:57:36] d2.evaluation.evaluator INFO: Inference done 434/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2281 s/iter. Total: 0.2816 s/iter. ETA=0:01:58
[04/21 16:57:41] d2.evaluation.evaluator INFO: Inference done 452/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2281 s/iter. Total: 0.2816 s/iter. ETA=0:01:53
[04/21 16:57:46] d2.evaluation.evaluator INFO: Inference done 470/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2281 s/iter. Total: 0.2817 s/iter. ETA=0:01:48
[04/21 16:57:51] d2.evaluation.evaluator INFO: Inference done 488/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2281 s/iter. Total: 0.2817 s/iter. ETA=0:01:43
[04/21 16:57:56] d2.evaluation.evaluator INFO: Inference done 506/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2281 s/iter. Total: 0.2817 s/iter. ETA=0:01:38
[04/21 16:58:01] d2.evaluation.evaluator INFO: Inference done 524/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2281 s/iter. Total: 0.2817 s/iter. ETA=0:01:33
[04/21 16:58:06] d2.evaluation.evaluator INFO: Inference done 542/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2282 s/iter. Total: 0.2817 s/iter. ETA=0:01:28
[04/21 16:58:11] d2.evaluation.evaluator INFO: Inference done 560/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2282 s/iter. Total: 0.2818 s/iter. ETA=0:01:23
[04/21 16:58:16] d2.evaluation.evaluator INFO: Inference done 578/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2282 s/iter. Total: 0.2818 s/iter. ETA=0:01:18
[04/21 16:58:22] d2.evaluation.evaluator INFO: Inference done 596/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2283 s/iter. Total: 0.2818 s/iter. ETA=0:01:13
[04/21 16:58:27] d2.evaluation.evaluator INFO: Inference done 614/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2284 s/iter. Total: 0.2819 s/iter. ETA=0:01:08
[04/21 16:58:32] d2.evaluation.evaluator INFO: Inference done 632/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2284 s/iter. Total: 0.2819 s/iter. ETA=0:01:03
[04/21 16:58:37] d2.evaluation.evaluator INFO: Inference done 650/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2284 s/iter. Total: 0.2820 s/iter. ETA=0:00:58
[04/21 16:58:42] d2.evaluation.evaluator INFO: Inference done 668/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2285 s/iter. Total: 0.2821 s/iter. ETA=0:00:53
[04/21 16:58:47] d2.evaluation.evaluator INFO: Inference done 686/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2286 s/iter. Total: 0.2821 s/iter. ETA=0:00:47
[04/21 16:58:52] d2.evaluation.evaluator INFO: Inference done 704/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2286 s/iter. Total: 0.2822 s/iter. ETA=0:00:42
[04/21 16:58:57] d2.evaluation.evaluator INFO: Inference done 722/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2287 s/iter. Total: 0.2822 s/iter. ETA=0:00:37
[04/21 16:59:02] d2.evaluation.evaluator INFO: Inference done 740/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2287 s/iter. Total: 0.2823 s/iter. ETA=0:00:32
[04/21 16:59:08] d2.evaluation.evaluator INFO: Inference done 758/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2288 s/iter. Total: 0.2823 s/iter. ETA=0:00:27
[04/21 16:59:13] d2.evaluation.evaluator INFO: Inference done 776/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2288 s/iter. Total: 0.2824 s/iter. ETA=0:00:22
[04/21 16:59:18] d2.evaluation.evaluator INFO: Inference done 794/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2289 s/iter. Total: 0.2824 s/iter. ETA=0:00:17
[04/21 16:59:23] d2.evaluation.evaluator INFO: Inference done 812/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2289 s/iter. Total: 0.2824 s/iter. ETA=0:00:12
[04/21 16:59:28] d2.evaluation.evaluator INFO: Inference done 830/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2289 s/iter. Total: 0.2825 s/iter. ETA=0:00:07
[04/21 16:59:33] d2.evaluation.evaluator INFO: Inference done 848/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2289 s/iter. Total: 0.2825 s/iter. ETA=0:00:02
[04/21 16:59:35] d2.evaluation.evaluator INFO: Total inference time: 0:04:00.496885 (0.282605 s / iter per device, on 1 devices)
[04/21 16:59:35] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052698 s / iter per device, on 1 devices)
[04/21 16:59:39] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/21 16:59:39] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/21 16:59:39] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/21 16:59:39] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/21 16:59:39] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.04 seconds.
[04/21 16:59:39] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/21 16:59:39] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[04/21 16:59:39] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/21 16:59:39] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/21 16:59:39] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/21 16:59:39] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/21 16:59:39] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.19 seconds.
[04/21 16:59:39] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/21 16:59:39] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.00 seconds.
[04/21 16:59:39] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 8.818 | 8.818  | 8.818  |  nan  |  nan  | 8.818 |
[04/21 16:59:39] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/21 16:59:39] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP    |
|:-----------|:-------|:-----------|:------|
| normal     | 14.131 | defect     | 3.505 |
[04/21 16:59:39] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/21 16:59:39] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/21 16:59:39] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/21 16:59:39] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/21 16:59:39] d2.evaluation.testing INFO: copypaste: Task: segm
[04/21 16:59:39] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/21 16:59:39] d2.evaluation.testing INFO: copypaste: 8.8177,8.8177,8.8177,nan,nan,8.8177
[04/21 17:00:50] detectron2 INFO: Rank of current process: 0. World size: 1
[04/21 17:00:52] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/21 17:00:52] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/21 17:00:52] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/21 17:00:52] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/21 17:00:52] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/21 17:00:53] d2.utils.env INFO: Using a generated random seed 54405877
[04/21 17:01:12] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/21 17:01:12] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/21 17:01:12] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/21 17:01:13] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/21 17:01:13] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/21 17:01:13] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/21 17:01:13] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/21 17:01:13] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/21 17:01:19] d2.evaluation.evaluator INFO: Inference done 1/856. Dataloading: 0.1458 s/iter. Inference: 6.0685 s/iter. Eval: 0.2197 s/iter. Total: 6.4350 s/iter. ETA=1:31:41
[04/21 17:01:24] d2.evaluation.evaluator INFO: Inference done 19/856. Dataloading: 0.0007 s/iter. Inference: 0.0529 s/iter. Eval: 0.2221 s/iter. Total: 0.2758 s/iter. ETA=0:03:50
[04/21 17:01:29] d2.evaluation.evaluator INFO: Inference done 38/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2229 s/iter. Total: 0.2763 s/iter. ETA=0:03:46
[04/21 17:01:34] d2.evaluation.evaluator INFO: Inference done 56/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2271 s/iter. Total: 0.2808 s/iter. ETA=0:03:44
[04/21 17:01:40] d2.evaluation.evaluator INFO: Inference done 74/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2270 s/iter. Total: 0.2807 s/iter. ETA=0:03:39
[04/21 17:01:45] d2.evaluation.evaluator INFO: Inference done 92/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2270 s/iter. Total: 0.2807 s/iter. ETA=0:03:34
[04/21 17:01:50] d2.evaluation.evaluator INFO: Inference done 110/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2271 s/iter. Total: 0.2808 s/iter. ETA=0:03:29
[04/21 17:01:55] d2.evaluation.evaluator INFO: Inference done 128/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2273 s/iter. Total: 0.2809 s/iter. ETA=0:03:24
[04/21 17:02:00] d2.evaluation.evaluator INFO: Inference done 146/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2273 s/iter. Total: 0.2808 s/iter. ETA=0:03:19
[04/21 17:02:05] d2.evaluation.evaluator INFO: Inference done 164/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2277 s/iter. Total: 0.2812 s/iter. ETA=0:03:14
[04/21 17:02:10] d2.evaluation.evaluator INFO: Inference done 182/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2281 s/iter. Total: 0.2816 s/iter. ETA=0:03:09
[04/21 17:02:15] d2.evaluation.evaluator INFO: Inference done 200/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2281 s/iter. Total: 0.2816 s/iter. ETA=0:03:04
[04/21 17:02:20] d2.evaluation.evaluator INFO: Inference done 218/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2282 s/iter. Total: 0.2816 s/iter. ETA=0:02:59
[04/21 17:02:25] d2.evaluation.evaluator INFO: Inference done 236/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2282 s/iter. Total: 0.2816 s/iter. ETA=0:02:54
[04/21 17:02:30] d2.evaluation.evaluator INFO: Inference done 254/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2282 s/iter. Total: 0.2817 s/iter. ETA=0:02:49
[04/21 17:02:35] d2.evaluation.evaluator INFO: Inference done 272/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2282 s/iter. Total: 0.2817 s/iter. ETA=0:02:44
[04/21 17:02:40] d2.evaluation.evaluator INFO: Inference done 290/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2284 s/iter. Total: 0.2818 s/iter. ETA=0:02:39
[04/21 17:02:46] d2.evaluation.evaluator INFO: Inference done 308/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2284 s/iter. Total: 0.2818 s/iter. ETA=0:02:34
[04/21 17:02:51] d2.evaluation.evaluator INFO: Inference done 326/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2284 s/iter. Total: 0.2818 s/iter. ETA=0:02:29
[04/21 17:02:56] d2.evaluation.evaluator INFO: Inference done 344/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2284 s/iter. Total: 0.2819 s/iter. ETA=0:02:24
[04/21 17:03:01] d2.evaluation.evaluator INFO: Inference done 362/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2285 s/iter. Total: 0.2819 s/iter. ETA=0:02:19
[04/21 17:03:06] d2.evaluation.evaluator INFO: Inference done 380/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2285 s/iter. Total: 0.2819 s/iter. ETA=0:02:14
[04/21 17:03:11] d2.evaluation.evaluator INFO: Inference done 398/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2287 s/iter. Total: 0.2821 s/iter. ETA=0:02:09
[04/21 17:03:16] d2.evaluation.evaluator INFO: Inference done 416/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2288 s/iter. Total: 0.2822 s/iter. ETA=0:02:04
[04/21 17:03:21] d2.evaluation.evaluator INFO: Inference done 434/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2288 s/iter. Total: 0.2822 s/iter. ETA=0:01:59
[04/21 17:03:26] d2.evaluation.evaluator INFO: Inference done 452/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2289 s/iter. Total: 0.2822 s/iter. ETA=0:01:54
[04/21 17:03:31] d2.evaluation.evaluator INFO: Inference done 470/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2289 s/iter. Total: 0.2822 s/iter. ETA=0:01:48
[04/21 17:03:36] d2.evaluation.evaluator INFO: Inference done 488/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2289 s/iter. Total: 0.2823 s/iter. ETA=0:01:43
[04/21 17:03:42] d2.evaluation.evaluator INFO: Inference done 506/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2289 s/iter. Total: 0.2823 s/iter. ETA=0:01:38
[04/21 17:03:47] d2.evaluation.evaluator INFO: Inference done 524/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2289 s/iter. Total: 0.2823 s/iter. ETA=0:01:33
[04/21 17:03:52] d2.evaluation.evaluator INFO: Inference done 542/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2290 s/iter. Total: 0.2824 s/iter. ETA=0:01:28
[04/21 17:03:57] d2.evaluation.evaluator INFO: Inference done 560/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2291 s/iter. Total: 0.2825 s/iter. ETA=0:01:23
[04/21 17:04:02] d2.evaluation.evaluator INFO: Inference done 578/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2291 s/iter. Total: 0.2825 s/iter. ETA=0:01:18
[04/21 17:04:07] d2.evaluation.evaluator INFO: Inference done 596/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2291 s/iter. Total: 0.2825 s/iter. ETA=0:01:13
[04/21 17:04:12] d2.evaluation.evaluator INFO: Inference done 614/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2292 s/iter. Total: 0.2827 s/iter. ETA=0:01:08
[04/21 17:04:17] d2.evaluation.evaluator INFO: Inference done 632/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2293 s/iter. Total: 0.2827 s/iter. ETA=0:01:03
[04/21 17:04:23] d2.evaluation.evaluator INFO: Inference done 650/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2293 s/iter. Total: 0.2828 s/iter. ETA=0:00:58
[04/21 17:04:28] d2.evaluation.evaluator INFO: Inference done 668/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2294 s/iter. Total: 0.2828 s/iter. ETA=0:00:53
[04/21 17:04:33] d2.evaluation.evaluator INFO: Inference done 686/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2294 s/iter. Total: 0.2828 s/iter. ETA=0:00:48
[04/21 17:04:38] d2.evaluation.evaluator INFO: Inference done 704/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2295 s/iter. Total: 0.2829 s/iter. ETA=0:00:43
[04/21 17:04:43] d2.evaluation.evaluator INFO: Inference done 722/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2295 s/iter. Total: 0.2830 s/iter. ETA=0:00:37
[04/21 17:04:48] d2.evaluation.evaluator INFO: Inference done 740/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2295 s/iter. Total: 0.2830 s/iter. ETA=0:00:32
[04/21 17:04:53] d2.evaluation.evaluator INFO: Inference done 758/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2296 s/iter. Total: 0.2830 s/iter. ETA=0:00:27
[04/21 17:04:58] d2.evaluation.evaluator INFO: Inference done 776/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2296 s/iter. Total: 0.2830 s/iter. ETA=0:00:22
[04/21 17:05:04] d2.evaluation.evaluator INFO: Inference done 794/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2296 s/iter. Total: 0.2831 s/iter. ETA=0:00:17
[04/21 17:05:09] d2.evaluation.evaluator INFO: Inference done 812/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2296 s/iter. Total: 0.2831 s/iter. ETA=0:00:12
[04/21 17:05:14] d2.evaluation.evaluator INFO: Inference done 830/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2297 s/iter. Total: 0.2831 s/iter. ETA=0:00:07
[04/21 17:05:19] d2.evaluation.evaluator INFO: Inference done 848/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2297 s/iter. Total: 0.2831 s/iter. ETA=0:00:02
[04/21 17:05:21] d2.evaluation.evaluator INFO: Total inference time: 0:04:01.011317 (0.283210 s / iter per device, on 1 devices)
[04/21 17:05:21] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052577 s / iter per device, on 1 devices)
[04/21 17:05:24] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/21 17:05:24] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/21 17:05:24] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/21 17:05:24] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/21 17:05:24] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.04 seconds.
[04/21 17:05:24] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/21 17:05:24] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[04/21 17:05:24] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/21 17:05:24] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/21 17:05:24] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/21 17:05:25] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/21 17:05:25] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.18 seconds.
[04/21 17:05:25] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/21 17:05:25] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.00 seconds.
[04/21 17:05:25] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 6.985 | 6.985  | 6.985  |  nan  |  nan  | 6.985 |
[04/21 17:05:25] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/21 17:05:25] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP    |
|:-----------|:-------|:-----------|:------|
| normal     | 13.352 | defect     | 0.618 |
[04/21 17:05:25] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/21 17:05:25] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/21 17:05:25] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/21 17:05:25] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/21 17:05:25] d2.evaluation.testing INFO: copypaste: Task: segm
[04/21 17:05:25] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/21 17:05:25] d2.evaluation.testing INFO: copypaste: 6.9847,6.9847,6.9847,nan,nan,6.9847
[04/22 14:23:52] detectron2 INFO: Rank of current process: 0. World size: 1
[04/22 14:23:53] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/22 14:23:53] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/22 14:23:53] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/22 14:23:53] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/22 14:23:53] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/22 14:23:53] d2.utils.env INFO: Using a generated random seed 55442294
[04/22 14:24:04] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/22 14:24:04] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/22 14:24:05] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/22 14:24:05] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/22 14:24:05] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/22 14:24:05] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/22 14:24:05] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/22 14:24:05] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/22 14:24:12] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0525 s/iter. Eval: 0.2393 s/iter. Total: 0.2925 s/iter. ETA=0:04:07
[04/22 14:24:17] d2.evaluation.evaluator INFO: Inference done 29/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2382 s/iter. Total: 0.2920 s/iter. ETA=0:04:01
[04/22 14:24:22] d2.evaluation.evaluator INFO: Inference done 46/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2394 s/iter. Total: 0.2931 s/iter. ETA=0:03:57
[04/22 14:24:27] d2.evaluation.evaluator INFO: Inference done 64/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2393 s/iter. Total: 0.2929 s/iter. ETA=0:03:51
[04/22 14:24:33] d2.evaluation.evaluator INFO: Inference done 82/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2391 s/iter. Total: 0.2925 s/iter. ETA=0:03:46
[04/22 14:24:38] d2.evaluation.evaluator INFO: Inference done 100/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2391 s/iter. Total: 0.2926 s/iter. ETA=0:03:41
[04/22 14:24:43] d2.evaluation.evaluator INFO: Inference done 118/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2392 s/iter. Total: 0.2926 s/iter. ETA=0:03:35
[04/22 14:24:48] d2.evaluation.evaluator INFO: Inference done 136/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2393 s/iter. Total: 0.2927 s/iter. ETA=0:03:30
[04/22 14:24:53] d2.evaluation.evaluator INFO: Inference done 153/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2395 s/iter. Total: 0.2929 s/iter. ETA=0:03:25
[04/22 14:24:58] d2.evaluation.evaluator INFO: Inference done 170/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2399 s/iter. Total: 0.2933 s/iter. ETA=0:03:21
[04/22 14:25:04] d2.evaluation.evaluator INFO: Inference done 188/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2400 s/iter. Total: 0.2934 s/iter. ETA=0:03:15
[04/22 14:25:09] d2.evaluation.evaluator INFO: Inference done 205/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2402 s/iter. Total: 0.2936 s/iter. ETA=0:03:11
[04/22 14:25:14] d2.evaluation.evaluator INFO: Inference done 222/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2403 s/iter. Total: 0.2937 s/iter. ETA=0:03:06
[04/22 14:25:19] d2.evaluation.evaluator INFO: Inference done 240/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2403 s/iter. Total: 0.2937 s/iter. ETA=0:03:00
[04/22 14:25:24] d2.evaluation.evaluator INFO: Inference done 258/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2403 s/iter. Total: 0.2937 s/iter. ETA=0:02:55
[04/22 14:25:29] d2.evaluation.evaluator INFO: Inference done 275/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2404 s/iter. Total: 0.2938 s/iter. ETA=0:02:50
[04/22 14:25:35] d2.evaluation.evaluator INFO: Inference done 293/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2404 s/iter. Total: 0.2938 s/iter. ETA=0:02:45
[04/22 14:25:40] d2.evaluation.evaluator INFO: Inference done 311/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2404 s/iter. Total: 0.2938 s/iter. ETA=0:02:40
[04/22 14:25:45] d2.evaluation.evaluator INFO: Inference done 328/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2405 s/iter. Total: 0.2939 s/iter. ETA=0:02:35
[04/22 14:25:50] d2.evaluation.evaluator INFO: Inference done 345/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2407 s/iter. Total: 0.2941 s/iter. ETA=0:02:30
[04/22 14:25:55] d2.evaluation.evaluator INFO: Inference done 362/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2408 s/iter. Total: 0.2942 s/iter. ETA=0:02:25
[04/22 14:26:00] d2.evaluation.evaluator INFO: Inference done 379/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2410 s/iter. Total: 0.2944 s/iter. ETA=0:02:20
[04/22 14:26:05] d2.evaluation.evaluator INFO: Inference done 396/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2412 s/iter. Total: 0.2946 s/iter. ETA=0:02:15
[04/22 14:26:10] d2.evaluation.evaluator INFO: Inference done 413/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2412 s/iter. Total: 0.2946 s/iter. ETA=0:02:10
[04/22 14:26:15] d2.evaluation.evaluator INFO: Inference done 430/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2413 s/iter. Total: 0.2947 s/iter. ETA=0:02:05
[04/22 14:26:20] d2.evaluation.evaluator INFO: Inference done 447/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2413 s/iter. Total: 0.2947 s/iter. ETA=0:02:00
[04/22 14:26:25] d2.evaluation.evaluator INFO: Inference done 464/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2414 s/iter. Total: 0.2948 s/iter. ETA=0:01:55
[04/22 14:26:30] d2.evaluation.evaluator INFO: Inference done 481/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2414 s/iter. Total: 0.2948 s/iter. ETA=0:01:50
[04/22 14:26:35] d2.evaluation.evaluator INFO: Inference done 498/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2415 s/iter. Total: 0.2949 s/iter. ETA=0:01:45
[04/22 14:26:40] d2.evaluation.evaluator INFO: Inference done 515/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2415 s/iter. Total: 0.2949 s/iter. ETA=0:01:40
[04/22 14:26:45] d2.evaluation.evaluator INFO: Inference done 532/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2415 s/iter. Total: 0.2949 s/iter. ETA=0:01:35
[04/22 14:26:50] d2.evaluation.evaluator INFO: Inference done 549/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2416 s/iter. Total: 0.2950 s/iter. ETA=0:01:30
[04/22 14:26:56] d2.evaluation.evaluator INFO: Inference done 566/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2415 s/iter. Total: 0.2950 s/iter. ETA=0:01:25
[04/22 14:27:01] d2.evaluation.evaluator INFO: Inference done 583/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2416 s/iter. Total: 0.2950 s/iter. ETA=0:01:20
[04/22 14:27:06] d2.evaluation.evaluator INFO: Inference done 600/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2416 s/iter. Total: 0.2950 s/iter. ETA=0:01:15
[04/22 14:27:11] d2.evaluation.evaluator INFO: Inference done 617/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2418 s/iter. Total: 0.2952 s/iter. ETA=0:01:10
[04/22 14:27:16] d2.evaluation.evaluator INFO: Inference done 634/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2418 s/iter. Total: 0.2952 s/iter. ETA=0:01:05
[04/22 14:27:21] d2.evaluation.evaluator INFO: Inference done 652/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:01:00
[04/22 14:27:26] d2.evaluation.evaluator INFO: Inference done 669/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:55
[04/22 14:27:31] d2.evaluation.evaluator INFO: Inference done 686/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:50
[04/22 14:27:36] d2.evaluation.evaluator INFO: Inference done 703/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:45
[04/22 14:27:41] d2.evaluation.evaluator INFO: Inference done 720/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:40
[04/22 14:27:46] d2.evaluation.evaluator INFO: Inference done 737/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:35
[04/22 14:27:51] d2.evaluation.evaluator INFO: Inference done 754/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:30
[04/22 14:27:56] d2.evaluation.evaluator INFO: Inference done 771/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:25
[04/22 14:28:01] d2.evaluation.evaluator INFO: Inference done 788/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:20
[04/22 14:28:06] d2.evaluation.evaluator INFO: Inference done 805/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:15
[04/22 14:28:11] d2.evaluation.evaluator INFO: Inference done 822/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2417 s/iter. Total: 0.2952 s/iter. ETA=0:00:10
[04/22 14:28:16] d2.evaluation.evaluator INFO: Inference done 839/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2418 s/iter. Total: 0.2952 s/iter. ETA=0:00:05
[04/22 14:28:21] d2.evaluation.evaluator INFO: Inference done 856/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2418 s/iter. Total: 0.2953 s/iter. ETA=0:00:00
[04/22 14:28:21] d2.evaluation.evaluator INFO: Total inference time: 0:04:11.323109 (0.295327 s / iter per device, on 1 devices)
[04/22 14:28:21] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052553 s / iter per device, on 1 devices)
[04/22 14:28:25] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/22 14:28:25] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/22 14:28:25] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/22 14:28:25] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/22 14:28:25] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.04 seconds.
[04/22 14:28:25] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/22 14:28:25] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[04/22 14:28:25] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/22 14:28:25] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/22 14:28:25] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/22 14:28:25] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/22 14:28:25] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.18 seconds.
[04/22 14:28:25] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/22 14:28:25] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.00 seconds.
[04/22 14:28:25] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 6.985 | 6.985  | 6.985  |  nan  |  nan  | 6.985 |
[04/22 14:28:25] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/22 14:28:25] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP    |
|:-----------|:-------|:-----------|:------|
| normal     | 13.352 | defect     | 0.618 |
[04/22 14:28:25] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/22 14:28:25] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/22 14:28:25] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/22 14:28:25] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/22 14:28:25] d2.evaluation.testing INFO: copypaste: Task: segm
[04/22 14:28:25] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/22 14:28:25] d2.evaluation.testing INFO: copypaste: 6.9847,6.9847,6.9847,nan,nan,6.9847
[04/25 14:28:22] detectron2 INFO: Rank of current process: 0. World size: 1
[04/25 14:28:24] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/25 14:28:24] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/25 14:28:24] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/25 14:28:24] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/25 14:28:24] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/25 14:28:24] d2.utils.env INFO: Using a generated random seed 28733182
[04/25 14:28:36] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/25 14:28:36] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/25 14:28:36] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/25 14:28:36] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/25 14:28:36] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/25 14:28:36] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/25 14:28:36] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/25 14:28:36] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/25 14:28:44] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0007 s/iter. Inference: 0.0525 s/iter. Eval: 0.2441 s/iter. Total: 0.2972 s/iter. ETA=0:04:11
[04/25 14:28:49] d2.evaluation.evaluator INFO: Inference done 28/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2448 s/iter. Total: 0.2982 s/iter. ETA=0:04:06
[04/25 14:28:54] d2.evaluation.evaluator INFO: Inference done 45/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2446 s/iter. Total: 0.2982 s/iter. ETA=0:04:01
[04/25 14:28:59] d2.evaluation.evaluator INFO: Inference done 62/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2441 s/iter. Total: 0.2976 s/iter. ETA=0:03:56
[04/25 14:29:04] d2.evaluation.evaluator INFO: Inference done 79/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2441 s/iter. Total: 0.2976 s/iter. ETA=0:03:51
[04/25 14:29:09] d2.evaluation.evaluator INFO: Inference done 96/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2442 s/iter. Total: 0.2977 s/iter. ETA=0:03:46
[04/25 14:29:14] d2.evaluation.evaluator INFO: Inference done 113/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2444 s/iter. Total: 0.2979 s/iter. ETA=0:03:41
[04/25 14:29:19] d2.evaluation.evaluator INFO: Inference done 130/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2446 s/iter. Total: 0.2980 s/iter. ETA=0:03:36
[04/25 14:29:24] d2.evaluation.evaluator INFO: Inference done 147/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2448 s/iter. Total: 0.2982 s/iter. ETA=0:03:31
[04/25 14:29:30] d2.evaluation.evaluator INFO: Inference done 164/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2453 s/iter. Total: 0.2987 s/iter. ETA=0:03:26
[04/25 14:29:35] d2.evaluation.evaluator INFO: Inference done 181/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2453 s/iter. Total: 0.2988 s/iter. ETA=0:03:21
[04/25 14:29:40] d2.evaluation.evaluator INFO: Inference done 198/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2453 s/iter. Total: 0.2988 s/iter. ETA=0:03:16
[04/25 14:29:45] d2.evaluation.evaluator INFO: Inference done 215/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2453 s/iter. Total: 0.2988 s/iter. ETA=0:03:11
[04/25 14:29:50] d2.evaluation.evaluator INFO: Inference done 232/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2455 s/iter. Total: 0.2989 s/iter. ETA=0:03:06
[04/25 14:29:55] d2.evaluation.evaluator INFO: Inference done 249/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2456 s/iter. Total: 0.2990 s/iter. ETA=0:03:01
[04/25 14:30:00] d2.evaluation.evaluator INFO: Inference done 266/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2456 s/iter. Total: 0.2991 s/iter. ETA=0:02:56
[04/25 14:30:05] d2.evaluation.evaluator INFO: Inference done 283/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2456 s/iter. Total: 0.2991 s/iter. ETA=0:02:51
[04/25 14:30:10] d2.evaluation.evaluator INFO: Inference done 300/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2455 s/iter. Total: 0.2990 s/iter. ETA=0:02:46
[04/25 14:30:15] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2456 s/iter. Total: 0.2990 s/iter. ETA=0:02:41
[04/25 14:30:20] d2.evaluation.evaluator INFO: Inference done 334/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2456 s/iter. Total: 0.2991 s/iter. ETA=0:02:36
[04/25 14:30:26] d2.evaluation.evaluator INFO: Inference done 351/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2457 s/iter. Total: 0.2991 s/iter. ETA=0:02:31
[04/25 14:30:31] d2.evaluation.evaluator INFO: Inference done 368/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2457 s/iter. Total: 0.2991 s/iter. ETA=0:02:25
[04/25 14:30:36] d2.evaluation.evaluator INFO: Inference done 385/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2457 s/iter. Total: 0.2991 s/iter. ETA=0:02:20
[04/25 14:30:41] d2.evaluation.evaluator INFO: Inference done 402/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2459 s/iter. Total: 0.2993 s/iter. ETA=0:02:15
[04/25 14:30:46] d2.evaluation.evaluator INFO: Inference done 419/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2459 s/iter. Total: 0.2993 s/iter. ETA=0:02:10
[04/25 14:30:51] d2.evaluation.evaluator INFO: Inference done 436/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2460 s/iter. Total: 0.2994 s/iter. ETA=0:02:05
[04/25 14:30:56] d2.evaluation.evaluator INFO: Inference done 453/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2461 s/iter. Total: 0.2996 s/iter. ETA=0:02:00
[04/25 14:31:01] d2.evaluation.evaluator INFO: Inference done 470/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2463 s/iter. Total: 0.2998 s/iter. ETA=0:01:55
[04/25 14:31:07] d2.evaluation.evaluator INFO: Inference done 487/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:01:50
[04/25 14:31:12] d2.evaluation.evaluator INFO: Inference done 504/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:01:45
[04/25 14:31:17] d2.evaluation.evaluator INFO: Inference done 521/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2998 s/iter. ETA=0:01:40
[04/25 14:31:22] d2.evaluation.evaluator INFO: Inference done 538/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:01:35
[04/25 14:31:27] d2.evaluation.evaluator INFO: Inference done 555/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:01:30
[04/25 14:31:32] d2.evaluation.evaluator INFO: Inference done 572/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:01:25
[04/25 14:31:37] d2.evaluation.evaluator INFO: Inference done 589/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:01:20
[04/25 14:31:42] d2.evaluation.evaluator INFO: Inference done 606/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:01:14
[04/25 14:31:47] d2.evaluation.evaluator INFO: Inference done 623/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2466 s/iter. Total: 0.3000 s/iter. ETA=0:01:09
[04/25 14:31:53] d2.evaluation.evaluator INFO: Inference done 640/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2466 s/iter. Total: 0.3000 s/iter. ETA=0:01:04
[04/25 14:31:58] d2.evaluation.evaluator INFO: Inference done 657/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2466 s/iter. Total: 0.3001 s/iter. ETA=0:00:59
[04/25 14:32:03] d2.evaluation.evaluator INFO: Inference done 674/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2466 s/iter. Total: 0.3001 s/iter. ETA=0:00:54
[04/25 14:32:08] d2.evaluation.evaluator INFO: Inference done 691/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2466 s/iter. Total: 0.3001 s/iter. ETA=0:00:49
[04/25 14:32:13] d2.evaluation.evaluator INFO: Inference done 708/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2466 s/iter. Total: 0.3001 s/iter. ETA=0:00:44
[04/25 14:32:18] d2.evaluation.evaluator INFO: Inference done 725/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2466 s/iter. Total: 0.3001 s/iter. ETA=0:00:39
[04/25 14:32:23] d2.evaluation.evaluator INFO: Inference done 742/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2466 s/iter. Total: 0.3001 s/iter. ETA=0:00:34
[04/25 14:32:28] d2.evaluation.evaluator INFO: Inference done 759/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2466 s/iter. Total: 0.3001 s/iter. ETA=0:00:29
[04/25 14:32:33] d2.evaluation.evaluator INFO: Inference done 776/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.3000 s/iter. ETA=0:00:24
[04/25 14:32:38] d2.evaluation.evaluator INFO: Inference done 793/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2465 s/iter. Total: 0.3000 s/iter. ETA=0:00:18
[04/25 14:32:44] d2.evaluation.evaluator INFO: Inference done 810/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.3000 s/iter. ETA=0:00:13
[04/25 14:32:49] d2.evaluation.evaluator INFO: Inference done 827/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.3000 s/iter. ETA=0:00:08
[04/25 14:32:54] d2.evaluation.evaluator INFO: Inference done 844/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.3000 s/iter. ETA=0:00:03
[04/25 14:32:57] d2.evaluation.evaluator INFO: Total inference time: 0:04:15.346981 (0.300055 s / iter per device, on 1 devices)
[04/25 14:32:57] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052558 s / iter per device, on 1 devices)
[04/25 14:33:01] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/25 14:33:01] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/25 14:33:01] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/26 15:40:01] detectron2 INFO: Rank of current process: 0. World size: 1
[04/26 15:40:02] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/26 15:40:02] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/26 15:40:02] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 15:40:02] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 15:40:03] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/26 15:40:03] d2.utils.env INFO: Using a generated random seed 3707536
[04/26 16:14:10] detectron2 INFO: Rank of current process: 0. World size: 1
[04/26 16:14:12] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/26 16:14:12] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/26 16:14:12] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:14:12] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:14:12] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/26 16:14:12] d2.utils.env INFO: Using a generated random seed 13564215
[04/26 16:14:24] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/26 16:14:24] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/26 16:14:24] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/26 16:14:24] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/26 16:14:24] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/26 16:14:24] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/26 16:14:24] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/26 16:14:24] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/26 16:14:32] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0007 s/iter. Inference: 0.0525 s/iter. Eval: 0.2471 s/iter. Total: 0.3003 s/iter. ETA=0:04:13
[04/26 16:14:37] d2.evaluation.evaluator INFO: Inference done 28/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2477 s/iter. Total: 0.3011 s/iter. ETA=0:04:09
[04/26 16:14:42] d2.evaluation.evaluator INFO: Inference done 45/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2473 s/iter. Total: 0.3008 s/iter. ETA=0:04:03
[04/26 16:14:47] d2.evaluation.evaluator INFO: Inference done 62/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2469 s/iter. Total: 0.3004 s/iter. ETA=0:03:58
[04/26 16:14:53] d2.evaluation.evaluator INFO: Inference done 79/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3002 s/iter. ETA=0:03:53
[04/26 16:14:58] d2.evaluation.evaluator INFO: Inference done 96/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2465 s/iter. Total: 0.2999 s/iter. ETA=0:03:47
[04/26 16:15:03] d2.evaluation.evaluator INFO: Inference done 113/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2465 s/iter. Total: 0.2999 s/iter. ETA=0:03:42
[04/26 16:15:08] d2.evaluation.evaluator INFO: Inference done 130/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2464 s/iter. Total: 0.2997 s/iter. ETA=0:03:37
[04/26 16:15:13] d2.evaluation.evaluator INFO: Inference done 147/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2464 s/iter. Total: 0.2997 s/iter. ETA=0:03:32
[04/26 16:15:18] d2.evaluation.evaluator INFO: Inference done 164/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2467 s/iter. Total: 0.3000 s/iter. ETA=0:03:27
[04/26 16:15:23] d2.evaluation.evaluator INFO: Inference done 181/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2467 s/iter. Total: 0.3000 s/iter. ETA=0:03:22
[04/26 16:15:28] d2.evaluation.evaluator INFO: Inference done 198/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2467 s/iter. Total: 0.3000 s/iter. ETA=0:03:17
[04/26 16:15:33] d2.evaluation.evaluator INFO: Inference done 215/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2467 s/iter. Total: 0.3000 s/iter. ETA=0:03:12
[04/26 16:15:38] d2.evaluation.evaluator INFO: Inference done 232/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3001 s/iter. ETA=0:03:07
[04/26 16:15:44] d2.evaluation.evaluator INFO: Inference done 249/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3001 s/iter. ETA=0:03:02
[04/26 16:15:49] d2.evaluation.evaluator INFO: Inference done 266/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2467 s/iter. Total: 0.3001 s/iter. ETA=0:02:57
[04/26 16:15:54] d2.evaluation.evaluator INFO: Inference done 283/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2467 s/iter. Total: 0.3000 s/iter. ETA=0:02:51
[04/26 16:15:59] d2.evaluation.evaluator INFO: Inference done 300/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2466 s/iter. Total: 0.2999 s/iter. ETA=0:02:46
[04/26 16:16:04] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2466 s/iter. Total: 0.2999 s/iter. ETA=0:02:41
[04/26 16:16:09] d2.evaluation.evaluator INFO: Inference done 334/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2466 s/iter. Total: 0.3000 s/iter. ETA=0:02:36
[04/26 16:16:14] d2.evaluation.evaluator INFO: Inference done 351/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2466 s/iter. Total: 0.3000 s/iter. ETA=0:02:31
[04/26 16:16:19] d2.evaluation.evaluator INFO: Inference done 368/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2466 s/iter. Total: 0.2999 s/iter. ETA=0:02:26
[04/26 16:16:24] d2.evaluation.evaluator INFO: Inference done 385/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2466 s/iter. Total: 0.3000 s/iter. ETA=0:02:21
[04/26 16:16:29] d2.evaluation.evaluator INFO: Inference done 402/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3001 s/iter. ETA=0:02:16
[04/26 16:16:35] d2.evaluation.evaluator INFO: Inference done 419/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3001 s/iter. ETA=0:02:11
[04/26 16:16:40] d2.evaluation.evaluator INFO: Inference done 436/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:02:06
[04/26 16:16:45] d2.evaluation.evaluator INFO: Inference done 453/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:02:00
[04/26 16:16:50] d2.evaluation.evaluator INFO: Inference done 470/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:01:55
[04/26 16:16:55] d2.evaluation.evaluator INFO: Inference done 487/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:01:50
[04/26 16:17:00] d2.evaluation.evaluator INFO: Inference done 504/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3001 s/iter. ETA=0:01:45
[04/26 16:17:05] d2.evaluation.evaluator INFO: Inference done 521/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:01:40
[04/26 16:17:10] d2.evaluation.evaluator INFO: Inference done 538/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3001 s/iter. ETA=0:01:35
[04/26 16:17:15] d2.evaluation.evaluator INFO: Inference done 555/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:01:30
[04/26 16:17:20] d2.evaluation.evaluator INFO: Inference done 572/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2468 s/iter. Total: 0.3001 s/iter. ETA=0:01:25
[04/26 16:17:26] d2.evaluation.evaluator INFO: Inference done 589/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2467 s/iter. Total: 0.3001 s/iter. ETA=0:01:20
[04/26 16:17:31] d2.evaluation.evaluator INFO: Inference done 606/856. Dataloading: 0.0009 s/iter. Inference: 0.0524 s/iter. Eval: 0.2467 s/iter. Total: 0.3001 s/iter. ETA=0:01:15
[04/26 16:17:36] d2.evaluation.evaluator INFO: Inference done 623/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:01:09
[04/26 16:17:41] d2.evaluation.evaluator INFO: Inference done 640/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:01:04
[04/26 16:17:46] d2.evaluation.evaluator INFO: Inference done 657/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:59
[04/26 16:17:51] d2.evaluation.evaluator INFO: Inference done 674/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:54
[04/26 16:17:56] d2.evaluation.evaluator INFO: Inference done 691/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:49
[04/26 16:18:01] d2.evaluation.evaluator INFO: Inference done 708/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:44
[04/26 16:18:06] d2.evaluation.evaluator INFO: Inference done 725/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:39
[04/26 16:18:12] d2.evaluation.evaluator INFO: Inference done 742/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:34
[04/26 16:18:17] d2.evaluation.evaluator INFO: Inference done 759/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:29
[04/26 16:18:22] d2.evaluation.evaluator INFO: Inference done 776/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:00:24
[04/26 16:18:27] d2.evaluation.evaluator INFO: Inference done 793/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:00:18
[04/26 16:18:32] d2.evaluation.evaluator INFO: Inference done 810/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2469 s/iter. Total: 0.3003 s/iter. ETA=0:00:13
[04/26 16:18:37] d2.evaluation.evaluator INFO: Inference done 827/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2469 s/iter. Total: 0.3003 s/iter. ETA=0:00:08
[04/26 16:18:42] d2.evaluation.evaluator INFO: Inference done 844/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:00:03
[04/26 16:18:46] d2.evaluation.evaluator INFO: Total inference time: 0:04:15.572964 (0.300321 s / iter per device, on 1 devices)
[04/26 16:18:46] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052501 s / iter per device, on 1 devices)
[04/26 16:18:49] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/26 16:18:49] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/26 16:18:49] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/26 16:18:49] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/26 16:18:49] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/26 16:18:49] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/26 16:18:50] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 6.985 | 6.985  | 6.985  |  nan  |  nan  | 6.985 |
[04/26 16:18:50] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/26 16:18:50] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP    |
|:-----------|:-------|:-----------|:------|
| normal     | 13.352 | defect     | 0.618 |
[04/26 16:18:50] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/26 16:18:50] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/26 16:18:50] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/26 16:18:50] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/26 16:18:50] d2.evaluation.testing INFO: copypaste: Task: segm
[04/26 16:18:50] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/26 16:18:50] d2.evaluation.testing INFO: copypaste: 6.9847,6.9847,6.9847,nan,nan,6.9847
[04/26 16:20:50] detectron2 INFO: Rank of current process: 0. World size: 1
[04/26 16:20:51] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/26 16:20:51] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/26 16:20:51] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:20:51] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:20:51] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/26 16:20:51] d2.utils.env INFO: Using a generated random seed 52210924
[04/26 16:21:03] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/26 16:21:03] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/26 16:21:03] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/26 16:21:03] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/26 16:21:03] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/26 16:21:03] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/26 16:21:03] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/26 16:21:03] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/26 16:21:11] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0527 s/iter. Eval: 0.2437 s/iter. Total: 0.2971 s/iter. ETA=0:04:11
[04/26 16:21:16] d2.evaluation.evaluator INFO: Inference done 28/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2438 s/iter. Total: 0.2973 s/iter. ETA=0:04:06
[04/26 16:21:21] d2.evaluation.evaluator INFO: Inference done 45/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2442 s/iter. Total: 0.2977 s/iter. ETA=0:04:01
[04/26 16:21:26] d2.evaluation.evaluator INFO: Inference done 62/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2446 s/iter. Total: 0.2981 s/iter. ETA=0:03:56
[04/26 16:21:31] d2.evaluation.evaluator INFO: Inference done 79/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2449 s/iter. Total: 0.2985 s/iter. ETA=0:03:51
[04/26 16:21:36] d2.evaluation.evaluator INFO: Inference done 96/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2451 s/iter. Total: 0.2987 s/iter. ETA=0:03:46
[04/26 16:21:41] d2.evaluation.evaluator INFO: Inference done 113/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2453 s/iter. Total: 0.2989 s/iter. ETA=0:03:42
[04/26 16:21:46] d2.evaluation.evaluator INFO: Inference done 130/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2456 s/iter. Total: 0.2991 s/iter. ETA=0:03:37
[04/26 16:21:51] d2.evaluation.evaluator INFO: Inference done 147/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2457 s/iter. Total: 0.2992 s/iter. ETA=0:03:32
[04/26 16:21:57] d2.evaluation.evaluator INFO: Inference done 164/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2461 s/iter. Total: 0.2996 s/iter. ETA=0:03:27
[04/26 16:22:02] d2.evaluation.evaluator INFO: Inference done 181/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2461 s/iter. Total: 0.2996 s/iter. ETA=0:03:22
[04/26 16:22:07] d2.evaluation.evaluator INFO: Inference done 198/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2461 s/iter. Total: 0.2997 s/iter. ETA=0:03:17
[04/26 16:22:12] d2.evaluation.evaluator INFO: Inference done 215/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2461 s/iter. Total: 0.2996 s/iter. ETA=0:03:12
[04/26 16:22:17] d2.evaluation.evaluator INFO: Inference done 232/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2462 s/iter. Total: 0.2998 s/iter. ETA=0:03:07
[04/26 16:22:22] d2.evaluation.evaluator INFO: Inference done 249/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:03:02
[04/26 16:22:27] d2.evaluation.evaluator INFO: Inference done 266/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:02:56
[04/26 16:22:32] d2.evaluation.evaluator INFO: Inference done 283/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:02:51
[04/26 16:22:37] d2.evaluation.evaluator INFO: Inference done 300/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.3000 s/iter. ETA=0:02:46
[04/26 16:22:43] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.3000 s/iter. ETA=0:02:41
[04/26 16:22:48] d2.evaluation.evaluator INFO: Inference done 334/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.3000 s/iter. ETA=0:02:36
[04/26 16:22:53] d2.evaluation.evaluator INFO: Inference done 351/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2466 s/iter. Total: 0.3001 s/iter. ETA=0:02:31
[04/26 16:22:58] d2.evaluation.evaluator INFO: Inference done 368/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.3001 s/iter. ETA=0:02:26
[04/26 16:24:33] detectron2 INFO: Rank of current process: 0. World size: 1
[04/26 16:24:35] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/26 16:24:35] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/26 16:24:35] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:24:35] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:24:35] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/26 16:24:35] d2.utils.env INFO: Using a generated random seed 36682812
[04/26 16:24:47] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/26 16:24:47] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/26 16:24:47] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/26 16:24:47] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/26 16:24:47] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/26 16:24:47] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/26 16:24:47] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/26 16:24:48] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/26 16:24:55] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0533 s/iter. Eval: 0.2459 s/iter. Total: 0.2999 s/iter. ETA=0:04:13
[04/26 16:25:00] d2.evaluation.evaluator INFO: Inference done 28/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2461 s/iter. Total: 0.2995 s/iter. ETA=0:04:08
[04/26 16:25:06] d2.evaluation.evaluator INFO: Inference done 45/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2463 s/iter. Total: 0.2999 s/iter. ETA=0:04:03
[04/26 16:25:11] d2.evaluation.evaluator INFO: Inference done 62/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2462 s/iter. Total: 0.2998 s/iter. ETA=0:03:58
[04/26 16:25:16] d2.evaluation.evaluator INFO: Inference done 79/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2464 s/iter. Total: 0.3001 s/iter. ETA=0:03:53
[04/26 16:25:21] d2.evaluation.evaluator INFO: Inference done 96/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2464 s/iter. Total: 0.3001 s/iter. ETA=0:03:48
[04/26 16:25:26] d2.evaluation.evaluator INFO: Inference done 113/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2466 s/iter. Total: 0.3003 s/iter. ETA=0:03:43
[04/26 16:25:31] d2.evaluation.evaluator INFO: Inference done 130/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2467 s/iter. Total: 0.3004 s/iter. ETA=0:03:38
[04/26 16:25:36] d2.evaluation.evaluator INFO: Inference done 147/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2468 s/iter. Total: 0.3005 s/iter. ETA=0:03:33
[04/26 16:25:41] d2.evaluation.evaluator INFO: Inference done 164/856. Dataloading: 0.0008 s/iter. Inference: 0.0528 s/iter. Eval: 0.2472 s/iter. Total: 0.3009 s/iter. ETA=0:03:28
[04/26 16:25:46] d2.evaluation.evaluator INFO: Inference done 181/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2472 s/iter. Total: 0.3008 s/iter. ETA=0:03:23
[04/26 16:25:52] d2.evaluation.evaluator INFO: Inference done 198/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2472 s/iter. Total: 0.3008 s/iter. ETA=0:03:17
[04/26 16:25:57] d2.evaluation.evaluator INFO: Inference done 215/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2471 s/iter. Total: 0.3007 s/iter. ETA=0:03:12
[04/26 16:26:02] d2.evaluation.evaluator INFO: Inference done 232/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2470 s/iter. Total: 0.3006 s/iter. ETA=0:03:07
[04/26 16:26:07] d2.evaluation.evaluator INFO: Inference done 249/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3006 s/iter. ETA=0:03:02
[04/26 16:26:12] d2.evaluation.evaluator INFO: Inference done 266/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:57
[04/26 16:26:17] d2.evaluation.evaluator INFO: Inference done 283/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:52
[04/26 16:26:22] d2.evaluation.evaluator INFO: Inference done 300/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:47
[04/26 16:26:27] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:41
[04/26 16:26:32] d2.evaluation.evaluator INFO: Inference done 334/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:36
[04/26 16:26:37] d2.evaluation.evaluator INFO: Inference done 351/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:31
[04/26 16:26:43] d2.evaluation.evaluator INFO: Inference done 368/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:26
[04/26 16:26:48] d2.evaluation.evaluator INFO: Inference done 385/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:21
[04/26 16:26:53] d2.evaluation.evaluator INFO: Inference done 402/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2471 s/iter. Total: 0.3006 s/iter. ETA=0:02:16
[04/26 16:26:58] d2.evaluation.evaluator INFO: Inference done 419/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2471 s/iter. Total: 0.3006 s/iter. ETA=0:02:11
[04/26 16:27:03] d2.evaluation.evaluator INFO: Inference done 436/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2471 s/iter. Total: 0.3006 s/iter. ETA=0:02:06
[04/26 16:27:08] d2.evaluation.evaluator INFO: Inference done 453/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2471 s/iter. Total: 0.3006 s/iter. ETA=0:02:01
[04/26 16:27:13] d2.evaluation.evaluator INFO: Inference done 470/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3006 s/iter. ETA=0:01:56
[04/26 16:27:18] d2.evaluation.evaluator INFO: Inference done 487/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2471 s/iter. Total: 0.3006 s/iter. ETA=0:01:50
[04/26 16:27:24] d2.evaluation.evaluator INFO: Inference done 504/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2471 s/iter. Total: 0.3006 s/iter. ETA=0:01:45
[04/26 16:27:29] d2.evaluation.evaluator INFO: Inference done 521/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2471 s/iter. Total: 0.3006 s/iter. ETA=0:01:40
[04/26 16:27:34] d2.evaluation.evaluator INFO: Inference done 538/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2471 s/iter. Total: 0.3007 s/iter. ETA=0:01:35
[04/26 16:27:39] d2.evaluation.evaluator INFO: Inference done 555/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2471 s/iter. Total: 0.3007 s/iter. ETA=0:01:30
[04/26 16:27:44] d2.evaluation.evaluator INFO: Inference done 572/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2472 s/iter. Total: 0.3008 s/iter. ETA=0:01:25
[04/26 16:27:49] d2.evaluation.evaluator INFO: Inference done 589/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2473 s/iter. Total: 0.3009 s/iter. ETA=0:01:20
[04/26 16:27:54] d2.evaluation.evaluator INFO: Inference done 606/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2473 s/iter. Total: 0.3008 s/iter. ETA=0:01:15
[04/26 16:27:59] d2.evaluation.evaluator INFO: Inference done 623/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3010 s/iter. ETA=0:01:10
[04/26 16:28:05] d2.evaluation.evaluator INFO: Inference done 640/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3010 s/iter. ETA=0:01:05
[04/26 16:28:10] d2.evaluation.evaluator INFO: Inference done 657/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3010 s/iter. ETA=0:00:59
[04/26 16:28:15] d2.evaluation.evaluator INFO: Inference done 674/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3010 s/iter. ETA=0:00:54
[04/26 16:28:20] d2.evaluation.evaluator INFO: Inference done 691/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3010 s/iter. ETA=0:00:49
[04/26 16:28:25] d2.evaluation.evaluator INFO: Inference done 708/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3010 s/iter. ETA=0:00:44
[04/26 16:28:30] d2.evaluation.evaluator INFO: Inference done 725/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:00:39
[04/26 16:28:35] d2.evaluation.evaluator INFO: Inference done 742/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:00:34
[04/26 16:28:40] d2.evaluation.evaluator INFO: Inference done 759/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:00:29
[04/26 16:28:46] d2.evaluation.evaluator INFO: Inference done 776/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:00:24
[04/26 16:28:51] d2.evaluation.evaluator INFO: Inference done 793/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:00:18
[04/26 16:28:56] d2.evaluation.evaluator INFO: Inference done 810/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:00:13
[04/26 16:29:01] d2.evaluation.evaluator INFO: Inference done 827/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3010 s/iter. ETA=0:00:08
[04/26 16:29:06] d2.evaluation.evaluator INFO: Inference done 844/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3010 s/iter. ETA=0:00:03
[04/26 16:29:10] d2.evaluation.evaluator INFO: Total inference time: 0:04:16.203205 (0.301061 s / iter per device, on 1 devices)
[04/26 16:29:10] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052601 s / iter per device, on 1 devices)
[04/26 16:29:13] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/26 16:29:13] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/26 16:29:13] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/26 16:29:13] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/26 16:29:13] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.04 seconds.
[04/26 16:29:13] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/26 16:29:13] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[04/26 16:29:13] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/26 16:29:13] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/26 16:29:13] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/26 16:29:13] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *segm*
[04/26 16:29:13] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.19 seconds.
[04/26 16:29:13] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/26 16:29:13] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.00 seconds.
[04/26 16:29:13] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 6.985 | 6.985  | 6.985  |  nan  |  nan  | 6.985 |
[04/26 16:29:13] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/26 16:29:13] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP    |
|:-----------|:-------|:-----------|:------|
| normal     | 13.352 | defect     | 0.618 |
[04/26 16:29:13] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/26 16:29:13] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/26 16:29:13] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/26 16:29:13] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/26 16:29:13] d2.evaluation.testing INFO: copypaste: Task: segm
[04/26 16:29:13] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/26 16:29:13] d2.evaluation.testing INFO: copypaste: 6.9847,6.9847,6.9847,nan,nan,6.9847
[04/26 16:29:53] detectron2 INFO: Rank of current process: 0. World size: 1
[04/26 16:29:54] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/26 16:29:54] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/26 16:29:54] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:29:54] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:29:54] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/26 16:29:54] d2.utils.env INFO: Using a generated random seed 55707285
[04/26 16:30:08] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/26 16:30:08] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/26 16:30:08] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/26 16:30:08] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/26 16:30:08] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/26 16:30:08] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/26 16:30:08] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/26 16:30:08] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/26 16:30:16] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0524 s/iter. Eval: 0.2474 s/iter. Total: 0.3004 s/iter. ETA=0:04:13
[04/26 16:30:21] d2.evaluation.evaluator INFO: Inference done 28/856. Dataloading: 0.0008 s/iter. Inference: 0.0524 s/iter. Eval: 0.2482 s/iter. Total: 0.3014 s/iter. ETA=0:04:09
[04/26 16:30:26] d2.evaluation.evaluator INFO: Inference done 45/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2482 s/iter. Total: 0.3015 s/iter. ETA=0:04:04
[04/26 16:30:31] d2.evaluation.evaluator INFO: Inference done 62/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2482 s/iter. Total: 0.3015 s/iter. ETA=0:03:59
[04/26 16:30:36] d2.evaluation.evaluator INFO: Inference done 79/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2481 s/iter. Total: 0.3016 s/iter. ETA=0:03:54
[04/26 16:30:41] d2.evaluation.evaluator INFO: Inference done 96/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2481 s/iter. Total: 0.3016 s/iter. ETA=0:03:49
[04/26 16:30:46] d2.evaluation.evaluator INFO: Inference done 113/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2480 s/iter. Total: 0.3015 s/iter. ETA=0:03:43
[04/26 16:30:52] d2.evaluation.evaluator INFO: Inference done 130/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2478 s/iter. Total: 0.3013 s/iter. ETA=0:03:38
[04/26 16:30:57] d2.evaluation.evaluator INFO: Inference done 147/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2477 s/iter. Total: 0.3012 s/iter. ETA=0:03:33
[04/26 16:31:02] d2.evaluation.evaluator INFO: Inference done 164/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2481 s/iter. Total: 0.3016 s/iter. ETA=0:03:28
[04/26 16:31:07] d2.evaluation.evaluator INFO: Inference done 181/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2479 s/iter. Total: 0.3014 s/iter. ETA=0:03:23
[04/26 16:31:12] d2.evaluation.evaluator INFO: Inference done 198/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2478 s/iter. Total: 0.3012 s/iter. ETA=0:03:18
[04/26 16:31:17] d2.evaluation.evaluator INFO: Inference done 215/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2477 s/iter. Total: 0.3011 s/iter. ETA=0:03:13
[04/26 16:31:22] d2.evaluation.evaluator INFO: Inference done 232/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2476 s/iter. Total: 0.3011 s/iter. ETA=0:03:07
[04/26 16:31:27] d2.evaluation.evaluator INFO: Inference done 249/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2476 s/iter. Total: 0.3011 s/iter. ETA=0:03:02
[04/26 16:31:32] d2.evaluation.evaluator INFO: Inference done 266/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2476 s/iter. Total: 0.3010 s/iter. ETA=0:02:57
[04/26 16:31:38] d2.evaluation.evaluator INFO: Inference done 283/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:02:52
[04/26 16:31:43] d2.evaluation.evaluator INFO: Inference done 300/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:02:47
[04/26 16:31:48] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2475 s/iter. Total: 0.3009 s/iter. ETA=0:02:42
[04/26 16:31:53] d2.evaluation.evaluator INFO: Inference done 334/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2474 s/iter. Total: 0.3008 s/iter. ETA=0:02:37
[04/26 16:31:58] d2.evaluation.evaluator INFO: Inference done 351/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3009 s/iter. ETA=0:02:31
[04/26 16:32:03] d2.evaluation.evaluator INFO: Inference done 368/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3009 s/iter. ETA=0:02:26
[04/26 16:32:08] d2.evaluation.evaluator INFO: Inference done 385/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2474 s/iter. Total: 0.3008 s/iter. ETA=0:02:21
[04/26 16:32:13] d2.evaluation.evaluator INFO: Inference done 402/856. Dataloading: 0.0009 s/iter. Inference: 0.0526 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:02:16
[04/26 16:32:18] d2.evaluation.evaluator INFO: Inference done 419/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:02:11
[04/26 16:32:24] d2.evaluation.evaluator INFO: Inference done 436/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2475 s/iter. Total: 0.3010 s/iter. ETA=0:02:06
[04/26 16:32:29] d2.evaluation.evaluator INFO: Inference done 453/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2475 s/iter. Total: 0.3009 s/iter. ETA=0:02:01
[04/26 16:32:34] d2.evaluation.evaluator INFO: Inference done 470/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2475 s/iter. Total: 0.3009 s/iter. ETA=0:01:56
[04/26 16:32:39] d2.evaluation.evaluator INFO: Inference done 487/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2474 s/iter. Total: 0.3009 s/iter. ETA=0:01:51
[04/26 16:32:44] d2.evaluation.evaluator INFO: Inference done 504/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2474 s/iter. Total: 0.3008 s/iter. ETA=0:01:45
[04/26 16:32:49] d2.evaluation.evaluator INFO: Inference done 521/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2474 s/iter. Total: 0.3008 s/iter. ETA=0:01:40
[04/26 16:32:54] d2.evaluation.evaluator INFO: Inference done 538/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2474 s/iter. Total: 0.3008 s/iter. ETA=0:01:35
[04/26 16:32:59] d2.evaluation.evaluator INFO: Inference done 555/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3008 s/iter. ETA=0:01:30
[04/26 16:33:04] d2.evaluation.evaluator INFO: Inference done 572/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3007 s/iter. ETA=0:01:25
[04/26 16:33:09] d2.evaluation.evaluator INFO: Inference done 589/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3007 s/iter. ETA=0:01:20
[04/26 16:33:15] d2.evaluation.evaluator INFO: Inference done 606/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3007 s/iter. ETA=0:01:15
[04/26 16:33:20] d2.evaluation.evaluator INFO: Inference done 623/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3008 s/iter. ETA=0:01:10
[04/26 16:33:25] d2.evaluation.evaluator INFO: Inference done 640/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3007 s/iter. ETA=0:01:04
[04/26 16:33:30] d2.evaluation.evaluator INFO: Inference done 657/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3007 s/iter. ETA=0:00:59
[04/26 16:33:35] d2.evaluation.evaluator INFO: Inference done 674/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3007 s/iter. ETA=0:00:54
[04/26 16:33:40] d2.evaluation.evaluator INFO: Inference done 691/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2472 s/iter. Total: 0.3007 s/iter. ETA=0:00:49
[04/26 16:33:45] d2.evaluation.evaluator INFO: Inference done 708/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2472 s/iter. Total: 0.3006 s/iter. ETA=0:00:44
[04/26 16:33:50] d2.evaluation.evaluator INFO: Inference done 725/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2472 s/iter. Total: 0.3006 s/iter. ETA=0:00:39
[04/26 16:33:55] d2.evaluation.evaluator INFO: Inference done 742/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2472 s/iter. Total: 0.3007 s/iter. ETA=0:00:34
[04/26 16:34:01] d2.evaluation.evaluator INFO: Inference done 759/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2472 s/iter. Total: 0.3007 s/iter. ETA=0:00:29
[04/26 16:34:06] d2.evaluation.evaluator INFO: Inference done 776/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3007 s/iter. ETA=0:00:24
[04/26 16:34:11] d2.evaluation.evaluator INFO: Inference done 793/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3007 s/iter. ETA=0:00:18
[04/26 16:34:16] d2.evaluation.evaluator INFO: Inference done 810/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3007 s/iter. ETA=0:00:13
[04/26 16:34:21] d2.evaluation.evaluator INFO: Inference done 827/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3008 s/iter. ETA=0:00:08
[04/26 16:34:26] d2.evaluation.evaluator INFO: Inference done 844/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2473 s/iter. Total: 0.3008 s/iter. ETA=0:00:03
[04/26 16:34:30] d2.evaluation.evaluator INFO: Total inference time: 0:04:16.049462 (0.300881 s / iter per device, on 1 devices)
[04/26 16:34:30] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052543 s / iter per device, on 1 devices)
[04/26 16:34:33] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/26 16:34:33] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/26 16:34:33] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/26 16:34:33] d2.evaluation.fast_eval_api INFO: Evaluate annotation type *bbox*
[04/26 16:34:33] d2.evaluation.fast_eval_api INFO: COCOeval_opt.evaluate() finished in 0.04 seconds.
[04/26 16:34:33] d2.evaluation.fast_eval_api INFO: Accumulating evaluation results...
[04/26 16:34:33] d2.evaluation.fast_eval_api INFO: COCOeval_opt.accumulate() finished in 0.01 seconds.
[04/26 16:36:28] detectron2 INFO: Rank of current process: 0. World size: 1
[04/26 16:36:30] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/26 16:36:30] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/26 16:36:30] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:36:30] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 16:36:30] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/26 16:36:30] d2.utils.env INFO: Using a generated random seed 31670760
[04/26 16:36:42] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/26 16:36:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/26 16:36:42] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/26 16:36:42] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/26 16:36:42] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/26 16:36:42] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/26 16:36:42] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/26 16:36:42] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/26 16:36:50] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0006 s/iter. Inference: 0.0526 s/iter. Eval: 0.2463 s/iter. Total: 0.2995 s/iter. ETA=0:04:13
[04/26 16:36:55] d2.evaluation.evaluator INFO: Inference done 28/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2466 s/iter. Total: 0.3000 s/iter. ETA=0:04:08
[04/26 16:37:00] d2.evaluation.evaluator INFO: Inference done 45/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.3000 s/iter. ETA=0:04:03
[04/26 17:01:04] detectron2 INFO: Rank of current process: 0. World size: 1
[04/26 17:01:06] detectron2 INFO: Environment info:
----------------------  ----------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
numpy                   1.22.3
detectron2              0.6 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.1
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.9.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0,1,2               NVIDIA RTX A6000 (arch=8.6)
Driver version          470.103.01
CUDA_HOME               /usr
Pillow                  9.0.1
torchvision             0.10.0 @/home/tqsang/miniconda3/envs/mask2former/lib/python3.8/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20220305
iopath                  0.1.9
cv2                     4.5.5
----------------------  ----------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.0-Product Build 20211112 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/26 17:01:06] detectron2 INFO: Command line arguments: Namespace(config_file='/home/tqsang/Mask2Former/output_coco_256_100/config.yaml', dist_url='tcp://127.0.0.1:50158', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', '/home/tqsang/Mask2Former/output_coco_256_100/model_final.pth'], resume=False)
[04/26 17:01:06] detectron2 INFO: Contents of args.config_file=/home/tqsang/Mask2Former/output_coco_256_100/config.yaml:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 17:01:06] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - chick_dataset_val
  TRAIN:
  - chick_dataset_train
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: false
  CROP:
    ENABLED: false
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 0.9
    - 0.9
    TYPE: relative_range
  DATASET_MAPPER_NAME: coco_instance_lsj
  FORMAT: RGB
  IMAGE_SIZE: 256
  MASK_FORMAT: polygon
  MAX_SCALE: 2.0
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SCALE: 0.1
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN:
  - 800
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: -1
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 0
    NAME: build_resnet_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_FORMER:
    CLASS_WEIGHT: 2.0
    DEC_LAYERS: 10
    DEEP_SUPERVISION: true
    DICE_WEIGHT: 5.0
    DIM_FEEDFORWARD: 2048
    DROPOUT: 0.0
    ENC_LAYERS: 0
    ENFORCE_INPUT_PROJ: false
    HIDDEN_DIM: 256
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NHEADS: 8
    NO_OBJECT_WEIGHT: 0.1
    NUM_OBJECT_QUERIES: 100
    OVERSAMPLE_RATIO: 3.0
    PRE_NORM: false
    SIZE_DIVISIBILITY: 32
    TEST:
      INSTANCE_ON: true
      OBJECT_MASK_THRESHOLD: 0.8
      OVERLAP_THRESHOLD: 0.8
      PANOPTIC_ON: false
      SEMANTIC_ON: false
      SEM_SEG_POSTPROCESSING_BEFORE_INFERENCE: false
    TRAIN_NUM_POINTS: 12544
    TRANSFORMER_DECODER_NAME: MultiScaleMaskedTransformerDecoder
    TRANSFORMER_IN_FEATURE: multi_scale_pixel_decoder
  MASK_ON: false
  META_ARCHITECTURE: MaskFormer
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 1
    - 1
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: basic
    STRIDE_IN_1X1: false
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 10.0
    - 10.0
    - 5.0
    - 5.0
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 256
    DEFORMABLE_TRANSFORMER_ENCODER_IN_FEATURES:
    - res3
    - res4
    - res5
    DEFORMABLE_TRANSFORMER_ENCODER_N_HEADS: 8
    DEFORMABLE_TRANSFORMER_ENCODER_N_POINTS: 4
    IGNORE_VALUE: 255
    IN_FEATURES:
    - res2
    - res3
    - res4
    - res5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    MASK_DIM: 256
    NAME: MaskFormerHead
    NORM: GN
    NUM_CLASSES: 2
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    TRANSFORMER_ENC_LAYERS: 6
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SWIN:
    APE: false
    ATTN_DROP_RATE: 0.0
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    DROP_PATH_RATE: 0.3
    DROP_RATE: 0.0
    EMBED_DIM: 96
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    OUT_FEATURES:
    - res2
    - res3
    - res4
    - res5
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMG_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    USE_CHECKPOINT: false
    WINDOW_SIZE: 7
  WEIGHTS: /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth
OUTPUT_DIR: ./output_coco_256_100
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 0.1
  BASE_LR: 0.0001
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 1
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 50000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 40000
  - 45000
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 10
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.05
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[04/26 17:01:06] detectron2 INFO: Full config saved to ./output_coco_256_100/config.yaml
[04/26 17:01:06] d2.utils.env INFO: Using a generated random seed 7430701
[04/26 17:01:18] d2.engine.defaults INFO: Model:
MaskFormer(
  (backbone): ResNet(
    (stem): BasicStem(
      (conv1): Conv2d(
        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
      )
    )
    (res2): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv1): Conv2d(
          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv2): Conv2d(
          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
        (conv3): Conv2d(
          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
      )
    )
    (res3): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv1): Conv2d(
          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv2): Conv2d(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
        )
        (conv3): Conv2d(
          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
      )
    )
    (res4): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
        (conv1): Conv2d(
          512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (3): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (4): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
      (5): BottleneckBlock(
        (conv1): Conv2d(
          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv2): Conv2d(
          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
        )
        (conv3): Conv2d(
          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
        )
      )
    )
    (res5): Sequential(
      (0): BottleneckBlock(
        (shortcut): Conv2d(
          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
        (conv1): Conv2d(
          1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (1): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
      (2): BottleneckBlock(
        (conv1): Conv2d(
          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv2): Conv2d(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
        )
        (conv3): Conv2d(
          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
        )
      )
    )
  )
  (sem_seg_head): MaskFormerHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(100, 256)
      (query_embed): Embedding(100, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (class_embed): Linear(in_features=256, out_features=3, bias=True)
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 2
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
)
[04/26 17:01:18] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/tqsang/Mask2Former/output_coco_256_100/model_final.pth ...
[04/26 17:01:18] d2.data.datasets.coco INFO: Loaded 856 images in COCO format from /home/tqsang/Mask2Former/datasets/coco_front2class/annotations/instances_val2017.json
[04/26 17:01:18] d2.data.build INFO: Distribution of instances among all 2 categories:
[36m|  category  | #instances   |  category  | #instances   |
|:----------:|:-------------|:----------:|:-------------|
|   normal   | 407          |   defect   | 449          |
|            |              |            |              |
|   total    | 856          |            |              |[0m
[04/26 17:01:18] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[04/26 17:01:18] d2.data.common INFO: Serializing 856 elements to byte tensors and concatenating them all ...
[04/26 17:01:18] d2.data.common INFO: Serialized dataset takes 2.88 MiB
[04/26 17:01:18] d2.evaluation.evaluator INFO: Start inference on 856 batches
[04/26 17:01:26] d2.evaluation.evaluator INFO: Inference done 11/856. Dataloading: 0.0007 s/iter. Inference: 0.0523 s/iter. Eval: 0.2461 s/iter. Total: 0.2991 s/iter. ETA=0:04:12
[04/26 17:01:31] d2.evaluation.evaluator INFO: Inference done 28/856. Dataloading: 0.0008 s/iter. Inference: 0.0529 s/iter. Eval: 0.2468 s/iter. Total: 0.3006 s/iter. ETA=0:04:08
[04/26 17:01:36] d2.evaluation.evaluator INFO: Inference done 45/856. Dataloading: 0.0008 s/iter. Inference: 0.0527 s/iter. Eval: 0.2465 s/iter. Total: 0.3001 s/iter. ETA=0:04:03
[04/26 17:01:41] d2.evaluation.evaluator INFO: Inference done 62/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2465 s/iter. Total: 0.2999 s/iter. ETA=0:03:58
[04/26 17:01:46] d2.evaluation.evaluator INFO: Inference done 79/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:03:52
[04/26 17:01:52] d2.evaluation.evaluator INFO: Inference done 96/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2998 s/iter. ETA=0:03:47
[04/26 17:01:57] d2.evaluation.evaluator INFO: Inference done 113/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2998 s/iter. ETA=0:03:42
[04/26 17:02:02] d2.evaluation.evaluator INFO: Inference done 130/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2998 s/iter. ETA=0:03:37
[04/26 17:02:07] d2.evaluation.evaluator INFO: Inference done 147/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2464 s/iter. Total: 0.2999 s/iter. ETA=0:03:32
[04/26 17:02:12] d2.evaluation.evaluator INFO: Inference done 164/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:03:27
[04/26 17:02:17] d2.evaluation.evaluator INFO: Inference done 181/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:03:22
[04/26 17:02:22] d2.evaluation.evaluator INFO: Inference done 198/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:03:17
[04/26 17:02:27] d2.evaluation.evaluator INFO: Inference done 215/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:03:12
[04/26 17:02:32] d2.evaluation.evaluator INFO: Inference done 232/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:03:07
[04/26 17:02:38] d2.evaluation.evaluator INFO: Inference done 249/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:03:02
[04/26 17:02:43] d2.evaluation.evaluator INFO: Inference done 266/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:02:57
[04/26 17:02:48] d2.evaluation.evaluator INFO: Inference done 283/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:02:52
[04/26 17:02:53] d2.evaluation.evaluator INFO: Inference done 300/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:02:46
[04/26 17:02:58] d2.evaluation.evaluator INFO: Inference done 317/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2469 s/iter. Total: 0.3003 s/iter. ETA=0:02:41
[04/26 17:03:03] d2.evaluation.evaluator INFO: Inference done 334/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2469 s/iter. Total: 0.3004 s/iter. ETA=0:02:36
[04/26 17:03:08] d2.evaluation.evaluator INFO: Inference done 351/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2469 s/iter. Total: 0.3004 s/iter. ETA=0:02:31
[04/26 17:03:13] d2.evaluation.evaluator INFO: Inference done 368/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2469 s/iter. Total: 0.3004 s/iter. ETA=0:02:26
[04/26 17:03:18] d2.evaluation.evaluator INFO: Inference done 385/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2469 s/iter. Total: 0.3004 s/iter. ETA=0:02:21
[04/26 17:03:24] d2.evaluation.evaluator INFO: Inference done 402/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:16
[04/26 17:03:29] d2.evaluation.evaluator INFO: Inference done 419/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:11
[04/26 17:03:34] d2.evaluation.evaluator INFO: Inference done 436/856. Dataloading: 0.0008 s/iter. Inference: 0.0526 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:06
[04/26 17:03:39] d2.evaluation.evaluator INFO: Inference done 453/856. Dataloading: 0.0008 s/iter. Inference: 0.0525 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:02:01
[04/26 17:03:44] d2.evaluation.evaluator INFO: Inference done 470/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2470 s/iter. Total: 0.3005 s/iter. ETA=0:01:55
[04/26 17:03:49] d2.evaluation.evaluator INFO: Inference done 487/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2470 s/iter. Total: 0.3004 s/iter. ETA=0:01:50
[04/26 17:03:54] d2.evaluation.evaluator INFO: Inference done 504/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2469 s/iter. Total: 0.3004 s/iter. ETA=0:01:45
[04/26 17:03:59] d2.evaluation.evaluator INFO: Inference done 521/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2469 s/iter. Total: 0.3003 s/iter. ETA=0:01:40
[04/26 17:04:04] d2.evaluation.evaluator INFO: Inference done 538/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:01:35
[04/26 17:04:09] d2.evaluation.evaluator INFO: Inference done 555/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:01:30
[04/26 17:04:14] d2.evaluation.evaluator INFO: Inference done 572/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:01:25
[04/26 17:04:20] d2.evaluation.evaluator INFO: Inference done 589/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3001 s/iter. ETA=0:01:20
[04/26 17:04:25] d2.evaluation.evaluator INFO: Inference done 606/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3001 s/iter. ETA=0:01:15
[04/26 17:04:30] d2.evaluation.evaluator INFO: Inference done 623/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3002 s/iter. ETA=0:01:09
[04/26 17:04:35] d2.evaluation.evaluator INFO: Inference done 640/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3002 s/iter. ETA=0:01:04
[04/26 17:04:40] d2.evaluation.evaluator INFO: Inference done 657/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3001 s/iter. ETA=0:00:59
[04/26 17:04:45] d2.evaluation.evaluator INFO: Inference done 674/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3001 s/iter. ETA=0:00:54
[04/26 17:04:50] d2.evaluation.evaluator INFO: Inference done 691/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3001 s/iter. ETA=0:00:49
[04/26 17:04:55] d2.evaluation.evaluator INFO: Inference done 708/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3001 s/iter. ETA=0:00:44
[04/26 17:05:00] d2.evaluation.evaluator INFO: Inference done 725/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3002 s/iter. ETA=0:00:39
[04/26 17:05:05] d2.evaluation.evaluator INFO: Inference done 742/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3002 s/iter. ETA=0:00:34
[04/26 17:05:11] d2.evaluation.evaluator INFO: Inference done 759/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2467 s/iter. Total: 0.3002 s/iter. ETA=0:00:29
[04/26 17:05:16] d2.evaluation.evaluator INFO: Inference done 776/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:24
[04/26 17:05:21] d2.evaluation.evaluator INFO: Inference done 793/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:18
[04/26 17:05:26] d2.evaluation.evaluator INFO: Inference done 810/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3002 s/iter. ETA=0:00:13
[04/26 17:05:31] d2.evaluation.evaluator INFO: Inference done 827/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:00:08
[04/26 17:05:36] d2.evaluation.evaluator INFO: Inference done 844/856. Dataloading: 0.0009 s/iter. Inference: 0.0525 s/iter. Eval: 0.2468 s/iter. Total: 0.3003 s/iter. ETA=0:00:03
[04/26 17:05:40] d2.evaluation.evaluator INFO: Total inference time: 0:04:15.604582 (0.300358 s / iter per device, on 1 devices)
[04/26 17:05:40] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:00:44 (0.052543 s / iter per device, on 1 devices)
[04/26 17:05:41] d2.evaluation.coco_evaluation INFO: Preparing results for COCO format ...
[04/26 17:05:41] d2.evaluation.coco_evaluation INFO: Saving results to ./output_coco_256_100/inference/coco_instances_results.json
[04/26 17:05:42] d2.evaluation.coco_evaluation INFO: Evaluating predictions with unofficial COCO API...
[04/26 17:05:43] d2.evaluation.coco_evaluation INFO: Evaluation results for bbox: 
|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |
|:-----:|:------:|:------:|:-----:|:-----:|:-----:|
| 0.000 | 0.000  | 0.000  |  nan  |  nan  | 0.000 |
[04/26 17:05:43] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/26 17:05:43] d2.evaluation.coco_evaluation INFO: Per-category bbox AP: 
| category   | AP    | category   | AP    |
|:-----------|:------|:-----------|:------|
| normal     | 0.000 | defect     | 0.000 |
[04/26 17:05:47] d2.evaluation.coco_evaluation INFO: Evaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |
|:------:|:------:|:------:|:-----:|:-----:|:------:|
| 83.663 | 83.663 | 83.663 |  nan  |  nan  | 83.663 |
[04/26 17:05:47] d2.evaluation.coco_evaluation INFO: Some metrics cannot be computed and is shown as NaN.
[04/26 17:05:47] d2.evaluation.coco_evaluation INFO: Per-category segm AP: 
| category   | AP     | category   | AP     |
|:-----------|:-------|:-----------|:-------|
| normal     | 86.716 | defect     | 80.611 |
[04/26 17:05:47] d2.engine.defaults INFO: Evaluation results for chick_dataset_val in csv format:
[04/26 17:05:47] d2.evaluation.testing INFO: copypaste: Task: bbox
[04/26 17:05:47] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/26 17:05:47] d2.evaluation.testing INFO: copypaste: 0.0000,0.0000,0.0000,nan,nan,0.0000
[04/26 17:05:47] d2.evaluation.testing INFO: copypaste: Task: segm
[04/26 17:05:47] d2.evaluation.testing INFO: copypaste: AP,AP50,AP75,APs,APm,APl
[04/26 17:05:47] d2.evaluation.testing INFO: copypaste: 83.6635,83.6635,83.6635,nan,nan,83.6635
